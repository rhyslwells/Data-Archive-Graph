[
  {
    "category": "OTHER",
    "filename": "Bag_of_Words.py",
    "sha": "9e3e9db8007cff80b8f4e2ae44e5912b2d183f06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Bag_of_Words.py.md",
    "text": "### Summary of What the Script Does:\n\n1. It takes a dataset of text (movie reviews in this case) and processes it to remove HTML tags, non-alphabetic characters, and stopwords.\n2. It transforms the cleaned text into numerical features using the **Bag of Words** model, where each word in the reviews is counted and represented as a feature.\n3. It prints a sample of the top features (words) that were extracted from the reviews.\n\nThis is a typical text preprocessing pipeline used to prepare textual data for machine learning models.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "bag_of_words.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Bandit example output",
    "sha": "7b9c76403b39c5b2ec839414d9024d557dc09d3a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Bandit%20example%20output.md",
    "text": "## Complicated example output of bandit\nRunning bandit on [[ML_Tools]] file [[Bandit_Example_Nonfixed.py]] gives. Fixing this gives [[Bandit_Example_Fixed.py]]\n\n```\n[main]  INFO    profile include tests: None\n[main]  INFO    profile exclude tests: None\n[main]  INFO    cli include tests: None\n[main]  INFO    cli exclude tests: None\n[main]  INFO    running on Python 3.10.8\nRun started:2025-01-11 17:19:41.806346\n\nTest results:\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: .\\Bandit_Example.py:1:0\n1       import subprocess\n2       import os\n3       import pickle\n\n--------------------------------------------------\n>> Issue: [B403:blacklist] Consider possible security implications associated with pickle module.\n   Severity: Low   Confidence: High\n   CWE: CWE-502 (https://cwe.mitre.org/data/definitions/502.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_imports.html#b403-import-pickle\n   Location: .\\Bandit_Example.py:3:0\n2       import os\n3       import pickle\n4\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: .\\Bandit_Example.py:16:4\n15          \"\"\"\n16          subprocess.call(f\"cmd /c echo {user_input}\", shell=True)\n17\n\n--------------------------------------------------\n>> Issue: [B105:hardcoded_password_string] Possible hardcoded password: 'SuperSecret123'\n   Severity: Low   Confidence: Medium\n   CWE: CWE-259 (https://cwe.mitre.org/data/definitions/259.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/plugins/b105_hardcoded_password_string.html\n   Location: .\\Bandit_Example.py:28:15\n27          \"\"\"\n28          password = \"SuperSecret123\"  # Example of hardcoded sensitive information\n29          print(password)\n\n--------------------------------------------------\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_calls.html#b307-eval\n   Location: .\\Bandit_Example.py:40:17\n39          try:\n40              result = eval(user_input)  # Evaluate the input\n41              print(f\"Result of eval: {result}\")  # Print the result\n\n--------------------------------------------------\n>> Issue: [B301:blacklist] Pickle and modules that wrap it can be unsafe when used to deserialize untrusted data, possible security issue.\n   Severity: Medium   Confidence: High\n   CWE: CWE-502 (https://cwe.mitre.org/data/definitions/502.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_calls.html#b301-pickle\n   Location: .\\Bandit_Example.py:53:11\n52          \"\"\"\n53          return pickle.loads(data)  # If data is malicious, it can execute arbitrary code.\n54\n\n--------------------------------------------------\n\nCode scanned:\n        Total lines of code: 77\n        Total lines skipped (#nosec): 0\n\nRun metrics:\n        Total issues (by severity):\n                Undefined: 0\n                Low: 3\n                Medium: 2\n                High: 1\n        Total issues (by confidence):\n                Undefined: 0\n                Low: 0\n                Medium: 1\n                High: 5\nFiles skipped (0):\n```\n## Simple example of bandit output\nWhen i run bandit on the following code.\n\n```python\nimport subprocess\n\nuser_input = input(\"Enter your name: \")\nsubprocess.call(f\"echo {user_input}\", shell=True)\n```\n\nit gives:\n\n```\nmain]  INFO    profile include tests: None\n[main]  INFO    profile exclude tests: None\n[main]  INFO    cli include tests: None\n[main]  INFO    cli exclude tests: None\n[main]  INFO    running on Python 3.10.8\nRun started:2025-01-11 16:56:32.096644\n\nTest results:\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess motettetted with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: .\\Bandit_ex1.py:1:0\n1       import subprocess\n2\n3       user_input = input(\"Enter your name: \")\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.0/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: .\\Bandit_ex1.py:4:0\n3       user_input = input(\"Enter your name: \")\n4       subprocess.call(f\"echo {user_input}\", shell=True)\n\n--------------------------------------------------\n\nCode scanned:\n        Total lines of code: 3\n        Total lines skipped (#nosec): 0\n\nRun metrics:\n        Total issues (by severity):\n                Undefined: 0\n                Low: 1\n                Medium: 0\n                High: 1\n        Total issues (by confidence):\n                Undefined: 0\n                Low: 0\n                Medium: 0\n                High: 2\nFiles skipped (0):\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "test"
    ],
    "normalized_filename": "bandit_example_output",
    "outlinks": [
      "ml_tools",
      "bandit_example_fixed.py",
      "bandit_example_nonfixed.py"
    ],
    "inlinks": [
      "tool.bandit"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Addressing_Multicollinearity.py",
    "sha": "ccc3ecce5b9042749e58a29aae04bdba5e6cb388",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Addressing_Multicollinearity.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Regression/Addressing_Multicollinearity.py",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "addressing_multicollinearity.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Bandit_Example_Fixed.py",
    "sha": "2584f6f457745911940c5dba918103b12217967a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Bandit_Example_Fixed.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Fixed.py",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "bandit_example_fixed.py",
    "outlinks": [],
    "inlinks": [
      "bandit_example_output"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Click_Implementation.py",
    "sha": "1e8a65fe5d3348f65838fbb6c16b8ce20559d58d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Click_Implementation.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Click_Implementation.py\n\nThis script implements a command-line interface (CLI) tool using Python's `click` library. The CLI allows users to interact with a JSON file, enabling them to view keys, retrieve values, and update key-value pairs.\n\n## Functionality Overview\n\n1. CLI Initialization (`cli` function)\n    - Serves as the main command group.\n    - Accepts a JSON file (`document`) as an argument.\n    - Reads the JSON file and stores its content in `ctx.obj`, making it accessible to all subcommands.\n\n2. Displaying Keys (`show_keys` command)\n    - Lists all top-level keys in the JSON document.\n\n3. Retrieving a Value (`get_value` command)\n    - Accepts a key as an argument.\n    - Prints the corresponding value if the key exists; otherwise, prints `\"Key not found\"`.\n\n4. Updating a Value (`update_value` command)\n    - Requires `-k/--key` (key to update) and `-v/--value` (new value).\n    - Updates the key’s value in memory.\n    - Saves the updated JSON data back to the file.\n\n## Example Usage\n\n### 1. Viewing Keys\n\n```sh\npython script.py data.json show_keys\n```\n\nExample Output (if `data.json` contains `{\"name\": \"Alice\", \"age\": 30}`):\n\n```\nKeys: ['name', 'age']\n```\n\n### 2. Retrieving a Value\n\n```sh\npython script.py data.json get_value name\n```\n\nOutput:\n\n```\nAlice\n```\n\n### 3. Updating a Value\n\n```sh\npython script.py data.json update_value -k name -v Bob\n```\n\nModifies `data.json` to:\n\n```json\n{\n    \"name\": \"Bob\",\n    \"age\": 30\n}\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "click_implementation.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Cross_Entropy_Single.py",
    "sha": "449888074a3c7e86b8c18e8f1a749bf18ba22aa0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Cross_Entropy_Single.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py\n## Example\n\nLet's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class label being A. The true label in one-hot encoded form would be [1, 0, 0].\n\nAssume the model predicts the following probabilities for this data point:\n\n- Probability of class A: 0.7\n- Probability of class B: 0.2\n- Probability of class C: 0.1\n\nThe predicted probability vector is [0.7, 0.2, 0.1].\n\nTo calculate the cross entropy loss for this example, we use the formula:\n\n$L = -\\sum_{i=1}^{N} y_i \\log(p_i)$\n\nSubstituting the values:\n\n- For class A: $y_1 = 1$ and $p_1 = 0.7$\n- For class B: $y_2 = 0$ and $p_2 = 0.2$\n- For class C: $y_3 = 0$ and $p_3 = 0.1$\n\nThe cross entropy loss $L$ is calculated as:\n\n$L = -(1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.2) + 0 \\cdot \\log(0.1))$\n\n$L = -(\\log(0.7))$\n\n$L \\approx -(-0.3567) = 0.3567$\n\nSo, the cross entropy loss for this example is approximately 0.3567. This value represents the penalty for the model's predicted probabilities not perfectly matching the true class distribution. The lower the loss, the better the model's predictions align with the true labels.\n\n### Script Description:\n\n1. **Cross Entropy Function**: Computes the cross entropy loss given true labels and predicted probabilities.\n2. **True and Predicted Probabilities Visualization**: Bar plots display the true one-hot encoded labels and the predicted probability distribution.\n3. **Cross Entropy Loss Calculation**: Prints the loss value for a sample data point.\n4. **Loss Curve**: A line graph shows how the loss changes as the predicted probability for the true class increases.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "cross_entropy_single.py",
    "outlinks": [],
    "inlinks": [
      "cross_entropy"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Cross_Entropy.py",
    "sha": "dae07a6a22c8dfd4c875a0361d1841836ef869e2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Cross_Entropy.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py\n### Generalized Script Description:\n\n1. **Dataset**: Uses the Iris dataset from `sklearn` to classify flower species.\n2. **Preprocessing**: One-hot encodes the target labels and splits the data into training and testing sets.\n3. **Model**: Trains a multinomial logistic regression model to predict probabilities for each class.\n4. **Cross Entropy Calculation**: Computes cross entropy loss for all predictions in the test set.\n5. **Visualization**: Plots a histogram to show the distribution of loss values across the test samples.\n6. **Summary Statistics**: Outputs mean, median, maximum, and minimum loss values for analysis.\n\nThis approach provides insight into the model's performance by analyzing the spread and typical values of cross entropy loss over multiple predictions.\n\n### Strengths:\n\n1. **Real-World Dataset**: The Iris dataset is well-known and intuitive, making it easier to follow and validate the results.\n2. **Generalization**: The script calculates the cross entropy loss for multiple predictions, demonstrating the loss function in a real-world, multi-class classification scenario.\n3. **Insights Through Visualization**: The histogram of losses provides a clear picture of how well the model performs across different test samples.\n4. **Statistical Summary**: The inclusion of mean, median, max, and min loss values gives a quick overview of the model's performance.\n5. **Numerical Stability**: The small epsilon value in the log computation ensures stability when dealing with probabilities close to zero.\n6. **Reproducibility**: Using `sklearn`'s preprocessing and modeling tools ensures that the example is easy to replicate.\n\n### Possible Enhancements:\n\n1. **Alternative Models**: Incorporating another model (e.g., a neural network) could showcase the versatility of cross entropy in various settings.\n2. **Analysis of Misclassifications**: Add a breakdown of where the model performed poorly and why (e.g., confusion matrix analysis).\n3. **Feature Exploration**: Include visualizations or explanations of feature importance to show how the model makes decisions.\n4. **Comparative Losses**: Compare cross entropy loss with other loss functions (e.g., mean squared error) to highlight its advantages in classification.\n\n**Distribution Insights**:\n\n- The histogram of loss values shows how well the model performs across the test dataset.\n    - A **narrow distribution** around a low value suggests consistent, accurate predictions.\n    - A **wide or skewed distribution** indicates variability in the model's performance, with some instances being predicted poorly.\n\n### [[Mean Squared Error]] versus [[Cross Entropy]]\n\n- **When Comparison Makes Sense**:\n    - MSE can highlight how \"far off\" the predicted probabilities are in terms of magnitude but doesn’t account for the probabilistic nature of classification tasks.\n    - Comparing cross entropy with MSE can show:\n        - How the model performs when considering confidence (cross entropy).\n        - How the model performs when focusing on numerical proximity (MSE).\n        \n- **Insights Gained**:\n    - If cross entropy is low but MSE is high, it might indicate that the model predicts probabilities close to the correct class but has poor numerical calibration for other classes.\n    - If both are high, the model is likely underperforming across the board.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "cross_entropy.py",
    "outlinks": [
      "mean_squared_error",
      "cross_entropy"
    ],
    "inlinks": [
      "cross_entropy"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Comparing_Ensembles.py",
    "sha": "ba06ccfae3c0e05c5734cca226e9796e2736f5f7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Comparing_Ensembles.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Comparing_Ensembles.py\n\nThis script explores various [[Model Ensemble]] techniques in machine learning, demonstrating their effectiveness in improving prediction accuracy through programmatic examples\n\n [!Important Notes]\n - {{Note 1: Consider adding more ensemble techniques like Stacking or Isolated Forests for a comprehensive comparison.}}\n - {{Note 2: Observe how different base estimators impact the performance of the ensemble methods.}}\n\n [!Follow-Up Questions]\n - {{How can ensemble methods be optimized for larger datasets?}}\n - {{What are the trade-offs between different ensemble techniques in terms of complexity and performance?}}",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "comparing_ensembles.py",
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Distribution_Analysis.py",
    "sha": "a14ca91a9928b1f1db5f806c66691064db4f8c09",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Distribution_Analysis.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Distribution_Analysis.py\n\nThe goodness-of-fit results represent the **p-values** from the **Kolmogorov-Smirnov (KS) test**, which assesses how well the data fits each distribution. Here's how to interpret these values:\n\n1. **Higher p-value** → The distribution is a better fit.\n2. **Lower p-value** → The distribution is a poor fit (likely not the correct model for the data).\n3. **Threshold**: A common significance level is **0.05**.\n    - If **p > 0.05**, we do **not** reject the hypothesis that the data follows this distribution.\n    - If **p < 0.05**, we reject the hypothesis, meaning the data **likely does not follow** that distribution.\n\nExample using penguins.csv column \"bill_depth_mm\"\n\nGoodness-of-fit results for bill_depth_mm\nGaussian: 0.026308596409291618\nT: 0.025906678848475195\nChi-squared: 1.4504381882536289e-15\nExponential: 2.8020502445188308e-14\nLogistic: 0.05019989765502264\n\n- **Gaussian (0.0263)** → **Poor fit** (p < 0.05). The data likely does not follow a normal distribution.\n- **T (0.0259)** → **Poor fit** (p < 0.05). The data does not fit a t-distribution well.\n- **Chi-squared (1.45e-15)** → **Very poor fit** (extremely low p-value). The data is **highly unlikely** to follow a chi-squared distribution.\n- **Exponential (2.80e-14)** → **Very poor fit** (extremely low p-value). The data is **not** exponentially distributed.\n- **Logistic (0.0502)** → **Acceptable fit** (p ≈ 0.05). The data could **potentially** follow a logistic distribution.\n\nThe **Logistic distribution** has the **highest p-value (0.0502)**, making it the **best candidate** among the tested distributions. However, since it's **borderline (≈0.05)**, you may want to visualize the distribution and compare the fits.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "distribution_analysis.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Debugging.py",
    "sha": "3611dcf36aedafcc64d7062384aae105d1fc641b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Debugging.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Debugging.py\n\nThis script includes examples of logging, using breakpoints, and reproducing a simple bug for practice\n### Key Concepts Demonstrated in the Script\n\n1. **Logging in Python**: The script uses Python's logging module to record debug, info, error, and warning messages. This helps track the flow of execution and diagnose issues.\n\n2. **Reproduce the Bug**: The script intentionally includes a division by zero bug to demonstrate how to identify and fix it.\n\n3. **Breakpoints**: You can set a breakpoint in your IDE at the line where `result = divide_numbers(num1, num2)` to inspect the values of `num1` and `num2`.\n\n4. **Automated Testing**: The script includes a simple assertion to test the `divide_numbers` function, ensuring it behaves as expected.\n\n5. **Static Analysis**: The commented line `unused_variable = 42` can be used to simulate a static analysis warning for an unused variable.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "debugging.py",
    "outlinks": [],
    "inlinks": [
      "debugging"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Factor_Analysis.py",
    "sha": "4303c893848b275a2088852d3b0d3dd59b9c500b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Factor_Analysis.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Factor_Analysis.py\n### **1. Factor Loadings Table**\n\nThis table shows how strongly each feature (e.g., `sepal length (cm)`) is correlated with the two extracted factors (Factor 1 and Factor 2).\n\n- **Rows**: Represent the extracted factors (`Factor 1` and `Factor 2`).\n- **Columns**: Represent the original features of the Iris dataset.\n- **Values**: Represent the \"loading\" or contribution of each feature to the factor. Higher absolute values indicate a stronger relationship between a feature and a factor.\n\n#### Interpretation:\n\n- **Factor 1 (Row 0)**:\n    - Strongly influenced by `petal length (cm)` (loading = 1.757902) and `petal width (cm)` (loading = 0.731005).\n    - Moderately influenced by `sepal length (cm)` (loading = 0.727461).\n    - Weak negative contribution from `sepal width (cm)` (loading = -0.180852).\n    - This factor might represent the size of petals and sepals.\n      \n- **Factor 2 (Row 1)**:\n    - Weak contributions from all features, with slightly negative contributions from `sepal length (cm)` and `sepal width (cm)`.\n    - This factor might capture a subtle relationship or orthogonal variance not well-defined by the dataset.\n\n---\n\n### **2. Explained Variance**\n\nThe explained variance values indicate how much of the dataset's total variance is captured by each factor.\n- **Factor 1 (0.9988)**: This factor explains ~99.88% of the variance among the features.\n- **Factor 2 (0.9039)**: This factor explains ~90.39% of the variance among the features.\n\n#### Combined Variance:\nThe two factors together capture a large portion of the total variance in the dataset. This suggests that most of the information in the dataset can be reduced to two latent factors, simplifying its structure while retaining the core relationships.\n\n### **Overall Interpretation**\n\n1. **Dimensionality Reduction**: The dataset with four features can effectively be reduced to two latent factors while retaining most of its variance.\n2. **Factor 1 Dominates**: Factor 1 has strong contributions from `petal length`, `petal width`, and `sepal length`. This factor likely represents size-related characteristics.\n3. **Factor 2 is Subtle**: Factor 2 shows weaker relationships with the features, potentially capturing noise or orthogonal variance.\n\n### Next Steps:\n\n1. Would you like to visualize the factors to understand how the data clusters in the new latent space?\n2. Should we explore the relationships between the factors and target classes (e.g., species in the Iris dataset)?\n\n### Breakdown of Extensions:\n\n1. **Visualization**:  \n    After performing factor analysis, we visualize how the data clusters in the new latent space (Factor 1 vs. Factor 2). The points are colored based on the species (target classes), which helps us see if the factors capture any clustering patterns related to species.\n    \n    - **Plot Details**:\n        - The x-axis represents **Factor 1**.\n        - The y-axis represents **Factor 2**.\n        - Each species is plotted in different colors to visualize possible separations.\n2. **Exploring Factor-Target Relationships**:  \n    We compute the **average factor values** for each species. This shows how the latent factors (Factor 1 and Factor 2) relate to the different species in the dataset.\n    \n    - **Interpretation**:\n        - If any species tends to cluster around specific values of Factor 1 and Factor 2, it suggests that the extracted factors capture some species-specific variance.\n\n### Next Steps:\n\n- The plot should give a clear idea of whether the latent factors allow for a meaningful separation of species.\n- The summary table of average factor values will help understand how the factors relate to the target variable.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "factor_analysis.py",
    "outlinks": [],
    "inlinks": [
      "factor_analysis"
    ]
  },
  {
    "category": "OTHER",
    "filename": "FastAPI_Example.py",
    "sha": "4c020d78573638ba1bd18d31189471c0730902ae",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/FastAPI_Example.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/FastAPI_Example.py\n### Explanation of New Features\n\n1. **Path and Query Parameter Metadata**: Added descriptions and constraints for better validation and autogenerated documentation.\n2. **Nested Models**: Demonstrated hierarchical data validation with the `User` model that includes a list of `Item` instances.\n3. **Partial Updates**: Introduced a `PATCH` endpoint to allow partial updates of fields using the `Body` method.\n4. **Static Data**: Provided a status endpoint that returns static information about the API.\n5. **Returning Key-Value Data**: Added a summary endpoint to showcase mock data.\n\n# Main\n\n### **What the Script Does**\n\n1. **Starts the FastAPI Application**  \n    When you run the script, it starts a web server using Uvicorn. This makes your FastAPI app accessible via the browser or API clients like Postman.\n    \n2. **Default Endpoint (`GET /`)**  \n    The browser sends a `GET` request to the root path (`http://127.0.0.1:8000/`).  \n    The root route (`@app.get(\"/\")`) is defined in the script to return:\n    \n    ```python\n    {\"message\": \"Welcome to the expanded FastAPI example!\"}\n    ```\n    \n    This is why you see that message in the browser or the API response.\n    \n3. **404 for `/favicon.ico`**  \n    The browser automatically tries to load a favicon (a small icon displayed in the browser tab). Since no favicon is defined in the script, a `404 Not Found` is returned, which is normal behavior.\n    \n\n---\n\n### **What the Script Offers Beyond the Root Endpoint**\n\nThe script defines several API endpoints that are not automatically accessed unless you explicitly call them. Here’s a summary of the key routes:\n\n|**Endpoint**|**HTTP Method**|**Description**|\n|---|---|---|\n|`/`|`GET`|Returns a welcome message.|\n|`/items/{item_id}`|`GET`|Fetches an item by its `item_id`, with an optional query parameter.|\n|`/search/`|`GET`|Searches items using `limit` and `offset` query parameters for pagination.|\n|`/items/`|`POST`|Creates a new item using the `Item` model for validation.|\n|`/items/{item_id}`|`PUT`|Updates an item by its `item_id` with a new `Item` object.|\n|`/items/{item_id}`|`DELETE`|Deletes an item by its `item_id`.|\n|`/users/`|`POST`|Creates a new user with optional nested items using the `User` model.|\n|`/items/{item_id}`|`PATCH`|Partially updates fields (like `price` or `on_offer`) of an item by its ID.|\n|`/status/`|`GET`|Returns static data, such as the API's status and version.|\n|`/summary/`|`GET`|Returns a dictionary with some mock data, such as total items and users.|\n\n---\n\n### **Testing the Script**\n\nYou need to explicitly visit or call other endpoints to explore more features of the script. For example:\n\n1. ==**Accessing the Swagger UI** Go to [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) in your browser.==  \n    This interactive interface shows all the available endpoints and lets you test them directly.\n    \n2. **Calling Specific Endpoints** You can call the endpoints via:\n    - **Browser** (e.g., `http://127.0.0.1:8000/items/123?q=test`).\n    - **API Clients** like Postman or Curl.\n\n---\n\n### **Why You See Only the Welcome Message**\n\nYou’ve accessed only the root endpoint (`/`). The other features of the script (like creating, updating, or searching for items) require you to call the respective endpoints explicitly.\n\n# Calling endpoints\n\nHere's how to call the respective endpoints of the script using various tools like **browser**, **Curl**, or **Postman**. Each example demonstrates an endpoint and its purpose.\n### **1. Welcome Endpoint (`GET /`)**\n\n- **Purpose:** Displays a welcome message.\n- **Call:**\n    - **Browser:** Open `http://127.0.0.1:8000/`.\n    - **Curl:**\n        ```bash\n        curl -X GET http://127.0.0.1:8000/\n        ```\n    - **Response:**\n        ```json\n        {\"message\": \"Welcome to the expanded FastAPI example!\"}\n        ```\n### **2. Retrieve an Item (`GET /items/{item_id}`)**\n\n- **Purpose:** Fetches item details by its `item_id` with an optional query parameter `q`.\n- **Example:** Retrieve item `3` with query `test`.\n- **Call:**\n    - **Browser:** Open `http://127.0.0.1:8000/items/3?q=test`.\n    - **Curl:**\n        \n        ```bash\n        curl -X GET \"http://127.0.0.1:8000/items/3?q=test\"\n        ```\n    - **Response:**\n        ```json\n        {\"item_id\": 3, \"query\": \"test\"}\n        ```\n\n---\n\n### **3. Search Items (`GET /search/`)**\n\n- **Purpose:** Uses `limit` and `offset` query parameters for pagination.\n- **Example:** Limit results to 5, skip the first 2.\n- **Call:**\n    - **Browser:** Open `http://127.0.0.1:8000/search/?limit=5&offset=2`.\n    - **Curl:**\n        ```bash\n        curl -X GET \"http://127.0.0.1:8000/search/?limit=5&offset=2\"\n        ```     \n    - **Response:**\n        ```json\n        {\"limit\": 5, \"offset\": 2}\n        ```\n### **4. Create an Item (`POST /items/`)**\n\n- **Purpose:** Adds a new item.\n- **Example:** Create an item named \"Laptop\" priced at 999.99.\n- **Call:**\n    - **Curl:**\n        ```bash\ncurl -X POST \"http://127.0.0.1:8000/items/\" -H \"Content-Type: application/json\" -d \"{\\\"name\\\": \\\"Laptop\\\", \\\"price\\\": 999.99, \\\"description\\\": \\\"High-end laptop\\\", \\\"on_offer\\\": true}\"\n        ```\n    - **Response:**\n        ```json\n        {\n          \"message\": \"Item created successfully\",\n          \"item\": {\n            \"name\": \"Laptop\",\n            \"price\": 999.99,\n            \"description\": \"High-end laptop\",\n            \"on_offer\": true\n          }\n        }\n        ```\n### **5. Update an Item (`PUT /items/{item_id}`)**\n\n- **Purpose:** Updates item details by its `item_id`.\n- **Example:** Update item `3` with new data.\n- **Call:**\n    - **Curl:**\n        \n        ```bash\n        curl -X PUT \"http://127.0.0.1:8000/items/3\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"name\": \"Smartphone\", \"price\": 499.99, \"description\": \"Updated phone\", \"on_offer\": false}'\n        ```\n\n    - **Response:**\n        \n        ```json\n        {\n          \"item_id\": 3,\n          \"updated_item\": {\n            \"name\": \"Smartphone\",\n            \"price\": 499.99,\n            \"description\": \"Updated phone\",\n            \"on_offer\": false\n          }\n        }\n        ```\n\n### **6. Delete an Item (`DELETE /items/{item_id}`)**\n\n- **Purpose:** Deletes an item by its `item_id`.\n- **Example:** Delete item `2`.\n- **Call:**\n    - **Curl:**\n        \n        ```bash\n        curl -X DELETE http://127.0.0.1:8000/items/2\n        ```\n        \n    - **Response:**\n        \n        ```json\n        {\"message\": \"Item 2 deleted successfully\"}\n        ```\n        \n\n---\n\n### **7. Create a User (`POST /users/`)**\n\n- **Purpose:** Creates a user, optionally with nested items.\n- **Example:** Create a user with items.\n- **Call:**\n    - **Curl:**\n        ```bash\n\t\tcurl -X POST \"http://127.0.0.1:8000/users/\" -H \"Content-Type: application/json\" -d \"{\\\"username\\\": \\\"john_doe\\\", \\\"email\\\": \\\"john@example.com\\\", \\\"full_name\\\": \\\"John Doe\\\", \\\"items\\\": [{\\\"name\\\": \\\"Tablet\\\", \\\"price\\\": 299.99, \\\"description\\\": \\\"Portable tablet\\\"}]}\"\n        ```\n        \n    - **Response:**\n        \n        ```json\n        {\n          \"message\": \"User created successfully\",\n          \"user\": {\n            \"username\": \"john_doe\",\n            \"email\": \"john@example.com\",\n            \"full_name\": \"John Doe\",\n            \"items\": [\n              {\n                \"name\": \"Tablet\",\n                \"price\": 299.99,\n                \"description\": \"Portable tablet\",\n                \"on_offer\": null\n              }\n            ]\n          }\n        }\n        ```\n### **8. Partially Update an Item (`PATCH /items/{item_id}`)**\n\n- **Purpose:** Partially updates an item using optional fields.\n- **Example:** Update the price of item `5`.\n- **Call:**\n    - **Curl:**\n        \n        ```bash\n        curl -X PATCH \"http://127.0.0.1:8000/items/5\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"price\": 79.99}'\n        ```\n        \n    - **Response:**\n        \n        ```json\n        {\n          \"item_id\": 5,\n          \"updates\": {\n            \"price\": 79.99\n          }\n        }\n        ```\n### **9. Check API Status (`GET /status/`)**\n\n- **Purpose:** Returns static information like API status.\n- **Call:**\n    - **Browser:** Open `http://127.0.0.1:8000/status/`.\n    - **Curl:**\n        \n        ```bash\n        curl -X GET http://127.0.0.1:8000/status/\n        ```\n        \n    - **Response:**\n        \n        ```json\n        {\"status\": \"API is running\", \"version\": \"1.0.0\"}\n        ```\n        \n\n### **10. Retrieve a Summary (`GET /summary/`)**\n\n- **Purpose:** Returns a mock summary of items and users.\n- **Call:**\n    - **Browser:** Open `http://127.0.0.1:8000/summary/`.\n    - **Curl:**\n        \n        ```bash\n        curl -X GET http://127.0.0.1:8000/summary/\n        ```\n        \n    - **Response:**\n        \n        ```json\n        {\"total_items\": 42, \"total_users\": 5, \"recent_activity\": \"Item purchase\"}\n        ```\n        \n\n---\n\n### **Testing Tips**\n\n- Use **Swagger UI** at `http://127.0.0.1:8000/docs` for interactive testing of all endpoints.\n- Use **Postman** for testing more complex requests with nested data.\n\n# New \n\n### **Explanation of Changes**:\n\n- **`created_items`**: Items that have been created using the `/items/` POST endpoint.\n- **`updated_items`**: Items that have been updated using the `/items/{item_id}` PUT endpoint.\n- **`deleted_items`**: Items that have been deleted using the `/items/{item_id}` DELETE endpoint.\n- **Summary Endpoint**:\n    - Returns counts of items created (`created_items_count`), updated (`updated_items_count`), and deleted (`deleted_items_count`).\n\n---\nHere's the formatted content for use in `cmd`:\n\n==TEST LATER==\n### **Testing with `curl`**:\n\n1. **Create an item**:\n    \n    ```bash\n    curl -X POST \"http://127.0.0.1:8000/items/\" -H \"Content-Type: application/json\" -d '{\"name\": \"Tablet\", \"price\": 299.99, \"description\": \"Portable tablet\"}'\n    ```\n    \n2. **Update an item**:\n    \n    ```bash\n    curl -X PUT \"http://127.0.0.1:8000/items/1\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"name\": \"Tablet\", \"price\": 250.00, \"description\": \"Updated portable tablet\"}'\n    ```\n    \n3. **Delete an item**:\n    \n    ```bash\n    curl -X DELETE \"http://127.0.0.1:8000/items/1\"\n    ```\n    \n4. **Get the summary**:\n    \n    ```bash\n    curl -X GET \"http://127.0.0.1:8000/summary/\"\n    ```\n    \n\n### **Expected Response**:\n\n```json\n{\n  \"total_items\": 0,\n  \"total_users\": 5,\n  \"recent_activity\": \"Item purchase\",\n  \"created_items_count\": 1,\n  \"updated_items_count\": 1,\n  \"deleted_items_count\": 1\n}\n```\n\nYou can copy and paste these commands directly into `cmd` to test the API.\n### **Note**:\n\nMake sure your FastAPI server is running on `http://127.0.0.1:8000` before executing these commands.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "fastapi_example.py",
    "outlinks": [],
    "inlinks": [
      "fastapi"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Forecasting_AutoArima.py",
    "sha": "8aff97c4a7a2984b03ba410a00fecd923a965060",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Forecasting_AutoArima.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_AutoArima.py\n## Tools and Resources\n\n- AutoARIMA: Automatically selects the best [[ARIMA]] [[Model Parameters]]. Available in the statsforecast library by [Nixtla](https://www.linkedin.com/company/nixtlainc/).\n- Implementation Example: See `TS_AutoArima.py` in [[ML_Tools]] for practical implementation.\n## Performance Insights\n\n- [[Evaluation Metrics]]: Marginal differences in evaluation metrics across ARIMA models may occur due to the volatile nature of the data.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "forecasting_autoarima.py",
    "outlinks": [
      "arima",
      "ml_tools",
      "evaluation_metrics",
      "model_parameters"
    ],
    "inlinks": [
      "arima",
      "forecasting_exponential_smoothing.py",
      "pmdarima",
      "time_series_forecasting"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Forecasting_Baseline.py",
    "sha": "a0850e23b53f91ebb9644c5551a8adcb24f34b6e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Forecasting_Baseline.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Baseline.py\n\nBaseline methods are essential for establishing a performance benchmark. They provide insights into the data's underlying patterns and help in assessing the effectiveness of more sophisticated forecasting models. By comparing advanced models against these baselines, you can determine if the added complexity is justified by improved accuracy.\n\n**Methods Implemented:**\n    - **Mean Forecasting:** Uses the average of all past values as the forecast for future periods.\n    - **Naive Forecasting:** The last observed value is used as the forecast for all future periods.\n    - **Seasonal Naive Forecasting:** Uses the value from the previous seasonal period to forecast the future.\n    - **Drift Method:** Predicts future values based on the trend between the first and last observations in the training data.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "forecasting_baseline.py",
    "outlinks": [],
    "inlinks": [
      "time_series_forecasting"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Forecasting_Exponential_Smoothing.py",
    "sha": "e0e7e4b733324099cf1e1d9b89a6feaf25f9a1ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Forecasting_Exponential_Smoothing.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Exponential_Smoothing.py\n\nExponential smoothing models are a set of [[Time Series Forecasting]] techniques that apply weighted averages of past observations, with the weights decaying exponentially over time. These methods are useful for capturing different components of time series data, such as level, trend, and seasonality.\n\nHowever, their effectiveness depends on the nature of the data. For [[Datasets]] with simple patterns, these models can be quite effective, but for more complex series, alternative methods may be necessary.\n\n## Methods Implemented\n\nAdvanced Alternatives: For complex datasets like stock prices, advanced models such as [[Forecasting_AutoArima.py]] may be more appropriate to capture the intricacies of the data.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "forecasting_exponential_smoothing.py",
    "outlinks": [
      "forecasting_autoarima.py",
      "datasets",
      "time_series_forecasting"
    ],
    "inlinks": [
      "time_series_forecasting"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Gaussian_Mixture_Model_Implementation.py",
    "sha": "c7fb76d62bc7f3e83cedf7afaaf289795d7a962c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Gaussian_Mixture_Model_Implementation.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/Gaussian_Mixture_Model_Implementation.py\n\nFollow-Up Questions\n\n- How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency?\n- What are the implications of choosing different covariance types in GMMs?",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "gaussian_mixture_model_implementation.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Imbalanced_Datasets_SMOTE.py",
    "sha": "f90401238829f11f7094c1e027cf1e17894f76d1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Imbalanced_Datasets_SMOTE.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Imbalanced_Datasets_SMOTE.py\n\n### Demonstrating the Value of Resampling in Imbalanced Classification\n\nThis example highlights the effectiveness of resampling techniques, such as [[SMOTE (Synthetic Minority Over-sampling Technique)|SMOTE]], in addressing [[Imbalanced Datasets|class imbalance]] issues in classification tasks. By implementing the following strategies, the setup ensures a measurable improvement in model performance:\n\n1. **Severe Imbalance and Dataset Size**:\n    - Utilizing a larger dataset with a severe imbalance ratio (e.g., 99:1) makes the impact of resampling more apparent. This imbalance necessitates resampling for the model to predict the minority class accurately.\n\n2. **Choice of Classifier**:\n    - Switching from robust classifiers like [[Random Forest]] to more sensitive ones like [[Logistic Regression]] or Support Vector Machine ([[Support Vector Machines|SVM]]) highlights the benefits of resampling. These simpler models struggle with imbalance, providing a clear contrast between resampling and non-resampling scenarios.\n\n3. **Feature Overlap**:\n    - Ensuring overlap in the feature space between minority and majority classes enhances the effectiveness of synthetic resampling techniques, such as SMOTE.\n\n4. **Focus on Minority Class Metrics**:\n    - Emphasizing evaluation metrics like [[Recall]] and F1-score for the minority class explicitly measures the model's ability to capture minority class instances, demonstrating the value of resampling in improving these metrics.\n\n### Results:\n\n#### Without Resampling:\n\n| Class | Precision | Recall | F1-Score | Support |\n|-------|-----------|--------|----------|---------|\n| 0     | 0.99      | 1.00   | 1.00     | 990     |\n| 1     | 0.67      | 0.20   | 0.31     | 10      |\n| **Accuracy** |       |        | 0.99     | 1000    |\n| **Macro Avg** | 0.83      | 0.60   | 0.65     | 1000    |\n| **Weighted Avg** | 0.99      | 0.99   | 0.99     | 1000    |\n\n- The minority class recall will likely be very low (close to 0), as the classifier may predict the majority class almost exclusively.\n- Overall [[Accuracy]] will be high because the majority class dominates.\n\n#### With SMOTE Resampling:\n\n| Class | Precision | Recall | F1-Score | Support |\n|-------|-----------|--------|----------|---------|\n| 0     | 1.00      | 0.82   | 0.90     | 990     |\n| 1     | 0.04      | 0.70   | 0.07     | 10      |\n| **Accuracy** |       |        | 0.82     | 1000    |\n| **Macro Avg** | 0.52      | 0.76   | 0.49     | 1000    |\n| **Weighted Avg** | 0.99      | 0.82   | 0.89     | 1000    |\n\n- Minority class recall and F1-score should improve significantly, as SMOTE provides synthetic samples to balance the training set.\n- Accuracy might decrease slightly due to more emphasis on minority class performance.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "imbalanced_datasets_smote.py",
    "outlinks": [
      "accuracy",
      "smote_(synthetic_minority_over-sampling_technique)",
      "logistic_regression",
      "recall",
      "random_forest",
      "imbalanced_datasets",
      "support_vector_machines"
    ],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Handling_Missing_Data_Basic.ipynb",
    "sha": "bafcfcb771c580d2111dad2042950740b5da84bd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Handling_Missing_Data_Basic.ipynb.md",
    "text": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data_Basic.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "handling_missing_data_basic.ipynb",
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Handling_Missing_Data.ipynb",
    "sha": "8ba197ac52837cdec55313f5da40cce80e428934",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Handling_Missing_Data.ipynb.md",
    "text": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "handling_missing_data.ipynb",
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ]
  },
  {
    "category": "OTHER",
    "filename": "K_Means.py",
    "sha": "b94b2a05db9621b8ede2c3542844dd2b62e6b843",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/K_Means.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/KMeans/K_Means.py\n## Key Concepts Used in the Script\n\n1. **Data Loading**:\n   - The script reads data from a CSV file (`penguins.csv`) and uses a sample dataset with random features for demonstration purposes.\n\n2. **Data Preprocessing**:\n   - **Standardization**: Features are standardized using `sklearn.preprocessing.scale` and `StandardScaler` to ensure that all features contribute equally to the clustering process.\n\n3. **Feature Selection**:\n   - Specific features, such as `bill_length_mm` and `bill_depth_mm`, are selected for clustering.\n\n4. **K-Means Clustering**:\n   - The core clustering algorithm is applied with `n_clusters=3`.\n   - Outputs include cluster centroids and labels for each data point.\n\n5. **Visualization**:\n   - Scatter plots are used to display the clustering results, highlighting the cluster centroids.\n\n6. **Evaluation of Optimal Clusters**:\n   - **Elbow Method**: This method iterates through different numbers of clusters to determine the optimal number based on the within-cluster sum of squares (WCSS).\n\n7. **Cluster Assignment**:\n   - Labels are assigned to data points, and the results are visualized to show the clustering outcome.\n\n8. **Exploratory Analysis**:\n   - The script examines the impact of different numbers of clusters using an example function (`scatter_elbow`).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "k_means.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "PCA_Analysis.ipynb",
    "sha": "8d01e349507db9848bfdff28af762e332846c845",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/PCA_Analysis.ipynb.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Analysis.ipynb\n\nThis script performs Principal Component Analysis (PCA) on the Iris dataset to reduce its dimensionality while preserving key variance. \n\nSee also [[Principal Component Analysis|PCA]]\n\nSummary:\n\n1. Load and Preprocess Data\n    - Loads the Iris dataset and extracts features and target labels.\n    - Scales the data to standardize feature ranges.\n\n2. Apply PCA (3 Components)\n    - Fits PCA to the scaled data and transforms it into three principal components.\n    - Stores the transformed data in a DataFrame with species labels.\n    \n3. Analyze PCA Loadings & Variance\n    - Computes and stores PCA loadings (weights of original features in principal components).\n    - Computes explained variance and cumulative variance to assess PCA effectiveness.\n\n4. Visualizations\n    - Explained variance: Bar plot of individual and cumulative variance contributions.\n    - PCA Scores: 3D scatter plots of transformed data, colored by species.\n    - PCA Loadings: 3D scatter plot showing feature contributions to principal components.\n    - Heatmap: Displays PCA component weights for feature importance analysis.\n\n5. Additional Full PCA Analysis\n    - Computes and prints explained variance for all components.\n    - Uses Seaborn to generate a heatmap of PCA component contributions.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pca_analysis.ipynb",
    "outlinks": [
      "principal_component_analysis"
    ],
    "inlinks": [
      "pca_principal_components",
      "principal_component_analysis"
    ]
  },
  {
    "category": "OTHER",
    "filename": "One_hot_encoding.py",
    "sha": "ee85b02a5ee8f2eef453e1c7b0db9ae38b29980d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/One_hot_encoding.py.md",
    "text": "Explorations\\Preprocess\\One_hot_encoding\\One_hot_encoding.py\n\nThis script demonstrates how to preprocess categorical variables and apply linear regression for house price prediction. Key steps include:\n\n1. **Data Loading**: It loads a dataset of house prices.\n2. **Dummy Variables**: It creates dummy variables for the 'town' column using `pd.get_dummies()` and merges them with the original dataframe.\n3. **Dummy Variable Trap**: It drops one dummy variable to avoid multicollinearity (dummy variable trap).\n4. **Feature and Target Split**: It separates the dataset into features (X) and the target variable (price).\n5. **Model Training**: A Linear Regression model is trained on the data.\n6. **Predictions**: It predicts house prices based on various features and evaluates the model's accuracy.\n7. **[[Label encoding]] and One-Hot Encoding**: It applies `LabelEncoder` to convert 'town' names into numbers and uses `OneHotEncoder` to create dummy variables for categorical columns.\n8. **Final Predictions**: It predicts prices using the transformed features and evaluates the model's performance.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "one_hot_encoding.py",
    "outlinks": [
      "label_encoding"
    ],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Momentum.py",
    "sha": "a3d309bb12bc28ab0920894e630c30ff3029c2c8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Momentum.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Momentum.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "momentum.py",
    "outlinks": [],
    "inlinks": [
      "momentum"
    ]
  },
  {
    "category": "OTHER",
    "filename": "PCA_Based_Anomaly_Detection.py",
    "sha": "e2bb120ff40c2642a1a4e8320144825805d7ad55",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/PCA_Based_Anomaly_Detection.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Based_Anomaly_Detection.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pca_based_anomaly_detection.py",
    "outlinks": [],
    "inlinks": [
      "pca-based_anomaly_detection"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Pandas_Stack.py",
    "sha": "8904a2dac0746517cd42cb0e2e7d87dce67b3581",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pandas_Stack.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Stack.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pandas_stack.py",
    "outlinks": [],
    "inlinks": [
      "pandas_stack"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Pycaret_Anomaly.ipynb",
    "sha": "4e128b8d74d1d56549d134ac7cc4d5d5b3fca520",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pycaret_Anomaly.ipynb.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/Pycaret_Anomaly.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pycaret_anomaly.ipynb",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Pandas_Common.py",
    "sha": "69cd3722d1b65e925a134db7ad630a49a48525f9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pandas_Common.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Common.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pandas_common.py",
    "outlinks": [],
    "inlinks": [
      "pandas",
      "pandas_stack"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Pycaret_Example.py",
    "sha": "3938412cd57af4eb00f86ea245516f988c8b20ad",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pycaret_Example.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Pycaret/Pycaret_Example.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pycaret_example.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Pydantic_More.py",
    "sha": "1e8d9ebcff3c83342781bf8d038e68d71b5a3ab4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pydantic_More.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic_More.py\n### Key Features Demonstrated in the Script:\n\n1. **Nested Models:** Use of the `Friend` model inside the `User` model.\n2. **Custom Validators:** Validating `age` and `email` fields with specific logic.\n3. **Dynamic Defaults:** Using `datetime.now` for `created_at`.\n4. **Field Aliases:** Supporting different key names during parsing and serialization.\n5. **Configuration Options:** Stripping whitespace and enabling strict typing.\n6. **Model Inheritance:** Extending the `User` model to create an `AdminUser` model.\n7. **Parsing Raw Data:** Demonstrating `parse_raw` for JSON strings.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pydantic_more.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Pydantic.py",
    "sha": "69fbc6f6a04492b3180fd2b8e0e458a5adbc7b62",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Pydantic.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic.py\n### Explanation:\n\n- **BaseModel**: This is the base class for creating data models in Pydantic. You define your model by subclassing `BaseModel` and specifying fields with type annotations.\n- **Optional**: Used to indicate that a field is optional.\n- **List**: Used to specify a list of items, in this case, a list of strings for friends.\n- **Validator**: A custom validator is used to enforce additional constraints, such as ensuring the age is positive.\n- **ValidationError**: This exception is raised when the input data does not conform to the model's constraints.\n\nThis script demonstrates how Pydantic can be used to validate and parse data, ensuring it meets the specified types and constraints.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "pydantic.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "ROC_Curve.py",
    "sha": "a93e96733fb543cfa21a92aaa596f0d45f7429e6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/ROC_Curve.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/ROC_Curve.py\n## **Overview**\n\nThis script demonstrates how to compute and interpret Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC) scores using Random Forest and Naive Bayes classifiers. Below is the step-by-step breakdown:\n## **Script Flow**\n\n- **Generate Synthetic Dataset**  \n  - Creates a binary classification dataset with 2,000 samples and 10 features.  \n  - Simulates a realistic classification problem.\n\n- **Add Noisy Features**  \n  - Appends random, irrelevant features to increase the dataset's complexity.  \n  - Mimics challenging real-world scenarios where not all features are informative.\n\n- **Split the Data**  \n  - Divides the dataset into training (80%) and testing (20%) subsets.  \n  - Ensures unbiased model evaluation on unseen data.\n\n- **Train Classification Models**  \n  - Builds two models:  \n    - **Random Forest**: A robust ensemble-based classifier.  \n    - **Naive Bayes**: A simple probabilistic model based on Bayes' theorem.\n\n- **Generate Prediction Probabilities**  \n  - Computes predicted probabilities for each class.  \n  - Retains probabilities for the positive class to construct the ROC curve.\n\n- **Compute AUROC and ROC Curve Values**  \n  - Calculates:  \n    - **AUROC**: Measures model performance (higher is better).  \n    - **ROC Values**: False Positive Rate (FPR) and True Positive Rate (TPR) across thresholds.\n\n- **Visualize ROC Curve**  \n  - Plots FPR (x-axis) against TPR (y-axis) for each model.  \n  - Includes AUROC scores in the legend for comparison.\n## **Key Outputs**\n- **AUROC Scores**  \n  - Evaluates the overall discriminative power of the classifiers.  \n\n- **ROC Plot**  \n  - Visualizes how well each model distinguishes between positive and negative classes across thresholds.  \n  - A random prediction baseline is included for reference.\n## **Conclusion**\nThis script illustrates the process of building, evaluating, and visualizing classification models using ROC curves. It highlights the strengths and weaknesses of different models in distinguishing classes.\n\n# Output\n\n### Interpretation of the Script Output\n\n- **Random (Chance) Prediction: AUROC = 0.500**\n    - An AUROC score of **0.500** represents a random guessing model with no predictive power.\n    - The model's True Positive Rate (TPR) is equal to its False Positive Rate (FPR) across all thresholds, resulting in a diagonal line on the ROC curve.\n    \n- **Random Forest: AUROC = 0.922**\n    - An AUROC score of **0.922** indicates excellent model performance.\n    - The Random Forest classifier has a high ability to distinguish between positive and negative classes, with a much higher TPR than FPR across thresholds.\n- \n- **Naive Bayes: AUROC = 0.993**\n    - An AUROC score of **0.993** suggests near-perfect model performance.\n    - The Naive Bayes classifier has an extremely high discriminative power, with TPR approaching 1 and FPR close to 0 for most thresholds.\n\n### Summary\n\n- The **Naive Bayes classifier** outperforms the **Random Forest classifier** in this specific setup.\n- Both models significantly outperform random guessing (baseline AUROC = 0.500), indicating their utility for this classification task.\n- However, such high performance (especially for Naive Bayes) may suggest that the dataset or features are particularly well-suited to the model, or there may be minimal noise in the classification task. Further evaluation (e.g., on new datasets) is recommended to confirm robustness.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "roc_curve.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Regression_Logistic_Metrics.ipynb",
    "sha": "f5deb57d639b3d754300172352985808f01545ab",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Regression_Logistic_Metrics.ipynb.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Regression_Logistic_Metrics.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "regression_logistic_metrics.ipynb",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "TS_Anomaly_Detection.py",
    "sha": "75a9c103f152ffc0b94b27b760ff0c372714714d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/TS_Anomaly_Detection.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/TS_Anomaly_Detection.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "ts_anomaly_detection.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "SVM_Example.py",
    "sha": "a7fa6fc91ae9b76698cc08160e7b1d79ac8b0859",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/SVM_Example.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main\\Explorations/Build/Classifiers/SVM/SVM_Example.py\n\n## **Overview**\n\n- **Objective**: To classify Iris flowers using SVM and explore various hyperparameters like kernel type, regularization (C), and gamma.\n- **Dataset**: The Iris dataset contains information about sepal and petal dimensions for three flower species.\n- To explore the effect of **soft boundaries** in SVMs, you can adjust the regularization parameter CCC. A smaller CCC allows a **softer boundary** (more margin violations), prioritizing generalization. A larger CCC enforces a **harder boundary** with fewer margin violations, but may lead to overfitting. Here's an extended version of the script to include this exploration:\n\n### **Steps in the Script**\n\n#### 1. **Data Loading and Preparation**\n\n- The Iris dataset is loaded using `sklearn.datasets.load_iris`.\n- A DataFrame is created with:\n    - Features: Sepal and petal dimensions.\n    - Target: Numerical representation of flower species.\n    - Flower name: Categorical species name derived from the target.\n\n#### 2. **Data Visualization**\n\n- The data is visualized to explore relationships between features:\n    - **Sepal Length vs. Sepal Width** for two species (Setosa vs. Versicolor).\n    - **Petal Length vs. Petal Width** for the same species.\n- Scatter plots are used to identify separable patterns.\n\n#### 3. **Model Training**\n\n- The data is split into training and testing sets (80%-20%).\n- An **SVM classifier** (`sklearn.svm.SVC`) is trained on the training set.\n- The model's performance is evaluated using the `.score()` method.\n\n#### 4. **Hyperparameter Tuning**\n\n- **Regularization (C)**:\n    - Adjusting `C` controls the trade-off between achieving a large margin and minimizing classification errors.\n    - Lower values of `C` allow a larger margin but can tolerate misclassified points.\n    - Higher values of `C` prioritize correct classification over a larger margin.\n- **Gamma**:\n    - Controls the influence of individual data points. A high value means data points closer to the hyperplane have more influence.\n- **Kernel**:\n    - Different kernels (e.g., `linear`, `rbf`) are tested to find the best mapping of data into higher dimensions for better separation.\n\n#### 5. **Prediction and Accuracy**\n\n- The model is used to predict flower species for new samples.\n- The accuracy of the model is reported for each combination of hyperparameters.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "svm_example.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Testing_Pytest.py",
    "sha": "81cd35d9421330edfc38e327211a612e4da500e2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Testing_Pytest.py.md",
    "text": "The `pytest` example script demonstrates several key features of the [[Pytest]] testing framework:\n\n1. **Fixtures**: The script uses a fixture named `sample_data` to provide common test data that can be reused across multiple test functions. This helps reduce code duplication and enhances test maintainability.\n\n2. **Parametrization**: The script employs the `@pytest.mark.parametrize` decorator to run a test function with multiple sets of arguments. This allows for testing a function with various inputs without writing separate test cases for each scenario.\n\n3. **Custom Markers**: A custom marker `@pytest.mark.slow` is used to categorize tests. This enables selective test execution based on markers, allowing you to run specific groups of tests using the `-m` option.\n\n4. **Mocking**: The script demonstrates how to use `unittest.mock.patch` with `pytest` to replace a function with a mock. This allows for controlling the behavior of the function during the test, facilitating isolated testing of units.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "testing_pytest.py",
    "outlinks": [
      "pytest"
    ],
    "inlinks": [
      "debugging"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Testing_unittest.py",
    "sha": "a416fb4eb814526a0eb4e3ec4759596cffc40b69",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Testing_unittest.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_unittest.py\n\nTo explore testing in Python, let's focus on some key concepts and provide a simple example using the `unittest` framework, which is a built-in module for writing and running tests.\n\nBy writing and running tests, you can ensure that your code behaves as expected and catch bugs early in the development process. \n\n[[Pytest]]\n### Key Concepts in Testing\n\n1. **Unit Testing**: Testing individual components or functions in isolation to ensure they work as expected. Unit tests are typically small and fast.\n\n2. **Test-Driven Development (TDD)**: A development approach where tests are written before the actual code. This helps define the expected behavior and ensures the code meets these expectations.\n\n3. **Assertions**: Statements in tests that check if a condition is true. If an assertion fails, the test fails.\n\n4. **Test Suites**: Collections of test cases that can be run together.\n\n5. **Mocking**: Simulating the behavior of complex objects or systems to isolate the unit being tested.\n\n### How to Run the Tests\n\n1. Save the script to a file, e.g., `test_math_operations.py`.\n2. Run the script using Python: `python test_math_operations.py`.\n3. The `unittest` framework will automatically discover and run the test cases, reporting any failures or errors.\n\n### Exploring Further\n\n- **Test Coverage**: Measure how much of your code is covered by tests. Tools like `coverage.py` can help identify untested parts of your code.\n- **Continuous Integration (CI)**: Automate the running of tests using CI tools like Jenkins, Travis CI, or [[categories/devops/Github Actions|GitHub Actions]] to ensure code quality in every commit.\n- **Behavior-Driven Development (BDD)**: An extension of TDD that uses natural language to describe the behavior of software, often using tools like `pytest-bdd` or `behave`.\n\n2. **Edge Cases and Error Handling**:\n    \n    - The `divide` function raises a `ValueError` if division by zero is attempted.\n    - The `test_divide` method includes a test case to check for this exception using `assertRaises`.\n3. **Mocking**:\n    - The `test_mock_add` method demonstrates how to use `unittest.mock.patch` to replace the `add` function with a mock that returns a fixed value.\n    - This is useful for isolating the unit under test and controlling its behavior.\n4. **Comprehensive Testing**:\n    \n    - Each function is tested with multiple inputs, including positive, negative, and zero values, to ensure robustness.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "testing_unittest.py",
    "outlinks": [
      "categories/devops/github_actions",
      "pytest"
    ],
    "inlinks": [
      "debugging"
    ]
  },
  {
    "category": "OTHER",
    "filename": "Vector_Embedding.py",
    "sha": "b27bb6ff1a7771e3533cae270d6e3e5c898f4995",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Vector_Embedding.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Vector_Embedding.py\n\n### Explanation of the Script\n\n1. **Vocabulary and Embedding Layer**:\n    - Terms are mapped to indices using a dictionary.\n    - The embedding layer learns continuous vector representations for these terms.\n      \n2. **Cosine Similarity**:\n    - The cosine similarity function measures how similar two terms are in the embedding space. Higher values indicate closer relationships.\n      \n3. **Visualization**:\n    - Embeddings are plotted in a 2D space to show semantic relationships. Terms with similar meanings (e.g., \"king\" and \"queen\") are expected to cluster together.\n      \n4. **t-SNE for Dimensionality Reduction**:\n    - If the embedding dimension is higher than 2, t-SNE can reduce it to 2D for visualization while preserving semantic relationships.\n\n### Outputs\n\n1. **Cosine Similarities**:\n    - Pairwise similarity scores between terms to quantify their semantic closeness.\n      \n2. **Visualization**:\n    - A scatter plot showing the positions of terms in the embedding space.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "vector_embedding.py",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Word2Vec.py",
    "sha": "08f2f08323d21cb7700ffe94380b4141742bfdfd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Word2Vec.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Word2Vec.py\n\nThe script can benefit from **Word2Vec embeddings** by replacing the randomly initialized embeddings with pretrained or trained embeddings generated using Word2Vec. These embeddings provide a meaningful semantic structure that is learned from a corpus of text, enhancing the visualization and [[Cosine Similarity]] calculations.\n\n#### Benefits:\n\n1. **Meaningful Relationships**: Words like \"king\" and \"queen\" will naturally be closer than \"king\" and \"apple.\"\n2. **Analogy Solving**: Word2Vec supports vector arithmetic to solve word analogies (e.g., \"man is to king as woman is to queen\").\n3. **Improved Visualizations**: The embeddings reflect real-world semantic and syntactic relationships, making the 2D plots more interpretable.\n\n### Further Enhancements\n\n1. **Train Your Word2Vec**:\n    - Train embeddings on a custom corpus using `gensim.models.[[Word2Vec]]` to reflect domain-specific semantics.\n    \n1. **Hybrid Embeddings**:\n    - Combine Word2Vec with other models (e.g., [[BERT]] or Sentence [[Transformer|Transformers]]) for tasks requiring contextual understanding.\n\n**Using `glove-wiki-gigaword-100`**:\n\n- A GloVe model with 100-dimensional embeddings trained on the Wikipedia Gigaword dataset.\n- Approximate size: ~100MB.\n\n### Expected Outcome\n\n1. **Visualization**:\n    - Terms from the same category (e.g., royalty, fruits, animals) will cluster together in the t-SNE plot.\n2. **Cosine Similarity**:\n    - Similar terms (e.g., \"king\" and \"queen\" or \"apple\" and \"orange\") will have higher cosine similarity scores.\n3. **Semantic Diversity**:\n    - The expanded list increases the diversity of semantic relationships and highlights the strength of embeddings in grouping similar concepts.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "word2vec.py",
    "outlinks": [
      "bert",
      "cosine_similarity",
      "word2vec",
      "transformer"
    ],
    "inlinks": []
  },
  {
    "category": "OTHER",
    "filename": "Wikipedia_API.py",
    "sha": "23921b4459bfa006c8e49bd97209a76aad1a2083",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/Wikipedia_API.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Wikipedia_API.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "wikipedia_api.py",
    "outlinks": [],
    "inlinks": [
      "api",
      "graphrag"
    ]
  },
  {
    "category": "OTHER",
    "filename": "transfer_learning.py",
    "sha": "90760b59a2876bedf753306114bbd2fc16b87908",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/OTHER/transfer_learning.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Neural_Network/transfer_learning.py\n\nFor deep learning, to do [[Transfer Learning]] we take out and replace a few end layers of the network. We can then train just the last layer of weights of a neural network. \n\nThe number of layers to remove and then added from pretrained depends on the similarity between tasks. Higher layers in networks are able to recognise higher detail components.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "transfer_learning.py",
    "outlinks": [
      "transfer_learning"
    ],
    "inlinks": [
      "transfer_learning"
    ]
  },
  {
    "category": "PAPER",
    "filename": "Attention Is All You Need",
    "sha": "4b80c655f25c7e89b466f150e5a0fbb8528a205a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/PAPER/Attention%20Is%20All%20You%20Need.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "attention_is_all_you_need",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "PAPER",
    "filename": "BERT Pretraining of Deep Bidirectional Transformers for Language Understanding",
    "sha": "1ebf4487dea3452b8af9c0e1aa248df08e49df8b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/PAPER/BERT%20Pretraining%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding",
    "outlinks": [],
    "inlinks": [
      "bert"
    ]
  },
  {
    "category": "CS",
    "filename": "Algorithms",
    "sha": "7676f84e25c161f243d99fcc4ed3f9984d781882",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Algorithms.md",
    "text": "### [[Recursive Algorithm]]\n\n### Backtracking\n\nBacktracking is a method for solving problems incrementally, by trying partial solutions and then abandoning them if they are not valid.\n\nExample: Graph coloring with the 4-color theorem.\n\n### Divide and Conquer\n\nDivide and conquer is a strategy that involves breaking a problem into smaller subproblems of the same type, solving these subproblems recursively, and then combining their solutions to solve the original problem.\n\nExample: Merge sort, where the array is split in half, and each smaller part is sorted.\n\nNote: Subproblems do not generally overlap.\n\n### Dynamic Programming\n\nDynamic programming is used for optimization problems and involves storing past results to find new results efficiently.\n\n- [[Memoization]]: This technique \"remembers\" past results to avoid redundant calculations.\n- It is characterized by overlapping substructure and overlapping subproblems.\n\nExamples: Fibonacci numbers, Towers of Hanoi, etc.\n\n### Greedy Algorithms\n\nA greedy algorithm builds up a solution piece by piece, always choosing the next piece that offers the most immediate benefit without regard for future consequences. The hope is that by choosing a local optimum at each step, a global optimum will be reached.\n\nExamples: Dijkstra's shortest path algorithm, Knapsack problem.\n\n### Branch and Bound\n\nBranch and bound algorithms are typically used for optimization problems and are similar to backtracking.\n\n- As the algorithm progresses, a tree of subproblems is formed, with the original problem as the \"root problem.\"\n- A method is used to construct upper and lower bounds for a given problem.\n- At each node, bounding methods are applied:\n    - If the bounds match, it is deemed a feasible solution for that subproblem.\n    - If the bounds do not match, the problem represented by that node is partitioned into two subproblems, which become child nodes.\n- The process continues, using the best known feasible solution to trim sections of the tree until all nodes have been solved or trimmed.\n\nExample: Traveling salesman problem.\n\n### Brute Force\n\nBrute force algorithms try all possible solutions until they find an optimized or satisfactory answer. Heuristics can be used to assist in this process.\n\n### Randomized Algorithms\n\nRandomized algorithms use random numbers to influence their behaviour.\n\nExample: [[K-means]] clustering initialization.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm"
    ],
    "normalized_filename": "algorithms",
    "outlinks": [
      "k-means",
      "memoization",
      "recursive_algorithm"
    ],
    "inlinks": [
      "checksum",
      "computer_science",
      "heap_data_structure",
      "kernel_machines",
      "machine_learning_algorithms",
      "one-hot_encoding",
      "symbolic_computation"
    ]
  },
  {
    "category": "CS",
    "filename": "BM25 (Best Match 25)",
    "sha": "11d69a247f82fdac7e4221c5e63a1aaa9a11510e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/BM25%20(Best%20Match%2025).md",
    "text": "BM25 (Best Match 25)\n\nBM25 is a **ranking function** widely used in search engines (e.g., Elasticsearch, Whoosh, Lucene). It’s an extension of **TF-IDF** with improvements for handling term frequency and document length.\n\n### Formula:\n\nFor a document \\$D\\$ and query \\$Q\\$, the BM25 score is:\n\n$$\n\\text{BM25}(D, Q) = \\sum_{t \\in Q} IDF(t) \\cdot \\frac{f(t,D) \\cdot (k_1 + 1)}{f(t,D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n$$\n\nWhere:\n\n* \\$f(t,D)\\$ = term frequency of term \\$t\\$ in document \\$D\\$\n* \\$|D|\\$ = length of document \\$D\\$ (number of words)\n* \\$\\text{avgdl}\\$ = average document length across all docs\n* \\$k\\_1\\$ (usually \\~1.2–2.0) controls **term frequency saturation** (diminishing returns after many repetitions).\n* \\$b\\$ (usually \\~0.75) controls **length normalization** (longer documents don’t get unfair advantage).\n* \\$IDF(t)\\$ = inverse document frequency:\n\n  $$\n  IDF(t) = \\log \\frac{N - n_t + 0.5}{n_t + 0.5}\n  $$\n\n  where \\$N\\$ is total number of docs, \\$n\\_t\\$ is number of docs containing \\$t\\$.\n\n### Intuition:\n\n* **Term Frequency**: More occurrences of a query term increase relevance, but with diminishing returns.\n* **Inverse Document Frequency**: Rare terms are more discriminative, so they’re weighted higher.\n* **Document Length Normalization**: A term in a shorter document is more meaningful than in a very long one.\n\n### Example (simplified):\n\nSuppose query = \"data science\".\n\n* Doc1: \"data science is growing fast\"\n* Doc2: \"big data in business and industry\"\n\nBM25 will:\n* Give high score to Doc1 since it contains both \"data\" and \"science\".\n* Score Doc2 lower because it lacks \"science\".\n* Adjust for length: if Doc2 were extremely long, the contribution of \"data\" would be dampened.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "bm25_(best_match_25)",
    "outlinks": [],
    "inlinks": [
      "search"
    ]
  },
  {
    "category": "CS",
    "filename": "Big O Notation",
    "sha": "4adc371b2a2ea3d192cab203d30c8d0e7c4f69f9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Big%20O%20Notation.md",
    "text": "Big-O Notation is an analysis of the algorithm using [Big – O asymptotic notation](https://www.geeksforgeeks.org/analysis-of-algorithms-set-3asymptotic-notations/).  \n\nMostly related to computing rather than storage\n\nDoing things not exponentially, such as copying the same data many times, will save lots of performance and money.\n\nWe can express algorithmic complexity using the big-O notation. For a problem of size N:\n-   A constant-time function/method is “order 1” : O(1)\n-   A linear-time function/method is “order N” : O(N)\n-   A quadratic-time function/method is “order N squared” : O(N^2)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "big_o_notation",
    "outlinks": [],
    "inlinks": [
      "mathematics"
    ]
  },
  {
    "category": "CS",
    "filename": "Computer Science",
    "sha": "bd6c374711459ed451d2f0079262bd2ce174b88f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Computer%20Science.md",
    "text": "[[Algorithms]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "computer_science",
    "outlinks": [
      "algorithms"
    ],
    "inlinks": [
      "data_science",
      "memory",
      "processes_vs_threads"
    ]
  },
  {
    "category": "CS",
    "filename": "Checksum",
    "sha": "87780dca06c216cb42566b5414c95c07ee463a12",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Checksum.md",
    "text": "A checksum is a value calculated from a data set that is used to verify the integrity of that data. It acts as a fingerprint for the data, allowing systems to detect errors or alterations that may occur during storage, processing, or transmission.\n\nWhen data is sent or stored, a checksum is generated based on the contents of the data. This checksum is then sent or stored alongside the data. Upon retrieval or receipt, the checksum is recalculated from the data and compared to the original checksum. If the two checksums match, it indicates that the data has remained unchanged and is likely intact. If they do not match, it suggests that the data may have been corrupted or tampered with.\n\nChecksums are commonly used in various applications, such as:\n\n- **File transfers**: To ensure that files are not corrupted during transfer.\n- **[[Data Storage]]**: To verify that data has not changed over time.\n- **Networking**: To check the integrity of packets sent over a network.\n### Example of a Checksum Calculation\n\n1. **Original Data**: Let's say we have the string \"Hello, World!\".\n   \n2. **Checksum Calculation**: A common method for calculating a checksum is to sum the ASCII values of each character in the string. \n\n   - ASCII values:\n     - H = 72\n     - e = 101\n     - l = 108\n     - l = 108\n     - o = 111\n     - , = 44\n     - (space) = 32\n     - W = 87\n     - o = 111\n     - r = 114\n     - l = 108\n     - d = 100\n     - ! = 33\n\n   - Sum of ASCII values:\n     $$ 72 + 101 + 108 + 108 + 111 + 44 + 32 + 87 + 111 + 114 + 108 + 100 + 33 =  1,  2,  0 $$\n\n   - Let's say we take the modulo 256 of the sum to get the checksum:\n     $$ 1,  2,  0 \\mod 256 =  1,  2,  0 $$\n\n3. **Sending Data**: The original data \"Hello, World!\" is sent along with the checksum value of 1, 2, 0.\n\n4. **Receiving Data**: Upon receiving the data, the receiver calculates the checksum again using the same method.\n\n5. **Verification**: If the calculated checksum matches the received checksum (1, 2, 0), the data is considered intact. If it does not match, it indicates that the data may have been corrupted during transmission.\n\nThis is a basic example, and in practice, checksums can be computed using more complex [[Algorithms]] (like CRC32, MD5, or SHA-1) to provide better error detection and [[Data Security]].",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "security"
    ],
    "normalized_filename": "checksum",
    "outlinks": [
      "data_security",
      "data_storage",
      "algorithms"
    ],
    "inlinks": [
      "data_integrity"
    ]
  },
  {
    "category": "CS",
    "filename": "Concurrency",
    "sha": "c6fab07cc012f4fb5f158ffcad10ee1a52df2e2c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Concurrency.md",
    "text": "> Concurrency is about *dealing with many things at once*, not necessarily *doing* them at the same time.\n\nConcurrency refers to the ability of a system to handle multiple tasks at once, either by:\n* Overlapping their execution (e.g. while one task waits for I/O, another runs), or\n* Structuring code to allow interleaved progress of multiple operations.\n\nIt does not necessarily mean tasks run simultaneously (that’s parallelism), but that their progress is interleaved.\n\nAnalogy:\n- Concurrent: Making dinner, and doing the dishes at the same time (yourself).\n- Parallel: Making dinner (you), doing the dishes (partner).\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "concurrency",
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "database_techniques",
      "java_vs_javascript",
      "multiprocessing_vs_multithreading",
      "processes_vs_threads",
      "sqlite",
      "transaction"
    ]
  },
  {
    "category": "CS",
    "filename": "Directed Acyclic Graph (DAG)",
    "sha": "02d55e2cb8ddc39372634ccf5c964df852b7ce34",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Directed%20Acyclic%20Graph%20(DAG).md",
    "text": "DAG stands for **Directed Acyclic Graph**. \n\nA DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point.\n\nIt's a popular way of building data pipelines in tools like [[Apache Airflow]], [[dagster]], [[Prefect]]. It clearly defines the data lineage. As well, it's made for a functional approach where you have the [idempotency](term/idempotency.md) to restart pipelines without side-effects.\n\n![](dag.png)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "orchestration"
    ],
    "normalized_filename": "directed_acyclic_graph_(dag)",
    "outlinks": [
      "prefect",
      "apache_airflow",
      "dagster"
    ],
    "inlinks": [
      "apache_airflow",
      "mathematics"
    ]
  },
  {
    "category": "CS",
    "filename": "Flask",
    "sha": "40055db60cb812a393726b08cca6c50d9c3192e3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Flask.md",
    "text": "[[frontend]]\nweb app framework for writing web pages\n\nuses decorators\n\n![[Pasted image 20240922202938.png]]\n\n# [[Flask]]\n## Flask app example\n\nhttps://www.youtube.com/watch?v=wBCEDCiQh3Q&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl\n\nYou can run a flask app in google colab and then share it publicly with ngrok.\n\nflask app saved on github.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "normalized_filename": "flask",
    "outlinks": [
      "flask",
      "frontend",
      "pasted_image_20240922202938.png"
    ],
    "inlinks": [
      "flask",
      "jinja_template",
      "model_deployment"
    ]
  },
  {
    "category": "CS",
    "filename": "Convex Optimisation",
    "sha": "c664c4ff0ca3d0032c49c344137f5d46710a3c06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Convex%20Optimisation.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "convex_optimisation",
    "outlinks": [],
    "inlinks": [
      "model_optimisation"
    ]
  },
  {
    "category": "CS",
    "filename": "Generators in Python",
    "sha": "848ea76c43fab2dad7c302ad61e8e4cdf940b6b0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Generators%20in%20Python.md",
    "text": "#### Why use generators for processing large data? \n\nGenerators are [[Python]] constructs that yield values lazily, which means they don’t load everything into [[LLM Memory]]. This is ideal for [[Data Streaming]] large datasets, such as:\n\n* Reading files line by line\n* Processing massive CSV or JSONL logs\n* Streaming from [[API]]s or sockets\n* Working with [[NLP]] corpora or web-scraped text\n\nBenefits:\n* Memory efficiency\n* Composability of operations (via generator chains)\n* Useful in I/O-heavy or parallelized workflows\n\nExample:\n```python\ndef read_lines(filepath):\n    with open(filepath, 'r') as f:\n        for line in f:\n            yield line.strip()\n\nfor line in read_lines(\"large_file.txt\"):\n    process(line)  # custom processing function\n```\n\nExploratory Questions:\n* Can we apply the same strategy for batched ML inference pipelines?\n* When should we materialize a generator vs. keep it lazy?\n* What limitations arise when [[Debugging]] generators?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "python"
    ],
    "normalized_filename": "generators_in_python",
    "outlinks": [
      "llm_memory",
      "debugging",
      "api",
      "python",
      "nlp",
      "data_streaming"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Heap Data Structure",
    "sha": "07357b7a1e2a6b30e37e4bf312ffd51f5787aa53",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Heap%20Data%20Structure.md",
    "text": "In [[Algorithms]], a heap is a specialized [[binary tree]]-based data structure that satisfies the heap property: \n\n==each parent node is either greater than or equal to (max-heap) or less than or equal to (min-heap) its children.==\n\nLocally ordered.\n## Key Characteristics\n\n| Feature        | Description                                                                     |\n| -------------- | ------------------------------------------------------------------------------- |\n| Structure  | Typically implemented as a binary heap stored in an array.              |\n| Use Cases  | Priority queues, heapsort, scheduling, Dijkstra’s and Prim’s algorithms.        |\n| Efficiency | `insert()` and `extract_min()` / `extract_max()` run in $O(\\log n)$ time. |\n\n## Why Use a Heap?\n\n### Efficient Priority Queue\n\n* Provides constant-time access to the highest- or lowest-priority element.\n* Suitable when priorities change over time or are not known up front.\n* Use cases: task schedulers, event-driven simulation, packet routing.\n\n### Repeated Min/Max Extraction\n\n* Ideal for problems that require frequent ==retrieval and removal of extremal elements while maintaining partial order==.\n* Time complexity:\n  * `insert()`: $O(\\log n)$\n  * `extract_min()` / `extract_max()`: $O(\\log n)$\n  * `peek_min()` / `peek_max()`: $O(1)$\n\n## When to Use a Heap\n\nUse a heap when your problem involves:\n* Maintaining a dynamic priority queue.\n* Fast access to the smallest or largest element.\n* Efficient partial ordering without full sorting.\n* Managing a sliding or streaming window of values (e.g. top-k, median).\n\n## Python Example (Min-Heap)\n\n```python\nimport heapq\n\nheap = []\nheapq.heappush(heap, 3)\nheapq.heappush(heap, 1)\nheapq.heappush(heap, 2)\n\nprint(heapq.heappop(heap))  # Output: 1\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure"
    ],
    "normalized_filename": "heap_data_structure",
    "outlinks": [
      "binary_tree",
      "algorithms"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Heap Memory",
    "sha": "26e804e1b96470d5b2796498d27cc59c39910a26",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Heap%20Memory.md",
    "text": "In systems and programming, the heap is a region of a program’s memory used for ==dynamic memory allocation.==\n\nCharacteristics:\n\n| Feature     | Description                                                                                         |\n| ----------- | --------------------------------------------------------------------------------------------------- |\n| Purpose     | Stores variables that are created at runtime and need to persist beyond the current function scope. |\n| Managed By  | The programmer or runtime environment (e.g., [[garbage collector]] in Python or Java).              |\n| Lifetime    | Data persists until manually deallocated (C/C++) or garbage-collected (Python, Java).               |\n| Access      | Slower than [[stack memory]] due to indirect referencing and fragmentation.                         |\n| Example Use | Objects, data structures (e.g., lists, trees), large arrays.                                        |\n\nIn Python:\n\n```python\na = [1, 2, 3]  # List stored on the heap, reference stored in variable `a`\n```\n\nThe actual list is allocated on the heap; `a` holds a reference to that memory.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "memory_management"
    ],
    "normalized_filename": "heap_memory",
    "outlinks": [
      "stack_memory",
      "garbage_collector"
    ],
    "inlinks": [
      "processes_vs_threads"
    ]
  },
  {
    "category": "CS",
    "filename": "Hash",
    "sha": "2686b5516df586d60b2cd837a3a75132bc11f597",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Hash.md",
    "text": "A hash is a fixed-size string of characters that is generated from input data of any size using a hash function. \n\nHashes are used to ensure [[Data Integrity]] by providing a unique representation of the data, making it easy to detect any changes or alterations.\n\n### Key Characteristics of Hashes:\n\n1. **Deterministic**: The same input will always produce the same hash output.\n2. **Fixed Size**: Regardless of the size of the input data, the output hash will always be of a fixed length (e.g., SHA-256 produces a 256-bit hash).\n3. **Fast Computation**: Hash functions are designed to compute the hash value quickly.\n4. **Pre-image Resistance**: It should be computationally infeasible to reverse-engineer the original input from its hash.\n5. **Collision Resistance**: It should be difficult to find two different inputs that produce the same hash output.\n\n### How Hashes are Used in Data Integrity:\n\n6. **Data Verification**: When data is stored or transmitted, a hash of the data is generated and stored or sent along with it. When the data is later accessed, the hash is recalculated and compared to the original hash. If they match, the data is considered intact; if not, it indicates potential corruption or tampering.\n\n7. **Digital Signatures**: Hashes are often used in digital signatures to ensure the authenticity and integrity of a message or document.\n\n8. **Password Storage**: Instead of storing passwords in plain text, systems often store the hash of the password. When a user logs in, the system hashes the entered password and compares it to the stored hash.\n\n### Example of Hashing:\n\nFor example, if we take the string \"Hello, World!\" and apply a hash function like SHA-256, it will produce a unique hash value:\n\n- Input: \"Hello, World!\"\n- Hash (SHA-256): `a591a6d40bf420404a011733cfb7b190d62c65bf0bcda190f4b6c3f0f3c3b8a`\n\nIf the input data changes even slightly (e.g., \"Hello, World\"), the hash will be completely different, making it easy to detect any alterations.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure"
    ],
    "normalized_filename": "hash",
    "outlinks": [
      "data_integrity"
    ],
    "inlinks": [
      "cryptography",
      "data_integrity"
    ]
  },
  {
    "category": "CS",
    "filename": "How to search within a graph",
    "sha": "afc9d4bc894d6579c9b9870d0364877797886557",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/How%20to%20search%20within%20a%20graph.md",
    "text": "### Vector Search with Graph Context\n\n[[Vector Embedding]] plays a crucial role in enhancing search capabilities:\n\nComparison of Vector-Only vs. Graph-RAG: \n  - Vector-only searches may lack context, while Graph-RAG utilizes graph traversal to provide richer, multi-step context.\n  - This leads to more complex and informative responses.\n\nContextual Prompts: \n  - Context is used to answer prompts (in JSON format). With graph traversal, this context involves more steps, allowing for more elaborate retrieval queries.\n\n[[Text2Cypher]]\n\n[[How to search within a graph]]\n#### Node Embedding\n\nUseful in [[GraphRAG]] is understanding the relationships of nodes in a [[Knowledge Graph]] using node embeddings.\n\n![[Pasted image 20241004074458.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "querying"
    ],
    "normalized_filename": "how_to_search_within_a_graph",
    "outlinks": [
      "vector_embedding",
      "text2cypher",
      "how_to_search_within_a_graph",
      "knowledge_graph",
      "pasted_image_20241004074458.png",
      "graphrag"
    ],
    "inlinks": [
      "graphrag",
      "how_to_search_within_a_graph",
      "vector_embedding"
    ]
  },
  {
    "category": "CS",
    "filename": "Immutable vs mutable",
    "sha": "c05bff23b865ea8be071a9981d3109b3e9fb5fe5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Immutable%20vs%20mutable.md",
    "text": "[[Python]]\n\nlist being mutable\n\nSide effect\n```python\ndef get_largest_numbers(numbers, n):\nnumbers. sort()\n\nreturn numbers[-n:]\n\nnums [2, 3, 4, 1,34, 123, 321, 1]\n\nprint(nums)\nlargest = get_largest_numbers(nums, 2)\nprint(nums)\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "python"
    ],
    "normalized_filename": "immutable_vs_mutable",
    "outlinks": [
      "python"
    ],
    "inlinks": [
      "python"
    ]
  },
  {
    "category": "CS",
    "filename": "Java vs JavaScript",
    "sha": "c17c0c3109451873c3c1bafbc27b36d7789d36b1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Java%20vs%20JavaScript.md",
    "text": "### Difference Between [[Java]] and [[JavaScript]]\n\nAlthough their names are similar, Java and JavaScript are fundamentally different languages designed for different purposes. Below is a comparison between the two:\n\n| Aspect                  | Java                                                                   | JavaScript                                                                              |\n| ----------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n| Type                    | Object-Oriented Programming Language                                   | Scripting Language                                                                      |\n| Use                     | General-purpose, used for desktop, mobile, and enterprise applications | Primarily used for web development (front-end and back-end)                             |\n| Execution               | Runs on the Java Virtual Machine (JVM)                                 | Runs in the browser or on server-side (Node.js)                                         |\n| Compiled or Interpreted | Compiled to bytecode, then executed by the JVM                         | Interpreted directly by the browser or Node.js [[Node.JS]]                              |\n| Syntax                  | Strongly typed; requires defining data types                           | Loosely typed; variables can change types                                               |\n| [[Concurrency]]         | Supports [[Multithreading]]                                            | Single-threaded, but supports asynchronous programming (e.g., with callbacks, promises) |\n| Platform Dependency     | Platform-independent (write once, run anywhere)                        | Platform-independent, mainly within the context of the web                              |\n| Main Use Case           | Enterprise applications, Android development, large systems            | Dynamic web pages, front-end and server-side scripting for web applications             |\n| Libraries/Frameworks    | Spring, Hibernate, JavaFX, Android SDK                                 | React, Angular, Vue.js (front-end), Node.js (back-end)                                  |\n| Syntax Example          | `System.out.println(\"Hello, World!\");`                                 | `console.log(\"Hello, World!\");`                                                         |\n\n### Key Points:\n- [[Java]] is used for building large-scale applications, including desktop apps and Android apps. It is strongly typed, compiled, and can handle multithreading.\n- [[JavaScript]] is mainly used for web development, both for the front-end (managing user interfaces) and back-end (using Node.js), and is more flexible with dynamic typing and asynchronous behavior.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "java_vs_javascript",
    "outlinks": [
      "javascript",
      "concurrency",
      "multithreading",
      "node.js",
      "java"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Java",
    "sha": "98f28c84d2a78ebd7e7f45a0ca7dc3e473ed7867",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Java.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "programming"
    ],
    "normalized_filename": "java",
    "outlinks": [],
    "inlinks": [
      "java_vs_javascript",
      "programming_languages",
      "scala",
      "strongly_vs_weakly_typed_language"
    ]
  },
  {
    "category": "CS",
    "filename": "JavaScript",
    "sha": "98f28c84d2a78ebd7e7f45a0ca7dc3e473ed7867",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/JavaScript.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "programming"
    ],
    "normalized_filename": "javascript",
    "outlinks": [],
    "inlinks": [
      "cryptography",
      "java_vs_javascript",
      "programming_languages",
      "quartz",
      "react",
      "strongly_vs_weakly_typed_language"
    ]
  },
  {
    "category": "CS",
    "filename": "Knowledge Graph",
    "sha": "886736896aa7df3ab1ba08be3dfbd4bd86cbdf9b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Knowledge%20Graph.md",
    "text": "Knowledge graphs (KGs) enable large language models (LLMs) to generate more accurate, trustworthy AI outputs. Neo4j is leader in this space and make use of KG through  such as [[Generative AI]] techniques like [[GraphRAG]]:\n \n - Knowledge graphs are critical for managing complex data relationships and making strategic AI-driven decisions.  \n - The combination of KGs and LLMs improves AI accuracy, diversity of viewpoints, and [[explainability]].  \n\n\n A knowledge graph is a structured representation of knowledge that captures entities (e.g., people, places, concepts) and the relationships between them. Knowledge graphs are often used to represent and store factual information in a way that machines can easily query and understand. They use a graph structure where: Nodes represent entities (like \"Company A\" or \"Person B\"). Edges represent relationships between those entities (like \"works at\" or \"founded\").\n\n![[Pasted image 20240921154214.png|600]]\n\nKGs act as a control for Large [[Language Models]] (LLMs) by enabling knowledge-based reasoning based on the connections in the data \n \nNo significant limitations or concerns were highlighted, but the implementation of KGs may require technical expertise and resources.  \n\nHow does the integration of knowledge graphs and LLMs improve explainability in AI-generated responses?\n\nhttps://neo4j.com/blog/genai-knowledge-graph-deep-understanding/\n\n## Key characteristics of knowledge graphs:\n- Structured data: Information is represented in a highly structured form (triples: subject, predicate, object) that allows efficient querying and reasoning.\n- [[Semantic Relationships]]: Entities are connected by meaningful relationships, often using ontologies and taxonomies to organize the knowledge.\n- Reasoning and inference: Some knowledge graphs can support reasoning capabilities, where new information can be inferred based on the existing relationships and rules (e.g., if \"Person A works at Company B,\" and \"Company B is in Industry C,\" it can infer that \"Person A works in Industry C\").\n\n## Example of a Knowledge Graph: \n- Nodes: `Barack Obama`, `United States`, `President`.\n- Edges: `Barack Obama -- President of -- United States`.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "NLP"
    ],
    "normalized_filename": "knowledge_graph",
    "outlinks": [
      "semantic_relationships",
      "generative_ai",
      "explainability",
      "pasted_image_20240921154214.png",
      "language_models",
      "graphrag"
    ],
    "inlinks": [
      "assessing_gen_ai_generated_content",
      "graphrag",
      "how_to_search_within_a_graph",
      "knowledge_graph_vs_rag_setup"
    ]
  },
  {
    "category": "CS",
    "filename": "Langchain",
    "sha": "625418e51cf42ed143840aa4cb43a589d587bf82",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Langchain.md",
    "text": "LangChain is a [[Python]] framework designed to facilitate the development of applications powered by [[LLM]]s (Large [[Language Models]]). It enables developers to build end-to-end, composable workflows that involve LLM interaction, integration with data sources, memory, tools, and agentic decision-making.\n\nPurpose: LangChain abstracts and orchestrates the logic needed to:\n* Prompt LLMs effectively\n* ==Chain together multiple LLM calls== or components\n* Maintain [[LLM Memory]] of interactions\n* Interact with external tools and APIs\n* Build autonomous or semi-autonomous [[Agentic Solutions]]\n* Enable retrieval and indexing over documents\n\nCore Modules:\n\n* [[Models]]: Interface layer to various LLM providers (e.g., OpenAI, Cohere).\n* [[Prompts]]: Templates and logic for prompt creation and formatting.\n* [[Chains]]: Composable sequences of LLM calls, enabling structured workflows.\n* [[LLM Memory]]: Mechanisms to persist context across interactions.\n* Indexes: Tools for embedding and retrieving documents using vector stores.\n* [[Agents and Tools]]: Constructs for dynamic decision-making, allowing the LLM to choose tools (e.g., calculator, web search) at runtime.\n\nExample Application:\n\n* [[Pandas Dataframe Agent]]: An agent that enables natural language interaction with tabular data using [[Pandas]] and LLM reasoning.\n\nUse Cases:\n\n* Chatbot that remembers prior inputs within a session\n* [[Querying]] a CSV file with natural language\n* Summarizing notes or documents interactively\n* A lightweight research assistant that chooses between tools to answer a question",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "python"
    ],
    "normalized_filename": "langchain",
    "outlinks": [
      "llm_memory",
      "agents_and_tools",
      "llm",
      "prompts",
      "querying",
      "chains",
      "models",
      "agentic_solutions",
      "pandas_dataframe_agent",
      "python",
      "language_models",
      "pandas"
    ],
    "inlinks": [
      "vector_database"
    ]
  },
  {
    "category": "CS",
    "filename": "Machine Learning Algorithms",
    "sha": "b9cb89195ea66d4bcbf91254bde9377ef0e4aad5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Machine%20Learning%20Algorithms.md",
    "text": "Machine learning [[Algorithms]] are used to automate tasks, extract insights, and make more informed decisions.\n\nChoosing the right algorithm for a specific problem involves understanding the task, the characteristics of the data, and the strengths and limitations of different algorithms.\n# [[Supervised Learning]]\n\nCommon [[Classification]] algorithms include:\n\n- [[Logistic Regression]]\n- [[Support Vector Machines]]\n- [[Naive Bayes Classifier]]\n- [[Decision Tree]]\n- [[Random Forest]]\n\nCommon [[Regression]] algorithms include:\n- [[Linear Regression]]\n- [[Support Vector Regression]]\n- [[Random Forest Regression]]\n\n# [[Unsupervised Learning]]\n\nCommon [[Clustering]] algorithms include:\n\n- [[K-means]]\n- [[Gaussian Mixture Models]]\n- [[Clustering]]\n- [[Dimensionality Reduction]]\n\nCommon [[Dimensionality Reduction]] algorithms include:\n\n- [[Principal Component Analysis]]\n- [[Manifold Learning]]\n## Strengths and Limitations of Machine Learning Algorithms\n\n##### Strengths:\n\nAutomation: Machine learning algorithms can automate complex tasks, freeing up human resources for other activities.\n\nAdaptability: Machine learning algorithms can adapt to changing data patterns, making them suitable for dynamic environments.\n\n[[Scalability]]: Machine learning algorithms can handle large datasets efficiently, making them applicable to big data problems.\n\nKnowledge Discovery: Machine learning algorithms can help discover hidden patterns and relationships in data, leading to new insights and knowledge.\n\n##### Limitations:\n\nData Dependence: The performance of machine learning algorithms heavily depends on the [[Data Quality]] and quantity of the training data.\n\n[[Overfitting]] occurs when the model learns the training data too well and fails to generalise to new, unseen data.\n\n[[Bias in ML]]: Machine learning algorithms can be biased, reflecting the biases present in the training data.\n\n[[Interpretability]]: Some machine learning algorithms, especially deep learning models, can be complex and difficult to interpret, making it challenging to understand the reasoning behind their predictions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "modeling"
    ],
    "normalized_filename": "machine_learning_algorithms",
    "outlinks": [
      "linear_regression",
      "principal_component_analysis",
      "dimensionality_reduction",
      "clustering",
      "manifold_learning",
      "gaussian_mixture_models",
      "support_vector_machines",
      "overfitting",
      "interpretability",
      "random_forest_regression",
      "classification",
      "regression",
      "logistic_regression",
      "support_vector_regression",
      "k-means",
      "decision_tree",
      "data_quality",
      "supervised_learning",
      "unsupervised_learning",
      "naive_bayes_classifier",
      "scalability",
      "random_forest",
      "algorithms",
      "bias_in_ml"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "gradient_boosting",
      "machine_learning",
      "model_building",
      "one-hot_encoding",
      "pandas_stack",
      "supervised_learning",
      "use_of_rnns_in_energy_sector",
      "z-normalisation"
    ]
  },
  {
    "category": "CS",
    "filename": "Monte Carlo Simulation",
    "sha": "a63e45804fdb13837887f21f28dff662ca525c85",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Monte%20Carlo%20Simulation.md",
    "text": "Resources:\n- https://www.youtube.com/watch?v=r7cn3WS5x9c\n\nAlgorithms that use repeated random sampling.\n\nMonte Carlo: random\n\nHow does the randomness in data generation impact the randomness of the paramaeter calculation.\n\nSimulation study:\n1) FIGURE OUT A WAY TO APPROXIMATE A PROCESS WITH A RANDOM NUMBER GENERATOR\n2) GENERATE THE DATA AND CALCULATE A VALUE OF INTEREST(I.E.MEAN, BIAS, COVERAGE)\n3) REPEAT STEPS 1& 2 MANY TIMES TO LEARN ABOUT THE UNCERTAINTY IN THIS VALUE\n\nSimulation studies:",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "statistics"
    ],
    "normalized_filename": "monte_carlo_simulation",
    "outlinks": [],
    "inlinks": [
      "impact_of_multicollinearity_on_model_parameters",
      "statistics"
    ]
  },
  {
    "category": "CS",
    "filename": "Multiprocessing vs Multithreading",
    "sha": "2915473f59ed14f28a7ecb6876b59149b0375211",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Multiprocessing%20vs%20Multithreading.md",
    "text": "The difference between [[Multiprocessing]] and multithreading lies primarily in how [[Concurrency]] is achieved, the resources used, and Python-specific constraints like the Global Interpreter Lock (GIL).\n\nRelated:\n- [[Global Interpreter Lock]]\n\n| Term                | Description                                                                     |\n| ------------------- | ------------------------------------------------------------------------------- |\n| [[Multiprocessing]] | Uses multiple processes, each with its own Python interpreter and memory space. |\n| [[Multithreading]]  | Uses multiple threads within a single process, sharing the same memory space.   |\n\nKey Differences:\n\n| Aspect              | Multiprocessing                                                  | Multithreading                                                              |\n| ------------------- | -------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n| Memory Space    | Each process has its own memory.                                 | Threads share the same memory space.                                        |\n| CPU Utilization | Can use multiple CPU cores fully.                                | Limited by GIL in CPython (only one thread runs Python bytecode at a time). |\n| Overhead        | Higher: more memory, process startup time.                           | Lower: threads are lightweight to spawn.                                        |\n| Data Sharing    | Requires IPC (Inter-Process Communication) like Queues or Pipes. | Shared memory makes data sharing easier but riskier.                            |\n| Best For        | CPU-bound tasks (e.g. image processing, computation).            | I/O-bound tasks (e.g. file/network access).                                 |\n| Crash Safety    | One process crash doesn't affect others.                         | A crash in one thread can crash the whole process.                              |\n\nUse Case Summary:\n\n| Use Case                                                             | Recommendation            |\n| -------------------------------------------------------------------- | ------------------------- |\n| ==CPU-intensive== (e.g. matrix multiplication, image classification) | Multiprocessing           |\n| ==I/O-intensive== (e.g. downloading files, database queries)         | Multithreading            |\n| Parallelizing EasyOCR on multiple images                             | Multiprocessing           |\n| Handling many simultaneous HTTP requests                             | Multithreading or AsyncIO |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "programming"
    ],
    "normalized_filename": "multiprocessing_vs_multithreading",
    "outlinks": [
      "global_interpreter_lock",
      "multithreading",
      "multiprocessing",
      "concurrency"
    ],
    "inlinks": [
      "multiprocessing"
    ]
  },
  {
    "category": "CS",
    "filename": "Multithreading",
    "sha": "bd6b76592de967e52c54036e838bf5c2e995b34f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Multithreading.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "programming",
      "system"
    ],
    "normalized_filename": "multithreading",
    "outlinks": [],
    "inlinks": [
      "global_interpreter_lock",
      "java_vs_javascript",
      "multiprocessing",
      "multiprocessing_vs_multithreading"
    ]
  },
  {
    "category": "CS",
    "filename": "Node.JS",
    "sha": "98f28c84d2a78ebd7e7f45a0ca7dc3e473ed7867",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Node.JS.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "programming"
    ],
    "normalized_filename": "node.js",
    "outlinks": [],
    "inlinks": [
      "aws_lambda",
      "cryptography",
      "html",
      "java_vs_javascript"
    ]
  },
  {
    "category": "CS",
    "filename": "Numpy",
    "sha": "5255cfa1c58497c264978ec8e3c0e328f0b931ef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Numpy.md",
    "text": "@ is equivalent to np.matmul() or np.dot() (for 2D arrays)\nUsed to perform linear algebra-style matrix multiplication\nCheck shapes: $(m \\times n) @ (n \\times p) \\Rightarrow (m \\times p)$",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "python"
    ],
    "normalized_filename": "numpy",
    "outlinks": [],
    "inlinks": [
      "pytorch",
      "vectorisation"
    ]
  },
  {
    "category": "CS",
    "filename": "Processes vs Threads",
    "sha": "8e29e892505ae6d334708c1e4ffcddca1300dba9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Processes%20vs%20Threads.md",
    "text": "The distinction between a process and a thread is foundational in [[Computer Science]] and systems programming. \n\n==Each process has one or more threads, but threads cannot exist outside a process.==\n\n| Term        | Description                                                                                                                |\n| ----------- | -------------------------------------------------------------------------------------------------------------------------- |\n| Process | An instance of a program in execution, with its own memory space, system resources, and execution context. |\n| Thread  | A unit of execution within a process. All threads in a process share the same memory and resources.                |\nAnalogy:\n* A process is like a house: self-contained, isolated, with its own address, rooms, and locks.\n* A thread is like a person inside the house: they share the same environment, can talk freely (shared memory), but can interfere with each other.\n\nIn Practice\n* Use processes when you want full isolation and need to bypass the GIL for CPU-bound tasks.\n* Use threads when tasks are I/O-bound and sharing memory is beneficial.\n## Core Differences\n\n| Aspect            | Process                                                                         | Thread                                                                          |\n| ----------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n| [[Memory]]        | Each process has separate memory (heap, stack, etc.).                           | Threads share process memory (global [[Heap Memory]]) but have separate stacks.        |\n| Isolation         | Fully isolated from other processes.                                            | Not isolated: changes in memory affect all threads.                             |\n| Communication     | Requires inter-process communication (IPC) mechanisms (pipes, queues, sockets). | Communication is simpler via shared memory, but prone to [[Race Conditions]].   |\n| Crash Impact      | Crash in one process typically does not affect others.                          | Crash in one thread may take down the whole process.                            |\n| Creation Overhead | Higher: requires setting up memory space, resources.                            | Lower: threads are more lightweight.                                            |\n| Context Switching | More costly: involves ==switching between separate memory contexts.==           | Faster: threads switch within the ==same process context.==                     |\n| Typical Use       | For [[parallelism]], running independent tasks.                                 | For [[Concurrency]], especially when tasks ==need to share data or resources.== |\n\n\n```python\nfrom multiprocessing import Process\nfrom threading import Thread\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "processes_vs_threads",
    "outlinks": [
      "race_conditions",
      "parallelism",
      "computer_science",
      "concurrency",
      "memory",
      "heap_memory"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "PyGraphviz",
    "sha": "f53cf48ad5bcdd42bd25ce3384483182d059fa6e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/PyGraphviz.md",
    "text": "PyGraphviz\nInterface: Thin wrapper around the C Graphviz API.\nBetter integration with NetworkX, especially with graphviz_layout.\n\nAdvantages:\n\nNative Graphviz object model (AGraph).\n\nSeamless conversion between NetworkX graphs and Graphviz objects.\n\nSupports advanced Graphviz features and layout options.\n\nLimitations:\n\nRequires Graphviz development libraries to be installed (can be hard to set up on Windows).\n\nSlightly more complex installation due to C bindings.\n\nExample with NetworkX:\n\npython\nfrom networkx.drawing.nx_agraph import graphviz_layout\npos = graphviz_layout(G, prog=\"dot\")",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "graph"
    ],
    "normalized_filename": "pygraphviz",
    "outlinks": [],
    "inlinks": [
      "graph_theory"
    ]
  },
  {
    "category": "CS",
    "filename": "Ranking models",
    "sha": "02ec511449759d79b58801c9350a9d443621e811",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Ranking%20models.md",
    "text": "Name query ranking models:\n- PageRank\n- TrustRank\n- BrowseRank\n- 3A Ranking\n- SimRank",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "ranking_models",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "QuickSort",
    "sha": "251046961f46aa188b38beafdc367d6f2408cf5c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/QuickSort.md",
    "text": "## [[Recursive Algorithm]]\n\n[Quicksort Algorithm in Five Lines of Code! - Computerphile](https://www.youtube.com/watch?v=OKc2hAmMOY4)\n\nFast algorithm (compared to say Insertion Sort)\n\n1) Pick pivot value\n2) Divide remaining numbers into two parts\n3) ==sort sublists in some way== <- apply alog again\n4) merge\n\nRecursion stops when nothing to pick for pivot value.\n\n\n```python\ndef quick_sort(arr, depth=0):\n    indent = \"  \" * depth  # Indentation for better readability in recursion\n    print(f\"{indent}Sorting: {arr}\")\n    \n    if len(arr) <= 1:\n        print(f\"{indent}Returning sorted: {arr}\")\n        return arr  # Base case: already sorted\n\n    pivot = arr[len(arr) // 2]  # Choosing pivot (middle element)\n    left = [x for x in arr if x < pivot]  # Elements smaller than pivot\n    middle = [x for x in arr if x == pivot]  # Elements equal to pivot\n    right = [x for x in arr if x > pivot]  # Elements greater than pivot\n    \n    print(f\"{indent}Pivot: {pivot}\")\n    print(f\"{indent}Left: {left}\")\n    print(f\"{indent}Middle: {middle}\")\n    print(f\"{indent}Right: {right}\")\n    \n    sorted_left = quick_sort(left, depth + 1)\n    sorted_right = quick_sort(right, depth + 1)\n    \n    sorted_array = sorted_left + middle + sorted_right\n    print(f\"{indent}Merged: {sorted_array}\")\n    \n    return sorted_array  # Recursively sort and merge\n\n# Example usage:\narr = [10, 7, 8, 9, 1, 5]\nprint(\"Starting QuickSort...\\n\")\nsorted_arr = quick_sort(arr)\nprint(\"\\nFinal sorted array:\", sorted_arr)\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm"
    ],
    "normalized_filename": "quicksort",
    "outlinks": [
      "recursive_algorithm"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Recursive Algorithm",
    "sha": "a6eda42759d6c2b39f8f45d33c4121e09daa39fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Recursive%20Algorithm.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm"
    ],
    "normalized_filename": "recursive_algorithm",
    "outlinks": [],
    "inlinks": [
      "algorithms",
      "common_table_expression",
      "quicksort"
    ]
  },
  {
    "category": "computer-science",
    "filename": "Science Portal",
    "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Science%20Portal.md",
    "text": "",
    "normalized_filename": "science_portal",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Times Series Python Packages",
    "sha": "269931a3eff8f9d773335bd11328ba7cd9248d24",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Times%20Series%20Python%20Packages.md",
    "text": "Darts",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "normalized_filename": "times_series_python_packages",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "Strongly vs Weakly typed language",
    "sha": "50e7fa5fb29a3a61116e31f50e701bdc7556815a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/Strongly%20vs%20Weakly%20typed%20language.md",
    "text": "A **strongly typed** programming language is one where ==types== are strictly enforced. This means that once a variable is assigned a type, it cannot be implicitly converted to another type without an explicit conversion. The goal is to ==minimize errors related to incorrect type handling,== as the compiler or interpreter will detect type mismatches early in the development process.\n\n### Characteristics of a Strongly Typed Language:\n1. **Type Enforcement**: The language does not allow operations between incompatible types (e.g., trying to add a string to an integer).\n2. **Explicit Conversions**: If you need to change the type of a variable, you must explicitly convert it (casting). The compiler or interpreter won't do it automatically.\n3. **Compile-time/Runtime Type Checking**: The language performs thorough checks either at compile time (for compiled languages) or at runtime (for interpreted languages) to ensure type safety.\n\n### Example: \n\nIn [[Java]], which is a strongly typed language:\n```java\nint number = 5;\nString text = \"Hello\";\n\n// The following line will result in an error since you cannot add an integer to a string directly:\ntext = text + number;  // Type mismatch error\n\n// You must explicitly convert the integer to a string to perform this operation:\ntext = text + Integer.toString(number);  // Correct\n```\n### Benefits:\n- **Error Prevention**: Type mismatches are caught early, reducing runtime errors.\n- **Code Clarity**: Since types are explicitly defined, it’s easier to understand what kind of data is being handled.\n- **Efficiency**: Some strongly typed languages can optimize code better due to the predictability of data types.\n\n### Contrast with Weakly Typed Languages:\n\nWeakly typed languages, like [[JavaScript]], allow implicit type conversions, leading to more flexibility but also potential runtime errors due to unexpected conversions:\n```javascript\nlet number = 5;\nlet text = \"Hello\";\n\n// JavaScript allows this, and it implicitly converts the number to a string:\ntext = text + number;  // No error, result is \"Hello5\"\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "strongly_vs_weakly_typed_language",
    "outlinks": [
      "javascript",
      "java"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "csv module",
    "sha": "df9e9e0ddaf0cdff3dac96b6226072329f7abba5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/csv%20module.md",
    "text": "[[Python]]\n\n| Feature                    | `csv` (standard lib)              | `pandas.to_csv()`                          |\n| -------------------------- | --------------------------------- | ------------------------------------------ |\n| Library                    | Built-in (`import csv`)           | External (`pandas`, requires install)      |\n| Input/Output               | Lists, dicts                      | `DataFrame`                                |\n| Header row                 | Manual (`writer.writerow([...])`) | Automatic from column names                |\n| Data formatting            | Manual                            | Handles types like datetime, NaNs, etc.    |\n| Performance (large data)   | Very fast for simple use cases    | Optimized for large tables                 |\n| Compression (gzip, bz2...) | Not supported natively            | Built-in support via `compression=`        |\n| Index export               | N/A                               | Optional (`index=False`)                   |\n| Multi-index export         | Not supported                     | Supported                                  |\n| Handling of nulls          | Manual                            | Automatic (e.g., NaN becomes empty string) |\n| Type preservation          | Manual                            | Preserves types (to the extent CSV allows) |\n| Append mode                | Easy (`open(..., 'a')`)           | Possible but requires care                 |\n\n## Example Comparison\n\n### 1. **Using `csv`**\n\n```python\nimport csv\n\ndata = [\n    ['name', 'age'],\n    ['Alice', 30],\n    ['Bob', 25]\n]\n\nwith open('people.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerows(data)\n```\n\nYou are responsible for formatting rows as lists (or dicts with `DictWriter`).\n\n### 2. **Using `pandas.to_csv()`**\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob'],\n    'age': [30, 25]\n})\n\ndf.to_csv('people.csv', index=False)\n```\n\nNo manual row handling — `pandas` handles headers, data alignment, and types.\n\n## When to Use Each\n\n### Use `csv` if:\n\n* You’re not using [[Pandas]].\n* You need lightweight scripts with zero dependencies.\n* You're working with custom file formats (e.g., pipe-delimited, legacy systems).\n* You're writing line-by-line or in memory-constrained environments.\n\n### Use `pandas.to_csv()` if:\n\n* You're already working with DataFrames.\n* You want fast, bulk export of structured tabular data.\n* You need features like compression, encoding control, or null handling.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "normalized_filename": "csv_module",
    "outlinks": [
      "pandas",
      "python"
    ],
    "inlinks": []
  },
  {
    "category": "CS",
    "filename": "garbage collector",
    "sha": "5ea6a173ffa5ae30db50849eaa804a8af3c1ef15",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/garbage%20collector.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "garbage_collector",
    "outlinks": [],
    "inlinks": [
      "heap_memory"
    ]
  },
  {
    "category": "CS",
    "filename": "neomodel",
    "sha": "60a8417183506cd1567ede42dc99e200e0893249",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/neomodel.md",
    "text": "**Neomodel** is a Python library that provides an Object-Graph Mapping (OGM) layer for [[neo4j]], a [[GraphRAG|graph database]]. It allows you to interact with a Neo4j graph database using Python objects, making it easier to work with the data stored in a graph structure without manually writing raw Cypher [[Querying]]. Neomodel simplifies the process of creating and manipulating nodes and relationships in a graph while providing an object-oriented approach.\n\n### Key Features:\n- **Object-Graph Mapping (OGM)**: Neomodel maps Python classes to Neo4j nodes and relationships, abstracting away the complexity of interacting directly with the database.\n- **[[Database Schema|Schema]]-based**: You define your data models using Python classes, and Neomodel automatically generates the corresponding graph structure in Neo4j.\n- **Easy to use**: Neomodel provides a simple and intuitive [[API]] for creating, querying, and updating graph data.\n- **Integrates with Neo4j**: Neomodel works seamlessly with the Neo4j graph database, allowing you to leverage its full capabilities while using Python.\n\n### Installation:\nTo install Neomodel, you can use `pip`:\n```bash\npip install neomodel\n```\n\n### Basic Example:\n\nHere’s an example of how you can define models, create nodes, and query the database using Neomodel:\n\n1. **Define Node Models**:\n   In Neomodel, you define models as Python classes. Each class represents a node in the graph.\n\n   ```python\n   from neomodel import StructuredNode, StringProperty, IntegerProperty, RelationshipTo\n\n   class Person(StructuredNode):\n       name = StringProperty()\n       age = IntegerProperty()\n       friends = RelationshipTo('Person', 'FRIEND')\n\n   # Define another node class, e.g., for a Company\n   class Company(StructuredNode):\n       name = StringProperty()\n       employees = RelationshipTo('Person', 'EMPLOYED_BY')\n   ```\n\n2. **Create and Save Nodes**:\n   You can create instances of these classes and save them to the Neo4j database.\n\n   ```python\n   # Create and save a person\n   john = Person(name=\"John\", age=30).save()\n\n   # Create another person and save\n   jane = Person(name=\"Jane\", age=25).save()\n\n   # Create a relationship\n   john.friends.connect(jane)\n   ```\n\n3. **Query the Graph**:\n   You can query the graph using the model’s methods.\n\n   ```python\n   # Find people named 'John'\n   johns = Person.nodes.filter(name='John')\n\n   for john in johns:\n       print(f\"{john.name}, {john.age} years old\")\n\n   # Get all friends of 'John'\n   for friend in john.friends:\n       print(f\"{friend.name} is a friend of John\")\n   ```\n\n### Key Concepts:\n- **StructuredNode**: This is the base class for defining nodes (entities) in your graph. You define properties using attributes (e.g., `StringProperty`, `IntegerProperty`).\n- **Relationships**: You can define relationships between nodes using `RelationshipTo` or `RelationshipFrom`, specifying the type of relationship (e.g., \"FRIEND\", \"EMPLOYED_BY\").\n- **Saving and Querying**: Nodes can be saved using the `.save()` method, and you can query nodes using `filter()`, `nodes`, or Cypher queries.\n\n### Advantages:\n- **Pythonic API**: The API is simple and follows Python conventions, making it easy to use for Python developers.\n- **Automatic Schema Handling**: Neomodel handles the schema for you by automatically creating constraints and indexes for unique node properties (e.g., unique constraints for primary keys).\n- **Integration with Neo4j**: It integrates tightly with Neo4j, making it easy to work with graph data without worrying about the underlying Cypher queries or database operations.\n\n### Use Case:\nNeomodel is especially useful when you need to interact with a Neo4j graph database while working within a Python application, providing a clean and simple interface to graph data without having to manually write raw Cypher queries. It’s ideal for applications that involve complex relationships, such as social networks, recommendation systems, fraud detection, and knowledge graphs.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "python"
    ],
    "normalized_filename": "neomodel",
    "outlinks": [
      "neo4j",
      "querying",
      "api",
      "database_schema",
      "graphrag"
    ],
    "inlinks": [
      "neo4j"
    ]
  },
  {
    "category": "CS",
    "filename": "programming languages",
    "sha": "185ac4dcbcee156b3cb13b78d74de1b0545a9d63",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/computer-science/programming%20languages.md",
    "text": "[[Python]]\n[[Java]]\n[[JavaScript]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "programming"
    ],
    "normalized_filename": "programming_languages",
    "outlinks": [
      "javascript",
      "python",
      "java"
    ],
    "inlinks": [
      "aws_lambda"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Altair",
    "sha": "850273e0e691a43c8eb74b51cadd92d4dbfd19f4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Altair.md",
    "text": "## What Is Altair?\n\nAltair provides a [[Declarative Data Pipeline]] interface where you specify *what* you want to visualize (e.g. variables, encodings, transformations), and the library handles *how* to render it.\n\nIt is powered by Vega-Lite, a JavaScript-based visualization grammar, and outputs charts as HTML/[[Json]] that can be embedded in Jupyter notebooks, web apps, and dashboards.\n\n### Core principles:\n\n* Declarative syntax: You describe *relationships* between data fields and visual properties (e.g. map `x` to time, `color` to category).\n* Interactive by default: Charts can support hover, zoom, selection, etc.\n* Data-aware: Built-in data transformations like aggregation, filtering, binning.\n* Consistent and safe: Prevents many bad visualization practices by design (e.g., overplotting, misaligned axes).\n\n## Why Use Altair?\n\n### 1. Readable and Concise Code\n\nAltair lets you build complex visuals with minimal syntax.\n\n```python\nimport altair as alt\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\nalt.Chart(df).mark_bar().encode(\n    x='category:N',\n    y='sales:Q',\n    color='region:N'\n)\n```\n\nYou don’t need to specify chart layout, axes, or legends — Altair infers them.\n\n### 2. Interactivity Built-in\n\nWith `.interactive()`, you can pan, zoom, and hover out-of-the-box — no extra code.\n\n```python\nalt.Chart(df).mark_circle().encode(\n    x='x_val', y='y_val'\n).interactive()\n```\n\nYou can also create complex interactivity like brushing, linking, or filtering across charts.\n\n### 3. Data Transformation Within the Chart\n\nAltair supports:\n\n* Filtering\n* Aggregating\n* Window functions\n* Binning\n* Calculated fields\n\nYou can express a lot of [[Preprocessing]] without leaving the plotting context.\n\n### 4. Best for:\n\n* Exploratory data analysis ([[EDA]])\n* Interactive data dashboards (e.g. with [[Streamlit]], Voila, or Panel)\n* Communicating insights with clean, web-ready charts\n\n## When Not to Use Altair\n\n* If you need fine-grained control over static plots for publication.\n* If you're working with large datasets (>5000 rows by default; though workarounds exist).\n* If you're deeply invested in Matplotlib or want full control over low-level rendering.\n\n## Example Use-Cases\n\n* [[Time Series]] dashboards: Interactively explore metrics across time.\n* Exploratory analysis: Quickly test variable relationships with scatter plots, box plots, etc.\n* Teaching/Presenting: Embed responsive charts in web slides or notebooks.\n* Data storytelling: Combine multiple linked views to guide a narrative.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "altair",
    "outlinks": [
      "declarative_data_pipeline",
      "preprocessing",
      "streamlit",
      "time_series",
      "eda",
      "json"
    ],
    "inlinks": [
      "altair_versus_seaborn"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Boxplot",
    "sha": "255904d0b4e89df0435ac2ae83bc5c4661797bef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Boxplot.md",
    "text": "A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the dataset.\n\nRelated:\n- [[Data Visualisation]]\n## Key Components\n\nUses:\n- Identifying [[uncategorised/Outliers]].\n- Understanding the spread and skewness of the data [[Distributions]].\n- Comparing distributions across different categories.\n- Need to remove then in order to do [[Data Cleansing]].\n\nComponents:\n- **Minimum:** The smallest data point excluding outliers.\n- **First Quartile (Q1):** The median of the lower half of the dataset.\n- **Median (Q2):** The middle value of the dataset.\n- **Third Quartile (Q3):** The median of the upper half of the dataset.\n- **Maximum:** The largest data point excluding outliers.\n- **Outliers:** Data points that fall outside 1.5 times the interquartile range (IQR) above Q3 or below Q1.\n- [[Interquartile Range (IQR) Detection]]\n## Implementation\n\n```python\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n\n# Create a boxplot\nplt.boxplot(data)\n\n# Add title and labels\nplt.title('Boxplot Example')\nplt.ylabel('Values')\n\n# Show plot\nplt.show()\n```\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n\n# Create a boxplot\nsns.boxplot(data=data)\n\n# Add title and labels\nplt.title('Boxplot Example')\nplt.ylabel('Values')\n\n# Show plot\nplt.show()\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#statistics",
      "cleaning",
      "visualization"
    ],
    "type": "term",
    "normalized_filename": "boxplot",
    "outlinks": [
      "data_visualisation",
      "interquartile_range_(iqr)_detection",
      "data_cleansing",
      "distributions",
      "uncategorised/outliers"
    ],
    "inlinks": [
      "anomaly_detection",
      "data_visualisation",
      "distributions",
      "variance",
      "violin_plot"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Binder",
    "sha": "95f431c2416f6b2e0fa02024b21bd9edda083711",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Binder.md",
    "text": "https://mybinder.org/\n\nSimilar to [[Google Colab]]. \n\n[[Data Visualisation]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "visualization"
    ],
    "normalized_filename": "binder",
    "outlinks": [
      "google_colab",
      "data_visualisation"
    ],
    "inlinks": [
      "data_visualisation"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Dash",
    "sha": "a9abf532a9880d2a2b77f07b4249fdf61085a8c2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Dash.md",
    "text": "Dash is an open-source framework for building interactive web applications using Python. \n\nIt is particularly well-suited for [[Data Visualisation]] and dashboard creation. \n\nDash integrates with popular libraries such as [[Plotly]], Pandas, and NumPy, making it ideal for creating dynamic and interactive visualizations.\n\nExamples: \n- https://dash.gallery/Portal/?search=[ML]\n- https://github.com/PacktPublishing/Interactive-Dashboards-and-Data-Apps-with-Plotly-and-Dash\n\nIn [[ML_Tools]] see [[Clustering_Dashboard.py]]\n\nKey Components of Dash\n1. Dash App: The main application instance, created using `dash.Dash(__name__)`.\n2. Dash HTML Components (`dash_html_components`): Provides wrappers for standard HTML elements (e.g., `html.Div`, `html.H1`).\n3. Dash Core Components (`dash_core_components`): Includes interactive UI components like graphs, dropdowns, sliders, and more (e.g., `dcc.Graph`, `dcc.Dropdown`).\n4. Callback Functions: Used to make components interactive by linking inputs (user actions) to outputs (changes in the UI).\n5. [[Plotly]] Integration: Dash apps leverage Plotly for creating interactive visualizations.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "frontend",
      "python",
      "visualization"
    ],
    "normalized_filename": "dash",
    "outlinks": [
      "plotly",
      "data_visualisation",
      "clustering_dashboard.py",
      "ml_tools"
    ],
    "inlinks": [
      "dashboarding"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Dashboards",
    "sha": "7721622f1b2287b979d3c88d055d3b1abb20c646",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Dashboards.md",
    "text": "Dashboards\n- [[Data Visualisation]]: \n- Whos the audiance, and [[granularity]]\n- What [[Metric]] are key\n- Add annotations\n- feedback from users\n- What actions are their from the insights\n- business\n- communication",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "communication",
      "visualization"
    ],
    "normalized_filename": "dashboards",
    "outlinks": [
      "granularity",
      "data_visualisation",
      "metric"
    ],
    "inlinks": [
      "github_actions"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Dashboarding",
    "sha": "866e1c7975f0668b432567edb3a5c4061ba5a91b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Dashboarding.md",
    "text": "[[Dash]]\n[[Streamlit.io]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "frontend"
    ],
    "normalized_filename": "dashboarding",
    "outlinks": [
      "dash",
      "streamlit.io"
    ],
    "inlinks": [
      "react"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Analysis",
    "sha": "f0fd87563dff2c0f840fe023b601566666cb175f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Analysis.md",
    "text": "Usually done with a [[Data Analyst]]. After processing, data is analyzed to extract meaningful insights and derive value from the data.\n### Types of analysis:\n\nExploration and understanding:\n- [[EDA]]: Involves exploring data sets to find patterns, anomalies, or relationships without having a specific hypothesis in mind. It is often used in the initial stages of data analysis to generate insights.\n- Descriptive: ==Focuses on summarizing historical data to understand what has happened== in the past. It often involves the use of [[Statistics]] measures and [[Data Visualisation]] tools to present data trends and patterns.\n- Diagnostic: ==Seeks to understand why something happened==. It involves examining data to identify causes and correlations, often using techniques like data mining and statistical analysis.\n\nForward looking:\n- Predictive: Uses historical data and statistical algorithms to ==forecast future outcome==s. It helps in identifying trends and making predictions about future events based on past data.\n- Prescriptive: Goes a step further by ==recommending actions based on the predictions made==. It uses optimization and simulation algorithms to suggest the best course of action for a given situation.\n- Inferential: Makes inferences and predictions about a population based on a sample of data. It often involves [[Hypothesis testing]] and [[Confidence Interval]].\n\n### [[LLM|Large Language Models]] can analyse graphs\n\nFor instance in [[Time Series Forecasting]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "normalized_filename": "data_analysis",
    "outlinks": [
      "data_analyst",
      "data_visualisation",
      "time_series_forecasting",
      "llm",
      "hypothesis_testing",
      "statistics",
      "eda",
      "confidence_interval"
    ],
    "inlinks": [
      "alternatives_to_batch_processing",
      "covariance_structures",
      "data_analyst",
      "data_lifecycle_management",
      "data_roles",
      "ds_&_ml_portal",
      "duckdb",
      "fact_table",
      "operational_resilience_for_growth_and_adaptability",
      "scientific_method",
      "statistical_assumptions"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Analysis Portal",
    "sha": "4690ede37cc233b5a6d966612513fd7a7c283e49",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Analysis%20Portal.md",
    "text": "- [[Data Analyst]]\n- [[Data Visualisation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "portal"
    ],
    "normalized_filename": "data_analysis_portal",
    "outlinks": [
      "data_analyst",
      "data_visualisation"
    ],
    "inlinks": []
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Distribution",
    "sha": "a3bf89dadc939b5c059071ef55575bcd79f9c5cf",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Distribution.md",
    "text": "Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. \n\nThis can involve supplying data to\n- business applications, \n- reporting systems, \n- or other data-driven processes, \n- ensuring that stakeholders\n\nhave access to the information they need for decision-making and operations.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "business",
      "communication"
    ],
    "normalized_filename": "data_distribution",
    "outlinks": [],
    "inlinks": [
      "data_lifecycle_management",
      "data_management",
      "when_and_why_not_to_us_regularisation"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Analyst",
    "sha": "93b3c4503e74d73ac45c4a3bed97d9d8c4cf86d5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Analyst.md",
    "text": "Summary:\n- Gathers and processes data to generate reports.\n- Communicates insights and findings to management\n- Conducts [[Data Analysis]].\n\n### Key responsibilities of a data analyst:\n\n- Define Objectives: Clearly outline the goals of the analysis to guide the process.\n  \n- [[Data Collection]]: Gather relevant data from various sources, ensuring accuracy, completeness, and timeliness.\n  \n- [[Data Cleansing]]: Clean the data to remove errors, duplicates, and inconsistencies for reliable findings.\n  \n- Data Exploration: Perform exploratory data analysis ([[EDA]]) to understand data structure, [[Distributions|distribution]], and relationships.\n  \n- Choose the Right Tools: Utilize appropriate tools and software for analysis, such as Excel, R, Python, SQL, or specialized platforms.\n  \n- [[Statistics]]: Apply various statistical methods and techniques, such as regression analysis, clustering, and [[Hypothesis testing]].\n  \n- [[Data Visualisation]]: Use visualization techniques to effectively present findings and communicate insights.\n  \n- Interpret Results: Analyze results in the context of objectives and consider implications for decision-making.\n  \n- Documentation: Maintain thorough [[Documentation & Meetings]] of the analysis process, including data sources, methodologies, and findings.\n  \n- Continuous Learning: Stay updated with the latest tools, techniques, and best practices in the evolving field of data analysis.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "normalized_filename": "data_analyst",
    "outlinks": [
      "data_visualisation",
      "data_cleansing",
      "hypothesis_testing",
      "data_analysis",
      "statistics",
      "eda",
      "distributions",
      "documentation_&_meetings",
      "data_collection"
    ],
    "inlinks": [
      "data_analysis",
      "data_analysis_portal",
      "data_roles"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Mining",
    "sha": "18a3a8fae6010307bf203a92c3597f1cf297ea55",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Mining.md",
    "text": "Data mining is the process of discovering useful patterns, relationships, and knowledge from large datasets by applying statistical, machine learning, and database techniques.\n\nData mining refers to the process of discovering useful patterns and insights from large datasets. It involves:  \n- Finding Useful Patterns: Identifying patterns in the data that can be used to make predictions or gain insights.\n- Building Predictive Models: Using these patterns to create models that can predict future outcomes or behaviors.\n\nThe term \"data mining\" has evolved and is now often referred to as traditional machine learning, particularly focusing on supervised machine learning where models are built to make predictions based on labeled data.\n\nConcise definition:\nData mining = ==turning raw data into actionable knowledge by uncovering hidden patterns and trends.==\n\nLaws of Data Mining:\n- There are always interactions & patterns\n### Key points:\nGoal: extract meaningful information from raw data that is not obvious.\n\nMethods used:\n  * Classification (assigning labels)\n  * Clustering (grouping similar items)\n  * Association rule mining (finding \"if–then\" patterns, e.g., market basket analysis)\n  * Anomaly detection (finding unusual patterns)\n  * Regression (predicting numerical values)\n\n### Related:\n- [[Data Mining - CRISP]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "normalized_filename": "data_mining",
    "outlinks": [
      "data_mining_-_crisp"
    ],
    "inlinks": [
      "business_understanding",
      "data_mining_-_crisp"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Product",
    "sha": "e3e0b921aa88558bacfcc648a35f72ec1d21e1d6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Product.md",
    "text": "A data product is\n\n\"a product that facilitates an end goal through data\".\n\nDelivering the final output, which could be dashboards, reports, or machine learning models. For example Recommendation systems or predictive analytics dashboards.\n\nIt applies more product thinking, whereas the \"Data Product\" essentially is a dashboard, report, and table in a [Data Warehouse](Data%20Warehouse.md) or a Machine Learning model.\n\nSometimes Data Products are also called [[data asset]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "business",
      "management"
    ],
    "normalized_filename": "data_product",
    "outlinks": [
      "data_asset"
    ],
    "inlinks": [
      "data_pipeline_to_data_products"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Reduction",
    "sha": "0ce9d92e5adf3c0494942596cb6651200a4c7e87",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Reduction.md",
    "text": "Reducing the volume of data through techniques:\n\n[[Dimensionality Reduction]]\n\n[[Resampling]]: Use subsets of data for training to speed up the process and address issues like imbalanced data representation.\n\nRemove features with zero or low [[Variance]] and redundant features to improve model performance.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "preprocessing"
    ],
    "normalized_filename": "data_reduction",
    "outlinks": [
      "resampling",
      "variance",
      "dimensionality_reduction"
    ],
    "inlinks": [
      "preprocessing"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Data Visualisation",
    "sha": "e04a729be651d00c5a2154019c9e450f88740b34",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Data%20Visualisation.md",
    "text": "Data visualization involves presenting data in a visual format, enabling stakeholders to quickly grasp insights and make informed decisions. Effective visualization tools include dashboards and reports.\n\nCan generate reports using:\n- [[Tableau]]\n- [[PowerBI]]\n- [[Looker Studio]]\n- [[Binder]]\n\nTypes of plots:\n- [[Scatter Plots]]\n- [[Boxplot]]\n- [[Histogram]]\n- [[Grouped plots]]\n- [[Q-Q Plot]]\n- [[Heatmap]]\n\nRelated to:\n- [[Multivariate Analysis]]\n- [[Univariate Analysis]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "normalized_filename": "data_visualisation",
    "outlinks": [
      "powerbi",
      "binder",
      "univariate_analysis",
      "boxplot",
      "histogram",
      "multivariate_analysis",
      "scatter_plots",
      "tableau",
      "grouped_plots",
      "heatmap",
      "q-q_plot",
      "looker_studio"
    ],
    "inlinks": [
      "anomaly_detection",
      "binder",
      "boxplot",
      "business_observability",
      "dash",
      "dashboards",
      "data_analysis",
      "data_analysis_portal",
      "data_analyst",
      "data_lifecycle_management",
      "dimensionality_reduction",
      "eda",
      "feature_selection",
      "melt",
      "multivariate_analysis",
      "seaborn",
      "tableau",
      "time_series_python_packages"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "EDA",
    "sha": "fbc100fb7b296061e2763f1abd9f383d133b2c56",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/EDA.md",
    "text": "EDA is an approach to analyzing datasets to summarize their main characteristics, often through visual and statistical methods. It helps with:\n* Understanding data structure and organization\n* Detecting patterns and trends\n* Choosing appropriate statistical techniques\n* Selecting and assessing variables\n* Addressing [[Data Quality]] issues\n* Identifying [[uncategorised/Outliers]] and [[storage/utils/file_getter/selected_files/Outliers|anomalies]]\n* Formulating and testing hypotheses\n* Verifying assumptions prior to modelling\n\nRelated to:\n- [[Data Mining - CRISP]]\n\n## Understanding Variable Behaviour\n\n[[univariate vs multivariate]] Analysis\n* Univariate: Focuses on single variable distributions, central tendency, and spread.\n* Multivariate: Explores interactions between variables and their collective behaviour.\n\nTechniques:\n* Descriptive statistics: mean, median, mode, percentiles, [[Standard deviation]]\n* [[Data Visualisation]]: histograms, box plots, bar charts\n* Pair plots: for relationships between multiple numerical variables\n* Correlation matrices: to assess linear relationships\n* Box plots: numeric vs categorical comparison\n\n[[Feature Importance]]Determine which variables matter most:\n* Check distributional shape and variance\n* Assess predictive separation (e.g. class imbalance in categorical predictors)\n* Compute correlation with the target variable\n## Distributions and [[Data Transformation]]\n\nUnderstand the shape and scale of each variable:\n* Use [[Log transformation]] to reduce skewness and approximate normal distributions\n* For imbalanced categorical features (e.g. 90% in one class), assess usefulness (see [[Imbalanced Datasets]])\n* Always interpret [[Distributions]] in domain context, not just statistical form\n\n## Data Relationships and [[Correlation]]\nExplore dependencies and associations between variables:\n* Use scatter plots and correlation coefficients\n* Investigate how multiple features interact with the target variable\n* Be mindful of spurious correlations-use domain knowledge to guide interpretation\n## Purpose-Driven Exploration\n\nEDA should be goal-oriented:\n* What modelling or analysis task will follow?\n* Are you preparing for [[Feature Engineering]], selecting variables, or cleaning data?\n* Tailor your EDA accordingly to inform future modelling or decision-making\n## Evaluating Limitations and Risk\n\nBe explicit about the constraints of your analysis:\n* Log issues such as missing data, small subgroup sizes, measurement bias\n* Check assumptions ([[Statistical Assumptions]]) where relevant (e.g., normality, linearity)\n## Summary and Action\n\nAlways conclude with a clear summary:\n* Key patterns, outliers, and relationships\n* Variables to engineer, discard, or investigate further\n* Questions or hypotheses to carry into the modelling phase\n\n## Continuous Development\n\nEDA is iterative-refine your insights as your understanding of the data deepens.\n\nUse implementation tools in [[ML_Tools]], e.g.:\n* [[EDA_Pandas.py]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "exploration",
      "transformation"
    ],
    "normalized_filename": "eda",
    "outlinks": [
      "eda_pandas.py",
      "univariate_vs_multivariate",
      "data_visualisation",
      "correlation",
      "statistical_assumptions",
      "data_transformation",
      "storage/utils/file_getter/selected_files/outliers",
      "feature_engineering",
      "data_mining_-_crisp",
      "uncategorised/outliers",
      "distributions",
      "feature_importance",
      "log_transformation",
      "imbalanced_datasets",
      "data_quality",
      "ml_tools",
      "standard_deviation"
    ],
    "inlinks": [
      "altair",
      "data_analysis",
      "data_analyst",
      "documentation_&_meetings",
      "factor_analysis",
      "github_actions",
      "knime",
      "preprocessing",
      "scientific_method",
      "t-sne"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "DuckDB",
    "sha": "ca081f154902d29ef1cf8e4ea866becc73158ef9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/DuckDB.md",
    "text": "DuckDB is an open-source analytical database management system designed for efficient data processing and analysis. It is optimized for running complex queries on large datasets and is particularly well-suited for data science and analytics tasks. Here are some key features and characteristics of DuckDB:\n\nResources:\n- https://duckdb.org/\n- [[DuckDB in python]]\n- [[DuckDB vs SQLite]] \n- https://www.youtube.com/watch?v=uHm6FEb2Re4&ab_channel=Fireship\n\nPerfect for:\n- [[Time Series]]\n- [[Columnar Storage]]\n\n### Key Features\n\n1. In-Memory Processing: DuckDB operates primarily in-memory, which allows for fast query execution and data manipulation.\n\n2. [[Columnar Storage]] It uses a columnar storage format, which is efficient for analytical queries that often involve aggregations and scans over large datasets.\n\n3. SQL Support: DuckDB supports SQL as its query language, making it accessible to users familiar with SQL syntax.\n\n4. Integration with Data Science Tools: DuckDB can be easily integrated with popular data science tools and programming languages, such as Python and R, allowing for seamless data analysis workflows.\n\n5. Lightweight and Easy to Use: It can be embedded in applications and does not require a separate server, making it lightweight and easy to deploy.\n\n6. Compatibility: DuckDB can read from various data formats, including CSV, [[Parquet]]\n\n### Use Cases\n- [[Data Analysis]]\n- [[Data Transformation]]\n- [[Querying]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "database"
    ],
    "normalized_filename": "duckdb",
    "outlinks": [
      "data_transformation",
      "parquet",
      "querying",
      "duckdb_vs_sqlite",
      "data_analysis",
      "time_series",
      "columnar_storage",
      "duckdb_in_python"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "vectorized_engine"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "ER Diagrams",
    "sha": "369086e3a17e8736c353c1a175ee8bfcd514f785",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/ER%20Diagrams.md",
    "text": "ER Diagrams are a visual representation of the database structure.\n\nRelated:\n- [[Why use ER diagrams]]\n- [[Mermaid]]\n#### Example\n\nEntities are tables in the database. Here, we have:\n\n- `Employee` table\n- `Department` table\n\nAttributes (Columns)\n- Each entity has attributes (columns):\n\nRelationship\n- ==Each Employee belongs to one Department==\n- Each Department has many Employees.\n- This is a One-to-Many (1:M) relationship.\n\nMermaid ER Diagram\n\n```mermaid\nerDiagram\n  Employee {\n    INTEGER id PK\n    STRING name\n    INTEGER dept_id FK\n  }\n  Department {\n    INTEGER dept_id PK\n    STRING name\n  }\n  Employee }o--|| Department : \"works_in\"\n```\n\nExplanation of the Arrows\n\n- `}o--||` → One-to-Many (1:M)\n- One Department (`||`) can have many Employees (`}o`).\n- Each Employee belongs to one Department.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "visualization"
    ],
    "normalized_filename": "er_diagrams",
    "outlinks": [
      "mermaid",
      "why_use_er_diagrams"
    ],
    "inlinks": [
      "conceptual_model",
      "data_modeling",
      "implementing_database_schema",
      "pgadmin",
      "relating_tables_together",
      "types_of_database_schema",
      "why_use_er_diagrams"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Heatmap",
    "sha": "03d9f344dec50dc62049204b9a685cf6ac63c02e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Heatmap.md",
    "text": "A **heatmap** is a two-dimensional graphical representation of data where individual values are represented by colors. It is particularly useful for visualizing numerical data organized in a table-like format. \n\nA heatmap is a graphical representation of data where individual values are represented by colors. It is useful for visualizing numerical data and analyzing the correlation between features.\n\nA heatmap is a visualization tool for analyzing the [[Correlation]] between features in a dataset. In the context of correlation analysis, a heatmap can display the correlation coefficients between different features in a dataset.\n\nBy using a heatmap, you can easily identify [[Multicollinearity]] and make informed decisions about which features to retain or remove, ultimately enhancing the performance and interpretability of your machine learning models.\n### Correlation Coefficients\nThe correlation coefficients range from -1 to 1:\n- **-1**: Indicates a perfect negative correlation; if one attribute is present, the other is almost certainly absent.\n- **0**: Indicates no correlation; there is no dependence between the attributes.\n- **1**: Indicates a perfect positive correlation; if one attribute is present, the other is also certainly present.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "visualization",
      "ML_Tools"
    ],
    "normalized_filename": "heatmap",
    "outlinks": [
      "multicollinearity",
      "correlation"
    ],
    "inlinks": [
      "correlated_time_series",
      "correlation",
      "data_visualisation",
      "feature_selection",
      "multicollinearity",
      "pca_principal_components"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Label encoding",
    "sha": "df056a0ae406cda549b904e039d6e64e9ca509a8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Label%20encoding.md",
    "text": "Label Encoding assigns an integer value to each unique category in a feature. For example, if you have three towns: `['West Windsor', 'Robbinsville', 'Princeton']`, Label Encoding would convert them into numerical values like this:\n    - West Windsor → 0\n    - Robbinsville → 1\n    - Princeton → 2\n\nInterpretation in the Model: When you use Label Encoding, the model interprets the numbers as continuous values, meaning it sees a numeric relationship between them (i.e., \"Princeton\" might be considered numerically higher than \"West Windsor\" and closer to \"Robbinsville\"). This can cause issues if the numeric values don’t have any ordinal relationship.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing"
    ],
    "normalized_filename": "label_encoding",
    "outlinks": [],
    "inlinks": [
      "encoding_categorical_variables",
      "label_encoding_vs_one-hot_encoding",
      "lightgbm_vs_xgboost_vs_catboost",
      "one-hot_encoding",
      "one_hot_encoding.py"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Linear Discriminant Analysis",
    "sha": "a284074cdc1b32cff915adb5c4652a7c08cb3388",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Linear%20Discriminant%20Analysis.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "normalized_filename": "linear_discriminant_analysis",
    "outlinks": [],
    "inlinks": [
      "dimensionality_reduction",
      "feature_scaling"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Log transformation",
    "sha": "e62e5f3191cabed0f9f333609277b72ccf4b3e73",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Log%20transformation.md",
    "text": "A log transformation is a mathematical operation that applies the logarithm function to each value in a dataset. This transformation is commonly used in data analysis and machine learning to address issues such as skewness, reduce the impact of outliers, and stabilize variance across a dataset.\n### Purpose of Log Transformation\n- Normalization of Data: Log transformation can help make a dataset more normally distributed, which is often an assumption for many statistical methods and machine learning algorithms.\n- Reducing Skewness: It is particularly effective for positively skewed data (where a majority of values are clustered at the lower end). By applying a log transformation, the distribution can be made more symmetric.\n- Dealing with Outliers: Log transformation can reduce the influence of extreme values (outliers) on statistical analyses, making the data more robust for modeling.\n- Linearizing Relationships: In regression analysis, log transformation can help linearize relationships between variables, making it easier to model.\n### When to Use Log Transformation\n\n- When the data is positively skewed.\n- When you want to stabilize variance across a dataset.\n- When you are working with multiplicative relationships in your data.\n- When preparing data for algorithms that assume normality or linearity.\n\n### Considerations\n\n- Log transformation cannot be applied to zero or negative values, as the logarithm of these values is undefined. In such cases, a common approach is to add a small constant to all values before applying the transformation (e.g., ).\n- Always visualize the data before and after transformation to assess the effectiveness of the log transformation in achieving normality or reducing skewness.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "transformation"
    ],
    "normalized_filename": "log_transformation",
    "outlinks": [],
    "inlinks": [
      "data_selection_in_ml",
      "eda",
      "stationary_time_series",
      "transformed_target_regressor",
      "trends_in_time_series"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "MariaDB vs MySQL",
    "sha": "9fbce63fa53ccbe4195187e5e723ab906ef6738f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/MariaDB%20vs%20MySQL.md",
    "text": "### Why Teams Pick One Over the Other\n\n* MariaDB:\n  * Teams that value open-source principles and community-driven development.\n  * Need advanced replication, scalability, or analytical features (ColumnStore).\n  * Organizations that want to avoid Oracle’s licensing costs.\n\n* MySQL:\n  * Enterprise users who already have Oracle support contracts.\n  * Companies that need guaranteed long-term support and Oracle ecosystem integration.\n  * Conservative environments that value stability and industry-standard adoption.\n### MariaDB vs MySQL: Key Differences\n\n| Feature                     | MariaDB                                                                             | MySQL                                                               |\n| --------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------- |\n| Origin                  | Forked from MySQL in 2009 by its original developers                                | Owned by Oracle Corporation                                         |\n| License                 | GPL (always open-source)                                                            | Dual-licensed (GPL + commercial license from Oracle)                |\n| Storage Engines         | Includes all MySQL engines + new ones like Aria, ColumnStore, MyRocks   | Includes core engines like InnoDB, MyISAM (fewer options)           |\n| Replication             | More advanced features (e.g., multi-source replication, Galera Cluster support)     | Standard replication, clustering via InnoDB Cluster                 |\n| Performance             | Often faster for complex queries, thread pooling, and analytical workloads          | Stable, widely used, but MariaDB tends to push optimizations faster |\n| Features                | JSON, dynamic columns, virtual columns, temporal data tables (system versioning)    | JSON support but less advanced in schema flexibility                |\n| Compatibility           | Aims to maintain drop-in compatibility with MySQL (same client libraries, commands) | Industry-standard, but may diverge from MariaDB over time           |\n| Community vs Enterprise | Strong open-source community, backed by MariaDB Foundation                          | Enterprise-driven development under Oracle priorities               |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "SQL",
      "storage"
    ],
    "normalized_filename": "mariadb_vs_mysql",
    "outlinks": [],
    "inlinks": [
      "mariadb"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Looker Studio",
    "sha": "d5f0ff85e1815f322219ab771e68ebea62450056",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Looker%20Studio.md",
    "text": "Looker studio is [[Google]] version of [[PowerBI]], but its free.\n### Connectors to data\n\nCan connect to data sources i.e:\n- [[Google Sheets|Google Sheets]]\n- [[PostgreSQL]]\n\n### Data Modelling\n\nData models are called Blends\n\n[Dashboard with Relational Database in Looker Studio Data Blending and Modeling](https://www.youtube.com/@virtual_school)",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "business",
      "communication"
    ],
    "normalized_filename": "looker_studio",
    "outlinks": [
      "powerbi",
      "google_sheets",
      "postgresql",
      "google"
    ],
    "inlinks": [
      "data_visualisation"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Melt",
    "sha": "a856281d12a45e1f55019d841941fee170787c60",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Melt.md",
    "text": "In pandas, the `melt` function is used to ==transform ([[Data Transformation]]) a DataFrame from a wide format to a long format==. This is especially useful for data analysis and visualization tasks where long-format data is preferred or required. The wide format typically has multiple columns for different variables, whereas the long format has a single column for variable names and a single column for values. \n\nRelated:\n- [[Database Techniques]]\n- [[Turning a flat file into a database]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb\n### Key Reasons to Use `melt`:\n\n1. [[Normalisation]]\n   - Wide to Long Transformation: `melt` helps in converting data with many columns (wide format) into a more normalized form with fewer columns (long format). This is useful for many statistical and visualization libraries that prefer long-format data.\n   \n1. Easier Analysis and [[Data Visualisation]]:\n   - Compatibility with Plotting Libraries: Many plotting libraries like `[[seaborn]]` and `ggplot` require data in a long format for creating certain types of plots, such as [[Grouped plots]].\n\n1. Simplifying Complex Data Structures:\n   - Handling [[Multi-level index]]: If a DataFrame has multiple levels of columns, `melt` can help flatten this structure, making it easier to work with.\n   \n3. Preparation for Aggregation:\n   - Facilitating [[Groupby]] Operations: Long-format data is often more suitable for these.\n\n### Parameters of `melt`:\n\n- `id_vars`: Columns to use as identifier variables. These columns are kept as-is in the output.\n- `value_vars`: Columns to unpivot. These columns are transformed into a single column.\n- `var_name`: Name to use for the `variable` column in the output.\n- `value_name`: Name to use for the `value` column in the output.\n\n### Example Usage:\n\nConsider a DataFrame in wide format:\n\n```python\nimport pandas as pd\n\n# Sample wide format data\ndata = {\n    'id': [1, 2, 3],\n    'math_score': [88, 92, 95],\n    'science_score': [85, 90, 89],\n    'english_score': [78, 85, 88]\n}\ndf_wide = pd.DataFrame(data)\nprint(df_wide)\n```\n\nOutput:\n\n```\n   id  math_score  science_score  english_score\n0   1          88             85             78\n1   2          92             90             85\n2   3          95             89             88\n```\n\nTo convert this wide-format DataFrame into a long-format DataFrame using `melt`:\n\n```python\n# Melt the DataFrame\ndf_long = pd.melt(df_wide, id_vars=['id'], \n                  value_vars=['math_score', 'science_score', 'english_score'], \n                  var_name='subject', value_name='score')\nprint(df_long)\n```\n\nOutput:\n\n```\n   id       subject  score\n0   1    math_score     88\n1   2    math_score     92\n2   3    math_score     95\n3   1  science_score     85\n4   2  science_score     90\n5   3  science_score     89\n6   1  english_score     78\n7   2  english_score     85\n8   3  english_score     88\n```",
    "aliases": [],
    "created": "2024-06-21 15:32",
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "type": null,
    "normalized_filename": "melt",
    "outlinks": [
      "data_visualisation",
      "turning_a_flat_file_into_a_database",
      "normalisation",
      "data_transformation",
      "database_techniques",
      "seaborn",
      "grouped_plots",
      "de_tools",
      "groupby",
      "multi-level_index"
    ],
    "inlinks": []
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Multiple Correspondence Analysis",
    "sha": "365903c76fdfbbd86124866b79c1bc89a5898dae",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Multiple%20Correspondence%20Analysis.md",
    "text": "Categorical version of [[Principal Component Analysis|PCA]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "multiple_correspondence_analysis",
    "outlinks": [
      "principal_component_analysis"
    ],
    "inlinks": [
      "principal_component_analysis"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Multivariate Analysis",
    "sha": "11ce8e32e115a3883cde2323644678ad6234248a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Multivariate%20Analysis.md",
    "text": "[[Data Visualisation]] for two variables:\n- Consider target categorical: and a input feature\n- [[Crosstab]], boxplots, or scatters graphs",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "multivariate_analysis",
    "outlinks": [
      "crosstab",
      "data_visualisation"
    ],
    "inlinks": [
      "data_visualisation",
      "univariate_vs_multivariate",
      "varmax"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "OLAP",
    "sha": "99060612fa73b773821098395cd98033ef00597b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/OLAP.md",
    "text": "Online Analytical Processing is a category of database technology. OLAP systems allow organizations to gain insights by examining data across various dimensions, such as time, product, and region.\n\nRelated:\n- [[Excel pivot table]]\n- [[Querying]]\n## Key Features of OLAP & Operations\n\n- Query Performance\n- Aggregation and Summarization across dimensions.\n- Slicing: Extracting a single layer of data from the cube by selecting a specific dimension (e.g., sales for Q1).\n- Dicing: Selecting a subcube by specifying values for multiple dimensions.\n- Drill Down: Moving from a summary level to a more detailed level (e.g., from yearly to monthly sales).\n- Roll Up: Aggregating data to a higher level (e.g., from daily to monthly sales).\n- Pivoting: Rotating the data to view it from a different perspective (e.g., switching rows and columns).\n### Use Cases of OLAP\n- [[business intelligence]] (BI): OLAP tools are integral to BI solutions, allowing for the analysis of financial data, sales performance, and other key metrics.\n- Data Warehousing: OLAP is commonly used with data warehouses, where large volumes of historical data are stored for reporting and analysis.\n### Visualization Tools\nTo interact with the OLAP cube, users typically utilize tools such as:\n- Microsoft PowerBI: For creating dashboards and visualizations.\n- [[Excel]] with Pivot Tables: For slicing, dicing, and reporting.\n- [[Tableau]]: For visual analysis.",
    "aliases": [
      "Online analytical processing"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "database",
      "tool"
    ],
    "normalized_filename": "olap",
    "outlinks": [
      "querying",
      "excel_pivot_table",
      "tableau",
      "excel",
      "business_intelligence"
    ],
    "inlinks": [
      "columnar_storage",
      "dimensions",
      "granularity",
      "star_schema"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Page Rank",
    "sha": "234c0e69799e8975c3d372659e80dce41430ac38",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Page%20Rank.md",
    "text": "PageRank is an algorithm originally developed by Larry Page and Sergey Brin (founders of Google) to rank web pages in [[Search]] engine results. It measures the relative importance of each node (e.g., webpage) in a directed graph based on the structure of incoming links.\n### Intuition\n\nThe core idea is:\n> A node is important if many other important nodes link to it.\n\nPageRank simulates a “random surfer” who clicks on links at random:\n- With probability $d$ (typically 0.85), the surfer follows a link from the current page.\n- With probability $1 - d$, the surfer jumps to a random page.\n\n### Mathematical Formulation\n\nGiven a graph with $N$ nodes, the PageRank of node $i$ is defined recursively as:\n\n$$\nPR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in \\text{In}(i)} \\frac{PR(j)}{L(j)}\n$$\n\nWhere:\n- $d$ is the damping factor (usually 0.85),\n- $\\text{In}(i)$ is the set of nodes linking to $i$,\n- $L(j)$ is the number of outbound links from node $j$.\n\n\n\n### Implementation (using NetworkX)\n\n```python\nimport networkx as nx\n\nG = nx.DiGraph()\nG.add_edges_from([\n    ('A', 'B'), ('B', 'C'), ('C', 'A'), ('A', 'D'), ('D', 'C')\n])\n\npagerank_scores = nx.pagerank(G, alpha=0.85)\nfor node, score in pagerank_scores.items():\n    print(f\"{node}: {score:.4f}\")\n```\n\n\n\n### 📊 Use Cases\n- Graph-based NLP: Keyword extraction (e.g., TextRank).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "visualization"
    ],
    "normalized_filename": "page_rank",
    "outlinks": [
      "search"
    ],
    "inlinks": [
      "graph_theory"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Parquet",
    "sha": "745a4a25b1b7896db8739b402a67b215a984bf63",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Parquet.md",
    "text": "A **Parquet file** is a **columnar storage file format** specifically designed for storing large amounts of data efficiently. It is commonly used in [[Big Data]] ecosystems due to its optimised performance for both storage and querying.\n\n[[Data Storage]]\n### Key Features of Parquet:\n\n1. **Columnar Storage Format:**\n    - Data is stored **column by column** instead of row by row.\n    - This design is highly efficient for analytical queries that access only specific columns, reducing the amount of data read.\n      \n2. **Optimised for Big Data:**\n    - Parquet is designed for distributed systems like Apache [[Hadoop]], [[Apache Spark]], and other big data processing tools.\n      \n3. **Compression:**\n    - It supports multiple compression algorithms (e.g., Snappy, GZIP) for reducing file size while maintaining fast read and write performance.\n      \n4. **[[Schema Evolution]]:**\n    - Parquet supports flexible schemas, allowing fields to be added or modified without breaking compatibility with older data.\n      \n5. **Efficient [[Metadata Handling]]:**\n    - Metadata is stored along with the data, making it easier to retrieve and query information about the dataset without scanning the entire file.\n\n### Advantages of Parquet:\n\n1. **Improved Query Performance:**\n    - Since data is stored column-wise, queries that require only a few columns read less data compared to row-based formats like CSV.\n      \n2. **Lower Storage Costs:**\n    - Built-in compression and columnar storage **significantly reduce file size.**\n      \n3. **Compatibility:**\n    - Parquet is compatible with most big data processing tools, such as Hadoop, Spark, Hive, Presto, and AWS Athena.\n\n1. **Efficient I/O:**\n    - Parquet’s columnar format minimises disk I/O, making it faster to process large datasets.\n\n---\n\n### When to Use Parquet:\n- **Analytical Workloads:** Ideal for scenarios where you need to perform aggregations, filtering, or processing large datasets.\n- **Big Data Processing:** Frequently used with tools like Spark, Hive, and Presto in data lakes.\n- **Cloud Storage:** Supported by [[Cloud Providers]] platforms.\n\n### Example Use Case:\n\nImagine a dataset with 10 million rows and 100 columns.\n\n- If you query just 3 columns in a row-based format (e.g., CSV), you must read all 100 columns for every row.\n- In Parquet, only the 3 relevant columns are read, significantly improving performance.\n\n---\n\n### File Structure:\n\n- **Row Group:** Data is divided into chunks called row groups, enabling efficient data retrieval.\n- **Columns:** Each column in a row group is stored together for fast access.\n- **Footer:** Contains metadata, such as schema definitions and row group locations, allowing quick navigation of the file.\n\n---\n\n### How to Work with Parquet in Python:\n\nYou can use libraries like **pandas** or **pyarrow**:\n\n```python\nimport pandas as pd\n\n# Write a DataFrame to a Parquet file\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\ndf.to_parquet('example.parquet', engine='pyarrow')\n\n# Read a Parquet file into a DataFrame\ndf_read = pd.read_parquet('example.parquet')\nprint(df_read)\n```\n\n---\n\nIn summary, Parquet is an efficient, compact, and scalable file format ideal for big data analytics and storage, providing faster performance and reduced costs.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "parquet",
    "outlinks": [
      "schema_evolution",
      "apache_spark",
      "hadoop",
      "metadata_handling",
      "cloud_providers",
      "big_data",
      "data_storage"
    ],
    "inlinks": [
      "apache_iceberg",
      "data_lakehouse",
      "delta_tables_in_databricks",
      "duckdb",
      "hugging_face"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "PowerBI",
    "sha": "d879a8d64a60db9b8dca7556db1d216e5c1c72ea",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/PowerBI.md",
    "text": "[tutorial](https://www.youtube.com/watch?v=TmhQCQr_DCA)\n\nBusiness analytics tool for data visualization and reporting.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software",
      "visualization"
    ],
    "normalized_filename": "powerbi",
    "outlinks": [],
    "inlinks": [
      "data_visualisation",
      "fabric",
      "looker_studio",
      "semantic_layer"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Plotly",
    "sha": "2b5e6afb6b2b4a9d902e43a14f9182790c6e2099",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Plotly.md",
    "text": "Python visualisation tool.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "visualization"
    ],
    "normalized_filename": "plotly",
    "outlinks": [],
    "inlinks": [
      "dash",
      "graph_theory",
      "hosting",
      "html",
      "pycaret",
      "streamlit",
      "time_series_python_packages"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Preprocessing Text Classification",
    "sha": "6421122485b984a5fb47da6b564c9843f142d398",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Preprocessing%20Text%20Classification.md",
    "text": "[[Preprocessing]] in [[Text Classification]] is the set of steps applied to raw text before feeding it into a machine learning or deep learning model. The goal is to normalize text, remove noise, and convert it into a numerical representation suitable for algorithms.\n\n### Typical Preprocessing Steps\n\n#### Text Cleaning\n\n* Remove punctuation, special characters, numbers (optional).\n* Lowercasing text: `\"The Cat\"` → `\"the cat\"`.\n* Remove unwanted whitespace.\n\n#### [[Tokenisation|Tokenization]]\n\n* Split text into tokens (words, subwords, or characters).\n\n  * Example: `\"I love NLP\"` → `[\"I\", \"love\", \"NLP\"]`.\n\n#### Stopword Removal\n\n* Remove common words that add little meaning:\n\n  * Example: `\"is\", \"and\", \"the\"`.\n\n#### Normalization\n\n* [[Stemming]]: Reduce words to their root form (e.g., `\"running\"` → `\"run\"`).\n* [[lemmatization]]: Use vocabulary and grammar to reduce to base form (`\"better\"` → `\"good\"`).\n\n#### Handling Categorical/Text Variants\n\n* Remove URLs, HTML tags, mentions (`@username`), hashtags.\n\n#### Handling Out-of-Vocabulary (OOV) & Rare Words\n\n* Replace rare words with `<UNK>` token or use subword tokenization (e.g., BPE).\n\n#### Encoding Text\n\nConvert processed tokens into numerical format:\n* [[Bag of Words]] (BoW)\n* [[TF-IDF]]\n* Word Embeddings (Word2Vec, GloVe, FastText)\n* Contextual embeddings (BERT, GPT tokenization).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "NLP",
      "preprocessing",
      "process"
    ],
    "normalized_filename": "preprocessing_text_classification",
    "outlinks": [
      "stemming",
      "lemmatization",
      "bag_of_words",
      "preprocessing",
      "tokenisation",
      "text_classification",
      "tf-idf"
    ],
    "inlinks": [
      "text_classification"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Preprocessing",
    "sha": "a06c7f5446128399fd3073cf821ace5f2c670764",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Preprocessing.md",
    "text": "### Data Preprocessing\n\nData Preprocessing refers to the overall process of cleaning and transforming raw data into a format that is suitable for analysis and modelling. This includes a variety of tasks, such as:\n\nUseful:\n- [[Data Cleansing]]\n- [[Data Transformation]]\n\nOthers:\n- [[Data Collection]]\n- [[Data Reduction]]\n\nEnd goal:\n- [[EDA]]\n\n### Feature Preprocessing\n\nFeature preprocessing refers to the process of transforming raw data into a clean data set for learning models, after Data Preprocessing. This step is crucial for improving model performance and ensuring accurate predictions\n\n2. **[[Feature Scaling]]**: Normalizing or standardizing features to ensure they are on a similar scale. Normalization and Scaling: Adjusting the range of features, often using techniques like min-max scaling or z-score normalization, to ensure that all features contribute equally to the model.\n\n4. [[Feature Selection]]: Identifying and retaining the most relevant features that contribute to the predictive power of the model, often using statistical tests or model-based approaches.\n\n5. [[Dimensionality Reduction]]: Reducing the number of features while preserving important information, using techniques like Principal Component Analysis (PCA).\n\n6. [[Feature Engineering]]: Creating new features from existing data to improve model performance, often based on domain knowledge.",
    "aliases": [
      "Data Preprocessing",
      "Feature Preprocessing",
      "Preprocess"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#optimisation",
      "cleaning",
      "portal",
      "preprocessing",
      "transformation"
    ],
    "type": null,
    "normalized_filename": "preprocessing",
    "outlinks": [
      "data_reduction",
      "data_transformation",
      "data_cleansing",
      "feature_scaling",
      "feature_engineering",
      "eda",
      "dimensionality_reduction",
      "feature_selection",
      "data_collection"
    ],
    "inlinks": [
      "altair",
      "clustermap",
      "data_hierarchy_of_needs",
      "data_lifecycle_management",
      "data_pipeline",
      "dimensionality_reduction",
      "feature_scaling",
      "handling_different_distributions",
      "heterogeneous_features",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "machine_learning_operations",
      "missing_data",
      "model_building",
      "model_deployment",
      "nlp",
      "normalisation_of_text",
      "one-hot_encoding",
      "preprocessing_text_classification",
      "pycaret",
      "standardisation",
      "time_series_python_packages"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "SQL Window functions",
    "sha": "deecff422e68082883b44870aeb229e5bd1806e7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/SQL%20Window%20functions.md",
    "text": "SQL Window Functions are a feature in SQL that allow you to perform calculations across a set of table rows that are somehow related to the current row. \n\nUnlike regular aggregate functions, which return a single value for a group of rows, window functions return a value for each row in the result set while still allowing access to the individual row data.\n\n### Key Characteristics of Window Functions\n\n1. Non-Aggregating: Window functions do not collapse rows into a single output row. Instead, they perform calculations across a defined \"window\" of rows related to the current row.\n\n2. OVER Clause: Window functions are defined using the `OVER` clause, which specifies the window of rows to be considered for the function.\n\n3. Partitioning: You can partition the result set into groups using the `PARTITION BY` clause within the `OVER` clause. Each partition is processed independently.\n\n4. Ordering: You can specify the order of rows within each partition using the `ORDER BY` clause within the `OVER` clause.\n### Example Use Case\n\nSuppose you have a table `sales` with columns `salesperson`, `region`, and `amount`. You can use window functions to calculate the total sales for each salesperson while still displaying individual sales records:\n\nInitial Table: `employees`\n\n| id | name    | department | salary |\n|----|---------|------------|--------|\n| 1  | Alice   | Sales      | 50000  |\n| 2  | Bob     | Sales      | 60000  |\n| 3  | Charlie | HR         | 55000  |\n| 4  | David   | HR         | 70000  |\n| 5  | Eve     | IT         | 80000  |\n| 6  | Frank   | IT         | 75000  |\n\n### Example [[Querying|Queries]] Using SQL Window Functions\n\n#### 1. Using `ROW_NUMBER()`\n\nThe `ROW_NUMBER()` function assigns a unique rank to each employee within their department based on their salary.\n\n```sql\nSELECT \n    id, \n    name, \n    department, \n    salary, \n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rank\nFROM employees;\n```\n\nResulting Table:\n\n| id | name    | department | salary | rank |\n|----|---------|------------|--------|------|\n| 2  | Bob     | Sales      | 60000  | 1    |\n| 1  | Alice   | Sales      | 50000  | 2    |\n| 4  | David   | HR         | 70000  | 1    |\n| 3  | Charlie | HR         | 55000  | 2    |\n| 5  | Eve     | IT         | 80000  | 1    |\n| 6  | Frank   | IT         | 75000  | 2    |\n\n#### 2. Using `SUM()` similar AVG\n\nThe `SUM()` function calculates the total salary for each department, showing the same total for each employee in that department.\n\n```sql\nSELECT \n    id, \n    name, \n    department, \n    salary, \n    SUM(salary) OVER (PARTITION BY department) AS total_department_salary\nFROM employees;\n```\n\nResulting Table:\n\n| id | name    | department | salary | total_department_salary |\n|----|---------|------------|--------|-------------------------|\n| 1  | Alice   | Sales      | 50000  | 110000                  |\n| 2  | Bob     | Sales      | 60000  | 110000                  |\n| 3  | Charlie | HR         | 55000  | 125000                  |\n| 4  | David   | HR         | 70000  | 125000                  |\n| 5  | Eve     | IT         | 80000  | 155000                  |\n| 6  | Frank   | IT         | 75000  | 155000                  |\n\n\n\n\n\n\n\n\n\n[[SQL Window functions]]\n   **Tags**:,",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "querying",
      "SQL"
    ],
    "normalized_filename": "sql_window_functions",
    "outlinks": [
      "querying",
      "sql_window_functions"
    ],
    "inlinks": [
      "sql_window_functions"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Seaborn",
    "sha": "5cf6ce9e40e8a83ade0c6f3ca3b0fc178309fd73",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Seaborn.md",
    "text": "https://seaborn.pydata.org/tutorial.html\n\nRelated:\n- [[Data Visualisation]]\n\n[[Distributions]]\nsns.displot",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "visualization"
    ],
    "normalized_filename": "seaborn",
    "outlinks": [
      "data_visualisation",
      "distributions"
    ],
    "inlinks": [
      "melt"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "Tableau",
    "sha": "747161769cbed000997c3dffb71a9c7604f3d60b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/Tableau.md",
    "text": "## Next Steps\n- Load a [[PostgreSQL]] database and perform analytics as an example.\n## Resources\n- [Tableau How-To Videos](https://public.tableau.com/app/learn/how-to-videos)\n- [Tutorial Link](https://public.tableau.com/app/learn/how-to-videos)\n- [Example Usage Video](https://www.youtube.com/watch?v=L5PL0gg1cPQ)\n## Features\n- Can publish to blogs and embed dashboards online ([[Data Visualisation]])\n- Dashboards can be shared online.\n- Easier than doing visualizations in Python.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "tableau",
    "outlinks": [
      "postgresql",
      "data_visualisation"
    ],
    "inlinks": [
      "data_visualisation",
      "olap",
      "pgadmin",
      "semantic_layer"
    ]
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "altair versus seaborn",
    "sha": "79d2cc61b6de973d9fe184a5999dc0939c4833c6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/altair%20versus%20seaborn.md",
    "text": "## 1. Declarative vs. Procedural API\n\n* [[Altair]] is declarative: you specify *what* you want (e.g. “plot *x* against *y* with color by group”) and Vega‑Lite handles *how* to draw it.\n* Seaborn is procedural: you call functions (e.g. `sns.scatterplot`) and pass in data and styling arguments each time.\n\n| Aspect            | Altair                                                | Seaborn                                            |\n| ----------------- | ----------------------------------------------------- | -------------------------------------------------- |\n| API style         | Declarative (chart grammar)                           | Procedural / function‑based                        |\n| Underlying engine | Vega‑Lite (JSON specification + JavaScript rendering) | Matplotlib (static plotting)                       |\n| Interactivity     | Built‑in (hover, zoom, selection)                     | Limited (requires extra work with `mpld3`, Bokeh…) |\n| Output formats    | HTML/JSON (notebooks, web apps, dashboards)           | Static images (PNG, SVG, PDF)                      |\n| Learning curve    | Moderate (chart grammar concepts)                     | Gentle (familiar Matplotlib semantics)             |\n\n## 2. Interactivity and Web Embedding\n\n* Altair charts are natively interactive and embed easily in Jupyter, Voila, or web apps.\n* Seaborn produces static figures; interactivity must be layered on separately.\n\n```python\n# Altair example\nimport altair as alt, pandas as pd\n\ndf = pd.DataFrame({\n    'height': [150, 160, 170, 180],\n    'weight': [55, 60, 65, 75],\n    'gender': ['F','F','M','M']\n})\n\nalt.Chart(df).mark_circle(size=100).encode(\n    x='height',\n    y='weight',\n    color='gender'\n).interactive()\n```\n\n```python\n# Seaborn equivalent\nimport seaborn as sns\nsns.scatterplot(data=df, x='height', y='weight', hue='gender')\n```\n\n## 3. Data Transformation and Pipelines\n\n* Altair can perform aggregations, binning, and joins *within* the chart specification using the `$transform` pipeline.\n* Seaborn expects you to preprocess data (e.g. with Pandas) before plotting.\n\n## 4. Ecosystem Integration\n\n* Altair integrates well with data‑streaming and dashboard tools (e.g. [[Streamlit]], Panel).\n* Seaborn sits atop Matplotlib and is ideal for quick exploratory plots and publication‑quality static figures.\n\n### When to choose which?\n\n| Scenario                                           | Altair                                     | Seaborn                   |\n| -------------------------------------------------- | ------------------------------------------ | ------------------------- |\n| Interactive dashboards                             | ✔︎                                         | ✘ (needs extra layers)    |\n| Quick exploratory static plots                     | ✘ (requires more setup)                    | ✔︎                        |\n| Complex data transformations within the chart spec | ✔︎                                         | ✘ (preprocess externally) |\n| Embedding in web pages                             | ✔︎                                         | ✘                         |\n| Publication‑quality static figures                 | ✘ (possible, but Matplotlib more flexible) | ✔︎                        |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "altair_versus_seaborn",
    "outlinks": [
      "altair",
      "streamlit"
    ],
    "inlinks": []
  },
  {
    "category": "DATA_ANALYSIS",
    "filename": "t-SNE",
    "sha": "e2bc632eda2ee5c76a616010c16737f2d8059286",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-analysis/t-SNE.md",
    "text": "t-SNE (t-distributed Stochastic Neighbor Embedding) is a [[Dimensionality Reduction]] technique used primarily for visualizing high-dimensional data. Unlike methods such as [[Principal Component Analysis|PCA]] (Principal Component Analysis), which are linear, t-SNE is a non-linear method that excels at preserving the local structure of the data. \n\nExample: https://dash.gallery/dash-tsne/\n\nRelated:\n- [[UMAP]]\n### Key Characteristics of t-SNE:\n- Non-linear Mapping: It attempts to capture non-linear relationships in the data by embedding it in a lower-dimensional space (usually 2D or 3D).\n- Local Similarities: t-SNE preserves the local structure of the data. This means that points that are close in the high-dimensional space remain close in the lower-dimensional space.\n- Global Structure: t-SNE may distort global structures to focus more on local relationships, which is both a strength and limitation.\n  \n### How t-SNE Works:\n1. Pairwise Similarities: t-SNE first calculates pairwise similarities between data points in the high-dimensional space.\n2. Probability Distribution: These similarities are transformed into probabilities representing how likely it is that two points are neighbors.\n3. Lower-Dimensional Mapping: t-SNE tries to replicate this distribution of neighbors in the lower-dimensional space by iteratively adjusting the positions of the points.\n\n### Applications:\n- Data Visualization: t-SNE is widely used in data visualization, especially when exploring clusters or patterns in high-dimensional datasets.\n- Exploratory Data Analysis ([[EDA]]): It helps in finding clusters or subgroups in complex datasets, such as in genomics, image processing, or [[NLP|natural language processing]].\n\n### Limitations:\n- Computationally Intensive: t-SNE can be slow and resource-heavy, particularly on large datasets.\n- Random Initialization: Results can vary due to its sensitivity to initialization and the perplexity parameter (which controls how t-SNE balances attention between local and global data structure).\n- Difficult to Interpret: While t-SNE is great for visualization, interpreting the precise distances and positions of points can be tricky.\n### Example\n\n```python\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Standardizing the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Applying t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\n# Plotting the results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\nplt.colorbar()\nplt.title('t-SNE visualization of Iris dataset')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()\n\n```\n\n\n![[Pasted image 20241015211844.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "t-sne",
    "outlinks": [
      "umap",
      "eda",
      "principal_component_analysis",
      "dimensionality_reduction",
      "pasted_image_20241015211844.png",
      "nlp"
    ],
    "inlinks": [
      "dimensionality_reduction",
      "evaluate_embedding_methods",
      "principal_component_analysis",
      "umap",
      "vector_embedding"
    ]
  },
  {
    "category": "DE",
    "filename": "AWS Lambda",
    "sha": "a8c3f674c3ef4fcbf4cba5b5b2f74e857c557898",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/AWS%20Lambda.md",
    "text": "AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that allows you to run code without provisioning or managing servers. \n\nAWS Lambda is a powerful tool for building scalable, event-driven applications without the overhead of managing server infrastructure.\n\nWith AWS Lambda, you can execute your code in response to various events, such as HTTP requests via Amazon API Gateway, changes to data in an [[Amazon S3]] bucket, updates to a DynamoDB table, or messages arriving in an Amazon SQS queue.\n\nKey features of AWS Lambda include:\n\n1. **[[Event Driven Events]]**: AWS Lambda functions are triggered by events, which can come from a wide range of AWS services or custom applications.\n\n2. **Automatic Scaling**: Lambda automatically scales your application by running code in response to each trigger. Your code runs in parallel and processes each trigger individually, scaling precisely with the size of the workload.\n\n3. **Pay-as-You-Go**: You are charged based on the number of requests for your functions and the time your code executes. This means you only pay for the compute time you consume.\n\n4. **No Server Management**: AWS Lambda abstracts the underlying infrastructure, so you don't need to manage servers, patch operating systems, or worry about scaling.\n\n5. **Supports Multiple Languages**: AWS Lambda supports several [[programming languages]], including Python, Java, [[Node.JS]], C#, Ruby, and Go, among others.\n\n6. **Integration with AWS Services**: Lambda integrates seamlessly with other AWS services, allowing you to build complex, scalable applications.\n\nHere's a simple example of how AWS Lambda might be used:\n\n- You have an [[Amazon S3]]] where users upload images.\n- An AWS Lambda function is triggered whenever a new image is uploaded.\n- The Lambda function processes the image, such as generating thumbnails or extracting metadata.\n- The processed data is then stored back in S3 or sent to another AWS service for further processing.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "aws_lambda",
    "outlinks": [
      "amazon_s3",
      "programming_languages",
      "node.js",
      "event_driven_events"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "ACID Transaction",
    "sha": "1a33ee5e9a2f815b359f1a67ec0daf2c0ee03a80",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ACID%20Transaction.md",
    "text": "An ACID [[Transaction]] ensures that either all changes are successfully committed or rolled back, preventing the database from ending up in an inconsistent state. This guarantees the integrity of the data throughout the transaction process.\n\n### Key Properties of ACID Transactions\n\n1. Atomicity: This property ensures that transactions are treated as a single, indivisible unit. If any part of the transaction fails, the entire transaction is rolled back, and none of the changes are applied. Users do not see intermediate states of the transaction.\n\n2. Consistency: Transactions must leave the database in a valid state, adhering to all defined constraints. If a transaction violates a constraint, it is rolled back to maintain the database's stable state.\n\n3. Isolation: This property ensures that concurrent transactions do not interfere with each other. Each transaction operates independently, and the results of one transaction are not visible to others until it is committed.\n\n4. Durability: Once a transaction has been committed, the changes are permanent, even in the event of a system failure. The data remains intact and recoverable.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "acid_transaction",
    "outlinks": [
      "transaction"
    ],
    "inlinks": [
      "data_lakehouse",
      "delta_tables_in_databricks",
      "schema_evolution",
      "transaction"
    ]
  },
  {
    "category": "DE",
    "filename": "Ada boosting",
    "sha": "2c4d432fd106b58dc1ed008a8a096a4717943579",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Ada%20boosting.md",
    "text": "Resources:\n[LINK](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n# Overview:\n\nAda Boosting short for ==Adaptive Boosting==, is a specific type of [[Boosting]] algorithm that focuses on improving the accuracy of predictions by ==combining multiple weak learners== into a strong learner. It is particularly known for its ==simplicity== and effectiveness in classification tasks.\n\n### How AdaBoost Works:\n\n1. **Base Learners**: In AdaBoost, the base learners are typically low-depth trees, also known as ==stumps==. These are simple models that perform slightly better than random guessing.\n\n2. **Sequential Training**: AdaBoost trains these stumps sequentially. Each stump is trained to correct the errors made by the previous stumps. This sequential approach ensures that each new model focuses on the data points that were misclassified by earlier models.\n\n3. **Weighting**: After each stump is trained, ==AdaBoost assigns a weight to it based on its accuracy==. More accurate stumps receive higher weights, giving them more influence in the final prediction.\n\n4. **Error Focus**: The algorithm ==increases the weights of the misclassified data points==, making them more prominent in the training of the next stump. This ensures that subsequent models pay more attention to the difficult-to-classify instances.\n\n5. **Final Prediction**: The final prediction is a weighted sum of the predictions from all the stumps. The stumps with higher accuracy have more say in the final classification.\n\n# Further Understanding\n\n### Creating a Forest with AdaBoost:\n\nTo create a forest using AdaBoost, you start with a [[Decision Tree]] or [[Random Forest]] approach, but instead of using full-sized trees, you use stumps. \n\nThese stumps are trained sequentially, with each one focusing on the errors of the previous stumps. \n\nThe final prediction is a weighted sum of the predictions from all the stumps, where more accurate stumps have more influence on the final outcome.\n\n### Key Differences from [[Random Forest]]:\n\n- **Tree Depth**: In [[Random Forest]], full-sized trees are used, and each tree gets an equal say in the final prediction. In contrast, AdaBoost uses low-depth trees (stumps) and assigns different weights to each stump based on its accuracy.\n\n- **Order and Sequence**: In AdaBoost, the order of the stumps is important because errors are passed on in sequence. In [[Random Forest]], trees are built independently and simultaneously.\n\n### Advantages of AdaBoost:\n\n- **Increased Accuracy**: By focusing on the errors of previous models, AdaBoost can significantly improve the accuracy of predictions.\n- **Simplicity**: AdaBoost is relatively simple to implement and understand compared to other ensemble methods.\n- **Flexibility**: It can be applied to various types of base models and is not limited to a specific algorithm.\n\n### Challenges of AdaBoost:\n\n- **Sensitivity to Noisy Data**: AdaBoost can be sensitive to noisy data and outliers, as it focuses heavily on correcting errors.\n- **Complexity**: While simpler than some other boosting methods, AdaBoost can still be computationally intensive due to its sequential nature.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "ada_boosting",
    "outlinks": [
      "decision_tree",
      "random_forest",
      "boosting"
    ],
    "inlinks": [
      "boosting"
    ]
  },
  {
    "category": "DE",
    "filename": "Apache Iceberg",
    "sha": "cf97e49d8d86838432bba9bcaa8f4deaa53329ad",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Apache%20Iceberg.md",
    "text": "Apache Iceberg is an open table format for huge analytic datasets. It manages large-scale data stored in files (like [[Parquet]] or ORC) on distributed storage systems (like HDFS, [[Amazon S3|S3]], or GCS).\n\nIn short, Iceberg turns your [[Data Lake]] into a [[Data Warehouse]]-like system with strong guarantees and high performance.\n### Key Features:\n\n* [[Schema Evolution]] and partition evolution without rewriting data.\n* Hidden partitioning: simplifies queries by abstracting partition logic.\n* ACID transactions for consistency during concurrent reads/writes.\n* Time travel and rollback via metadata snapshots.\n* Optimized for engines like [[Apache Spark|Spark]], Trino, Flink, Dremio, and [[Snowflake]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "storage"
    ],
    "normalized_filename": "apache_iceberg",
    "outlinks": [
      "schema_evolution",
      "parquet",
      "amazon_s3",
      "snowflake",
      "apache_spark",
      "data_lake",
      "data_warehouse"
    ],
    "inlinks": [
      "data_lakehouse"
    ]
  },
  {
    "category": "DE",
    "filename": "Adding a database to PostgreSQL",
    "sha": "34d0163426465933c3420bba02a0b65b76391489",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Adding%20a%20database%20to%20PostgreSQL.md",
    "text": "### How to Add a Database to PostgreSQL  \n\n### Using pgAdmin (GUI)  \n\n1. Open pgAdmin and log in.  \n2. In the Object Explorer, right-click Databases → Create → Database.  \n3. Enter Database Name (e.g., `mydatabase`).  \n4. Choose an Owner (optional).  \n5. Click Save.  \n### Using Python (`psycopg2`)  \n\nIf you're using Python (e.g., in a Jupyter Notebook), install the `psycopg2` package if needed:  \n\n```python\n!pip install psycopg2-binary\n```\n\nThen, run this script to create a PostgreSQL database:  \n\n```python\nimport psycopg2\n\n# Connect to the PostgreSQL server (default 'postgres' database)\nconn = psycopg2.connect(\n    dbname=\"postgres\",  # Default DB to connect before creating a new one\n    user=\"postgres\",\n    password=\"your_password\",\n    host=\"localhost\"\n)\nconn.autocommit = True  # Required for CREATE DATABASE\ncursor = conn.cursor()\n\n# Create a new database\ncursor.execute(\"CREATE DATABASE mydatabase;\")\n\n# Close connection\ncursor.close()\nconn.close()\nprint(\"Database 'mydatabase' created successfully!\")\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "SQL"
    ],
    "normalized_filename": "adding_a_database_to_postgresql",
    "outlinks": [],
    "inlinks": [
      "pgadmin"
    ]
  },
  {
    "category": "DE",
    "filename": "Aggregation",
    "sha": "da2af2e30f9927b21ed2a1ee04a2d1a9a89c101e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Aggregation.md",
    "text": "Summarizing data for analysis ([[Pandas Pivot Table]] and [[Groupby]]).\n\nIn [[DE_Tools]] see:\n\t- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/group_by.ipynb\n\t- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "transformation"
    ],
    "normalized_filename": "aggregation",
    "outlinks": [
      "de_tools",
      "groupby",
      "pandas_pivot_table"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ]
  },
  {
    "category": "DE",
    "filename": "Attack mitigation",
    "sha": "41334d321ed7828318c7df6f0d9a7c814a571587",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Attack%20mitigation.md",
    "text": "### Bot Detection and MFA\n\n- Bot detectors analyze user behavior (e.g., rapid typing, unusual navigation patterns).\n- Multi-Factor Authentication (MFA):  \n  Even if credentials are leaked, the attacker can't easily log in without the second factor (e.g., SMS, authenticator app).\n\n### Least Privilege Permissions\n\nLeast Privilege Principle:  \n  Every user, service, and program should operate with only the minimum privileges needed to perform their tasks.\n  \n- Benefit:  \n  Limits the potential damage if a system is compromised.\n\nExample:  \nA web server should not have database admin rights unless necessary.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "security"
    ],
    "normalized_filename": "attack_mitigation",
    "outlinks": [],
    "inlinks": [
      "attack_types",
      "data_security"
    ]
  },
  {
    "category": "DE",
    "filename": "Attack types",
    "sha": "5e1cb1ec9de5415affa43c072a3efb3fd54b8ac5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Attack%20types.md",
    "text": "Related notes:\n- [[Attack mitigation]]\n### Cache Poisoning and Denial of Service\n\n| Attack | Idea | Example |\n|:------|:-----|:--------|\n| Cache Poisoning | Attacker manipulates cache content to serve malicious responses. | Poisoning a CDN cache to distribute bad scripts. |\n| Denial of Service (DoS) | Overwhelm servers so they can't respond to legitimate requests. | Botnet floods a login page until it crashes. |\n\n### Credential Stuffing and Password Risks\n\nCredential Stuffing:  \n  Attackers use previously leaked usernames/passwords from breaches to try logging into other sites.\n\nWhy it works:  \n  People reuse the same passwords across multiple accounts.\n\nMitigation/Defenses:\n- Enforce strong password policies.\n- Deploy bot detection (e.g., CAPTCHA, behavior analysis).\n- Use MFA (Multi-Factor Authentication) to require an extra step beyond password.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "security"
    ],
    "normalized_filename": "attack_types",
    "outlinks": [
      "attack_mitigation"
    ],
    "inlinks": [
      "data_security"
    ]
  },
  {
    "category": "DE",
    "filename": "Benefits of Data Transformation",
    "sha": "cefd40f3e80953d0323d3647eb6c073d727b0c78",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Benefits%20of%20Data%20Transformation.md",
    "text": "## Benefits of [[Data Transformation]]  \n\n- Efficiency: Faster query performance.  \n- [[Interoperability]]: Converting data into the required format for target systems.  \n- Enrichment: Adding contextual data for better insights.  \n- [[Data Quality]]: Validating, cleansing, and deduplicating data.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "benefits_of_data_transformation",
    "outlinks": [
      "data_quality",
      "interoperability",
      "data_transformation"
    ],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "DE",
    "filename": "Azure",
    "sha": "c2628525093fda60c9f93004455c48cfdf871341",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Azure.md",
    "text": "Public cloud computing platform from Microsoft offering various services like infrastructure, data storage, and machine learning.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "storage"
    ],
    "normalized_filename": "azure",
    "outlinks": [],
    "inlinks": [
      "data_engineering_tools",
      "github_actions"
    ]
  },
  {
    "category": "DE",
    "filename": "Big Data",
    "sha": "25efa6b1389a635cebd3b422f67c94a17963523d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Big%20Data.md",
    "text": "The concept of Big Data revolves around datasets that are too large or complex to be managed using traditional data processing techniques. It’s characterized by four main attributes, commonly referred to as the ==Four V’s:==\n\n- **Volume**: The sheer amount of data being generated, often in terabytes, petabytes, or even exabytes.\n- **Variety**: The diversity in data types, including structured, semi-structured, and unstructured data (e.g., text, images, videos).\n- **Velocity**: The speed at which data is generated and needs to be processed in real-time or near-real-time.\n- **Veracity**: The uncertainty or quality of the data, addressing issues like noise, biases, or incomplete data.\n\nBig Data Technologies\n- [[Apache Spark]]\n- [[Hadoop]]\n- [[Scala]]\n- [[Databricks|Databricks]]\n\nHandling big data involves:\n- Distributed storage systems/ [[Data Storage]]: Ensuring that data is split and stored across multiple machines for redundancy and speed.\n- Processing frameworks: Using tools like [[Apache Spark|Spark]] or Hadoop to process data efficiently in parallel.\n- Cloud platforms: Leveraging cloud infrastructure (e.g., Azure, AWS, Google Cloud) to scale resources dynamically based on workload.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "storage"
    ],
    "normalized_filename": "big_data",
    "outlinks": [
      "apache_spark",
      "databricks",
      "hadoop",
      "data_storage",
      "scala"
    ],
    "inlinks": [
      "databricks",
      "hadoop",
      "parquet",
      "pyspark",
      "scala"
    ]
  },
  {
    "category": "DE",
    "filename": "BigQuery",
    "sha": "d680d2a1961c0c5d82a27fad65dc45e31da84d2b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/BigQuery.md",
    "text": "cloud-based [[Data Warehouse]]\n\nBigQuery is a fully managed, serverless data warehouse offered by [[Google]] Cloud Platform (GCP). It is designed to handle large-scale data analytics and allows users to run fast SQL queries on massive datasets. \n\n1. **Serverless Architecture:** BigQuery is serverless, meaning users do not need to manage any infrastructure. Google handles the provisioning of resources, scaling, and maintenance, allowing users to focus on analyzing data.\n\n2. **Scalability:** BigQuery can scale to handle petabytes of data, making it suitable for large datasets and complex queries.\n\n3. **SQL Support:** BigQuery supports standard SQL, making it accessible to users familiar with SQL syntax. It also offers extensions for advanced analytics.\n\n4. **Real-Time Analytics:** BigQuery can ingest streaming data and perform real-time analytics, enabling users to gain insights from data as it arrives.\n\n5. **Integration:** BigQuery integrates seamlessly with other Google Cloud services, such as Google Cloud Storage, Google Sheets, and Google Data Studio, as well as third-party tools for data visualization and ETL (Extract, Transform, Load).\n\n6. **Machine Learning:** BigQuery ML allows users to build and deploy machine learning models directly within BigQuery using SQL, without needing to move data to another platform.\n\n7. **Security and Compliance:** BigQuery provides robust security features, including data encryption, identity and access management, and compliance with various industry standards.\n\n8. **Cost-Effective:** BigQuery uses a pay-as-you-go pricing model, where users are charged based on the amount of data processed by queries and the amount of data stored.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "database"
    ],
    "normalized_filename": "bigquery",
    "outlinks": [
      "data_warehouse",
      "google"
    ],
    "inlinks": [
      "data_engineering_tools",
      "dbt",
      "elt",
      "google_cloud_platform",
      "types_of_database_schema"
    ]
  },
  {
    "category": "DE",
    "filename": "CUDA",
    "sha": "969965fd7bb5fd2789ddee5760fe667c866cafd9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/CUDA.md",
    "text": "GPU support for [[Deep Learning]] operations.\n## Example \nHere is the performance comparison for 1 [[Epoch]],\n\n| Epoch | CPU | GPU  |\n|:------|:------|:------|\n| 1 | 43 sec | 3 sec |\n| 10 | 7 min 26 sec | 30 sec |\n\nYou can see that GPU is almost 15 times faster. We ran only one epoch for benchmarking but for actual training we have to run many epochs and also when data volume is big running deep learning without GPU can consume so much time. This is the reason why GPUs are becoming popular in the field of deep learning",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "cuda",
    "outlinks": [
      "deep_learning",
      "epoch"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "CRUD",
    "sha": "dc17177715feb31758da33b009bbfc4379a9d12d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/CRUD.md",
    "text": "Create,Read,Update,Delete.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "crud",
    "outlinks": [],
    "inlinks": [
      "database",
      "database_management_system_(dbms)",
      "graph_query_language",
      "rest_api",
      "row-based_storage"
    ]
  },
  {
    "category": "DE",
    "filename": "Cassandra",
    "sha": "e09b8c5dd6b1d2c50240bc902bbe2ed1dd0eb747",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Cassandra.md",
    "text": "an open source distributed database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. \n\nHigh Availablity & Partion\nOnly eventually consistent\n\nkey value store\n\nBest for heavy writes, medium reads\n\nBad at updates\n\na complex system in return for a lot of power and flexibility\n\ncircle storage metaphor\n\nOriginally an internal Facebook project,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "cassandra",
    "outlinks": [],
    "inlinks": [
      "database_management_system_(dbms)"
    ]
  },
  {
    "category": "DE",
    "filename": "Cloud Providers",
    "sha": "fb55b17c04f5b9b3eb31b3c9f4b818cacb89b3d8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Cloud%20Providers.md",
    "text": "Among the biggest cloud providers are [AWS](https://aws.amazon.com/), [Microsoft Azure](https://azure.microsoft.com/), [Google Cloud](https://cloud.google.com/). \n\nWhereas [[categories/devops/Databricks]] ( [Databrick](https://www.databricks.com/)) and [Snowflake](https://www.snowflake.com/) provide dedicated [[Data Warehouse]]and [[Data Lakehouse|Lakehouse]] solutions\n\n## Features\n\n[[Scaling Server]]\n[[Load Balancing]]\n[[Memory Caching]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "cloud_providers",
    "outlinks": [
      "scaling_server",
      "categories/devops/databricks",
      "memory_caching",
      "data_lakehouse",
      "load_balancing",
      "data_warehouse"
    ],
    "inlinks": [
      "data_storage",
      "parquet",
      "storage_layer_object_store"
    ]
  },
  {
    "category": "DE",
    "filename": "Coaching & Mentoring",
    "sha": "c1e3ee1fa130d68e7faa3a64765ba6fdae124d7d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Coaching%20&%20Mentoring.md",
    "text": "Coaching\n  - Short-to-medium term.\n  - Goal-oriented: focuses on specific performance improvements or skill acquisition.\n  - Typically structured and time-bound (e.g., 6 sessions over 3 months).\n  - Coach doesn’t need direct experience in the coachee’s role — they facilitate thinking.\n  - Uses questions more than advice.\n\nMentoring\n  - Long-term relationship.\n  - Development-oriented: broader career growth, professional identity, and guidance.\n  - Less structured, more relational.\n  - Mentor usually has more experience in the mentee’s field.\n  - Uses advice, sharing, and role-modelling.\n\n#### Frameworks\n\nGROW Model (most common)\n  - Goal: Define what success looks like.\n  - Reality: Explore the current situation.\n  - Options: Brainstorm possibilities.\n  - Way forward: Decide next steps.\n\nAlso see:\n- OSKAR Model (solution-focused)\n- CLEAR Model (for behavioural change)\n\nGeneral Techniques:\n  - Questioning (open-ended, reflective).\n  - Active listening.\n  - Reframing perspectives.\n  - Accountability check-ins.\n\nMentoring Approaches:\n  - Sharing experiences (stories, lessons learned).\n  - Career advice and networking introductions.\n  - Shadowing opportunities.\n  - Encouraging reflection.\n  - Building confidence and identity in a profession.\n\nSTAR (useful in mentoring conversations)\n  - Situation, Task, Action, Result\n\nABC of Mentoring\n  - Availability,Building rapport, Commitment",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "coaching_&_mentoring",
    "outlinks": [],
    "inlinks": [
      "managing_people"
    ]
  },
  {
    "category": "DE",
    "filename": "Columnar Storage",
    "sha": "cc7fbca7b317890a5c7199712457d0d7513bf6fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Columnar%20Storage.md",
    "text": "A database storage technique that stores ==data by columns== rather than rows, \n\nUseful for read-heavy operations and ==large-scale data analytics==, as it enables the retrieval of specific columns without the need to access the entire row. \n\nColumnar Storage Example (Analytical Workloads)**:\n\n| `order_id`  | `customer_id` | `order_date` | `order_amount` |\n|-------------|---------------|--------------|----------------|\n| 1           | 101           | 2024-10-01   | $100           |\n| 2           | 102           | 2024-10-02   | $150           |\n| 3           | 103           | 2024-10-03   | $200           |\nIn **columnar storage**, the data would be stored by columns, like:\n- `customer_id`: [101, 102, 103]\n\nIf you're querying for the total sales (`order_amount`) in a specific period, only the `order_amount` and `order_date` columns are accessed. \n\n\nUse case: **Data Analytics/[[OLAP]] (Online Analytical Processing)**\n- Running a query to get the **total sales for October** only needs to scan the `order_amount` and `order_date` columns, rather than scanning entire rows, faster [[Querying]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "columnar_storage",
    "outlinks": [
      "olap",
      "querying"
    ],
    "inlinks": [
      "database_storage",
      "duckdb",
      "duckdb_vs_sqlite",
      "row-based_storage",
      "types_of_database_schema",
      "vectorized_engine"
    ]
  },
  {
    "category": "DE",
    "filename": "Command Prompt",
    "sha": "7b13389ad58d1eb18ff3c30d66d15bc92f3a434d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Command%20Prompt.md",
    "text": "Command Prompt (cmd) is a text-based command-line interpreter for Windows. Although more limited than [[PowerShell]]—which supports object-oriented scripting—`cmd` remains useful for legacy compatibility, simple file and process operations, and lightweight scripting.\n\nRelated to: [[Bash]], [[PowerShell]], [[bat]]\n\nSummary Characteristics\n* Text-Based Interface: Commands operate on plain text, without the object-passing capabilities of PowerShell.\n* Limited Built-in Commands: Focused on core utilities like file handling, process control, and system inspection.\n* Direct Execution: Commands are interpreted and executed line-by-line, without support for pipelines.\n\n#### Navigating the File System\n* Change Directory:\n  ```cmd\n  cd C:\\path\\to\\directory\n  ```\n* List Directory Contents:\n  ```cmd\n  dir\n  ```\n* List in Bare Format:\n  ```cmd\n  dir /b\n  ```\n\n#### Managing Files and Directories\n- Creating many folders\n```cmd\nmkdir time_series_analysis\ncd time_series_analysis\ntype nul > 01_data_prep.py\ntype nul > 02_exploration.py\ntype nul > 03_decomposition_stats.py\n```\n\n* Create a Directory:\n  ```cmd\n  mkdir newfolder\n  ```\n* Delete a Directory (with contents):\n  ```cmd\n  rmdir /s /q newfolder\n  ```\n* Copy a File:\n  ```cmd\n  copy C:\\source\\file.txt D:\\destination\\\n  ```\n* Rename a File:\n  ```cmd\n  ren oldfile.txt newfile.txt\n  ```\n* Delete a File:\n  ```cmd\n  del file.txt\n  ```\n\n#### Viewing System and Network Information\n* Network Configuration:\n  ```cmd\n  ipconfig\n  ```\n* Detailed System Info:\n  ```cmd\n  systeminfo\n  ```\n\n#### Managing Processes\n* List Running Processes:\n  ```cmd\n  tasklist\n  ```\n* Kill a Process by PID:\n  ```cmd\n  taskkill /F /PID 1234\n  ```\n\n\n#### Scripting with Batch Files\n* Simple `.bat` Script Example:\n  `example.bat`\n  ```cmd\n  @echo off\n  echo Hello, World!\n  pause\n  ```\n* Run the Script:\n  ```cmd\n  example.bat\n  ```\n\n#### Environment Variables\n* View All Variables:\n  ```cmd\n  set\n  ```\n* Set a Variable:\n  ```cmd\n  set MYVAR=Hello\n  ```\n\nRedirecting Output\n* Send Command Output to File:\n\n  ```cmd\n  dir > output.txt\n  ```\n\nViewing Command History\n* Current Session Only:\n\n  ```cmd\n  doskey /history\n  ```",
    "aliases": [
      "cmd"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "command_prompt",
    "outlinks": [
      "bat",
      "powershell",
      "bash"
    ],
    "inlinks": [
      "bat",
      "command_line",
      "powershell",
      "powershell_versus_command_prompt"
    ]
  },
  {
    "category": "DE",
    "filename": "Common Table Expression",
    "sha": "6de62a4c235fac66ffbffc7de854a6b6a953d1de",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Common%20Table%20Expression.md",
    "text": "A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. \n\nThe CTE can also be used in a [[Views]]. Serve as temporary views for a single [[Querying|Queries]].\n\n```sql\nWITH cte_query AS\n(SELECT … subquery ...)\nSELECT main query ... FROM/JOIN with cte_query ...\n```\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/ExplorationsSQLite/Utilities/Common_Table_Expression.ipynb\n### Non-Recursive CTE\n\nThe non-recursive are simple where CTE is used to ==avoid SQL duplication== by referencing a name instead of the actual SQL statement. See [[Views]] simplification usage.\n\n```sql\nWITH avg_per_store AS\n  (SELECT store, AVG(amount) AS average_order\n   FROM orders\n   GROUP BY store)\nSELECT o.id, o.store, o.amount, avg.average_order AS avg_for_store\nFROM orders o\nJOIN avg_per_store avg\nON o.store = avg.store;\n```\n\n### Recursive CTE\n\nCTEs can be used in [[Recursive Algorithm]]. The recursive query calls itself until the query satisfied the condition. In a recursive CTE, we should provide a where condition to terminate the recursion.\n\nA recursive CTE is useful in querying hierarchical data such as organization charts where one employee reports to a manager or multi-level bill of materials when a product consists of many components, and each component itself also consists of many other components.\n\n```sql\nWITH levels AS (\n  SELECT\n    id,\n    first_name,\n    last_name,\n    superior_id,\n    1 AS level\n  FROM employees\n  WHERE superior_id IS NULL\n  UNION ALL\n  SELECT\n    employees.id,\n    employees.first_name,\n    employees.last_name,\n    employees.superior_id,\n    levels.level + 1\n  FROM employees, levels\n  WHERE employees.superior_id = levels.id\n)\n \nSELECT *\nFROM levels;\n```",
    "aliases": [
      "CTE"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "querying"
    ],
    "normalized_filename": "common_table_expression",
    "outlinks": [
      "recursive_algorithm",
      "querying",
      "views",
      "de_tools"
    ],
    "inlinks": [
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Components of the database",
    "sha": "3461f716e0654eca0de54015db87df31554d7e3a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Components%20of%20the%20database.md",
    "text": "[[Fact Table]] in main table that [[Dimension Table]] connect to them.\n\n![[Obsidian_CSP0FnAVD1.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "components_of_the_database",
    "outlinks": [
      "dimension_table",
      "fact_table",
      "obsidian_csp0fnavd1.png"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ]
  },
  {
    "category": "DE",
    "filename": "Covering Index",
    "sha": "fd56c434f03795299556b0da443ad8ed638cd95e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Covering%20Index.md",
    "text": "Like an [[Database Index|Index]] but for partial indexes?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "covering_index",
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "database_index"
    ]
  },
  {
    "category": "DE",
    "filename": "Crosstab",
    "sha": "4f867de2201977395f095fb2b8ebf28ce0c91587",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Crosstab.md",
    "text": "Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables.\n\n```python\n# Sample DataFrame\ndf = pd.DataFrame({\n    'Category': ['A', 'B', 'A', 'B', 'A'],\n    'Subcategory': ['X', 'X', 'Y', 'Y', 'X']\n})\n\n# Cross-tabulation of 'Category' and 'Subcategory'\ncrosstab = pd.crosstab(df['Category'], df['Subcategory'])\nprint(crosstab)\n```\n\nInput\n```\n  Category Subcategory\n0        A           X\n1        B           X\n2        A           Y\n3        B           Y\n4        A           X\n```\n\nOutput:\n```\nSubcategory  X  Y\nCategory         \nA            2  1\nB            1  1\n```\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "crosstab",
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "groupby_vs_crosstab",
      "multivariate_analysis"
    ]
  },
  {
    "category": "DE",
    "filename": "Curse of dimensionality",
    "sha": "5094def8259e7e2873fb1ba5d993c067d3c51ffd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Curse%20of%20dimensionality.md",
    "text": "The **curse of dimensionality** refers to the various phenomena that arise when working with data in high-dimensional spaces.\n\n- **Increased Data ==Sparsity==:** As the number of dimensions grows, the available data becomes increasingly sparse, making it difficult for algorithms to find ==meaningful patterns==. This sparsity can lead to poor generalization performance, as the algorithm might not have enough data points in each region of the input space to learn a robust model.\n\n- ==**Distance Metric Issues:**== In high-dimensional spaces, traditional distance metrics like Euclidean distance become less effective, as the relative difference between the nearest and farthest points diminishes. This can make it difficult for algorithms like k-nearest neighbours to identify meaningful neighbours.\n\n- **Difficulty in Visualization:** Visualizing data beyond three dimensions becomes incredibly challenging, making it difficult to gain insights from the data and understand the behaviour of machine learning models.\n\n### Examples of the Curse of Dimensionality\n\n**Vulnerability of [[Ngrams]] [[Language Models]]:** \n\nClassical n-gram language models in [[NLP]], which rely on counting the occurrences of word sequences, are particularly vulnerable to the curse of dimensionality. As the vocabulary size and the value of 'n' increase, the number of possible n-grams grows exponentially, making it impossible to observe most of them in even a massive training set.\n### Addressing the Curse of Dimensionality\n\nWhile the curse of dimensionality presents significant challenges, there are techniques to mitigate its effects:\n\n- **[[Dimensionality Reduction]]:** Techniques like Principal Components Analysis (PCA), Factor Analysis, and [[Multidimensional Scaling]] (MDS) can reduce the number of features while retaining essential information, making it easier to visualize and analyze data and train machine learning models.\n\n- **[[Feature Selection]]:** Identifying and selecting the most relevant features for a given task can improve model performance and reduce computational complexity.\n\n- **Distributed Representations:** Using distributed representations, where information is encoded across multiple features rather than a single one, can help overcome the limitations of one-hot encodings in high-dimensional spaces.\n\n- **[[Regularisation]]:** Techniques like weight decay in neural networks can help prevent overfitting and improve generalization performance, particularly in high-dimensional settings\n\n- **[[Manifold Learning]]:** Manifold learning methods assume that the data lies on a lower-dimensional manifold embedded in a high-dimensional space. By learning this manifold structure, these methods can reduce dimensionality while preserving nonlinear relationships in the data.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning"
    ],
    "normalized_filename": "curse_of_dimensionality",
    "outlinks": [
      "regularisation",
      "manifold_learning",
      "ngrams",
      "dimensionality_reduction",
      "language_models",
      "nlp",
      "feature_selection",
      "multidimensional_scaling"
    ],
    "inlinks": [
      "cluster_seperation",
      "dimensionality_reduction"
    ]
  },
  {
    "category": "DE",
    "filename": "Cypher",
    "sha": "388c0b026ccf19c6b5d7b19a7c8ef244cf9317a9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Cypher.md",
    "text": "Cypher is a graph query language designed to be:\n\n- Declarative: you say *what* you want, not *how* to get it.\n- Pattern-based: you express queries as graph patterns.\n- Human-readable\n\nRelated to:\n- [[neo4j]]\n- [[Querying]]\n- [[APOC Procedures]]\n- [[Indexing in cypher]]\n\n## Core Components\n\n| Concept         | Syntax                  | Example                          |\n|-----------------|--------------------------|----------------------------------|\n| Node        | `(n)`                    | `(n:Person)`                     |\n| Relationship| `-[r:TYPE]->`             | `-[:FRIENDS_WITH]->(friend)`      |\n| Property    | `{key: value}`            | `{name: 'Alice'}`                |\n| Labels      | `:Label`                  | `:Person`                        |\n| Alias       | `variable` names          | `alice`, `friend`, etc.           |\n\n## Common Keywords\n\n| Keyword   | Purpose                      | Example                               |\n|-----------|-------------------------------|---------------------------------------|\n| `MATCH`   | Find pattern                  | `MATCH (n:Person)`                    |\n| `WHERE`   | Add condition                 | `WHERE n.age > 30`                    |\n| `RETURN`  | Output results                | `RETURN n.name`                       |\n| `CREATE`  | Insert nodes/relationships    | `CREATE (n:Person {name: 'Bob'})`      |\n| `MERGE`   | Create if not exists           | `MERGE (n:Person {name: 'Charlie'})`   |\n| `SET`     | Update properties             | `SET n.age = 40`                      |\n| `DELETE`  | Remove nodes/relationships    | `DELETE n`                            |\n\n## Example 1: Find Alice’s Friends\n\n```cypher\nMATCH (alice:Person {name: 'Alice'})-[:FRIENDS_WITH]->(friend)\nRETURN friend.name\n```\n\n- `MATCH`: pattern starts with `alice` node.\n- `-[:FRIENDS_WITH]->(friend)`: outgoing relationship to `friend`.\n- `RETURN`: names of friends.\n\n## Example 2: Create a New Person Node\n\n```cypher\nCREATE (p:Person {name: 'David', age: 29})\n```\n\n- `CREATE`: create node `p`.\n- `:Person`: label.\n- Properties: `name`, `age`.\n\n## Example 3: Connect Two People\n\n```cypher\nMATCH (alice:Person {name: 'Alice'}), (bob:Person {name: 'Bob'})\nCREATE (alice)-[:FRIENDS_WITH]->(bob)\n```\n\n- Find both `alice` and `bob`.\n- Create a `FRIENDS_WITH` relationship from Alice to Bob.\n\n## Example 4: Conditional Query\n\n```cypher\nMATCH (p:Person)\nWHERE p.age > 30\nRETURN p.name, p.age\n```\n\n- Find all `Person` nodes with `age > 30`.\n- Return their `name` and `age`.\n\n## Example 5: Shortest Path\n\n```cypher\nMATCH path = shortestPath((a:Person {name: 'Alice'})-[:FRIENDS_WITH*]-(b:Person {name: 'Charlie'}))\nRETURN path\n```\n\n- Find the shortest path between Alice and Charlie.\n- `*` means any number of `FRIENDS_WITH` hops (edges).\n\n# Tips for Writing Cypher\n\n- Use Aliases: `(a:Person)` makes later referencing (`a.name`) easier.\n- Direction Matters: `-[:TYPE]->` vs `<-[:TYPE]-`.\n- Wildcard Relationships: `-[:TYPE*1..3]->` allows 1 to 3 hops.\n- Optional Matches: `OPTIONAL MATCH` to avoid dropping records if some parts are missing.\n\n\nExcellent — let's go through these topics carefully and systematically, with simple examples and clear breakdowns.  \n\n---\n\n# 1. Aggregations in Cypher (#cypher)\n\nIn Cypher, you can aggregate data much like SQL — examples include `COUNT()`, `AVG()`, `SUM()`, `MIN()`, `MAX()`.\n\n## Basic Syntax\n\n| Function | Purpose                      | Example                        |\n|----------|-------------------------------|--------------------------------|\n| `COUNT()`| Count records                 | `COUNT(p)`                     |\n| `AVG()`  | Average a numeric field       | `AVG(p.age)`                   |\n| `SUM()`  | Sum numeric fields            | `SUM(p.age)`                   |\n| `MIN()`  | Find minimum                  | `MIN(p.age)`                   |\n| `MAX()`  | Find maximum                  | `MAX(p.age)`                   |\n\n---\n\n## Example: Count number of people\n\n```cypher\nMATCH (p:Person)\nRETURN COUNT(p) AS number_of_people\n```\n\n- `MATCH` all nodes with label `Person`.\n- `COUNT(p)` counts how many nodes.\n\n---\n\n## Example: Average age of friends\n\n```cypher\nMATCH (:Person {name: 'Alice'})-[:FRIENDS_WITH]->(friend)\nRETURN AVG(friend.age) AS avg_friend_age\n```\n\n- Find Alice’s friends.\n- Compute their average age.\n\n---\n\n# 2. Returning Paths and Subgraphs (#cypher)\n\nIn Neo4j, you can return not only *nodes* but entire *paths*.\n\n## Path Syntax\n\n```cypher\nMATCH path = (a:Person {name: 'Alice'})-[:FRIENDS_WITH*]->(b:Person)\nRETURN path\n```\n\n- `path = (...)` captures the whole path (nodes + relationships).\n\n## Visualizing\n\nIn Neo4j Browser or Bloom, returning a `path` shows an actual graph visualization, not just rows.\n\n---\n\n## Example: Return all paths of up to 2 hops\n\n```cypher\nMATCH path = (a:Person)-[:FRIENDS_WITH*1..2]->(b:Person)\nRETURN path\n```\n\n- `*1..2` means 1 to 2 relationships deep.\n\n---\n\n# 3. `CREATE` vs `MERGE` (#cypher)\n\nThis is very important to understand:\n\n| Action | Meaning | Key Behavior |\n|--------|---------|--------------|\n| `CREATE` | Always create a new node or relationship. | No checking if it already exists. |\n| `MERGE`  | Search first, create only if not found.   | Like SQL `INSERT OR IGNORE`. |\n\n---\n\n## Example: CREATE\n\n```cypher\nCREATE (p:Person {name: 'Alice'})\n```\n- Every time you run it, a new node will be created.\n\nIf you run it 5 times, you get 5 nodes!\n\n---\n\n## Example: MERGE\n\n```cypher\nMERGE (p:Person {name: 'Alice'})\n```\n- If a `Person` with `name: 'Alice'` already exists, no new node is created.\n- If none exists, a new node is created.\n\n---\n\n## Important Note\n\n- `MERGE` matches the entire pattern: both labels and properties.\n- If only partial matching is done, it may create *extra* unwanted nodes.\n\nTip: Be careful when using `MERGE` with relationships — you often have to `MATCH` the nodes first, then `MERGE` the relationship separately.\n\nExample:\n\n```cypher\nMATCH (a:Person {name: 'Alice'}), (b:Person {name: 'Bob'})\nMERGE (a)-[:FRIENDS_WITH]->(b)\n```\n\n---",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "querying"
    ],
    "normalized_filename": "cypher",
    "outlinks": [
      "querying",
      "indexing_in_cypher",
      "neo4j",
      "apoc_procedures"
    ],
    "inlinks": [
      "graph_query_language",
      "neo4j",
      "text2cypher"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Architect",
    "sha": "60dbf3af01c6369cc4b9a493bd210eefc83de3d1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Architect.md",
    "text": "Data Architect\n  - Designs and manages the data infrastructure.\n  - Ensures data is stored, organized, and accessible for analysis.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "data_architect",
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Architecture",
    "sha": "6608d0b53b395824884de24921395ccbec9f90aa",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Architecture.md",
    "text": "Data Architecture Definition:\n  - Identifies organizational data needs\n  - Models data knowledge\n  - Designs data management features\n  - Bridges business strategy and technology\n\nImportance:\n  - Ensures data availability, integrity, and usability\n  - Provides unique data vision\n  - Standardizes data structure\n  - Improves data quality\n  - Enables Data-as-a-Product approach\n\nThree Main Layers:\n  1. Business Layer:\n     - Focuses on business processes\n     - Contains Data Domains and Business Objects\n  2. Logical Layer:\n     - Defines dimensional business model\n     - Contains Functional Objects\n     - Technology-agnostic but business-aware\n  3. Technical Layer:\n     - Implements physical solutions\n     - Contains Technical Objects\n     - Includes development documentation\n\nFramework Goals:\n  - Better data asset management\n  - Improved data accessibility\n  - Clear roles (RACI matrix)\n  - Standardized data rules\n  - Enhanced data quality",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_architecture",
    "outlinks": [],
    "inlinks": [
      "framework_for_models"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Cleansing",
    "sha": "803c54696dbbb73f072ff1ed90d03acdd5142577",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Cleansing.md",
    "text": "Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [[Data Quality]] for analysis. Involves:\n\n- [[uncategorised/Outliers|Handling Outliers]]\n- [[Handling Missing Data]]\n- [[Handling Different Distributions]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning/Dataframe_Cleaing.ipynb\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning\n\nRelated terms:\n- [[Data Selection]]\n\nFollow-up questions:\n- [[Deleting rows or filling them with the mean is not always best ]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "portal",
      "transformation"
    ],
    "type": "term",
    "normalized_filename": "data_cleansing",
    "outlinks": [
      "handling_different_distributions",
      "handling_missing_data",
      "data_selection",
      "deleting_rows_or_filling_them_with_the_mean_is_not_always_best",
      "uncategorised/outliers",
      "de_tools",
      "data_quality"
    ],
    "inlinks": [
      "anomaly_detection",
      "boxplot",
      "data_analyst",
      "data_transformation",
      "fuzzywuzzy",
      "pandas_stack",
      "preprocessing",
      "why_use_er_diagrams"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Contract",
    "sha": "85f98a293c763eb0cc9e942a61a818851a01ffb7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Contract.md",
    "text": "Pattern to handle schema changes\n\nPattern to apply to organisation using tools they have.\n\nTooling:\n- [[dbt]]\n\nData contracts help prevent preventable data issues while increasing collaboration and reducing costs.\n\nA data contract is an agreed interface between\nthe generators of data and its consumers. It sets\nthe expectations around that data, defines how\nit should be governed, and facilitates the\nexplicit generation of quality data that meets\nthe business requirements.\n\nInterfaces:\n- [[API]] for data.\n\nA document to codify what has been agreed.\n\nQ: How does the Data Contract allow for contextual rules? Example the same schema can support multiple products in our org but the DQ rules can be different for different Products\nA: Data contracts are particular business. Could use ==inheritance== of rules in data contracts - basic template. To get standardisation across products.\n\n[[Data Contract]]\nBy establishing a [[Data Contract]] and building interfaces based on it, organizations can improve data quality. Implementing structured agreements and automated change management processes can help business users, who may not be data experts, produce higher-quality data ([[Data Quality]]).\n\n### Images\n\n![[Pasted image 20250312163351.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "data_contract",
    "outlinks": [
      "dbt",
      "api",
      "pasted_image_20250312163351.png",
      "data_contract",
      "data_quality"
    ],
    "inlinks": [
      "data_contract",
      "data_quality"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Deployment",
    "sha": "c188058e3ab7983c6218d1b33cc79687a34e0534",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Deployment.md",
    "text": "Purpose\n\n* Move the final model into a production environment.\n* Enable the model to generate actionable outputs, automate processes, or support decision-making.\n* Ensure deployment is aligned with business context and prior analysis.\n\nKey Considerations\n\n* Use documentation from [[Data Understanding]] and [[Data Preparation]] to provide context, especially if deployment occurs long after initial data work.\n* Ensure that deployment supports the intended business objectives and integrates seamlessly with existing systems.\n* Maintain traceability and reproducibility of the model and its results.\n\nDeployment Activities\n\n* Integrate the model into operational systems or pipelines.\n* Automate reporting or decision-support functions.\n* Monitor model performance and maintain logs for updates or retraining.\n\nRelated Concepts\n\n* [[Model Deployment]]: The broader process of operationalizing predictive or analytical models.\n\nOutcome\n\n* A working, production-ready model that delivers value and can be maintained over time.\n* Documentation ensures understanding of model context, data sources, and preparation steps.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_deployment",
    "outlinks": [
      "data_understanding",
      "model_deployment",
      "data_preparation"
    ],
    "inlinks": [
      "data_mining_-_crisp"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Dictionary",
    "sha": "183e9405b4e63abd57feade3dd91251d911ccf4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Dictionary.md",
    "text": "A mapping that explains the variables of a [[Data Sources]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_dictionary",
    "outlinks": [
      "data_sources"
    ],
    "inlinks": [
      "data_understanding"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Drift",
    "sha": "19b53e599c2b9d58754223c6303a48083fdc4406",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Drift.md",
    "text": "Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead to decreased model performance, as the model may struggle to make accurate predictions on data that differ from its training set. \n\nRegular monitoring and prompt response to data drift are essential to maintain the effectiveness of ML models in dynamic production environments.\n\nConcepts:\n\n- Data drift involves changes in input data distributions\n- Concept drift pertains to alterations in the relationship between inputs and outputs.\n- [[Performance Drift]] drift relates to changes in model outputs. \n\n**Training-Serving Skew:** This refers to discrepancies between training data and production data, which can arise from data drift or other factors, leading to performance issues. \n\n==**Detecting Data Drift:**==\n\nIdentifying data drift is crucial for maintaining model accuracy. Techniques include:\n\n- **Statistical Hypothesis Testing:** Assessing whether differences between training and production data distributions are statistically significant.\n\n- **Distance Metrics:** Quantifying the divergence between data distributions using measures like Kullback-Leibler divergence or Kolmogorov-Smirnov tests.\n\n- **Monitoring Summary Statistics:** Regularly reviewing key statistical indicators (e.g., mean, variance) of input features to detect anomalies.\n\n**Addressing Data Drift:**\n\nOnce detected, strategies to manage data drift include:\n\n1. **Data Quality Checks:** Ensure that the drift isn't due to data quality issues, such as errors in data collection or processing. \n\n2. **Investigate the Drift:** Analyze the source and nature of the drift to understand its implications.\n\n3. **Model Retraining:** Update the model using recent data to help it adapt to new patterns.\n\n4. **Model Rebuilding:** In cases of significant drift, it may be necessary to redesign the model architecture or feature engineering processes.\n\n5. **Fallback Strategies:** Implement alternative decision-making processes, such as rule-based systems or human judgment, when the model's reliability is compromised.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "statistics"
    ],
    "normalized_filename": "data_drift",
    "outlinks": [
      "performance_drift"
    ],
    "inlinks": [
      "challenges_to_model_deployment",
      "machine_learning_operations",
      "model_observability",
      "performance_drift"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Engineering Portal",
    "sha": "44656863db6d876a222d0e6d1bb630005238f7e1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Engineering%20Portal.md",
    "text": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include:\n\n- [[MySql]]\n- [[PostgreSQL]]\n\n\nThey facilitate efficient [CRUD.md](obsidian://open?vault=content&file=standardised%2FCRUD.md) operations and transactional processing ([OLTP.md](obsidian://open?vault=content&file=standardised%2FOLTP.md)), structured by a [Database Schema.md](obsidian://open?vault=content&file=standardised%2FDatabase%20Schema.md) that organizes data into tables and relationships.\n\n## Key Features\n\n- **[[structured data]]**: Organized for efficient CRUD operations, allowing reliable access.\n- **Relational Databases**: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy.\n\nStructure\n- Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval.\n\nFlexibility\n- Databases have a flexible schema that adapts to evolving requirements, unlike static solutions like spreadsheets.\n\nRelated Ideas:\n- [[Spreadsheets vs Databases]]\n- [[Database Management System (DBMS)]]\n- [[Components of the database]]\n- [[Relating Tables Together]]\n- [[Turning a flat file into a database]]\n- [[Database Techniques]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "data_engineering_portal",
    "outlinks": [
      "mysql",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "spreadsheets_vs_databases",
      "database_techniques",
      "postgresql",
      "structured_data",
      "components_of_the_database",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "data_engineer"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Engineering",
    "sha": "bc4d840609cf4d980baadf549eb033105748fa1f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Engineering.md",
    "text": "The definition from the [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/), as it’s one of the most recent and complete: \n\n> Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, [[Data Management]], DataOps, data architecture, orchestration, and software engineering.\n\nA [[Data Engineer]] today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the [Data Engineering Lifecycle](Data%20Lifecycle%20Management.md) and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and [[Interoperability]].\n\nData Engineering helps also overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):\n- More transparency as tools are open-source mostly\n- More frequent data loads\n- Supporting [Machine Learning](Machine%20Learning.md) capabilities \n\nCompared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.\n\nWith that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** which is used in engineering with tools alike [[Apache Airflow]], [dagster](dagster.md), [[Prefect]] as well as data science with powerful libraries.\n\nAs a data engineer, you use mainly [SQL](SQL.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [[Data Pipeline]] with the tools mentioned above.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "data_engineering",
    "outlinks": [
      "data_engineer",
      "data_pipeline",
      "interoperability",
      "apache_airflow",
      "data_management",
      "prefect"
    ],
    "inlinks": [
      "data_storage",
      "data_transformation_in_data_engineering",
      "normalisation"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Engineering Tools",
    "sha": "9b8d10d1fee266be3667758e6398401d39feb239",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Engineering%20Tools.md",
    "text": "- [[Snowflake]]: [[Cloud]]-based data warehousing for scalable storage and processing.\n  - Microsoft SQL Server: [[SQL]]-based [[Relational Database]] management.\n  - [[Azure]] SQL Database: Managed relational database service on Azure.\n  - Azure Data Lake Storage: Scalable storage for big data analytics.\n  - SQL and T-SQL: Query languages for managing and querying relational databases.\n  - AWS [[Amazon S3|S3]]: Storage for data lakes.\n\n[[Data Ingestion]] Tools and Technologies:\n- [[Apache Kafka]]\n- AWS Kinesis: A cloud service for real-time data processing, enabling the collection and analysis of streaming data.\n- Google Pub/Sub: A messaging service that allows for asynchronous communication between applications, supporting real-time data ingestion.\n\n[[Data Storage]]\nTools: Amazon [[Amazon S3|S3]], Google [[BigQuery]], Snowflake.\n\n[[dbt]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "tool"
    ],
    "normalized_filename": "data_engineering_tools",
    "outlinks": [
      "data_ingestion",
      "sql",
      "data_storage",
      "apache_kafka",
      "bigquery",
      "dbt",
      "amazon_s3",
      "snowflake",
      "relational_database",
      "azure",
      "cloud"
    ],
    "inlinks": [
      "data_engineer",
      "data_ingestion",
      "data_storage"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Evaluation",
    "sha": "4a36e4d47ca5195c2024a8da83ea1112d8559463",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Evaluation.md",
    "text": "The model is thoroughly evaluated to ensure it meets the business objectives and provides actionable insights.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_evaluation",
    "outlinks": [],
    "inlinks": [
      "data_mining_-_crisp"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Hierarchy of Needs",
    "sha": "e685887a66f93f063d8d3cfa3e13654abcb43993",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Hierarchy%20of%20Needs.md",
    "text": "The Data Hierarchy of Needs is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow’s hierarchy, progressing from basic data needs to advanced capabilities:\n\n1. [[Data Collection]]: (bottom)\n   Start by collecting raw data from various sources, ensuring it's stored securely and reliably.\n\n2. [[Data Storage]] and Access:  \n   Organize and store data so it's easily accessible for those who need it, using databases or data warehouses.\n\n3. Data Cleaning and Preparation:  \n   Clean, [[Preprocessing|preprocess]], and transform data to ensure it’s accurate, consistent, and ready for analysis.\n\n4. Data Analytics:  \n   Analyze the prepared data to generate insights, identify patterns, and create reports.\n\n5. Data-Driven Decision Making:  \n   Use the insights from data analytics to inform and improve decision-making across the organization.\n\n6. Advanced Data Capabilities (AI/ML): (top)\n   Once the foundation is in place, apply advanced techniques like machine learning and artificial intelligence for predictive and prescriptive insights.\n\n![[Pasted image 20241005170237.png|500]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "data_hierarchy_of_needs",
    "outlinks": [
      "preprocessing",
      "data_storage",
      "pasted_image_20241005170237.png",
      "data_collection"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Data Integration",
    "sha": "c78f6f41b50bf879bde973144a06cf3c626d1340",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Integration.md",
    "text": "Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [[Single Source of Truth]].\n\n## Manual Integration\nManual integration involves analysts manually logging into source systems, analyzing and/or exporting data, and creating reports. \n\n### Disadvantages of Manual Integration:\n- **Time-consuming**: The process requires significant time investment.\n- **Security Risks**: Analysts need access to multiple operational systems.\n- **Performance Issues**: Running analytics on non-optimized systems can interfere with their functioning.\n- **Outdated Reports**: Data changes frequently, leading to quickly outdated reports.\n\n## [[Data Virtualization]]\n\nData virtualization is a method that allows access to data without needing to replicate it, providing a unified view of data from multiple sources.\n\n## Application Integration\nApplication integration links multiple applications to move data directly between them. \n\n### Methods of Application Integration:\n- **Point-to-Point Communications**: Direct connections between applications.\n- **Middleware Layer**: Using tools like an Enterprise Service Bus (ESB).\n- **Application Integration Tools**: Specialized tools for integrating applications.\n\n### Disadvantages of Application Integration:\n- **Data Redundancy**: May result in multiple copies of the same data across systems.\n- **Increased Costs**: Managing multiple copies can lead to higher costs.\n- **Point-to-Point Traffic**: Can create excessive traffic between systems.\n- **Performance Impact**: Executing analytics on operational systems may interfere with their functioning.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "storage"
    ],
    "normalized_filename": "data_integration",
    "outlinks": [
      "data_virtualization",
      "single_source_of_truth"
    ],
    "inlinks": [
      "apache_kafka",
      "data_virtualization",
      "digital_twin",
      "symbolic_computation"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Integrity",
    "sha": "d3e93ec0a8e436e32f274807e93106dab3914d0b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Integrity.md",
    "text": "Data integrity refers to the \n- accuracy, \n- consistency, and \n- reliability of data\n\nthroughout its lifecycle. It ensures that data remains ==unaltered== and ==trustworthy==, whether it is being \n- stored, \n- processed, \n- or transmitted. \n\nMaintaining data integrity involves implementing measures to prevent unauthorized access, corruption, or loss of data.\n\nIn the context of [[Database]] and information systems, data integrity can be enforced through:\n\n1. **Validation Rules**: Ensuring that data entered into a system meets certain criteria.\n2. **Access Controls**: Limiting who can view or modify data.\n3. **Backups**: Regularly saving copies of data to prevent loss.\n4. **Error Checking**: Using [[Checksum]] or [[Hash]] to verify data integrity during transmission.\n\n\n\n[[Data Integrity]]\n   **Tags**:,",
    "aliases": [
      "Integrity"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "management",
      "term"
    ],
    "normalized_filename": "data_integrity",
    "outlinks": [
      "database",
      "data_integrity",
      "checksum",
      "hash"
    ],
    "inlinks": [
      "anomaly_detection",
      "apache_kafka",
      "data_integrity",
      "data_lifecycle_management",
      "data_pipeline",
      "data_principles",
      "hash",
      "performance_dimensions",
      "relating_tables_together",
      "soft_deletion",
      "transaction"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Lake",
    "sha": "676a4442a81c52d1f14ceeffc79bf05b08c42bc7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Lake.md",
    "text": "A Data Lake is a storage system with vast amounts of [[unstructured data]] and [[structured data]], stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a [[Relational Database]], or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).\n\n**Definition**: A repository that ==stores diverse data types==, including structured, semi-structured, and unstructured data. If cant fit into a database.\n\nFeatures:\n- **Versatility**: Can accommodate various data formats, including videos, images, documents, and more.\n- **Raw Data Storage**: Preserves data in its raw form, suitable for advanced analytics, particularly in machine learning and AI.\n- **Data Usability**: Raw data ==may require cleaning and transformation for analytical use==, often transferred to databases or data warehouses.\n- **Use Case**: Valuable for storing large volumes of raw data, especially in contexts requiring advanced analytics and experimentation.\n\n[[unstructured data]] for predictive modeling and analysis. This leads to the creation of a **data lake**, which stores raw data without predefined schemas. \n\nThe data lake supports the following capabilities:\n-   To capture and store raw data at scale for a low cost\n-   To store many types of data in the same repository\n-   To perform [Data Transformation](Data%20Transformation.md) on the data where the purpose may not be defined\n-   To perform new types of data processing\n-   To perform single-subject analytics based on particular use cases\n\nComponents of a data lake\n\t1. [Storage Layer](term/storage%20layer%20object%20store.md)\n\t2. [Data Lake File Format](term/data%20lake%20file%20format.md)\n\t3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](Apache%20Iceberg.md), and [Apache Hudi](Apache%20Iceberg.md)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "data_lake",
    "outlinks": [
      "relational_database",
      "structured_data",
      "unstructured_data"
    ],
    "inlinks": [
      "apache_iceberg",
      "data_lakehouse",
      "data_storage",
      "databricks",
      "event_driven_events",
      "fabric",
      "hadoop",
      "snowflake"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Lakehouse",
    "sha": "adf85472f56e70c7323b4325d54284343d13c9f4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Lakehouse.md",
    "text": "A Data Lakehouse open [[Data Management]] architecture that combines the flexibility, cost-efficiency, and scale of [[Data Lake]] with the data management and [[ACID Transaction]]s of [[Data Warehouse]]  with Data Lake Table Formats ([[Delta Lake]], [[Apache Iceberg]]) that enable Business Intelligence (BI) and Machine Learning (ML) on all data.\n\nA data lakehouse is an emerging architectural approach that combines the best features of data lakes and data warehouses to provide a unified platform for storing, processing, and analyzing large volumes of structured and unstructured data.\n### Key Characteristics\n\n1. Unified Storage:\n   - Data lakehouses store data in a single repository, accommodating both structured data (like tables in a database) and unstructured data (like images, videos, and text). This eliminates the need for separate systems, simplifying data management.\n\n1. Support for Multiple Data Types:\n   - They can handle various data formats, such as CSV, JSON, [[Parquet]], and Avro, enabling flexibility in how data is ingested and stored.\n\n3. [[ACID Transaction]]:\n   - Unlike traditional data lakes, data lakehouses provide [[ACID Transaction]] which ensure reliable data operations and integrity, even in concurrent processing environments.\n\n1. Schema Enforcement:\n   - Data lakehouses can enforce [[Database Schema|schema]] at the time of data write, allowing users to define data structures while still benefiting from the flexibility of a data lake.\n\n1. Performance Optimization:\n   - They incorporate various optimization techniques, such as indexing and caching, to improve query performance and provide faster access to data.\n\n1. Integration with BI Tools:\n   - Data lakehouses are designed to work seamlessly with business intelligence (BI) tools and data analytics platforms, enabling users to derive insights without needing extensive data preparation.\n\n### Benefits\n\n1. Cost-Effectiveness:\n   - By merging the functionalities of data lakes and data warehouses, organizations can reduce the costs associated with maintaining separate systems for structured and unstructured data.\n\n1. Scalability:\n   - Data lakehouses leverage cloud storage solutions, allowing for scalable data storage that can grow with the organization’s needs.\n\n1. Data Accessibility:\n   - With a unified architecture, data from different sources can be accessed and analyzed together, breaking down silos and fostering a more holistic view of the organization’s data landscape.\n\n1. Simplified Data Pipelines:\n   - Data lakehouses streamline the [[Data Ingestion]] process, enabling organizations to build more efficient data pipelines that accommodate a variety of data sources.\n\n1. Support for Advanced Analytics:\n   - They provide a robust foundation for advanced analytics, including machine learning and real-time data processing, allowing organizations to extract actionable insights more effectively.\n\nPlatforms that implement the data lakehouse architecture include:\n- [[categories/devops/Databricks]] Lakehouse Platform: Combines data engineering, data science, and BI capabilities with a focus on collaboration.\n- [[Apache Iceberg]]: A high-performance table format for large analytic datasets that supports ACID transactions and [[Schema Evolution]].\n- [[Apache Iceberg vs Databricks]]",
    "aliases": [
      "Lakehouse"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "data_lakehouse",
    "outlinks": [
      "schema_evolution",
      "data_ingestion",
      "apache_iceberg_vs_databricks",
      "parquet",
      "apache_iceberg",
      "delta_lake",
      "acid_transaction",
      "database_schema",
      "categories/devops/databricks",
      "data_management",
      "data_lake",
      "data_warehouse"
    ],
    "inlinks": [
      "cloud_providers",
      "databricks",
      "single_source_of_truth"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Leakage",
    "sha": "c96c4997d74bec3892d2b019ebd2cd6582735ec8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Leakage.md",
    "text": "**Data Leakage** refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly optimistic model performance. It occurs when the model has access to data it shouldn't during training, such as future information or test data, which can result in misleading evaluation metrics and poor generalization to new data.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "memory_management",
      "storage"
    ],
    "normalized_filename": "data_leakage",
    "outlinks": [],
    "inlinks": [
      "cross_validation",
      "data_selection_in_ml",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "time_series_forecasting"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Lifecycle Management",
    "sha": "66eda261032c9cda3245e88842dae2a7ca5c5af5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Lifecycle%20Management.md",
    "text": "This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. \n\nUsed for maintaining [[Data Integrity]], optimizing performance, and ensuring that data-driven decisions are based on accurate and timely information. \n\nNot the same as the [[Software Development Life Cycle]]\n\nKey Stages of Full Lifecycle Management\n\n1. [[Data Ingestion]]\n2. [[Data Storage]]\n3. [[Preprocessing]]\n4. [[Data Analysis]]\n5. [[Data Visualisation]]\n6. [[Data Distribution]]\n\nData engineers must evaluate and select tools and technologies based on several [[Performance Dimensions]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "portal"
    ],
    "normalized_filename": "data_lifecycle_management",
    "outlinks": [
      "data_integrity",
      "data_ingestion",
      "data_visualisation",
      "preprocessing",
      "data_analysis",
      "data_distribution",
      "software_development_life_cycle",
      "performance_dimensions",
      "data_storage"
    ],
    "inlinks": [
      "dagster",
      "data_lineage",
      "data_principles"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Management",
    "sha": "29dc794ea4c7aad41cffed822036f58eaab38521",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Management.md",
    "text": "Data management involves overseeing processes to maintain data integrity and quality. It includes:\n\n- **Responsibility**: Identifying accountable individuals or teams.\n- **Issue Resolution**: Mechanisms for detecting and addressing data-related problems.\n\nData management ensures that a [[Data Pipeline]] operates efficiently, focusing on monitoring errors, performance issues, and [[Data Quality]].\n\n**Tools**:\n- [[Apache Airflow]]\n- Prefect\n- [[dagster]]\n\nRelated Concepts:\n- [[Database Management System (DBMS)]]\n- [[master data management]]\n- [[Data Distribution]]\n\n\n\n[[Data Management]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "management"
    ],
    "normalized_filename": "data_management",
    "outlinks": [
      "data_pipeline",
      "data_distribution",
      "dagster",
      "master_data_management",
      "apache_airflow",
      "data_quality",
      "data_management",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "data_engineer",
      "data_engineering",
      "data_lakehouse",
      "data_management",
      "data_pipeline",
      "data_principles",
      "data_roles",
      "data_steward",
      "data_storage",
      "database_schema",
      "digital_twin",
      "master_data_management",
      "snowflake_vs_hadoop",
      "spreadsheets_vs_databases"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Modeling",
    "sha": "9d38d58bcde6e9d8d7f9cb5994046c4420a2d4fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Modeling.md",
    "text": "Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. \n\nThis helps in organizing and structuring the data so it can be efficiently managed and utilized.\n\nData modelling ensures that data is logically structured and organized, making it easier to store, retrieve, and manipulate in a database.\n\nThe modelers job is to wisely choose an underlying model that reflects the reality of nature, and then use data to estimate the parameters of the model.\n\nWorkflow of Data Modeling:\n1) [[Conceptual Model]]\n2) [[Logical Model]]\n3) [[Physical Model]]\n\nTypes of Modeling:\n- Relational: Organizes data into tables.\n- Object-Oriented: Focuses on objects and their state changes, e.g., robots in a car factory.\n- Entity: Uses [[ER Diagrams]] to represent data entities and relationships.\n- Network: An extension of hierarchical models.\n- Hierarchical: Organizes data in a tree-like structure.\n\nFor flat files:\n- Various modeling techniques are applied to the data, and parameters are tuned to improve performance. Models like decision trees, neural networks, or [[Clustering]] may be used.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "data_modeling",
    "outlinks": [
      "clustering",
      "physical_model",
      "er_diagrams",
      "logical_model",
      "conceptual_model"
    ],
    "inlinks": [
      "data_mining_-_crisp",
      "database_schema"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Observability",
    "sha": "d6a49081fde56272bc989dd1ad83cfe778119f61",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Observability.md",
    "text": "Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [[Data Quality]], reliability, and availability. \n\nIt covers various aspects, such as data quality, pipeline health, metadata management, and infrastructure performance. By tracking key metrics and [[uncategorised/Outliers|anomalies]], it helps detect issues like data freshness problems, schema changes, or pipeline failures before they impact downstream processes or users.\n### Categories of Observability\n\nAuto-profiling Data:\n\nAutomatically tracks data attributes, such as row count, column types, data distributions, and schema changes.\n - Bigeye: Provides ML-driven threshold tests and automatic alerts when data drifts beyond expected ranges.\n - Datafold: Integrates with GitHub to run data diffs between environments, offering insights into differences between datasets during development.\n - Monte Carlo: Enterprise-focused with data lake integrations for comprehensive observability.\n - Metaplane: Offers a high level of configuration and both out-of-the-box and custom tests.\n\nPipeline Testing:\n\nEnsures that data transformation pipelines are functioning correctly by verifying the quality and accuracy of data as it moves through different stages.\n - Great Expectations: An open-source tool that allows you to define tests and automatically generate documentation for those tests, promoting transparency in data quality checks.\n - Soda: Offers pipeline testing with the flexibility of a self-hosted option for more control over data quality monitoring.\n - [[dbt]]tests: Integrated with [[dbt]] Core and dbt Cloud, allowing testing during the transformation process in a dbt project.\n\n Infrastructure Monitoring:\n \nMonitors the health and performance of the underlying data infrastructure, such as databases, pipelines, and servers, to prevent failures and bottlenecks.\n - DataDog: Provides deep monitoring capabilities, including for Airflow, containers, and custom metrics, allowing visibility at various layers of the data stack.\n\n### Managing Metadata\n\nManaging metadata is critical for observability, as it provides context and lineage for your data. Metadata can include:\n\n- Technical Metadata: Information about the dataset’s structure, such as table schema, data types, and column descriptions.\n- Operational Metadata: Information about the dataset’s freshness, when it was last updated, and the number of records processed.\n- Business Metadata: Describes the meaning of data, such as field definitions and business rules, helping stakeholders understand the context and usage of the dataset.\n\nHow to Manage Metadata:\n\n- Manual Documentation: Teams may manually document metadata, but this can be prone to human error and inconsistency.\n- Automated Metadata Management: Many modern data tools, such as data catalogs (e.g., Atlan, Alation), automatically track and manage metadata, offering insights into data lineage, schema changes, and data usage.\n- Integration with Data Pipelines: Tools like dbt also generate metadata about transformations, which can be included in downstream monitoring systems to ensure consistency and traceability.\n\n[[Data Observability]]\n- Tracking the issues.\n- Alerting and ensuring data owners fix it.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#orchestration",
      "management"
    ],
    "normalized_filename": "data_observability",
    "outlinks": [
      "data_quality",
      "data_observability",
      "uncategorised/outliers",
      "dbt"
    ],
    "inlinks": [
      "data_governance",
      "data_observability",
      "data_quality",
      "declarative_data_pipeline",
      "guardrails",
      "model_observability",
      "performance_drift",
      "prevention_is_better_than_the_cure"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Principles",
    "sha": "cf9ace59013a87b61414b0db7d8ac34ce62e7ad9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Principles.md",
    "text": "Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically.\n\n1. [[Data Quality]] Ensure data is accurate, complete, reliable, and up-to-date. High-quality data is crucial for making informed decisions.\n\n2. [[Data Governance]]: Establish clear policies and procedures for data management, including roles and responsibilities, to ensure [[Data Integrity]] and compliance with regulations.\n\n3. Data Privacy: Protect personal and sensitive information by adhering to privacy laws and regulations, such as GDPR or CCPA, and implementing appropriate security measures.\n\n4. Data Security: Safeguard data against unauthorized access, breaches, and other security threats through encryption, access controls, and regular [[Data Security]] audits.\n\n5. Data Accessibility: Ensure that data is easily accessible to those who need it while maintaining appropriate security and privacy controls. This includes providing the necessary tools and training for data access.\n\n6. Data Transparency: Maintain transparency about data collection, usage, and sharing practices. This helps build trust with stakeholders and ensures accountability.\n\n7. Data Consistency: Standardize data formats and definitions across the organization to ensure consistency and [[Interoperability]].\n\n8. Data Stewardship: Assign data stewards to oversee [[Data Management]] practices, ensuring data quality, compliance, and proper usage.\n\n9. [[Data Lifecycle Management]] Manage data throughout its lifecycle, from creation and storage to archiving and deletion, ensuring that data is retained only as long as necessary.\n\n10. Ethical Data Use: Use data ethically and responsibly, considering the potential impact on individuals and society. Avoid biases and ensure fairness in data-driven decisions.\n\n11. Data [[Documentation & Meetings]]: Maintain thorough documentation of data sources, definitions, and processes to facilitate understanding and reproducibility.\n\n12. Data Sharing and Collaboration: Encourage data sharing and collaboration within and across organizations to maximize the value of data, while respecting privacy and security constraints.\n    \n13. DRY\n\nRelated:\n- [[Performance Dimensions]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "governance",
      "portal"
    ],
    "normalized_filename": "data_principles",
    "outlinks": [
      "performance_dimensions",
      "data_integrity",
      "data_governance",
      "data_quality",
      "data_lifecycle_management",
      "interoperability",
      "data_security",
      "data_management",
      "documentation_&_meetings"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Data Quality",
    "sha": "405d4964eef2ac74f04d1d92bd3d5a0dfa20e7fc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Quality.md",
    "text": "Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis.\n\n**Definition**: Data quality refers to the ==accuracy, consistency, and reliability of data.== It is essential for maintaining trust in data-driven processes and outcomes. \n\n**Importance**: The principle of \"garbage in, garbage out\" highlights that poor-quality data leads to poor model performance.\n\nRelated terms:\n- [[Data Observability]]\n- [[Change Management]]\n- [[Prevention Is Better Than The Cure]]\n\n\nRelated terms:\n- [[Data Observability]]\n- [[Data Contract]]\n- [[Change Management]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#data_quality"
    ],
    "normalized_filename": "data_quality",
    "outlinks": [
      "prevention_is_better_than_the_cure",
      "change_management",
      "data_observability",
      "data_contract"
    ],
    "inlinks": [
      "benefits_of_data_transformation",
      "business_value_of_anomaly_detection",
      "challenges_to_model_deployment",
      "data_cleansing",
      "data_collection",
      "data_contract",
      "data_ingestion",
      "data_management",
      "data_observability",
      "data_principles",
      "data_roles",
      "data_selection_in_ml",
      "data_steward",
      "data_understanding",
      "data_validation",
      "declarative_data_pipeline",
      "determining_threshold_values",
      "digital_transformation",
      "ds_&_ml_portal",
      "eda",
      "feature_engineering_for_time_series",
      "machine_learning_algorithms",
      "master_data_management",
      "neural_network_classification",
      "performance_dimensions",
      "prevention_is_better_than_the_cure",
      "why_use_er_diagrams"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Security",
    "sha": "a619da881d495a9d0768d5c43c8a88acefa620de",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Security.md",
    "text": "Related notes:\n- [[Security Vulnerabilities]]\n- [[SQL Injection]]\n- [[Security Researcher]]\n- [[Security mitigation]]\n- [[Attack types]]\n- [[Attack mitigation]]\n\nSummary\n```plaintext\n- Attackers constantly evolve; defenses must be layered.\n- Critical risks: passwords, injection attacks, secret leaks.\n- Secure-by-default coding (e.g., React) is a good first step.\n- Security is not one-time: continuous monitoring and response is essential.\n```\n\nFlow\n1) Assume You Are Under Attack\n2) Design Multiple Defense Layers (Defense in Depth)\n3) Secure Inputs (Escape, Sanitize, Validate)\n4) Restrict Privileges (Least Privilege Principle)\n5) Monitor for Compromise (IoCs)\n6) Respond and Recover Quickly\n\n### Mindset: Always Assume You Are Under Attack\n\nThe \"always under attack\" mindset means:\n  - Assume systems are *already* being probed.\n  - Assume credentials, APIs, services can be targeted *at any moment*.\n  - Focus on reducing risk and early detection, not just reacting after breaches.\n\n### Defense in Depth: Swiss Cheese Model\n\nDefense in Depth:  \n  Multiple independent layers of security, so even if one fails, others still protect.\n\nSwiss Cheese Model:  \n  Every defense layer has holes (weaknesses), but when layered, holes are less likely to align and create a clear path for attackers.\n\nLayers might include:\n- Firewall\n- Application security\n- Authentication policies\n- Monitoring and alerting\n- Encryption\n\n### Indicators of Compromise (IoCs)\n\nIoCs are signs that a system may have been breached. Used for incident detection and threat hunting.\n\nExamples:\n- Unexpected outgoing traffic\n- Unusual user behavior\n- Presence of known malware files\n- Unauthorized configuration changes",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "governance",
      "security"
    ],
    "normalized_filename": "data_security",
    "outlinks": [
      "security_researcher",
      "attack_mitigation",
      "security_vulnerabilities",
      "attack_types",
      "security_mitigation",
      "sql_injection"
    ],
    "inlinks": [
      "amazon_s3",
      "checksum",
      "cryptography",
      "data_principles",
      "operational_resilience_for_growth_and_adaptability",
      "security_vulnerabilities",
      "software_design_patterns",
      "sql_injection",
      "view_use_case",
      "why_json_is_better_than_pickle_for_untrusted_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Selection",
    "sha": "ae9f91c7f485d82e9eb916911ac7dc45810aefa1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Selection.md",
    "text": "Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame.\n\nIn [[DE_Tools]] we explore how to do Data Selection with Pandas\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/selection.ipynb\n\nRelated:\n- [[Data Selection in ML]]\n## Examples\n### Selecting Columns\n\nYou can select a single column from a DataFrame using either bracket notation or dot notation:\n\n```python\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ncolumn_a = df['A']  # or df.A\n```\n### Selecting Rows by Index\n\nTo select rows by their index position, you can use slicing:\n```python\nrows_0_to_2 = df[0:3]  # Selects the first three rows\n```\n### Selecting Rows by Date Range\n\nIf your DataFrame has a DateTime index, you can select rows within a specific date range:\n\n```python\ndate_rng = pd.date_range(start='2013-01-01', end='2013-01-06', freq='D')\ndf = pd.DataFrame(date_rng, columns=['date'])\ndf.set_index('date', inplace=True)\nselected_dates = df['2013-01-02':'2013-01-04']\n```\n### Label-based Selection\n\nUse `.loc` or `.at` to select rows by label:\n\n```python\ndf = pd.DataFrame({'Weather': ['Sunny', 'Rain', 'Cloudy'], 'Temp': [30, 22, 25]})\ndf.set_index('Weather', inplace=True)\nrain_row = df.loc['Rain']  # or df.at['Rain']\n```\n### Position-based Selection\n\nUse `.iloc` or `.iat` to select rows by position:\n\n```python\nthird_row = df.iloc[2]  # Selects the third row\nspecific_value = df.iat[1, 1]  # Selects the value at row 1, column 1\n```\n### Conditional Selection\n\nCreate a new DataFrame based on a condition:\n```python\ndf_new = df[df['var1'] >= 999]  # Selects rows where 'var1' is greater than or equal to 999\n```\nThe condition `df[\"var1\"] >= 999` creates a boolean Series that filters the rows of `df`.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "selection",
      "transformation"
    ],
    "normalized_filename": "data_selection",
    "outlinks": [
      "de_tools",
      "data_selection_in_ml"
    ],
    "inlinks": [
      "data_cleansing",
      "data_transformation",
      "how_do_you_do_the_data_selection",
      "pandas"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Sources",
    "sha": "bbbd47a84af37dc563536364dcf7bb87bf36e257",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Sources.md",
    "text": "Try get many sources from many departments, to understand the interactions between business components.\n\n\n\n\n\n\n[[Data Sources]]\nData Sources & Literature\n* Conduct literature search for prior work and benchmarks.\n* Identify relevant data sources: availability, cost, usefulness, update frequency, and granularity.\n* Evaluate data quality: completeness, patterns in missing data, formatting, outliers, and timeliness.\n* Consider impact of data issues on downstream processes.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_sources",
    "outlinks": [
      "data_sources"
    ],
    "inlinks": [
      "data_collection",
      "data_dictionary",
      "data_sources",
      "good_enough_principle_in_data_projects"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Storage",
    "sha": "d528af75cfd3ec5722b904431aa6eae8d550190f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Storage.md",
    "text": "Data storage is a fundamental aspect of [[Data Engineering]], influencing processes such as \n- (occurring after [[Data Ingestion]])\n- [[Data Transformation]]\n- [[Querying]]\n- [[Data Management]].\n\nStoring the [[Data Transformation]] data in a database or [[Data Warehouse]] for easy access and analysis.\n## Types of Storage\n\nData storage encompasses various methods and technologies for storing, retrieving, and managing data. The choice of storage method significantly impacts ==data retrieval efficiency== and consistency\n\n| Storage Type                                 | Description                                                                                           |\n| -------------------------------------------- | ----------------------------------------------------------------------------------------------------- |\n| [[storage layer object store\\|Object Store]] | The gold standard for data lakes, ideal for unstructured data such as images, audio, and text.        |\n| [[Database]]                                 | The most widely deployed database globally is [[SQLite]]. Suited for transaction recording.           |\n| [[NoSQL]]                                    |                                                                                                       |\n| [[Data Warehouse]]                           | Excels in analytics and reporting.                                                                    |\n| [[Data Lake]]                                | Offers versatility for storing raw data, particularly beneficial for advanced analytics applications. |\n## Follow-Up Questions\n- How do different data storage methods impact data retrieval speed in large datasets?\n- What are the trade-offs between using relational versus [[NoSQL]] databases in specific applications?\n## Related Resources\n- [[Cloud Providers]]\n- [[Amazon S3]]\n- [[Data Governance]]\n- [[Data Engineering Tools]]",
    "aliases": [
      "data_management"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "data_storage",
    "outlinks": [
      "data_engineering",
      "data_ingestion",
      "sqlite",
      "data_governance",
      "data_transformation",
      "database",
      "querying",
      "amazon_s3",
      "data_engineering_tools",
      "storage_layer_object_store\\",
      "cloud_providers",
      "data_lake",
      "data_management",
      "nosql",
      "data_warehouse"
    ],
    "inlinks": [
      "apache_kafka",
      "big_data",
      "checksum",
      "data_engineering_tools",
      "data_hierarchy_of_needs",
      "data_lifecycle_management",
      "data_pipeline",
      "data_warehouse",
      "parquet",
      "snowflake"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Transformation in Data Engineering",
    "sha": "37b22991a81650159c2860cc3a65a8c2cd6f976d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Transformation%20in%20Data%20Engineering.md",
    "text": "Data transformation in [[Data Engineering]] is a key step in data pipelines, often part of:  \n- [ETL (Extract, Transform, Load)](ETL.md) [[ETL]]: Data is transformed before loading into the target system.  \n- [ELT (Extract, Load, Transform)](term/elt.md) [[ELT]]: Data is loaded first, then transformed for analysis.  \n- EtLT (Extract, “tweak”, Load, Transform: A hybrid approach combining elements of ETL and ELT.  \n\nRelated:\n- [[ETL vs ELT]]for a comparison.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "data_transformation_in_data_engineering",
    "outlinks": [
      "data_engineering",
      "etl",
      "elt",
      "etl_vs_elt"
    ],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Transformation with Pandas",
    "sha": "7e6efc810c9212877f234eb4195219c0c4bc0d3d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Transformation%20with%20Pandas.md",
    "text": "Using [[Pandas]] we can do the following:\n\n- [[Merge]]\n- [[Concatenate]]\n- [[Joining Datasets]] \n- [[Pandas join vs merge]]\n- [[Multi-level index]]\n\n- [[Aggregation]]\n\n- [[Pandas Stack]]\n- [[Crosstab]]\n\nA summary of transformations steps can be helpful:\n\n|Step|Operation|Result|\n|---|---|---|\n|1|`set_index`|Rows get hierarchical keys|\n|2|`stack`|Wide → long with 3-level row index|\n|3|`reset + extract`|Parse variable names into fields|\n|4|`pivot`|Tidy format with metric columns|\n|5|`unstack`|Wide format with MultiIndex columns|\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation\n\nRelated terms:\n\n### Split-Apply-Combine\n\n![[Pasted image 20250323081817.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "data_transformation_with_pandas",
    "outlinks": [
      "pandas_stack",
      "concatenate",
      "merge",
      "pasted_image_20250323081817.png",
      "crosstab",
      "pandas",
      "aggregation",
      "pandas_join_vs_merge",
      "de_tools",
      "joining_datasets",
      "multi-level_index"
    ],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Transformation",
    "sha": "e4585751f2206415cd50df7e77ac62ab41f6351b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Transformation.md",
    "text": "Data transformation is the process of converting data from one format to another. \n\nData transformation may involve:  \n- [[Data Cleansing]]\n- [[Structuring and organizing data]]\n- [[Aggregation]]\n- [[Data Selection]]\n- [[Joining Datasets]]\n- [[Normalisation of data]]\n- [[Normalised Schema]]\n- [[Imputation Techniques]]\n\nOthers:\n- Sorting: Arranging data in a logical order.  \n- Validating: Ensuring data integrity and accuracy.  \n- Data Type Conversion: Changing data types (e.g., converting strings to integers).  \n- Schema Normalization: Ensuring a consistent data structure for efficiency.  \nRelated:\n- [[Data Transformation with Pandas]]  \n- [[Data Transformation in Data Engineering]]\n- [[Data Transformation in Machine Learning]]\n- [[Benefits of Data Transformation]]\n- [[Feature Transformations]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "portal",
      "transformation"
    ],
    "normalized_filename": "data_transformation",
    "outlinks": [
      "data_transformation_in_data_engineering",
      "structuring_and_organizing_data",
      "imputation_techniques",
      "feature_transformations",
      "data_cleansing",
      "benefits_of_data_transformation",
      "normalised_schema",
      "data_selection",
      "normalisation_of_data",
      "data_transformation_with_pandas",
      "aggregation",
      "data_transformation_in_machine_learning",
      "joining_datasets"
    ],
    "inlinks": [
      "activation_function",
      "benefits_of_data_transformation",
      "data_pipeline",
      "data_selection_in_ml",
      "data_storage",
      "dbt",
      "duckdb",
      "eda",
      "etl",
      "handling_missing_data",
      "knime",
      "melt",
      "pandas",
      "pandas_stack",
      "powerquery",
      "preprocessing",
      "standardisation",
      "stationary_time_series"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Validation",
    "sha": "72004f0de4c3ccb89e6e96a7362a62de4f59f293",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Validation.md",
    "text": "Data Validation:\n- **Error Prevention**: It ensures data accuracy by preventing incorrect or inappropriate data entries.\n- **Consistent Data Entry**: Helps maintain consistency across large datasets by controlling what users can input.\n- **Efficiency**: By providing drop-down lists or constraints, it reduces the chances of manual errors.\n- **Better [[Data Quality]]: Validating input ensures that your data is clean and ready for analysis or reporting without requiring additional checks.\n- [[type checking]]\n- [[TypeScript]]\n- [[Pydantic]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "governance",
      "management"
    ],
    "normalized_filename": "data_validation",
    "outlinks": [
      "typescript",
      "data_quality",
      "pydantic",
      "type_checking"
    ],
    "inlinks": [
      "anomaly_detection",
      "excel",
      "pydantic",
      "pyright_vs_pydantic"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Virtualization",
    "sha": "408315152010e627a745add11a0a2152e4ba71fb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Virtualization.md",
    "text": "Organizations may also consider adopting a data virtualization solution to integrate their data. \n\nIn this type of [[Data Integration]], data from multiple sources is left in place and is ==accessed== via a virtualization layer so that it ==_appears_== as a single data store. \n\nThis virtualization layer makes use of adapters that translate queries executed on the virtualization layer into a format that each connected source system can execute. \n\nThe virtualization layer then combines the responses from these source systems into a single result. This data integration strategy is sometimes used when a BI tool like Tableau needs to access data from multiple data sources.\n\nOne disadvantage of data virtualization is that analytics workloads are executed on operational systems, which could interfere with their functioning. Another disadvantage is that the virtualization layer may act as a bottleneck on the performance of analytics operations.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "orchestration"
    ],
    "normalized_filename": "data_virtualization",
    "outlinks": [
      "data_integration"
    ],
    "inlinks": [
      "data_integration",
      "semantic_layer"
    ]
  },
  {
    "category": "DE",
    "filename": "DataOps",
    "sha": "e90b1977b333f98e870551e387c92210d5eea246",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/DataOps.md",
    "text": "- Cross functional\n- stake holder involvement\n- Governance\n- Increased speed to value.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "dataops",
    "outlinks": [],
    "inlinks": [
      "devops"
    ]
  },
  {
    "category": "DE",
    "filename": "Data Warehouse",
    "sha": "7e55fcb7cb75c5400ce67d72db7b8213530ecea7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Data%20Warehouse.md",
    "text": "A Data Warehouse (DWH) is a centralized repository designed for [[Querying]] and analysis, storing large volumes of structured data from various sources within an organization. It supports reporting and decision-making by providing a consolidated view of data.\n### Key Features\n\n[[Data Ingestion]] Integration: Combines data from diverse sources (e.g., transactional databases, CRM systems) into a single repository, ensuring consistency.\n  \nSubject-Oriented: Organizes data around key business areas (e.g., sales, finance) rather than operational processes.\n\nNon-Volatile: Data remains unchanged once entered, preserving historical data for long-term analysis.\n\nTime-Variant: Stores data with a time dimension, enabling historical analysis and trend identification.\n\n### Components\n\nData Sources: Internal (e.g., ERP systems) and external (e.g., market research data) origins of data.\n\n[[ETL]]\n\n[[Data Storage]]\n\nMetadata/[[Documentation & Meetings]]: Information about the data, including definitions and transformation rules, aiding in data management.\n\nAccess Tools: Tools for querying and analyzing data, such as SQL clients and business intelligence tools.\n\n##### Resources\n- [Designing a Data Warehouse](https://www.youtube.com/watch?v=patBYUGwsHE)\n- [Why a Data Warehouse?](https://www.youtube.com/watch?v=jmwGNhUXn_o)",
    "aliases": [
      "DWH",
      "Warehouse"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "data_warehouse",
    "outlinks": [
      "data_ingestion",
      "querying",
      "etl",
      "data_storage",
      "documentation_&_meetings"
    ],
    "inlinks": [
      "apache_iceberg",
      "bigquery",
      "cloud_providers",
      "data_lakehouse",
      "data_storage",
      "databricks",
      "databricks_vs_snowflake",
      "dimensional_modelling",
      "fabric",
      "fact_table",
      "single_source_of_truth",
      "snowflake"
    ]
  },
  {
    "category": "DE",
    "filename": "Database Index",
    "sha": "7e8c4f567a733a8e7464ec779bf785aa5ad4e9f2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database%20Index.md",
    "text": "In [[DE_Tools]] see: \n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Indexing/Indexing.ipynb\n\nRelated terms:\n- [[Covering Index]]\n- Partial Index (Index with where clause)\n\nIndexing is a technique used to ==speed up data retrieval== in [[Database]]. It achieves this by creating a separate structure, known as an index, that organizes specific columns of data for faster access. Better than scanning.\n\nCommonly created on ==primary keys== (unique for item) and foreign keys.\n\nIndexes can also be created across multiple tables to enhance the performance of complex queries, especially those that involve joins. A special type of index, called a ==covering index==, includes all the necessary data within the index itself, further improving efficiency.\n\nExample:  \nFor instance, creating an index on the \"title\" column in the \"movies\" table can significantly reduce the time it takes to execute [[Querying|queries]] that search for movie titles.\n\n## Using Indexes\n\nKeep in mind that indexes consume additional storage space.\n\nCreating Indexes: To improve search performance, create indexes on relevant columns. For example:\n  ```sql\n  CREATE INDEX idx_title ON movies(title);\n  ```\n\nAnalyzing Queries: Use the `EXPLAIN QUERY PLAN` command to check if a query is utilizing an index effectively.\n\nDropping Indexes: If an index is no longer needed, it can be removed using:\n  ```sql\n  DROP INDEX idx_title;\n  ```\n\n## Space and Time Trade-offs\n\nSpace: Indexes require extra storage because they are built using B-Trees, which are hierarchical data structures.\nTime: While indexes speed up data retrieval, creating and updating them can slow down data insertion and modification processes.\n\n## How Indexes Work\n\n- Data Structure: Indexes typically use a [[B-tree]] data structure, which allows for efficient searching.\n- Node Structure: A B-tree organizes data into nodes, where each node contains links to the corresponding rows in the table. The data is sorted, enabling quick access.\n- Search Mechanism: When searching, a binary search method is employed. This involves checking the middle of the data and deciding which side to search next, taking advantage of the ordered nature of B-trees for efficiency.",
    "aliases": [
      "Index",
      "Indexing"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "optimisation"
    ],
    "normalized_filename": "database_index",
    "outlinks": [
      "b-tree",
      "database",
      "querying",
      "covering_index",
      "de_tools"
    ],
    "inlinks": [
      "covering_index",
      "database_techniques",
      "indexing_in_cypher",
      "query_optimisation",
      "querying"
    ]
  },
  {
    "category": "DE",
    "filename": "Database Management System (DBMS)",
    "sha": "e1ced9651bdc4ca86bf6222ee02d1cb4a224eb9e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database%20Management%20System%20(DBMS).md",
    "text": "A **Database Management System** (DBMS) is software that allows you to interact with and manage databases.\nEasiest to use:\n- [[SQLite]]\n- [[PostgreSQL]]\n\nOthers:\n- [[MySql]]\n- [[MongoDB]]\n- [[Oracle]]\n- [[Cassandra]]\n- [[MariaDB]]\n\nThese systems enable users to perform [[CRUD]] operations while maintaining data integrity and providing tools for backup, security, and optimization.\n\nCan be proprietary (paid, with support) or Open source (free, self-supported).",
    "aliases": [
      "DBMS"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "management"
    ],
    "normalized_filename": "database_management_system_(dbms)",
    "outlinks": [
      "mysql",
      "sqlite",
      "postgresql",
      "crud",
      "cassandra",
      "mariadb",
      "mongodb",
      "oracle"
    ],
    "inlinks": [
      "data_engineering_portal",
      "data_management",
      "database",
      "sqlite",
      "transaction"
    ]
  },
  {
    "category": "DE",
    "filename": "Database Schema",
    "sha": "2e9de895d0cdebc035ea29fe225e530902af255f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database%20Schema.md",
    "text": "A [[Database Schema|schema]] is the structure that defines how data is organized in a [[Database]], used in [[Data Management]]. It specifies the tables, columns, relationships, and constraints within the database. The schema is used for ensuring data is stored consistently and can be queried efficiently.\n\n1. Definition and Components: A database schema represents the ==structure== around the data, including tables, views, fields, relationships, and various other elements like indexes and triggers. It provides a framework for organizing and understanding data.\n\n2. Importance of ==Structure==: Without a schema, data can be chaotic and difficult to ==interpret==. A well-defined schema organizes data, making it ==manageable and meaningful.==\n\n3. Schema on Read vs. Schema on Write: \n   - Schema on Read: Structure is applied when the data is read, useful for unstructured data stores.\n   - Schema on Write: Structure is enforced when data is written, typical of traditional databases.\n\n1. Design Influences: The design of a schema impacts database behavior. For example, schemas designed with tables connected by primary keys are optimized for transactional applications, while star schemas are designed for efficient read operations in data warehouses.\n\n2. Performance Impact: A good schema can significantly ==improve query performance==, reducing processing ==time== and ==cost==, and simplifying query complexity.\n\n3. [[Data Modeling]]: Despite being considered an old concept, data modeling remains crucial for creating effective schemas, particularly in the context of big data and analytics.\n\n4. Iterative Process: Developing a data warehouse schema involves iterative refinement, starting with interviews to create a [[conceptual data model]], which is then tested and refined through multiple iterations before being implemented.\n\n5. Strategic Importance: The strategic design and deployment of a database schema are vital for efficient data warehousing and analytics. Intracity specializes in this process, helping organizations define and execute their data strategies.\n\nRelated to: \n- [[Types of Database Schema]]\n- [[Implementing Database Schema]]\n#### Resources\n[link](https://www.youtube.com/watch?v=3BZz8R7mqu0)",
    "aliases": [
      "schema",
      "Schema"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "database_schema",
    "outlinks": [
      "database",
      "conceptual_data_model",
      "database_schema",
      "data_modeling",
      "implementing_database_schema",
      "types_of_database_schema",
      "data_management"
    ],
    "inlinks": [
      "data_lakehouse",
      "database",
      "database_schema",
      "fact_table",
      "how_is_schema_evolution_done_in_practice_with_sql",
      "neomodel",
      "relational_database",
      "schema_evolution",
      "soft_deletion",
      "structured_data",
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Database Storage",
    "sha": "88ca881bb3232b4cb86671d5e66d3f36608f6b45",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database%20Storage.md",
    "text": "Methods and optimizations for storing, retrieving, and processing data in [[Database]] systems. \n\n[[Columnar Storage]]\n\n[[Row-based Storage]]\n\n[[Vectorized Engine]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "database",
      "storage"
    ],
    "normalized_filename": "database_storage",
    "outlinks": [
      "database",
      "vectorized_engine",
      "columnar_storage",
      "row-based_storage"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Database Techniques",
    "sha": "9106c033204e33f9389b4aacd9530991d76158e2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database%20Techniques.md",
    "text": "Techniques:\n- [[Soft Deletion]]\n- [[Concurrency]] \n\t- [[Race Conditions]]\n- [[Querying]]\n\t- [[SQL Joins]]\n\t- [[Stored Procedures]]\n\t- Cleaning: Use **Levenshtein Distance** (if SQLite extension is available) to group similar entries.\n- [[Database Index|Indexing]]\n\t- [[Querying]]\n\t- [[Vacuum]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "portal"
    ],
    "normalized_filename": "database_techniques",
    "outlinks": [
      "race_conditions",
      "stored_procedures",
      "querying",
      "database_index",
      "concurrency",
      "soft_deletion",
      "vacuum",
      "sql_joins"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "melt",
      "sql"
    ]
  },
  {
    "category": "DE",
    "filename": "Database",
    "sha": "80d3800b9830ed89c08a0ec11f527dc37f821517",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Database.md",
    "text": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include:\n\n- [[MySql]]\n- [[PostgreSQL]]\n- [[MongoDB]]\n\nThey facilitate efficient [[CRUD]] operations and transactional processing ([[OLTP]]) structured by [[Database Schema|schema]] that organizes data into tables and relationships.\n\nKey Features\n- **[[structured data]]**: Organized for efficient [[CRUD]] operations, allowing reliable access.\n- [[Relational Database]]: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy.\n\nStructure\n- Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval.\n\nFlexibility\n- Databases have a flexible schema that adapts to evolving requirements, unlike static solutions like spreadsheets.\n\nRelated Ideas:\n- [[Spreadsheets vs Databases]]\n- [[Database Management System (DBMS)]]\n- [[Components of the database]]\n- [[Relating Tables Together]]\n- [[Turning a flat file into a database]]\n- [[Database Techniques]]\n\nTasks:\n- [x] Investigate [[pyodbc]] ✅ 2025-09-28",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "database",
    "outlinks": [
      "mysql",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "spreadsheets_vs_databases",
      "database_techniques",
      "postgresql",
      "database_schema",
      "crud",
      "structured_data",
      "components_of_the_database",
      "pyodbc",
      "relational_database",
      "mongodb",
      "database_management_system_(dbms)",
      "oltp"
    ],
    "inlinks": [
      "data_ingestion",
      "data_integrity",
      "data_storage",
      "database_index",
      "database_schema",
      "database_storage",
      "delta_tables_in_databricks",
      "file_management",
      "microsoft_access",
      "neo4j",
      "relational_database",
      "rollup",
      "single_source_of_truth",
      "slowly_changing_dimension",
      "software_design_patterns",
      "sql",
      "structured_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Digital twin",
    "sha": "8f213091e87c1c5aa49a2e8ca686986fdc1cab00",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Digital%20twin.md",
    "text": ">[!Summary]\n> A **digital twin** is a virtual representation of a physical object, system, or process that mirrors its real-world counterpart in real-time. This digital model is used to simulate, monitor, analyze, and optimize the physical entity by continuously updating based on data collected from sensors, devices, or other inputs. The concept is widely applied in industries such as manufacturing, healthcare, [[Energy]], smart cities, and more to improve decision-making, predictive maintenance, and efficiency. A digital twin is a powerful tool for enhancing real-time decision-making, optimizing processes, and predicting future performance by bridging the physical and digital worlds. Its applications continue to expand across various industries, helping organizations to reduce costs, improve efficiency, and innovate faster.\n\n>[!Example]\n>Consider a **digital twin of a wind turbine**. Sensors installed on the turbine gather data on operational conditions such as wind speed, blade position, temperature, and vibration. This data is continuously transmitted to the digital twin, which mirrors the turbine’s state in real-time. The digital twin runs simulations to predict when parts of the turbine might fail due to wear and tear. Maintenance teams can use this information to schedule repairs before a breakdown occurs, minimizing downtime and improving the turbine's efficiency.\n\n### Key Components of a Digital Twin:\n\n1. **Physical Object or Process**:\n   - The real-world entity (such as a machine, a building, a production line, or even a human body) that the digital twin replicates.\n\n2. **Digital Model**:\n   - A virtual replica of the physical entity, designed using data models, physics-based simulations, and other analytical tools. The digital model reflects the structure, behavior, and function of the physical object or process.\n\n3. **[[Data Integration]]**:\n   - The digital twin ==relies on real-time data from sensors,== IoT devices, or historical databases connected to the physical counterpart. This data flow enables the twin to reflect current operating conditions and states.\n\n4. **Analytics and Simulation**:\n   - Advanced analytics (e.g., machine learning, artificial intelligence) and simulations are applied to the digital twin to gain insights into the performance, predict future behavior, and test scenarios that would be difficult or expensive to replicate in the real world.\n\n5. **Feedback Loop**:\n   - A digital twin allows for continuous interaction between the physical and digital worlds. Insights or predictions from the digital twin can inform changes to the physical system, and any updates in the physical system feed back into the digital twin, maintaining accuracy and alignment.\n\n### Types of Digital Twins:\n\n1. **Component/Asset Twin**:\n   - Represents individual components or parts of a larger system (e.g., the digital twin of a jet engine or an electric motor).\n\n2. **System or Unit Twin**:\n   - Models entire systems or units, such as a production line in a factory or the electrical system of a building.\n\n3. **Process Twin**:\n   - Focuses on simulating and optimizing processes, such as a manufacturing workflow or supply chain operations.\n\n4. **Environment Twin**:\n   - Used to simulate larger, more complex systems like cities, ecosystems, or large-scale infrastructure (e.g., smart city initiatives or environmental monitoring).\n\n### Applications of Digital Twins:\n\n1. **Manufacturing**:\n   - In smart factories, digital twins are used to simulate production processes, predict machine failures, optimize maintenance schedules, and improve product design by running real-time simulations of manufacturing conditions.\n\n2. **Healthcare**:\n   - Digital twins of patients are being developed to model individual health profiles, allowing for personalized treatment plans and predictive diagnostics. A digital twin of a human organ, for example, could simulate medical treatments before they are applied to the patient.\n\n3. **[[Energy]]**:\n   - In energy systems, digital twins help optimize the operation of power plants, monitor grid performance, and simulate the impacts of renewable energy integration, improving reliability and efficiency.\n\n4. **Smart Cities**:\n   - Urban planners use digital twins to model traffic flow, infrastructure usage, or environmental conditions. This allows them to simulate different scenarios and optimize city operations, reduce congestion, and improve public services.\n\n5. **Aerospace and Automotive**:\n   - Digital twins are used extensively in designing, testing, and maintaining complex systems like aircraft, satellites, and autonomous vehicles. Engineers can simulate operational conditions to identify potential problems before they occur in the physical system.\n\n6. **Building Management**:\n   - Digital twins of buildings or infrastructure monitor and control systems like HVAC, lighting, and security, improving energy efficiency and safety. They are also used for simulating how a building will perform under different conditions (e.g., weather events or occupancy changes).\n\n### Benefits of Digital Twins:\n\n1. **Real-time Monitoring**:\n   - Provides live feedback from the physical entity, which enables organizations to make faster, more informed decisions.\n\n2. **Predictive Maintenance**:\n   - Predicts when equipment or systems are likely to fail based on real-time data and simulations, reducing downtime and maintenance costs.\n\n3. **Optimization**:\n   - Enables the continuous improvement of processes by testing scenarios in a virtual environment without disrupting real-world operations.\n\n4. **Improved Design and Innovation**:\n   - Digital twins allow engineers and designers to experiment with different configurations, materials, or processes virtually, leading to faster, cheaper, and more innovative solutions.\n\n5. **Reduced Risk**:\n   - By simulating potential failures or dangerous scenarios in the digital world, organizations can assess risk and plan mitigation strategies without putting the physical system at risk.\n\n### Challenges:\n\n1. **[[Data Management]]**:\n   - Digital twins require a large amount of real-time data to maintain accuracy. Collecting, managing, and processing this data efficiently can be complex and costly.\n\n2. **Integration**:\n   - Integrating the digital twin with physical systems, particularly in legacy environments, can be challenging due to compatibility issues and the need for IoT infrastructure.\n\n3. **Security**:\n   - Because digital twins rely on real-time data transmission, they are vulnerable to cyberattacks, which can lead to compromised systems or intellectual property theft.\n\n4. **[[Scalability]]**:\n   - Scaling digital twin models to encompass entire cities or large systems involves high computational and infrastructural requirements.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "modeling"
    ],
    "normalized_filename": "digital_twin",
    "outlinks": [
      "energy",
      "data_integration",
      "data_management",
      "scalability"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Distributed Computing",
    "sha": "a2f2e13d2091ea536062ab7fb8ef39dc7f5699bc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Distributed%20Computing.md",
    "text": "**Distributed Computing** is essential for managing **massive data volumes** by distributing tasks across multiple servers or machines. This enables scalability and efficient data processing.\n\n**[[Hadoop]]** is a framework that handles both data storage and processing across **clusters of servers**:\n  - It ensures **scalability**: can easily grow as more data is added.\n  - Provides **redundancy**: data is replicated across servers to prevent loss in case of failures.\n\nTools like [[Apache Spark]] are built to process data in these **distributed environments**, allowing for fast, parallel processing across the cluster.\n\nDistributed computing is central to modern data handling, driven by frameworks like Spark and Hadoop, supported by cloud infrastructure, and expanding into real-time and [[edge computing]].\n### Distributed Computing: Current State\n\nDistributed computing enables the processing of massive datasets and computational tasks by distributing them across multiple machines. This approach increases [[Scalability]], parallelism, and fault tolerance, making it essential for modern data processing.\n#### Key Frameworks\n\n- **Hadoop**: An early pioneer, Hadoop introduced distributed storage via **HDFS** and data processing with **MapReduce**, allowing tasks to be split across clusters of servers.\n- **Apache Spark**: A faster alternative to MapReduce, Spark uses in-memory computing for real-time, iterative tasks, improving speed and efficiency. It has become the leading tool for distributed data processing.\n\n#### Distributed Storage\n\n- **HDFS** and cloud storage systems like **[[Amazon S3]]** break data into smaller parts and distribute them across multiple servers. This setup provides high throughput, redundancy, and fault tolerance.\n- **Distributed databases** such as **Cassandra** and **Bigtable** offer scalable storage for structured data, ensuring availability across nodes.\n\n#### Real-Time Processing ([[Data Streaming]])\n\n- Frameworks like **Apache Flink** and **Kafka Streams** are critical for real-time data processing, enabling continuous data handling as it is generated. They are commonly used in applications requiring instant processing, such as fraud detection or live analytics.\n\n#### Cloud-Native Computing\n\n- **Cloud platforms** (e.g., AWS, Google Cloud, Azure) have made distributed computing accessible through services like **Amazon EMR** and **Google Dataproc**. These services simplify the deployment and management of distributed applications.\n- [[kubernetes]] has become the standard for orchestrating distributed applications, managing containers and ensuring high availability across clusters.\n\n#### Trends and Challenges\n\n- **Edge computing** is gaining momentum, enabling data to be processed closer to the source (e.g., IoT devices), reducing latency and bandwidth usage.\n- Challenges include **fault tolerance**, **network [[Latency]]**, and **data consistency**. Innovations in consensus algorithms and fault-tolerant storage systems are working to mitigate these issues.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "management"
    ],
    "normalized_filename": "distributed_computing",
    "outlinks": [
      "kubernetes",
      "scalability",
      "edge_computing",
      "amazon_s3",
      "apache_spark",
      "hadoop",
      "data_streaming",
      "latency"
    ],
    "inlinks": [
      "batch_processing",
      "map_reduce",
      "publish_and_subscribe",
      "pyspark"
    ]
  },
  {
    "category": "DE",
    "filename": "DuckDB in python",
    "sha": "38b9d5545de8645c8b548b7dbe42617acd55565c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/DuckDB%20in%20python.md",
    "text": "To use **DuckDB** in Python, you can follow these steps to install the DuckDB library and perform basic operations such as creating a database, running queries, and manipulating data. Here's a simple guide:\n\n### Step 1: Install DuckDB\n\nYou can install DuckDB using pip. Open your terminal or command prompt and run:\n\n```bash\npip install duckdb\n```\n### Example: Full Code\n\nHere’s a complete example that incorporates all the steps:\n\n```python\nimport duckdb\n\n# Step 1: Connect to an in-memory database\nconn = duckdb.connect(database=':memory:')\n\n# Step 2: Create a table\nconn.execute(\"\"\"\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR,\n    age INTEGER\n)\n\"\"\")\n\n# Step 3: Insert data\nconn.execute(\"\"\"\nINSERT INTO users VALUES\n(1, 'Alice', 30),\n(2, 'Bob', 25),\n(3, 'Charlie', 35)\n\"\"\")\n\n# Step 4: Query data\nresult = conn.execute(\"SELECT * FROM users\").fetchall()\n\n# Print the results\nfor row in result:\n    print(row)\n\n# Step 5: Close the connection\nconn.close()\n```\n\n### Additional Features\n\nDuckDB also supports advanced features such as:\n\n- **Reading from CSV and Parquet files**: You can load data directly from these formats using commands like `READ_CSV` or `READ_PARQUET`.\n- **Integration with Pandas**: You can easily convert DuckDB query results to Pandas DataFrames for further analysis.\n\n### Example of Reading from a CSV\n\n```python\n# Load data from a CSV file into a DuckDB table\nconn.execute(\"CREATE TABLE my_data AS SELECT * FROM read_csv_auto('path/to/your/file.csv')\")\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "python"
    ],
    "normalized_filename": "duckdb_in_python",
    "outlinks": [],
    "inlinks": [
      "duckdb"
    ]
  },
  {
    "category": "DE",
    "filename": "DuckDB vs SQLite",
    "sha": "cffe96b69ae71aca84ccbe9951354388b7d28dda",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/DuckDB%20vs%20SQLite.md",
    "text": "Choosing between **[[DuckDB]]** and **[[SQLite]]** for data processing in [[Python]] depends on your specific use case and requirements.\n\nWhile **SQLite** is an excellent choice for lightweight applications, local data storage, and simple transactional workloads.\n\n**DuckDB** shines in scenarios involving complex analytical queries, large datasets, and data science workflows.\n\nIf your primary focus is on data analysis and you need high performance for analytical tasks, DuckDB may be the better option. However, if you need a simple, lightweight database for small-scale applications, SQLite could be more appropriate.\n\n### 1. **Performance for Analytical Queries**\n- **DuckDB** is optimized for analytical workloads and can handle complex queries involving large datasets more efficiently than SQLite. It uses a [[Columnar Storage]] format, which is particularly beneficial for aggregation and analytical operations.\n- **SQLite**, while fast for transactional workloads, may not perform as well with large-scale analytical queries.\n\n### 2. **In-Memory Processing**\n- DuckDB can operate entirely in-memory, allowing for faster data processing and query execution. This is especially useful for data science tasks where speed is critical.\n- SQLite can also work in-memory, but its performance may not match that of DuckDB for analytical tasks.\n\n### 3. **Support for Complex Data Types**\n- DuckDB supports more complex data types and operations, such as nested data structures and advanced analytical functions, which can be advantageous for data analysis.\n- SQLite has a more limited set of data types and may not support some advanced analytical functions.\n\n### 4. **Integration with Data Science Tools**\n- DuckDB is designed to integrate seamlessly with data science libraries like Pandas, making it easier to perform data analysis and manipulation directly within your Python workflows.\n- While SQLite can also be used with Pandas, DuckDB's integration is often more straightforward for analytical tasks.\n\n### 5. **Cloud and Big Data Compatibility**\n- DuckDB is designed to work well with cloud-based data warehouses and big data environments, making it suitable for modern data workflows that involve large datasets stored in cloud storage.\n- SQLite is more suited for lightweight applications and local data storage.\n\n### 6. **Columnar Storage Format**\n- DuckDB's columnar storage format allows for more efficient data compression and faster query performance on analytical workloads, as it reads only the necessary columns for a query.\n- SQLite uses a row-based storage format, which can be less efficient for certain types of analytical queries.\n\n### 7. **Ease of Use for Data Transformation**\n- DuckDB simplifies data transformation processes, allowing you to perform transformations directly within the database after loading the data. This can streamline workflows and reduce the need for additional data processing steps.\n- SQLite requires more manual handling for data transformations, especially when dealing with large datasets.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "python",
      "SQL"
    ],
    "normalized_filename": "duckdb_vs_sqlite",
    "outlinks": [
      "columnar_storage",
      "sqlite",
      "duckdb",
      "python"
    ],
    "inlinks": [
      "duckdb"
    ]
  },
  {
    "category": "DE",
    "filename": "Durability",
    "sha": "0f5d9919efafa52af1c97aeb61d0a827a301fa05",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Durability.md",
    "text": "Once committed (completed), the results of a transaction are permanent and survive future system and media failures. \n\nIf the airline reservation system computer gives you seat 22A and crashes a millisecond later, it won't have forgotten that you are sitting in 22A and also give it to someone else. \n\nFurthermore, if a programmer spills coffee into a disk drive, it will be possible to install a new disk and recover the transactions up to the coffee spill, showing that you had seat 22A.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "durability",
    "outlinks": [],
    "inlinks": [
      "performance_dimensions"
    ]
  },
  {
    "category": "DE",
    "filename": "ELT",
    "sha": "4a2cfc31beee030800bb2c420c871d804c937def",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ELT.md",
    "text": "**ELT** (Extract, Load, Transform) is a data integration approach that involves three main steps:\n\n1. **Extract (E)**: Data is extracted from a source system.\n2. **Load (L)**: The raw data is loaded into a destination system, such as a data warehouse.\n3. **Transform (T)**: Transformation of the data occurs within the destination system after the data has been loaded.\n\nThis approach contrasts with the traditional **ETL** (Extract, Transform, Load) method, where data is transformed before reaching the destination. For a detailed comparison, see [[ETL vs ELT]]\n\n### Advantages of ELT\nThe shift from ETL to ELT has been facilitated by several factors:\n\n- **Cost Efficiency**: The decreasing costs of cloud-based storage and computation have reduced the advantages of ETL's pre-loading data transformation.\n  \n- **Cloud-Based Data Warehouses**: The emergence of cloud-based data warehouses like Redshift, [[BigQuery]], and [[Snowflake]] has made the ELT approach more feasible and efficient.\n### Historical Context\nHistorically, ETL was preferred for reasons that are now less relevant:\n\n- **Cost Savings**: ETL was believed to save costs by filtering out unwanted data before loading. However, this is less significant with modern cloud solutions.\n  \n- **Complexity Management**: ETL minimizes the complexity of post-loading transformations. Yet, contemporary tools like [[dbt]] simplify this process, making it easier to perform transformations after loading.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "elt",
    "outlinks": [
      "snowflake",
      "bigquery",
      "etl_vs_elt",
      "dbt"
    ],
    "inlinks": [
      "data_transformation_in_data_engineering",
      "dbt",
      "etl"
    ]
  },
  {
    "category": "DE",
    "filename": "ETL 1",
    "sha": "88e2e6c5f340cf768c29e0a5d8710df079a84284",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ETL%201.md",
    "text": "- ETL (extract transform load) is the process of creating new database objects by extracting data from multiple data sources, transforming it on a local or third party machine, and loading the transformed data into a data warehouse.\n- ELT (extract load transform) is a more recent process of creating new database objects by first extracting and loading raw data into a data warehouse and then transforming that data directly in the warehouse.\n- The new ELT process is made possible by the introduction of cloud-based data warehouse technologies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "etl_1",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "ETL Pipeline Example",
    "sha": "fce12c1fb415b76430ace42f395201a5628fca7c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ETL%20Pipeline%20Example.md",
    "text": "[Link](https://www.youtube.com/watch?v=uqRRjcsUGgk)\n\n[Github](https://github.com/syalanuj/youtube/blob/main/de_fundamentals_python/etl.py\n\n#### 1. Extract using a API\n\nGet data via api or download.\n\n#### 2. Transform \n\nPut into a pandas df.\n\n#### 3. Load\n\nSave df as a database. Save SQLite database. Save as table.\n\nRun functions\n\n\n```python\n\"\"\"\nPython Extract Transform Load Example\n\"\"\"\n\n# %%\nimport requests\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef extract()-> dict:\n    \"\"\" This API extracts data from\n    http://universities.hipolabs.com\n    \"\"\"\n    API_URL = \"http://universities.hipolabs.com/search?country=United+States\"\n    data = requests.get(API_URL).json()\n    return data\n\ndef transform(data:dict) -> pd.DataFrame:\n    \"\"\" Transforms the dataset into desired structure and filters\"\"\"\n    df = pd.DataFrame(data)\n    print(f\"Total Number of universities from API {len(data)}\")\n    df = df[df[\"name\"].str.contains(\"California\")]\n    print(f\"Number of universities in california {len(df)}\")\n    df['domains'] = [','.join(map(str, l)) for l in df['domains']]\n    df['web_pages'] = [','.join(map(str, l)) for l in df['web_pages']]\n    df = df.reset_index(drop=True)\n    return df[[\"domains\",\"country\",\"web_pages\",\"name\"]]\n\ndef load(df:pd.DataFrame)-> None:\n    \"\"\" Loads data into a sqllite database\"\"\"\n    disk_engine = create_engine('sqlite:///my_lite_store.db')\n    df.to_sql('cal_uni', disk_engine, if_exists='replace')\n\n# %%\ndata = extract()\ndf = transform(data)\nload(df)\n\n\n# %%\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "etl_pipeline_example",
    "outlinks": [
      "\"domains\",\"country\",\"web_pages\",\"name\""
    ],
    "inlinks": [
      "etl"
    ]
  },
  {
    "category": "DE",
    "filename": "ETL vs ELT",
    "sha": "f6d7de609785759e1bb03772d3c3a97b48c93ce9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ETL%20vs%20ELT.md",
    "text": "[ETL](ETL.md) (Extract, Transform, and Load) and [ELT](term/elt.md) (Extract, Load, and Transform) are two paradigms for moving data from one system to another.\n\n==ELT is most friendly for analysts==\n\nThe main difference between them is that when an ETL approach is used, data is transformed before it is loaded into a destination system. On the other hand, in the case of ELT, any required transformations are done after the data has been written to the destination and are _then_ done _inside_ the destination -- often by executing SQL commands. The difference between these approaches is easier to understand by a visual comparison of the two approaches. \n\nThe image below demonstrates the ETL approach to [data integration](term/data%20integration.md):\n\n![](etl-tool.png)\n\nWhile the following image demonstrates the ELT approach to data integration:\n\n![](elt-tool.png)\n\n[[ETL]] was originally used for [Data Warehousing](Data%20Warehouse.md) and ELT for creating a [Data Lake](Data%20Lake.md). \n\n## Disadvantages of ETL compared to ELT\n\n**ETL** has several **disadvantages compared to ELT**, including the following:\n\n- Generally, only transformed data is stored in the destination system, and so ==analysts must know beforehand every way== they are going to use the data, and every report they are going to produce.  \n- Modifications to requirements can be costly, and often require re-ingesting data from source systems.\n- Every transformation that is performed on the data may obscure some of the underlying information, and analysts only see what was kept during the transformation phase. \n- Building an ETL-based data pipeline is often beyond the technical capabilities of analysts.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "etl_vs_elt",
    "outlinks": [
      "etl"
    ],
    "inlinks": [
      "data_transformation_in_data_engineering",
      "elt",
      "etl"
    ]
  },
  {
    "category": "DE",
    "filename": "ETL",
    "sha": "bf6021d0c581015435e75440b7f7b20956ef13a0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/ETL.md",
    "text": "**ETL** (Extract, Transform, Load) is a data integration process that involves moving data from one system to another. It consists of three main stages:\n\n1. **Extract**: Collecting data from various sources, such as databases, APIs, or flat files. This may involve setting up API connections to pull data from multiple sources.\n\n2. **Transform**: Cleaning and converting the data into a usable format. This includes filtering, aggregating, and joining data to create a unified dataset. See [[Data Transformation]].\n\n3. **Load**: Inserting the transformed data into a destination system, such as a data warehouse (organized), database, or data lake (unorganized).\n\n### Historical Context\nThe ETL paradigm emerged in the 1970s and was traditionally preferred for its ability to transform data before it reaches the destination. This approach ensures that data is standardized and passes quality checks, enhancing overall data quality.\n\n### Transition to ELT\nIn recent years, the data movement paradigm has shifted towards [[ELT]] (Extract, Load, Transform). This approach emphasizes keeping raw data accessible in the destination system, allowing for more flexibility in data processing. For a comparison of ETL and ELT, see [[ETL vs ELT]].\n\nReasons for Change to see [[ELT]]\n\n### Modern ETL Tools\nCurrent ETL processes are often managed using tools like [[Apache Airflow]], [[dagster]], and [Temporal](term/temporal.md).\n\n### Enhancing the ETL Process\nTo improve an ETL process, consider the following enhancements:\n\n- **Error Handling**: Implement error handling to manage exceptions and prevent silent failures.\n- **Logging**: Include logging to track the process flow and facilitate debugging.\n- **Parameterization**: Make scripts flexible by parameterizing file paths and database connections.\n- **Data Validation and Cleaning**: Incorporate steps to validate and clean the data.\n- **Database Indexing and Constraints**: Optimize database tables with proper indexing and constraints for better performance.\n##### Related Notes\n- [[ETL Pipeline Example]]\n- [[ETL vs ELT]]\n\n\n\n\n[[ETL]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "etl",
    "outlinks": [
      "elt",
      "data_transformation",
      "etl_pipeline_example",
      "etl",
      "etl_vs_elt",
      "apache_airflow",
      "dagster"
    ],
    "inlinks": [
      "data_transformation_in_data_engineering",
      "data_warehouse",
      "etl",
      "etl_vs_elt",
      "excel",
      "fabric",
      "loading_google_sheets_into_databricks",
      "reverse_etl",
      "slowly_changing_dimension"
    ]
  },
  {
    "category": "DE",
    "filename": "Estimator",
    "sha": "c6ebdef6bb54f5ab0c391beceb6e0cb7d3567247",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Estimator.md",
    "text": "Given a sample ([[Resampling]]) an estimator is a formula that approximates a population parameter i.e feature",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "estimator",
    "outlinks": [
      "resampling"
    ],
    "inlinks": [
      "maximum_likelihood_estimation",
      "statistical_tests",
      "statistics"
    ]
  },
  {
    "category": "DE",
    "filename": "EtLT",
    "sha": "4fe2a882e00d3e8e56fb4011cb21fd35b926d5df",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/EtLT.md",
    "text": ">[!important]\n> EtLT refers to Extract, “tweak”, Load, [Transform](Data%20Transformation.md), and can be thought of an extension to the [ELT](term/elt.md) approach to [data integration](term/data%20integration.md). \n\nWhen compared to ELT, the EtLT approach incorporates an additional light “tweak” (small “t”) transformation, which is done on the data after it is extracted from the source and before it is loaded into the destination.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "etlt",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Event Driven Microservices",
    "sha": "735a852812bc166ea714d9be55a5c46db94254ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Event%20Driven%20Microservices.md",
    "text": "Event-driven microservices refer to a [[software architecture]] pattern where [[microservices]] communicate and coordinate their actions through the production, detection, consumption, and reaction to [[Event Driven Events]]. This approach is designed to create a more decoupled and scalable system, where services can operate independently and react to changes in real-time.\n\nEvent-driven microservices are particularly useful for applications that require high scalability, real-time processing, and flexibility. \n\nThey are commonly used in domains like e-commerce, IoT, financial services, and any system where real-time data processing and responsiveness are critical. \n\nHowever, they also introduce challenges in terms of event schema management, eventual consistency, and debugging, which need to be carefully addressed.\n\nKey characteristics of event-driven microservices include:\n\n1. **Event Producers and Consumers**: In this architecture, services can act as event producers, consumers, or both. An event producer generates events when something of interest happens (e.g., a new order is placed), and event consumers listen for these events to perform actions (e.g., processing the order).\n\n2. **Asynchronous Communication**: Events are typically communicated asynchronously, meaning that the producer does not wait for the consumer to process the event. This allows services to operate independently and improves system responsiveness and scalability.\n\n3. **Event Brokers**: An event broker or message broker (such as Apache Kafka, RabbitMQ, or AWS SNS/SQS) is often used to facilitate the distribution of events between services. The broker decouples producers from consumers, allowing them to evolve independently.\n\n4. **Loose Coupling**: Services are loosely coupled because they do not need to know about each other directly. They only need to know about the events they produce or consume, which reduces dependencies and increases flexibility.\n\n5. **Real-Time Processing**: Event-driven architectures are well-suited for real-time processing and can handle high volumes of events efficiently, making them ideal for applications that require immediate responses to changes.\n\n6. **Scalability and Resilience**: The decoupled nature of event-driven microservices allows for independent scaling of services. If one service fails, it does not necessarily affect others, enhancing the system's resilience.\n\n7. **Event Sourcing and CQRS**: Event-driven architectures often use patterns like event sourcing (storing the state of an entity as a sequence of events) and CQRS (Command Query Responsibility Segregation) to manage data consistency and separation of concerns.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "software"
    ],
    "normalized_filename": "event_driven_microservices",
    "outlinks": [
      "software_architecture",
      "microservices",
      "event_driven_events"
    ],
    "inlinks": [
      "event_driven",
      "event_driven_events"
    ]
  },
  {
    "category": "DE",
    "filename": "Event-Driven Architecture",
    "sha": "d468755d07807e198104711380210e069b272855",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Event-Driven%20Architecture.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "event-driven_architecture",
    "outlinks": [],
    "inlinks": [
      "alternatives_to_batch_processing",
      "event_driven",
      "publish_and_subscribe"
    ]
  },
  {
    "category": "DE",
    "filename": "Fabric",
    "sha": "d521e505b0511a5bb82c54b32f8ad1ba8a958380",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Fabric.md",
    "text": "Fabric is a unified analytics platform that operates in the cloud, eliminating the need for local installations. It provides an integrated environment for data analysis, similar to how [[Microsoft]] Office serves as a suite for productivity tasks.\n\n## Key Features\n\n- Unified Platform: Combines various data tools and services into a single platform, streamlining data analysis and management.\n- Cloud-Based: Operates entirely in the cloud, ensuring accessibility and scalability without the need for local installations.\n- Integrated Environment: Offers a cohesive environment for data analysis, integrating tools like [[Data Factory]], [[Synapse]], and [[PowerBI]].\n\n## Components\n\n- Data Factory: Facilitates data integration and transformation.\n- Synapse: Acts as a [[Relational Database]] and [[Data Warehouse]], supporting [[T-SQL]] for data management.\n- [[Data Lake]]: Fabric provides open data storage solutions, allowing for efficient data management and analysis.\n- OneLake: Offers workspaces for different departments, enabling data sharing and referencing without duplication.\n\n## Technologies\n\n- Programming Languages: Supports [[Scala]] and [[PySpark]] for data processing within the [[Data Lake]].\n- PowerBI Integration: Enhances data visualization and querying capabilities within Fabric, offering faster insights.\n\n## Advantages\n\n- Data as Fuel: Recognizes data as the essential component powering AI and analytics.\n- No ETL Required: Mirrors data sources, eliminating the need for [[ETL]] processes when source data is edited.\n- Cross-Workspace Shortcuts: Allows departments to reference data across workspaces without creating copies.\n- Copilot with PowerBI: Integrates AI-driven insights and automation within PowerBI for enhanced data analysis.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cloud"
    ],
    "normalized_filename": "fabric",
    "outlinks": [
      "powerbi",
      "data_factory",
      "pyspark",
      "synapse",
      "etl",
      "relational_database",
      "t-sql",
      "data_lake",
      "data_warehouse",
      "scala",
      "microsoft"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Faker",
    "sha": "75fe4ed53453c0f4f65c6057dc61e179c5400077",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Faker.md",
    "text": "Generating datasets\n\nFaker, python library\n\nhttps://pypi.org/project/Faker/\n\ndiffernet types, names, emails ect..\n\nNumerical data too:",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "normalized_filename": "faker",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "File Management",
    "sha": "7598f1a76094248bd99a3ed3e1f1e85b59443e7c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/File%20Management.md",
    "text": "It is possible to create a [[SQLite]] [[Database]] for a folder and then [[Querying|Query]] it.\n\nTo get:\n- files of large size\n- file types\n- date last modified\n\nUseful for a repository health check.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "file_management",
    "outlinks": [
      "database",
      "sqlite",
      "querying"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Folder Tree Diagram",
    "sha": "bf0b41d06eb121a0869b540473baedc6a51fea36",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Folder%20Tree%20Diagram.md",
    "text": "## Links \n\nSimple method\nhttps://www.digitalcitizen.life/how-export-directory-tree-folder-windows/\n\nMore general\n\nhttps://superuser.com/questions/272699/how-do-i-draw-a-tree-file-structure\n\n[Treeviz](randelshofer.ch/treeviz/)\n[Graphviz](https://graphviz.org/)\n\n\ntree /a /f >output.doc",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "folder_tree_diagram",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Foreign Key",
    "sha": "2c77ccae9379fa8a8a8524e8ef713536d9f384b2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Foreign%20Key.md",
    "text": "A foreign key is a field in one table that uniquely identifies a row in another table, linking to the primary key of that table.\n\nFor example, `DepartmentID` in the ==Employees== table links to `DepartmentID` in the ==Departments== table.  \n\nForeign keys establish relationships between tables and ==maintain **referential integrity** by ensuring valid connections between records.==  \nThis means:\n- You cannot insert a `DepartmentID` in the Employees table unless it already exists in the Departments table.  \n- You cannot delete a department from the Departments table if employees are still assigned to it (unless cascading rules are defined).  \n- This ==prevents orphaned records== and enforces consistency across related tables.  \n### Example\n\n**Departments Table**\n\n| DepartmentID | DepartmentName   |\n|--------------|------------------|\n| 1            | Human Resources  |\n| 2            | IT               |\n| 3            | Marketing        |\n\n**Employees Table**\n\n| EmployeeID | EmployeeName | DepartmentID |\n|------------|--------------|--------------|\n| 101        | Alice        | 1            |\n| 102        | Bob          | 2            |\n| 103        | Charlie      | 1            |\n| 104        | Dana         | 3            |",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "foreign_key",
    "outlinks": [],
    "inlinks": [
      "relating_tables_together",
      "turning_a_flat_file_into_a_database"
    ]
  },
  {
    "category": "DE",
    "filename": "Github Actions",
    "sha": "bf36cb1546db3a24775b96d96ffb6f0f8eaa4508",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Github%20Actions.md",
    "text": "**Purpose:**\nGitHub Actions is a **CI/CD platform** integrated into GitHub that allows you to automate tasks in your repository. You can define workflows that **run on specific events** such as pushes, pull requests, or scheduled times.\n\n**Summary:**\nGitHub Actions lets you **automate code workflows** directly in GitHub, improving efficiency, reproducibility, and integration with other services while keeping secrets and automation steps secure.\n\n**Key Points:**\n\n* **Automation of workflows:** Build, test, and deploy code automatically.\n* **Event-driven:** Workflows trigger on GitHub events (e.g., `push`, `pull_request`).\n* **Customizable jobs:** Define steps that run sequentially or in parallel.\n* **Reusable actions:** Use pre-built actions from the marketplace, or write your own.\n* **Integration:** Can interact with GitHub APIs, external services, or your own infrastructure.\n* **Secrets management:** Store sensitive data (like API tokens) securely and reference them in workflows.\n\n**Common Use Cases:**\n\n* Continuous Integration (CI) for building and testing code.\n* Continuous Deployment (CD) to deploy applications automatically.\n* Automating repetitive tasks, like updating a Gist, sending notifications, or generating documentation.\n* Running scheduled tasks or maintenance scripts.\n\n**Example Workflow Trigger:**\n\n```yaml\nname: Update Resume Gist\non: push\njobs:\n  update-resume-gist:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n      - name: Update Resume Gist\n        uses: exuanbo/actions-deploy-gist@v1\n        with:\n          token: ${{ secrets.TOKEN }}\n          gist_id: 7194d0d073190269ab1a4fedb2f1cb97\n          file_path: resume.json\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "automation",
      "devops",
      "software"
    ],
    "normalized_filename": "github_actions",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Google Sheet Pivots Table",
    "sha": "f580929aefc8f3eda30d411588799bada262ee2a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Google%20Sheet%20Pivots%20Table.md",
    "text": "Sort by rows.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "google_sheet_pivots_table",
    "outlinks": [],
    "inlinks": [
      "google_sheets"
    ]
  },
  {
    "category": "DE",
    "filename": "Grain",
    "sha": "e3016bf9a59355b10abc46653fea9a7b6d5432c3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Grain.md",
    "text": "Grain\n   - Definition: The level of detail or [[granularity]] of the data stored in the fact table.\n   - Importance: Defining the grain is crucial as it determines what each record in the fact table represents (e.g., individual [[Transaction|transactions]], daily summaries).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "grain",
    "outlinks": [
      "granularity",
      "transaction"
    ],
    "inlinks": [
      "dimensional_modelling"
    ]
  },
  {
    "category": "DE",
    "filename": "Graph Query Language",
    "sha": "94d3afd749cdf151ebe2908d58e2dab520a05b3c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Graph%20Query%20Language.md",
    "text": "GQL is an emerging ISO standard for querying graph databases.\n\nNot yet fully finalized, but intended to unify concepts from:\n  - [[neo4j]] [[Cypher]]\n  - SQL/PGQL\n\n==Think of GQL as a future \"standard [[SQL]]\" for graphs.==\n\nTutorial Lesson 1: [GraphQL Crash Course #1 - What is GraphQL?](https://www.youtube.com/watch?v=xMCnDesBggM)\n\nAlternative to using a [[REST API]].\n\n[[REST API]]\n- GET or POST to endpoints, many endpoints\n- Server interacts with a database using [[CRUD]]\n- drawbacks at scale and complexity\n\t- Overfetching of data: GraphQL get specific\n\t- Underfetching, need to go to many different places.\n\nGraphQL:\n- request goes to a single endpoint\n- There is a querying format for this request.\n- Dont need many queries.\n- Mutations similar to CRUD.",
    "aliases": [
      "GQL"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "graph_query_language",
    "outlinks": [
      "neo4j",
      "sql",
      "rest_api",
      "crud",
      "cypher"
    ],
    "inlinks": [
      "neo4j"
    ]
  },
  {
    "category": "DE",
    "filename": "Groupby vs Crosstab",
    "sha": "f783dd688c3aedb2e2ca9f6d4e76932ca7b81138",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Groupby%20vs%20Crosstab.md",
    "text": "In pandas, [[Groupby]] and [[Crosstab]] serve related but distinct purposes for data ==aggregation== and summarization.\n\n- groupby is more flexible for aggregation and transformations,\n- whereas `crosstab` is specifically designed for creating frequency tables and exploring the relationship between categorical variables.\n\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb\n### Key Differences\n\n1. **Purpose**:\n   - `groupby`: Used for performing aggregate functions (sum, mean, count, etc.) on grouped data.\n   - `crosstab`: Used for generating frequency tables or contingency tables.\n\n2. **Output**:\n   - `groupby`: Returns a DataFrame with aggregated values.\n   - `crosstab`: Returns a DataFrame with counts or specified aggregation functions applied across two or more columns.\n\n3. **Usage**:\n   - `groupby`: Can be used with multiple aggregation functions and complex groupings.\n   - `crosstab`: Typically used for counting occurrences and exploring the relationship between two categorical variables.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "groupby_vs_crosstab",
    "outlinks": [
      "crosstab",
      "de_tools",
      "groupby"
    ],
    "inlinks": [
      "groupby"
    ]
  },
  {
    "category": "DE",
    "filename": "Groupby",
    "sha": "89288e8e014d244301e490f4af99f24c78972122",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Groupby.md",
    "text": "Groupby is a versatile method in pandas used to group data based on one or more columns, and then perform aggregate functions on the grouped data. \n\nRelated:\n- [[Groupby vs Crosstab]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation/group_by.ipynb\n### Implementation\n```python\n# Sample DataFrame\ndf = pd.DataFrame({'Category': ['A', 'B', 'A', 'B', 'A'],'Values': [10, 20, 30, 40, 50]})\n# Group by 'Category' and calculate the sum of 'Values'\ngrouped = df.groupby('Category').sum()\nprint(grouped)\n```\nOutput:\n```\n          Values\nCategory        \nA              90\nB              60\n```\n\n\n![[Pasted image 20250323081619.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "type": "grouping",
    "normalized_filename": "groupby",
    "outlinks": [
      "de_tools",
      "pasted_image_20250323081619.png",
      "groupby_vs_crosstab"
    ],
    "inlinks": [
      "aggregation",
      "google_sheets",
      "groupby_vs_crosstab",
      "imputation_techniques",
      "melt",
      "multi-level_index",
      "pandas_stack",
      "pd.grouper"
    ]
  },
  {
    "category": "DE",
    "filename": "Honkit",
    "sha": "60ff1e408dd6560d5623f62964e897bc82d06303",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Honkit.md",
    "text": "https://honkit.netlify.app/examples\n\nhttps://flaviocopes.com/how-to-create-ebooks-markdown/#:~:text=honkit%20works%20great.,and%20let%20CloudFlare%20distribute%20it.\n\nhttps://github.com/rhyslwells/Note_Compiler",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "tool"
    ],
    "normalized_filename": "honkit",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "How is schema evolution done in practice with SQL",
    "sha": "cc39423c2b8a0a41530cc0e9e7f23392ca9441cc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/How%20is%20schema%20evolution%20done%20in%20practice%20with%20SQL.md",
    "text": "Related:\n- [[Database Schema|schema]]\n- [[SQL]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#question",
      "SQL"
    ],
    "normalized_filename": "how_is_schema_evolution_done_in_practice_with_sql",
    "outlinks": [
      "database_schema",
      "sql"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Hosting",
    "sha": "f2f037696609184de226cd1fc93e4580ddf3d8f9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Hosting.md",
    "text": "### Using [[Plotly]]\n\nOnce you have a simple Dash application.\n#### What You Can Do With It\n\n### Free Public Hosting Options\n\n#### a) Render (https://render.com)\n1. Create a free account.\n2. Create a new \"Web Service.\"\n3. Connect your GitHub repo (with `app.py` and `requirements.txt`).\n4. Set the start command: `python app.py`.\n\n**Pros**: \n- ✅ Simple and free, supports Dash\n\n**Cons**: \n- ❌ Cold start delay on the free tier\n\n#### b) Railway (https://railway.app)\n1. Sign up for a free plan.\n2. Link your GitHub project.\n3. Define your start command and Python version.\n\n**Pros**: \n- ✅ Very fast to set up\n\n**Cons**: \n- ❌ May require a credit card to unlock some features",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "hosting",
    "outlinks": [
      "plotly"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "How to normalise a merged table",
    "sha": "43475b85bf60c5ffdbb96d1829c5f3e554360f73",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/How%20to%20normalise%20a%20merged%20table.md",
    "text": "See: [[Normalised Schema]]\n\nSplitting out tables.\n\nResource:\n[Database Normalization for Beginners | How to Normalize Data w/ Power Query (full tutorial!)](https://www.youtube.com/watch?v=rcrsqyFtJ_4)",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "how_to_normalise_a_merged_table",
    "outlinks": [
      "normalised_schema"
    ],
    "inlinks": [
      "normalisation",
      "normalised_schema",
      "powerquery"
    ]
  },
  {
    "category": "DE",
    "filename": "Implementing Database Schema",
    "sha": "269f072f24c26c08b7c2a606c6b3ba16d488f8c7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Implementing%20Database%20Schema.md",
    "text": "To manage and create a database schema in SQLite, you can use the following commands:\n\n- To view all commands used to create a database, execute:\n  ```\n  .schema\n  ```\n- To view the schema for a specific table, use:\n  ```\n  .schema table\n  ```\n- To run a schema from a file, use:\n  ```\n  .read schema.sql\n  ```\n\n## Creating a Database Schema\n\nWhen creating a database schema, follow these steps:\n\n1. **Identify the Tables**: Determine which tables are necessary for your data.\n2. **Define Columns**: Specify the columns for each table.\n3. **Choose Data Types**: Select appropriate data types for each column.\n4. **Establish Keys**: Define primary and foreign keys to maintain data integrity.\n5. **Set Column Constraints**: Ensure that values adhere to specified conditions.\n\nNote that constraints do not need to apply to primary and foreign keys. Common constraints include:\n\n- `CHECK`: Ensures values meet certain criteria (e.g., amount must be greater than 0).\n- `DEFAULT`: Sets a default value for a column.\n- `NOT NULL`: Ensures a column cannot have a NULL value.\n- `UNIQUE`: Ensures all values in a column are distinct.\n\n### Example Schema Creation\n\nHere’s an example of how to create tables for a database:\n\n```sql\nCREATE TABLE cards (\n    \"id\" INTEGER PRIMARY KEY\n);\n\nCREATE TABLE stations (\n    \"id\" INTEGER PRIMARY KEY,\n    \"name\" TEXT UNIQUE NOT NULL,\n    \"line\" TEXT NOT NULL\n);\n\nCREATE TABLE swipes (\n    \"id\" INTEGER PRIMARY KEY,\n    \"card_id\" INTEGER,\n    \"station_id\" INTEGER,\n    \"type\" TEXT NOT NULL CHECK(\"type\" IN ('enter', 'exit', 'deposit')),\n    \"datetime\" NUMERIC NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \"amount\" NUMERIC NOT NULL CHECK(\"amount\" != 0),\n    FOREIGN KEY(\"card_id\") REFERENCES \"cards\"(\"id\"),\n    FOREIGN KEY(\"station_id\") REFERENCES \"stations\"(\"id\")\n);\n```\n\n## Modifying the Schema\n\nTo change an existing schema, you can use commands such as `RENAME`, `ADD COLUMN`, and `DROP COLUMN`. \n\n```sql\nALTER TABLE visits\nRENAME TO swipes;\n\nALTER TABLE swipes\nADD COLUMN \"swipetype\" TEXT;\n\nDROP TABLE \"riders\"; \n```\n\n## Relating Entities\n\nDone using foreign keys.\n\n```sql\nCREATE TABLE visits (\n    \"rider_id\" INTEGER,\n    \"station_id\" INTEGER,\n    FOREIGN KEY(\"rider_id\") REFERENCES \"riders\"(\"id\"),\n    FOREIGN KEY(\"station_id\") REFERENCES \"stations\"(\"id\")\n);\n```\n\nAlso See:\n- [[Many-to-Many Relationships]]\n- [[ER Diagrams]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "database"
    ],
    "normalized_filename": "implementing_database_schema",
    "outlinks": [
      "er_diagrams",
      "many-to-many_relationships"
    ],
    "inlinks": [
      "database_schema"
    ]
  },
  {
    "category": "DE",
    "filename": "Imputation Techniques",
    "sha": "9b949df8b9ed2eea37948d5c75cccfed0088ce06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Imputation%20Techniques.md",
    "text": "For columns with say less than 30% missing data, you can fill in missing values using various imputation techniques.\n\nQuestions when doing:\n- How to do this properly?\n- Does it make sense to do \n- Are there related variables, group based on these\n\nAlso see: [[KNIME]]\n\n```python\nfrom sklearn.preprocessing import Imputer\n```\n#### Column Average Imputation\nFill missing values with the column's average:\n```python\ndf['var1'] = df['var1'].fillna(df['var1'].mean())\n```\n\n#### Using [[Groupby]]\n\nUse `groupby` to calculate averages for a variable with respect to another variable and fill missing values:\n\nUsing `groupby` to fill in missing values involves aggregating data based on certain groups and then using the aggregated values to fill in the blanks. This method is particularly useful when you want to fill missing values with statistics (like mean, median, or mode) calculated within specific groups of data. Here's how you can do it:\n\nSuppose you have a DataFrame with missing values in a column, and you want to fill these missing values with the mean of the group to which each row belongs.\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndata = {\n    'Category': ['A', 'A', 'B', 'B', 'C', 'C','B'],\n    'Value': [10, np.nan, 20, 25, np.nan, 30,np.nan]\n}\ndf = pd.DataFrame(data)\n\n# Display the original DataFrame\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Group by 'Category' and calculate the mean for each group\ngrouped_means = df.groupby('Category')['Value'].transform('mean')\n\n# Fill missing values with the group mean\ndf['Value'] = df['Value'].fillna(grouped_means)\n\n# Display the DataFrame after filling missing values\nprint(\"\\nDataFrame after filling missing values with group means:\")\nprint(df)\n\n  Category  Value\n0        A   10.0\n1        A   10.0\n2        B   20.0\n3        B   25.0\n4        C   30.0\n5        C   30.0\n6        B   22.5\n```\n\nExplanation:\n1. Grouping: The `groupby('Category')` groups the DataFrame by the 'Category' column.\n2. Transformation: The `transform('mean')` function calculates the mean of the 'Value' column for each group and returns a Series with the same index as the original DataFrame. This allows you to align the group means with the original data.\n3. Filling Missing Values: The `fillna(grouped_means)` function fills the missing values in the 'Value' column with the corresponding group mean.\n\nBenefits:\n- Contextual Filling: By using group-specific statistics, you ensure that the imputed values are more contextually relevant compared to using a global statistic like the overall mean.\n- Preservation of Group Characteristics: This method helps maintain the ==inherent characteristics of each group==, which might be lost if a single value is used for imputation across all groups.\n\nThis approach is part of data transformation techniques that help in cleaning and preparing data for analysis or modeling, ensuring that the data is as accurate and representative as possible.\n#### Using functions\n```python\ndef funct(cols):\n    var1 = cols[0]\n    var2 = cols[1]\n    if pd.isnull(var1): # It uses predefined means (`element_1_mean`, `element_2_mean`, `generic_mean`) depending on the value of `var2`.\n        if var2 == 0:\n            return element_1_mean\n        elif var2 == 1:\n            return element_2_mean\n        else:\n            return generic_mean\n    else:\n        return var1\ndf['var1'] = df[['var1', 'var2']].apply(funct, axis=1)\n```\n\n#### Filling with Specific Values\n\nFill missing values with specific common values:\n```python\ntest.loc[test.KitchenQual.isnull(), 'KitchenQual'] = 'TA'\ndata.loc[data['Electrical'].isnull(), 'Electrical'] = 'SBrkr'\ntest.loc[test['LotFrontage'].isnull(), 'LotFrontage'] = test['LotFrontage'].mean()\n```\n#### Other Imputation Methods\n\n##### KNN\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n\nK-nearest neighbors (KNN) imputation: Uses the average of the K most similar data points.\n\t- Example: `from sklearn.impute import KNNImputer` initializes the KNN imputer.\n##### Other\n- Hot deck imputation: Randomly selects existing data points from the group.\n- Cold deck imputation: Replaces missing values with a constant value, often a default like \"0\".",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning"
    ],
    "normalized_filename": "imputation_techniques",
    "outlinks": [
      "knime",
      "groupby",
      "'var1',_'var2'"
    ],
    "inlinks": [
      "data_transformation",
      "handling_missing_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Indexing in cypher",
    "sha": "b178a42e981ac1c1cb3b6091f0674c460f58fe72",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Indexing%20in%20cypher.md",
    "text": "On large graphs, performance becomes critical. Neo4j supports [[Database Index|Index]] and tuning.\n\n## When to Use Indexes\n\n- You often `MATCH` nodes by a property (e.g., `name`, `id`).\n- These searches benefit greatly from an index.\n## How to Create an Index\n\n```cypher\nCREATE INDEX person_name_index IF NOT EXISTS\nFOR (p:Person)\nON (p.name)\n```\n\n- Creates an index on the `name` property for `Person` nodes.\n- `IF NOT EXISTS` avoids errors if the index already exists.\n## Types of Indexes\n\n| Type | Use Case | Example |\n|------|----------|---------|\n| Property Index | Speed up lookups by property | `CREATE INDEX FOR (p:Person) ON (p.name)` |\n| Composite Index | Search by multiple properties | `CREATE INDEX FOR (p:Person) ON (p.name, p.age)` |\n| Full-text Index | Search unstructured text fields | More advanced setup |\n\n## Performance Tips\n\n- Always MATCH on indexed properties for faster queries.\n- Avoid Cartesian Products: if you forget to connect nodes in `MATCH`, Neo4j will combine everything (`n * m`) — extremely slow!\n- Limit the depth when matching variable-length paths (`*1..3`) to avoid exploring the entire graph.\n\nExample of a bad Cartesian product:\n\n```cypher\nMATCH (a:Person), (b:Person) \nRETURN a, b\n```\n\n(no relationship specified between `a` and `b`!)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "indexing_in_cypher",
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "cypher"
    ]
  },
  {
    "category": "DE",
    "filename": "Input is Not Properly Sanitized",
    "sha": "5058247881e4f60190c951087b0c5ca781e2730a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Input%20is%20Not%20Properly%20Sanitized.md",
    "text": "When we say that ==\"input is not properly sanitized,\"== it means that the input data from users or external sources is not being adequately checked or cleaned before being processed by the application. Proper sanitization involves validating and filtering input to ensure it is safe and expected, preventing malicious data from causing harm. Without proper sanitization, applications can be vulnerable to various attacks, such as:\n\n- Command Injection: Malicious commands can be executed on the server.\n- [[SQL Injection]]: Malicious SQL queries can be executed against a database.\n- Cross-Site Scripting (XSS): Malicious scripts can be injected into web pages.\n\nSanitization typically involves:\n- Validating input against expected formats or values.\n- Escaping special characters that could be interpreted as code.\n- Removing or encoding potentially harmful content.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "SQL"
    ],
    "normalized_filename": "input_is_not_properly_sanitized",
    "outlinks": [
      "sql_injection"
    ],
    "inlinks": [
      "security_vulnerabilities"
    ]
  },
  {
    "category": "DE",
    "filename": "Joining Datasets",
    "sha": "e0ae7a487a8f52218e07492be0a7a6d32721efe1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Joining%20Datasets.md",
    "text": "### Joining Datasets\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb\n\n```python\n# [[Merge]]\ndf1 = pd.DataFrame({'key': ['A', 'B'], 'value': [1, 2]})\ndf2 = pd.DataFrame({'key': ['A', 'B'], 'value': [3, 4]})\nmerged_df = pd.merge(df1, df2, on='key')\n\n# Concat\nconcat_df = pd.concat([df1, df2])\n\n# Join\ndf1.set_index('key', inplace=True)\ndf2.set_index('key', inplace=True)\njoined_df = df1.join(df2, lsuffix='_left', rsuffix='_right')\n```\n\nMerging datasets for completeness (also see [[SQL Joins]]).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "joining_datasets",
    "outlinks": [
      "sql_joins",
      "de_tools",
      "merge"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ]
  },
  {
    "category": "DE",
    "filename": "Junction Tables",
    "sha": "4a637ce8c04af4b28c3d81b37519258bcc74e06e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Junction%20Tables.md",
    "text": "Use to facilitate many to many relationships.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "junction_tables",
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ]
  },
  {
    "category": "DE",
    "filename": "KNIME",
    "sha": "56ab8ad373d211600b0dbd54415328e538164aa8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/KNIME.md",
    "text": "- [ ] How to simply implement [[Logistic Regression]] in [[KNIME]]\n\nETL tool\n\nextract form sheets,\ntransform: groupby\nload: looker studio ext\n\nsmall steps\nautomation steps\nno code platform\n\nvisual\n\nNodes:\n- Extract: read organse\n- Transform yello\n- Load red\n- brown \n\nrow filters\n\nAnalytical nodes: custom logic\nrule engine node\n\n\nwork flow annotations: boxes and labeling to group nodes\nreadability\n\nExtensions?\n\nautomation?\n\nAn open source tool for data exploration.\n\nRelated:\n- [[EDA]]\n- [[Data Transformation]]\n\n\n\nVisual workflows are useful for agentic systems\n\nUse workflows to ensure language in text docuements are of a given standard.\n\nTypes of workflows\n\nTerminology updater\n\n### Handling Imbalanced Data\n\nE.g Identifiying fraud: happens rarely\n\nUneven mix of data\n\nClass seperation\n\nHigh accuracy\n\nWhat do the evaluation metrics say\n\nHigh accuracy in prediction for majority but poor for minority class.\n\nPatterns appear most heavily in the majority class.\n\n##### Class-Imbalance Problem:\n aim to ensure that the prediction algorithm pays equal attention to the patterns presented by all classes—majority, minority\n Possible sol: change the distrubiton in the training data\n\n\nData Sampling Methods:\n- Oversampling:  increase the number of minority class\n\t- SMOTE\n- Undersampling\n- Boot straping\n- Combinations\n\nCost Sensitive Methods:  focusing on the cost of making mistakes – the cost of misclassifications.\n- unequal cositing, threshold based\n\nAlgorithimic methods\n: SVM,Decisaino trees,clustering\none classed based\n\n\nEnsemble Methods:\nbagging\nboosting\nactive learning\n\n\n### Knime\n\nCan open google sheets in it\nhttps://www.youtube.com/watch?v=YIzyhRVIpHQ&ab_channel=EasifyDataScience\n\nCan do imputation for missing values",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "transformation"
    ],
    "normalized_filename": "knime",
    "outlinks": [
      "logistic_regression",
      "eda",
      "knime",
      "data_transformation"
    ],
    "inlinks": [
      "imputation_techniques",
      "knime"
    ]
  },
  {
    "category": "DE",
    "filename": "Many-to-Many Relationships",
    "sha": "0975d17f33f642c4115028615706c35bb87dbde9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Many-to-Many%20Relationships.md",
    "text": "### Many-to-Many Relationships\n\nOccurs when multiple records in one table are associated with multiple records in another table.\n\nNeed to use a **junction table** (also known as a bridge table or associative entity). This table will contain foreign keys that reference the primary keys of the two tables involved in the relationship.\n\n### Steps to Implement Many-to-Many Relationships\n\n1. **Identify the Entities**: Determine the two entities that will participate in the many-to-many relationship. For example, consider `students` and `courses`.\n\n2. **Create the Junction Table**: Create a new table that will serve as the junction table. This table will hold the foreign keys from both entities. In our example, we can create a table called `enrollments`.\n\n3. **Define Foreign Keys**: In the junction table, define foreign keys that reference the primary keys of the two related tables. This establishes the relationship between the entities.\n\n4. **Add Additional Attributes (if necessary)**: If needed, you can also include additional attributes in the junction table that are relevant to the relationship itself. For instance, you might want to track the enrollment date.\n\nExample Schema\n\n```sql\n-- Create the students table\nCREATE TABLE students (\n    \"id\" INTEGER PRIMARY KEY,\n    \"name\" TEXT NOT NULL,\n    \"email\" TEXT UNIQUE NOT NULL\n);\n\n-- Create the courses table\nCREATE TABLE courses (\n    \"id\" INTEGER PRIMARY KEY,\n    \"title\" TEXT NOT NULL,\n    \"description\" TEXT\n);\n\n-- Create the junction table for the many-to-many relationship\nCREATE TABLE enrollments (\n    \"student_id\" INTEGER,\n    \"course_id\" INTEGER,\n    \"enrollment_date\" DATE NOT NULL,\n    PRIMARY KEY(\"student_id\", \"course_id\"), -- Composite primary key\n    FOREIGN KEY(\"student_id\") REFERENCES \"students\"(\"id\"),\n    FOREIGN KEY(\"course_id\") REFERENCES \"courses\"(\"id\")\n);\n```\n\nA composite primary key (`student_id`, `course_id`) ensures that each student can enroll in a course only once.\n\nTo retrieve data from a many-to-many relationship, you can use SQL JOIN statements. For example, to find all courses a specific student is enrolled in, you can run:\n\n```sql\nSELECT courses.title\nFROM courses\nJOIN enrollments ON courses.id = enrollments.course_id\nWHERE enrollments.student_id = 1;  -- Replace 1 with the desired student ID\n```\n\n\n\n[[Many-to-Many Relationships]]\n   - Records in Table A relate to multiple records in Table B, and vice versa.\n   - ==Requires a junction table== to manage the relationships.\n   - Example: Students and Courses tables with a junction table Enrollments.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "database"
    ],
    "normalized_filename": "many-to-many_relationships",
    "outlinks": [
      "many-to-many_relationships"
    ],
    "inlinks": [
      "implementing_database_schema",
      "many-to-many_relationships",
      "relating_tables_together"
    ]
  },
  {
    "category": "DE",
    "filename": "Logical Model",
    "sha": "7924646e84eca9ad5e0a8c78da13fe167392d5ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Logical%20Model.md",
    "text": "Logical Model\n   - Customer: CustomerID, Name, Email\n   - Order: OrderID, OrderDate, CustomerID\n   - Book: BookID, Title, Author\n   - Order-Book Relationship: OrderID, BookID\n\nLogical Model\n   - Details the attributes of each data entity.\n   - Specifies relationships without depending on a specific database management system.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "documentation"
    ],
    "normalized_filename": "logical_model",
    "outlinks": [],
    "inlinks": [
      "data_modeling"
    ]
  },
  {
    "category": "DE",
    "filename": "MariaDB",
    "sha": "41ed6410769b6586fa119fcbcc61b489416514ce",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/MariaDB.md",
    "text": "\\#database\\_storage\n\n**MariaDB** is an **open-source relational database management system (RDBMS)**. It was originally created as a fork of **MySQL** in 2009, after concerns arose that Oracle Corporation’s acquisition of [[MySql]] might limit its open-source future.\n\n### Key Points:\n\n* **Origin**: Developed by the original creators of MySQL, led by Michael \"Monty\" Widenius.\n* **Compatibility**: Designed to remain highly compatible with MySQL (same commands, APIs, and table definitions), so applications built for MySQL often work with MariaDB without modification.\n* **Storage Engines**: Supports MySQL storage engines but also introduces its own, such as **Aria** and **ColumnStore** (for analytical workloads).\n* **Performance**: Generally optimized for speed, scalability, and stability, with features like thread pooling and query optimizations.\n* **License**: Released under the **GPL (General Public License)**, ensuring it remains open-source.\n* **Use Cases**:\n  * Web applications (often used as a backend in LAMP/LEMP stacks).\n  * Data warehousing (with MariaDB ColumnStore).\n  * Enterprise systems that need high availability and replication.\n\n### Example:\n\n```sql\n-- Creating a database in MariaDB\nCREATE DATABASE company;\n\n-- Creating a table\nCREATE TABLE employees (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100),\n    department VARCHAR(50),\n    salary DECIMAL(10,2)\n);\n\n-- Inserting data\nINSERT INTO employees (name, department, salary)\nVALUES ('Alice', 'Engineering', 60000.00);\n```\n\nRelated:\n- [[MariaDB vs MySQL]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data",
      null,
      "SQL"
    ],
    "normalized_filename": "mariadb",
    "outlinks": [
      "mysql",
      "mariadb_vs_mysql"
    ],
    "inlinks": [
      "database_management_system_(dbms)"
    ]
  },
  {
    "category": "DE",
    "filename": "Merge",
    "sha": "ea909b78eddfa1601aa18668bce4c51849ef4c3b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Merge.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "merge",
    "outlinks": [],
    "inlinks": [
      "data_transformation_with_pandas",
      "joining_datasets",
      "pandas_join_vs_merge"
    ]
  },
  {
    "category": "DE",
    "filename": "Missing Data",
    "sha": "61d58749376520cd1c30a90729b9357808295adc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Missing%20Data.md",
    "text": "Purpose\n* Investigate gaps in the dataset to understand their causes and impact.\n* Decide how to treat missing values (retain, remove, or impute).\n* Ensure the dataset remains useful for analysis and modeling.\n* Assess missingness: how much, patterns, and impact.\n* Impute or drop as appropriate based on project context.\n* Document decisions for reproducibility and stakeholder communication.\n\nKey Questions\n* How much data is missing overall and per variable?\n* Are there patterns in missingness (random vs systematic)?\n* Do related columns share similar missing values?\n* Are missing values clustered (e.g., certain groups, neighborhoods, or time periods)?\n* Can the chosen model handle missing data directly, or is [[Preprocessing]] required?\n\nExploration Approaches\n* Use data exploration tools (Python, JavaScript, etc.) to summarize each column:\n  * Mean, [[Variance]], skewness, min, max, sum.\n  * Count of zeros and count of missing values.\n* Inspect repeating patterns in missingness.\n* Compare across related features (column dependencies).\n\nTechniques\n* Simple imputation: mean, median, or mode.\n* Predictive imputation: regression or ML-based methods (more complex).\n* Row/column filtering: dropping rows or columns when appropriate.\n* Type conversion: e.g., numbers → strings to preserve categorical meaning.\n* Some models (e.g., tree-based methods) can handle missing values directly.\n\nOutcome\n* A documented set of working theories on why values are missing.\n* A strategy (or multiple strategies) for dealing with missingness before modeling ([[Handling Missing Data]]).\n\nReference\n* NASA (2023). *Dealing with Missing Data: The Art and Science of Imputation*. [Link](https://www.nasa.gov/wp-content/uploads/2023/06/01-dealing-with-missing-data-the-art-and-science-of-imputation.pdf)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning"
    ],
    "normalized_filename": "missing_data",
    "outlinks": [
      "preprocessing",
      "handling_missing_data",
      "variance"
    ],
    "inlinks": [
      "em_algorithm",
      "feature_engineering_for_time_series",
      "handling_missing_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Microsoft Access",
    "sha": "a2b5133fb6b3bdc252f2bda7207732b82db9537b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Microsoft%20Access.md",
    "text": "### Tasks\n\n- [ ] How to update (or more so insert) into multiple related tables. How to insert existing data into a [[Database]]. SQL triggers?\n- [ ] Investigate: Helper tables gather many small tables into one.\n- [ ] What are typical types of databases.\n- [x] Questions: Make a form that accesses multiple tables. ✅ 2025-07-20\n### Resources\n[Tutorial](https://www.youtube.com/watch?v=ubmwp8kbfPc)\n\n[Best Practices](https://www.youtube.com/watch?v=ymc9CYnziS4)\n\n[LINK](https://youtu.be/ymc9CYnziS4)\n\n[TIME](https://youtu.be/ymc9CYnziS4?t=1042)\n### Notes\n\nWhy use access:\n\tHandles lots of data better than excel. Understand relationships between sources of data.\n \n[[Querying]]:\n\tCan do querying. Which might be hard to do in excel.\n\tGraphical way to make queries.\n\nForms:\n\tAccess can make it easier for user interfaces \n\tforms- opening other forms, drop downs, user interface\n\tsecure fields on forms so users can only see so much \n\nFeatures:\n\tHas user security.\n\tHas control over the types of data input.\n\tUser control feature\n\tuser friendly. \n\nIssues:\n\tPossible limitations when scale increases. Next steps, can upscale to SQL server.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "software"
    ],
    "normalized_filename": "microsoft_access",
    "outlinks": [
      "database",
      "querying"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Model Deployment",
    "sha": "ba9f993760b0574b51ed6c5b6c945c1c4d0f13a7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Model%20Deployment.md",
    "text": "Deploying a machine learning model involves moving it from a development environment to a production environment where it can make predictions on new data.\n## Steps for Model Deployment\n\nModel Exporting\n   - Use tools like `joblib` or `pickle` to serialize the model.\n     ```python\n     import joblib\n     joblib.dump(model, 'linear_regression_model.pkl')\n     ```\n\nDeployment Options\n   - Application Integration: Embed the model into an application for real-time predictions.\n   - [[API]] Deployment: Use frameworks like [[Flask]] or [[FastAPI]] to create an API endpoint for the model.\n   - Automated Workflows: Integrate the model into automated data processing pipelines.\n## Tools and Platforms\n\n- [[Sklearn Pipeline]]: Streamline the deployment process by integrating [[Preprocessing]] and model steps.\n- [[Gradio]]: Create user-friendly interfaces for model interaction.\n- [[Streamlit.io]]\n\n## Considerations\n\n- [[Scalability]]: Ensure the deployment solution can handle the expected load.\n- [[Model Observability]]: Implement monitoring to track model performance and detect issues.\n\nRelated:\n- [[Model Deployment using PyCaret]]\n- [[Challenges to Model Deployment]]",
    "aliases": [
      "Deployment"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "model_deployment",
    "outlinks": [
      "streamlit.io",
      "preprocessing",
      "model_deployment_using_pycaret",
      "flask",
      "challenges_to_model_deployment",
      "api",
      "scalability",
      "model_observability",
      "fastapi",
      "sklearn_pipeline",
      "gradio"
    ],
    "inlinks": [
      "continuous_delivery_-_deployment",
      "data_deployment",
      "gitlab-ci.yml",
      "machine_learning_operations",
      "pycaret",
      "testing",
      "vercel"
    ]
  },
  {
    "category": "DE",
    "filename": "Monolith Architecture",
    "sha": "152f06fe2580081685b04586c059837690d3cc52",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Monolith%20Architecture.md",
    "text": "A monolith, in the context of [[software architecture]], refers to a ==single, unified application where all components and functionalities are interconnected and interdependent==. In a monolithic architecture, the entire application is typically built as a ==single codebase==, and all functions and modules are tightly coupled.\n\nWhile monolithic architectures can be simpler to develop and deploy initially, they can become cumbersome as the application grows in complexity. Many organizations eventually transition to [[microservices]] or other modular architectures to improve scalability, flexibility, and [[Maintainability]]. However, monoliths can still be effective for smaller applications or teams with limited resources.\n\nWhen we talk about a \"function call-driven\" monolith, we are referring to the way in which different parts of the application interact with each other. In such a system:\n\n1. **Tightly Coupled Components**: All components of the application are part of a single codebase and often share the same resources, such as databases and libraries.\n\n2. **Function Calls**: Communication between different parts of the application is primarily done through direct function or method calls. This means that one part of the application can directly invoke functions or methods in another part.\n\n3. **Single Deployment Unit**: The entire application is deployed as a single unit. Any changes to one part of the application require redeploying the whole application.\n\n4. **Shared Memory Space**: Since all components are part of the same application, they often share the same memory space, which can simplify data sharing but also lead to issues with scalability and fault isolation.\n\n5. **Challenges with Scalability and Flexibility**: As the application grows, a monolithic architecture can become difficult to manage, scale, and update. Changes in one part of the system can have unintended consequences elsewhere, making it challenging to innovate quickly.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "monolith_architecture",
    "outlinks": [
      "maintainability",
      "software_architecture",
      "microservices"
    ],
    "inlinks": [
      "event_driven_events"
    ]
  },
  {
    "category": "DE",
    "filename": "Multi-level index",
    "sha": "76d7dea8df2bde8aa96d8da0c349db293725965f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Multi-level%20index.md",
    "text": "Multi-level indexing in pandas—also called hierarchical indexing—enables you to work with higher-dimensional data in a 2D DataFrame. It's particularly useful for working with grouped or nested data structures.\n\nWhy use multi-level index:\n- MultiIndex makes your data [[interoperable]]\n- Enables systematic slicing and aggregation\n- Logical grouping of variables\n\nOperations like `.stack()` and `.unstack()` rely on MultiIndex to move between long and wide formats.\n- In a flat DataFrame, reshaping often requires column renaming or pivoting.\n- With MultiIndex, it's structured and reversible.\n- Stack can be used to make a multi index from a flat dataframe.\n\nIf you need frequent slicing/aggregation across multiple levels, MultiIndex saves effort and code.\n\nWhen _not_ to use it\n- If your data is simple or small.\n- If you're just loading, cleaning, and exporting CSVs.\n- If you don’t need `.groupby(level=...)`, `.stack()`, or `.xs()` operations.\n\nSimilar to:\n- SQL composite keys\n- Python nested dictionaries\n- [[Json]] hierarchical structures\n\nRelated:\n- [[Groupby]]\n- [[Pandas Stack]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/multi__level_index.ipynb\n\nHow this mimics a 3D array:\n- You can think of each (Product, Store) pair as defining a \"slice\" of a 2D array.\n- The columns (Jan, Feb) represent time-like progression (3rd axis).\n- Visually, it’s like you’ve flattened a cube into a matrix while retaining the ability to slice along all original axes.\n\n```python\nMonth             Jan  Feb\nProduct   Store            \nProduct A Store X  100  110\n          Store Y  120  115\nProduct B Store X   90  105\n          Store Y   95  100\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "multi-level_index",
    "outlinks": [
      "pandas_stack",
      "interoperable",
      "json",
      "de_tools",
      "groupby"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "melt",
      "pandas_stack",
      "pd.grouper",
      "structuring_and_organizing_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Multiprocessing",
    "sha": "9ccbfb60c8899c664b15002bce197d4bc2d06b95",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Multiprocessing.md",
    "text": "## What Is Multiprocessing?\n\nMultiprocessing is a form of *parallelism* that allows a program to execute multiple tasks simultaneously by using multiple CPU cores. Unlike [[Multithreading]] (which shares the same memory space), multiprocessing involves separate memory spaces, each with its own Python interpreter. This avoids Python’s [[Global Interpreter Lock]] (GIL), making it well-suited for CPU-bound tasks.\n\nRelated:\n[[Multiprocessing vs Multithreading]]\n## Why Multiprocessing in Python?\n\nPython's GIL prevents true multithreading in CPython when performing CPU-bound operations. Multiprocessing sidesteps this by creating independent processes, thus taking advantage of multiple CPU cores.\n\nPython provides this functionality via the `multiprocessing` module in the standard library.\n## How Is It Done in Python?\n\n### 1. Basic Usage: `multiprocessing.Process`\n\n```python\nfrom multiprocessing import Process\n\ndef task():\n    print(\"Running in a separate process\")\n\np = Process(target=task)\np.start()\np.join()\n```\n\nEach `Process` runs in its own memory space and executes independently.\n\n### 2. Using `multiprocessing.Pool` (simplified parallel mapping)\n\n```python\nfrom multiprocessing import Pool\n\ndef square(x):\n    return x * x\n\nwith Pool(processes=4) as pool:\n    results = pool.map(square, [1, 2, 3, 4, 5])\n    print(results)  # Output: [1, 4, 9, 16, 25]\n```\n\n`Pool` is useful when you want to apply a function to a collection of data in parallel.\n\n### 3. Using `multiprocessing.Queue` and `multiprocessing.Pipe` for Interprocess Communication (IPC)\n\n```python\nfrom multiprocessing import Process, Queue\n\ndef f(q):\n    q.put('Hello from process')\n\nq = Queue()\np = Process(target=f, args=(q,))\np.start()\nprint(q.get())  # prints 'Hello from process'\np.join()\n```\n\n### 4. Shared Memory via `Value` or `Array`\n\n```python\nfrom multiprocessing import Process, Value\n\ndef f(n):\n    n.value += 1\n\nnum = Value('i', 0)\np = Process(target=f, args=(num,))\np.start()\np.join()\nprint(num.value)  # 1\n```\n\n## Use Cases of Multiprocessing\n\n### 1. CPU-Bound Tasks\n\nTasks that require intensive computation:\n* Image or video processing\n* Mathematical simulations\n* Data transformations or encodings\n* Cryptographic computations\n\n### 2. [[Batch Processing]] of Independent Jobs\n* Applying a model to independent chunks of data\n* OCR on many files (e.g., with EasyOCR)\n\n### 3. Pipeline Stages in Data Processing\n\nDifferent processes handling different stages:\n* Process 1: Reading data\n* Process 2: Transformation\n* Process 3: Writing to disk or DB\n\nThis can be coordinated using `Queue`s or `Pipe`s.\n\n## When Not to Use It\n\n* When the task is I/O-bound → prefer `asyncio` or `multithreading`.\n* When overhead from starting processes outweighs benefits (e.g., tiny jobs).\n* When memory sharing is important — inter-process communication is more complex than threads.\n## Related Tools and Libraries\n\n* `joblib`: High-level interface, useful in [[Scikit-Learn]].\n* `concurrent.futures.ProcessPoolExecutor`: Modern interface for multiprocessing.\n* `ray`, `dask`, `loky`: Advanced distributed and parallel processing.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "multiprocessing",
    "outlinks": [
      "multiprocessing_vs_multithreading",
      "global_interpreter_lock",
      "multithreading",
      "batch_processing",
      "scikit-learn"
    ],
    "inlinks": [
      "global_interpreter_lock",
      "multiprocessing_vs_multithreading"
    ]
  },
  {
    "category": "DE",
    "filename": "MySql",
    "sha": "7ef94522265e1b194023e783c1332445d341a340",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/MySql.md",
    "text": "MySQL has more ==[[granularity]]== with types than [[SQLite]]. For example, an integer could be `TINYINT`, `SMALLINT`, `MEDIUMINT`, `INT` or `BIGINT` based on the size of the number we want to store. \n\nThe following table shows us the size and range of numbers we can store in each of the integer types.\n    \n![\"Table of integer types in MySQL\"|500](https://cs50.harvard.edu/sql/2024/notes/6/images/12.jpg)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "management",
      "SQL"
    ],
    "normalized_filename": "mysql",
    "outlinks": [
      "granularity",
      "sqlite"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "database_management_system_(dbms)",
      "google_cloud_platform",
      "mariadb",
      "sql_injection",
      "sqlalchemy"
    ]
  },
  {
    "category": "DE",
    "filename": "NoSQL",
    "sha": "78c898af4f697a359c1e4d80531e4e476ea1b733",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/NoSQL.md",
    "text": "(Not Only SQL):** ==Non-relational== database management systems offering flexibility and scalability for unstructured or document-based data.\n\n **NoSQL Databases**: Accommodate unstructured data and can be represented through graph theory or document-based structures, allowing for flexible data models.\n\n\nDefinition: A family of databases that store data in non-tabular formats, often optimized for scalability, flexibility, or specific access patterns.\n\n#### Types:\n* Document (e.g., [[MongoDB]], Couchbase)\n* Key-Value (e.g., Redis, DynamoDB)\n* Column-Family (e.g., Cassandra, HBase)\n* Graph (e.g., Neo4j, ArangoDB)\n#### Characteristics:\n\n* Flexible or schema-less design\n* Eventually consistent (in distributed systems)\n* Optimized for horizontal scaling\n* Better suited for unstructured or semi-structured data\n#### Use Cases:\n* Real-time analytics\n* Content management\n* IoT and sensor data\n* Social networks\n* Mobile and web apps",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "SQL"
    ],
    "normalized_filename": "nosql",
    "outlinks": [
      "mongodb"
    ],
    "inlinks": [
      "data_storage",
      "google_cloud_platform",
      "sql_vs_nosql"
    ]
  },
  {
    "category": "DE",
    "filename": "Normalised Schema",
    "sha": "6a46d147195a7e0a6b191bb25b130ec5b206c923",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Normalised%20Schema.md",
    "text": "In a normalized schema, data is organized into multiple related tables to minimize redundancy and dependency, and improve data integrity.\n\nThis approach is often used in transactional databases (OLTP) to ensure data integrity. However, it can lead to complex queries and slower performance for analytical queries.\n\nNormalization involves organizing the columns (attributes) and tables (relations) in a database to ensure proper enforcement of dependencies through database integrity constraints. \n\nThis is achieved by applying formal rules during the creation of a new database design or ==decomposition== (improvement of an existing database design) process.\n\n1.  **First Normal Form (1NF)**:\n    - Eliminate duplicate data by ensuring each attribute contains only atomic values and each table has a unique primary key.\n2.  **Second Normal Form (2NF)**:\n    - Meet all requirements of 1NF and ==remove partial dependencies== by ensuring that ==every non-prime attribute (attribute not part of any candidate key) entirely depends on the primary key.==\n3.  **Third Normal Form (3NF)**:\n    - Meet all requirements of 2NF and remove ==transitive dependencies== by ensuring that no non-prime attribute is transitively dependent on the primary key.\n\n[See here for an example](https://youtu.be/rcrsqyFtJ_4?t=885)\n\n[[How to normalise a merged table]]\n## Denormalization\n\n**Denormalization**, on the other hand, is the process of intentionally introducing redundancy into a database design by combining tables or adding redundant data, aiming to improve query performance or simplify the database structure. Denormalization is the **opposite of normalization**. Please consider the trade-offs between data integrity and query performance. This technique is used with [Dimensional Modeling](Dimensional%20Modelling.md) in [OLAP](OLAP.md) cubes, for example.\n\n# Related to:\n\n[[Normalisation of data]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "normalised_schema",
    "outlinks": [
      "normalisation_of_data",
      "how_to_normalise_a_merged_table"
    ],
    "inlinks": [
      "data_transformation",
      "how_to_normalise_a_merged_table",
      "normalisation",
      "snowflake_schema",
      "types_of_database_schema",
      "why_use_er_diagrams"
    ]
  },
  {
    "category": "DE",
    "filename": "OLTP",
    "sha": "957f2afcf9e288ac05d6a12217c64a2ca1d4b67a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/OLTP.md",
    "text": "In online transaction processing (**OLTP**), information systems typically facilitate and manage **transaction-oriented** applications. It's the opposite of [OLAP (Online Analytical Processing)](OLAP.md).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "oltp",
    "outlinks": [],
    "inlinks": [
      "database",
      "row-based_storage"
    ]
  },
  {
    "category": "DE",
    "filename": "Object Relational Mapper",
    "sha": "543d65eaada35f802ed9989b43987cc88cee3e88",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Object%20Relational%20Mapper.md",
    "text": "[[SQLAlchemy]]",
    "aliases": [
      "ORM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "programming",
      "SQL"
    ],
    "normalized_filename": "object_relational_mapper",
    "outlinks": [
      "sqlalchemy"
    ],
    "inlinks": [
      "sql_injection",
      "sqlalchemy",
      "sqlalchemy_vs._sqlite3"
    ]
  },
  {
    "category": "DE",
    "filename": "Overfitting",
    "sha": "64537d15de8eb1f71800fdc5ec55e1ed34980432",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Overfitting.md",
    "text": "Overfitting in machine learning occurs when a model captures not only the underlying patterns in the training data ==but also the noise==, leading to poor performance on unseen data, and is unable to generalise.\n \nMathematically, overfitting results in a model with ==low bias but high variance==, meaning it adapts too closely to the training data and fails to generalize well.\n\nKey methods to address overfitting include [[Regularisation]] (such as $L_1$ and $L_2$ regularization), [[Cross Validation]], and simpler models.\n\nIn statistical terms, it indicates a model with high complexity and too many parameters relative to the amount of training data, which results in $f(x)$ poorly representing the population distribution.\n\n Key Components:  \n - Regularization (Lasso: $L_1$, Ridge: $L_2$) to penalize model complexity.  \n - [[Cross Validation]] to ensure model generalization.  \n - Early stopping in training to avoid learning noise.  \n - Simplification of models to prevent fitting irrelevant patterns.\n\nImportant\n - Overfitting indicates high variance in the model’s performance, which can be ==identified by a significant drop in accuracy between training and test datasets.==  \n - Regularization adds penalty terms to the cost function, reducing model complexity and mitigating overfitting.\n\nAttention\n - Overfitting is more common in models with high-dimensional datasets.  \n - Excessive model tuning (hyperparameter optimization) may inadvertently increase overfitting.\n\n[!Follow up questions]  \n - How does the choice of regularization type (e.g., $L_1$ vs. $L_2$) affect model generalization in overfitting scenarios?  \n - What role does the size of the training dataset play in mitigating overfitting?\n\n[!Related Topics]  \n - [[Cross Validation]] techniques (e.g., $k$-fold, Leave-One-Out cross-validation)  \n - [[Bias in ML]]radeoff in machine learning models  \n\n\nThe distinction:\n* High bias -> underfitting\n  * Model is too simple.\n  * Strong assumptions.\n  * Fails to capture the true relationship.\n\n* High variance -> overfitting\n  * Model is too complex / too sensitive to training data.\n  * Captures noise as if it were signal.\n  * Performs well on training data but poorly on unseen data.\n\nSo:\n* Underfitting = high bias, low variance\n* Overfitting = low bias, high variance",
    "aliases": [
      "high variance models",
      "model overfitting"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "overfitting",
    "outlinks": [
      "regularisation",
      "bias_in_ml",
      "cross_validation"
    ],
    "inlinks": [
      "backpropagation",
      "bagging",
      "batch_normalisation",
      "bias-variance_trade_off",
      "cross_validation",
      "decision_tree",
      "dropout",
      "ds_&_ml_portal",
      "feed_forward_neural_network",
      "gradient_boosting",
      "learning_curve",
      "machine_learning_algorithms",
      "model_evaluation",
      "parsimonious",
      "ridge"
    ]
  },
  {
    "category": "DE",
    "filename": "Pandas Stack",
    "sha": "e4fdad6a549608c11e5a81cef71f03274ac8a6b4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pandas%20Stack.md",
    "text": "Tool for reshaping data, particularly when you need to pivot a DataFrame ([[Pandas Pivot Table]]) from a wide format to a long format. \n\nSee:\n- [[Pandas_Common.py]]\n- [[Pandas_Stack.py]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb\n#### Why Use `stack`? ([[Data Transformation]])\n\nData Reshaping:\n   - ==Wide to Long Format==: Convert a DataFrame from a wide format to a long format, which is often preferred for statistical models and visualizations.\n\nHandling Multi-Index DataFrames:\n   - Simplifying Structure: Move the inner level of a column MultiIndex to the row index, simplifying the DataFrame's structure.\n\n[[Data Cleansing]]:\n   - Aggregation and Operations: Facilitate data cleaning by allowing aggregation or operations across columns in a more manageable long format.\n\nPreparing Data for Grouping or Aggregation ([[Groupby]]):\n   - Ease of Grouping: Simplify group-by operations and aggregations on data with columns representing different categories or time periods.\n#### Example of Using `stack`\n\nConsider the following example to illustrate how `stack` works:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Using stack\nstacked_df = df.stack()\nprint(\"\\nStacked DataFrame:\")\nprint(stacked_df)\n```\n\nOutput:\n```\nOriginal DataFrame:\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nStacked DataFrame:\n0  A    1\n   B    4\n   C    7\n1  A    2\n   B    5\n   C    8\n2  A    3\n   B    6\n   C    9\ndtype: int64\n```\n\nIn this example:\n- The original DataFrame has three columns ('A', 'B', 'C') and three rows.\n- After stacking, the DataFrame is transformed into a Series with a MultiIndex/[[Multi-level index]]. The outer level of the index corresponds to the original DataFrame’s row index, and the inner level corresponds to the original column labels.\n\n#### When Not to Use `stack`\n\n- Wide Format Requirements: If your analysis or processing requires a wide format, such as some [[Machine Learning Algorithms]], stacking may not be appropriate.\n- Complexity: If stacking makes the data too complex to manage or understand, it might be better to keep the original structure.\n- Simplicity: When the current structure of your DataFrame already suits your analysis needs, stacking may be unnecessary.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "pandas_stack",
    "outlinks": [
      "pandas_pivot_table",
      "data_transformation",
      "data_cleansing",
      "machine_learning_algorithms",
      "pandas_stack.py",
      "de_tools",
      "groupby",
      "pandas_common.py",
      "multi-level_index"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "multi-level_index"
    ]
  },
  {
    "category": "DE",
    "filename": "Pandas Pivot Table",
    "sha": "c44308cf38ea48c6e81af6f0093ba4c2e8bf3efd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pandas%20Pivot%20Table.md",
    "text": "Pivot Table: Summarize Data\n```python\ndf = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': ['one', 'two', 'one'], 'C': [1, 2, 3]})\npivot_table = df.pivot_table(values='C', index='A', columns='B', aggfunc='sum')\n```\n\nRelevant links:\n- https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "transformation"
    ],
    "normalized_filename": "pandas_pivot_table",
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "aggregation",
      "pandas_stack"
    ]
  },
  {
    "category": "DE",
    "filename": "Pandas join vs merge",
    "sha": "21cf7d152463665b77a5d3f7e55071ead3d52fb8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pandas%20join%20vs%20merge.md",
    "text": "In pandas, both `.join()` and `pd.merge()` are used to combine DataFrames, but they differ in **syntax**, **defaults**, and **use cases**.\n\n[[Merge]] is better than Join.\n\n|Feature|`df.join()`|`pd.merge()`|\n|---|---|---|\n|**Default key**|Uses index of caller and index/column of other|Requires explicit column(s) to merge on|\n|**Syntax style**|Method on a DataFrame|Function (`pd.merge(left, right)`)|\n|**Column join**|Must specify `on=` and use one column from each|Can merge on multiple columns|\n|**Index join**|Default behavior (index-to-index)|Requires `left_index=True`, `right_index=True`|\n|**Suffixes**|`lsuffix`, `rsuffix`|`suffixes=('_x', '_y')`|\n|**Complex joins**|Not well-suited|Supports full SQL-style joins|\n|**Use case**|Simple joins on index or one column|Complex joins with control over join behavior|",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "transformation"
    ],
    "normalized_filename": "pandas_join_vs_merge",
    "outlinks": [
      "merge"
    ],
    "inlinks": [
      "data_transformation_with_pandas"
    ]
  },
  {
    "category": "DE",
    "filename": "Pandas",
    "sha": "a80f9305eac94bf177ea80b2c2278a47898b150f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pandas.md",
    "text": "In [[ML_Tools]] see:\n- [[Pandas_Common.py]]\n\nAreas:\n- [[Handling Missing Data]] \n- [[Data Selection]]\n- [[Data Transformation]]\n\n[[Pandas Series vs DataFrame]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "transformation"
    ],
    "type": "grouping",
    "normalized_filename": "pandas",
    "outlinks": [
      "data_transformation",
      "handling_missing_data",
      "data_selection",
      "pandas_series_vs_dataframe",
      "ml_tools",
      "pandas_common.py"
    ],
    "inlinks": [
      "csv_module",
      "data_transformation_with_pandas",
      "fuzzywuzzy",
      "langchain",
      "pyspark",
      "sqlalchemy",
      "vectorisation"
    ]
  },
  {
    "category": "DE",
    "filename": "Pgadmin Permissions on Windows",
    "sha": "5e4b5f76c2317d5ebe11e2f5621c7aed8bec4e9e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pgadmin%20Permissions%20on%20Windows.md",
    "text": "Pgadmin Permissions on Windows\n\nWindows changing permissions of folders for pgadmin access\n\nIf you are using Windows, the following steps should set the permissions:\n\n1. Right click the folder containing the file and select Properties\n2. Select the Security tab\n3. Click the Edit button\n4. Click Add...\n5. Enter everyone in the text box\n6. Click Check names (the word Everyone should now be underlined)\n7. Click OK\n8. With the Everyone item selected in the Group or user names box, select all the checkboxes in the Allow column\n9. Click Apply\n10. Click Apply again",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "pgadmin_permissions_on_windows",
    "outlinks": [],
    "inlinks": [
      "pgadmin"
    ]
  },
  {
    "category": "DE",
    "filename": "Physical Model",
    "sha": "4766b52504f5657c2f717c7387b2bafb6b678262",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Physical%20Model.md",
    "text": "Physical Model\n\n(for a SQL database):\n   ```sql\n   CREATE TABLE Customer (\n       CustomerID INT PRIMARY KEY,\n       Name VARCHAR(100),\n       Email VARCHAR(100)\n   );\n\n   CREATE TABLE Order (\n       OrderID INT PRIMARY KEY,\n       OrderDate DATE,\n       CustomerID INT,\n       FOREIGN KEY (CustomerID) REFERENCES Customer(CustomerID)\n   );\n\n   CREATE TABLE Book (\n       BookID INT PRIMARY KEY,\n       Title VARCHAR(100),\n       Author VARCHAR(100)\n   );\n\n   CREATE TABLE OrderBook (\n       OrderID INT,\n       BookID INT,\n       PRIMARY KEY (OrderID, BookID),\n       FOREIGN KEY (OrderID) REFERENCES Order(OrderID),\n       FOREIGN KEY (BookID) REFERENCES Book(BookID)\n   );\n   ```\n\n\n\nPhysical Model\n   - Implements the logical model in a specific database system.\n   - Includes table structures, columns, data types, and constraints.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "documentation",
      "SQL"
    ],
    "normalized_filename": "physical_model",
    "outlinks": [],
    "inlinks": [
      "data_modeling"
    ]
  },
  {
    "category": "DE",
    "filename": "Pickle",
    "sha": "e0631fd5954c6bffba71514f7e87c7840e6def17",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pickle.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type",
      "storage"
    ],
    "normalized_filename": "pickle",
    "outlinks": [],
    "inlinks": [
      "why_json_is_better_than_pickle_for_untrusted_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Poetry",
    "sha": "e3a1c6eca3c1dab38d781c3ba15fcd29552ef340",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Poetry.md",
    "text": "Modern version of setting up dependencies instead of requirements.txt ([[dependency manager]])\n\nPrimary Purpose: Poetry is a [[dependency manager]] and packaging tool for Python projects.\n    \nMain Features:\n    - Dependency management: Allows you to declare, install, and lock dependencies in the `pyproject.toml` file.\n    - Package management: Can help package your Python project for distribution (e.g., publishing to PyPI).\n    - Virtual environment management: Poetry automatically creates and manages a virtual environment for your project.\n    - Version management: Ensures that all your project dependencies use compatible versions through its `poetry.lock` file, similar to `npm` or `yarn` in the JavaScript ecosystem.\n    - Built-in publishing: Simplifies the process of publishing your Python package to PyPI.\n\nIdeal Use Case:\n    - If you need to manage dependencies for a Python project, create virtual environments, and ensure reproducibility (using `poetry.lock`).\n    - If you're developing a Python package that you want to distribute or manage versions for, Poetry is a great choice.\n\nUse:\n\n```cmd\npip install poetry\npoetry init\npoetry add numpy\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "system"
    ],
    "normalized_filename": "poetry",
    "outlinks": [
      "dependency_manager"
    ],
    "inlinks": [
      "dependency_manager",
      "virtual_environments"
    ]
  },
  {
    "category": "DE",
    "filename": "Polars",
    "sha": "b26b015d3552a3d0e2bf2ec4159574763c8d08ba",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Polars.md",
    "text": "Large scale data processing\n\nWritten in Rust\n\nSimilar to Pandas\n\nPolars API is strict\n\nPolars uses slice instead of loc and iloc in pandas\n\nPolars vs Pandas:\n\n- **Pandas**: Mature tool with a large community, wide range of features, and seamless integration with other Python libraries. However, it can be slow and memory-intensive with large datasets.\n- **Polars**: Designed for speed and memory efficiency, especially with large datasets. It uses Rust for performance and supports lazy evaluation for optimized query execution. It has a modern API similar to Pandas but is still growing in terms of features and library integration.\n- **Usage Recommendations**: Use Pandas for smaller datasets and when you need extensive functionality and integration. Opt for Polars when working with large datasets and performance is crucial. Both libraries can be used together in different parts of your data science workflows.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "polars",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "PostgreSQL",
    "sha": "789bbbc92c89a0acd68e01c9a9d0352bd73a7ed3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/PostgreSQL.md",
    "text": "Think of PostgreSQL as the “brains” — it does the actual work of managing data.\n\n- Type: Database management system (DBMS).\n- Role: The actual database engine that stores, processes, and manages your data.\n- Nature: Runs as a server process and is accessed via SQL queries (e.g., through `psql`, scripts, or applications).\n- Usage:\n    - Define schemas, tables, indexes, and relationships.\n    - Insert, query, update, and delete data.\n    - Handle transactions, concurrency, and security.\n- Interface: Can be used purely via command line or programmatically through APIs/drivers (Python `psycopg2`, JDBC, etc.).\n\nRelated:\n- [[pgAdmin]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "management",
      "SQL"
    ],
    "normalized_filename": "postgresql",
    "outlinks": [
      "pgadmin"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "database_management_system_(dbms)",
      "looker_studio",
      "normalisation_of_data",
      "pgadmin",
      "sqlalchemy",
      "tableau"
    ]
  },
  {
    "category": "DE",
    "filename": "Postman",
    "sha": "71ae3ec15b090115226e29df203fd271f4aef30a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Postman.md",
    "text": "www.postman.com\n\nPublic [[API]]s:\nhttps://github.com/public-apis/public-apis",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "postman",
    "outlinks": [
      "api"
    ],
    "inlinks": [
      "rest_api"
    ]
  },
  {
    "category": "DE",
    "filename": "PowerShell",
    "sha": "563e8977685ebc039e4e932f7de7a0cfcf32f0b5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/PowerShell.md",
    "text": "PowerShell is a task automation and configuration management framework developed by Microsoft, consisting of a [[Command Line]] shell and an associated scripting language. It is designed to automate system administration tasks across local and remote Windows systems and, with PowerShell Core (now known as PowerShell 7+), across macOS and Linux as well.\n\nRelated:\n- [[Command Prompt]]\n- [[Powershell versus Command Prompt]]\n- [[Powershell scripts]]\n\n\nObject-Oriented Architecture: Unlike `cmd`, which handles plain text, PowerShell processes .[[NET]] objects. This allows for structured data manipulation and precise control of outputs. Example:\n    \n```powershell\nGet-ChildItem | Select-Object Name, Length, LastWriteTime\n```\n    \nIntegrated with .[[NET]]: PowerShell scripts can invoke .NET classes and use assemblies directly. Example:\n    \n```powershell\n[System.IO.File]::ReadAllText(\"C:\\path\\to\\file.txt\")\n```\n\nCmdlets: PowerShell uses built-in, standardized command modules (cmdlets) for common tasks. These provide consistent naming and behavior, unlike the ad-hoc commands in `cmd`. Example:\n\n```powershell\nRestart-Service -Name \"Spooler\"\n```\n\nAdvanced Scripting Support: Supports rich scripting constructs such as functions, conditionals, loops, and error handling. Example:\n\n```powershell\nif (Test-Path \"file.txt\") { Write-Output \"Exists\" } else { Write-Output \"Missing\" }\n```\n\nPipeline with Objects: PowerShell supports object-based pipelines, enabling powerful command chaining and transformations.\n\nRemote Management: PowerShell includes native support for remote system management via PowerShell Remoting (e.g., `Enter-PSSession`, `Invoke-Command`), a capability not present in `cmd`.\n\nCross-Platform Compatibility: PowerShell Core (now PowerShell 7+) runs on Windows, macOS, and Linux, while `cmd` is exclusive to Windows.\n\nBuilt-in Help System: Use `Get-Help` to access comprehensive documentation directly from the console.  \nExample:\n\n```powershell\nGet-Help Get-Process -Full\n    ```\n\n## Supported Script Types\n\nPowerShell can interact with multiple script formats and scripting environments:\n\n- `.ps1` (PowerShell scripts): Primary automation and configuration scripts.\n    \n- `.[[bat]]` / `.cmd` (Batch files): Legacy support for traditional Windows scripting.\n    \n- `.vbs` (VBScript): Executes legacy Visual Basic scripts.\n    \n- WMI Scripts: Interfaces with Windows Management Instrumentation.\n    \n- .NET Code Snippets: Full access to .NET APIs.\n    \n- External Languages: Executes other scripts (e.g., Python) via CLI:\n    ```powershell\n    python script.py\n    ```\n    \n- Structured Data (JSON/XML): Built-in cmdlets to parse, query, and write structured data:\n    ```powershell\n    $data = Get-Content config.json | ConvertFrom-Json\n    ```\n\n\nTo Do:\n- [ ]  Explore the uses of [[PowerShell]]: see [[Powershell scripts]]\n\nhttps://www.youtube.com/watch?v=18hUejOK0qk\n\n```cmd\nprompt $g\n```\n\n### powershell\n```powershell\n$profile\n\nmicrosfot_Powershell_profile have\n\nfunction prompt{\n$p = -path\n\"$p> \"\n}\n```\n\ngetting the script working \n\nhttps://stackoverflow.com/questions/41117421/ps1-cannot-be-loaded-because-running-scripts-is-disabled-on-this-system",
    "aliases": [],
    "created": "2024-06-22 21:06",
    "date modified": "27-09-2025",
    "tags": [
      "software",
      "system"
    ],
    "type": null,
    "normalized_filename": "powershell",
    "outlinks": [
      "net",
      "powershell_scripts",
      "powershell_versus_command_prompt",
      "command_line",
      "command_prompt",
      "bat",
      "powershell"
    ],
    "inlinks": [
      "bat",
      "batch_vs_powershell_scripts",
      "command_line",
      "command_prompt",
      "powershell",
      "powershell_scripts",
      "powershell_versus_command_prompt",
      "powershell_vs_bash"
    ]
  },
  {
    "category": "DE",
    "filename": "Prevention Is Better Than The Cure",
    "sha": "b53caef4ca082a0942a680f5fcccec6a2017b577",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Prevention%20Is%20Better%20Than%20The%20Cure.md",
    "text": "To ensure data products are effective essential to prioritize prevention over remediation of [[Data Quality]]\n### Prevention\nPreventing data quality issues is the most effective strategy. This involves identifying and addressing potential problems at the source, ensuring that data is accurately entered from the beginning.\n\n### Remediation\nWhen data quality issues do arise, organizations should implement remediation strategies, including:\n- **[[Data Observability]] Tools**: Monitor data quality continuously to detect issues early.\n- **Alerting Systems**: Notify stakeholders when data quality problems are identified.\n- **Complex ETL Processes**: Manage data effectively to minimize errors.\n- **Trust Building**: Address the erosion of trust that can result from poor data quality.\n\n### Consequences of Poor Data Quality\nFailing to address data quality issues can lead to significant opportunity costs and hinder the ability to meet business goals. The sooner these issues are resolved, the cheaper and easier it is to manage them.\n\n### Motivating and Maintaining Data-Driven Value\n\nTo foster a culture of data quality, it is essential to motivate data producers by demonstrating the value of high-quality data. \n\nEffective [[Change Management]] is vital for maintaining data quality. This includes: Clear Communication to ensure all stakeholders are informed about data quality standards.\n\n### Addressing Data Quality Issues\n\nTo effectively handle data quality issues, organizations should focus on:\n1. **Detecting**: Identify issues as they arise through user reports, failed tests, or monitoring alerts.\n2. **Understanding**: Analyze the root causes of data quality problems.\n3. **Fixing**: Implement solutions to correct identified issues.\n4. **Reducing**: Minimize the occurrence of future data quality problems.\n\n### Questions for Consideration\n**Q:** What if data producers are not part of the data team but are business users (e.g., entering data into Google Sheets or Excel) with naming convention issues?  \n**A:** Encourage these users to apply the same data quality patterns by establishing agreements on data structure and implementing alerting and automated change management processes.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality"
    ],
    "normalized_filename": "prevention_is_better_than_the_cure",
    "outlinks": [
      "change_management",
      "data_quality",
      "data_observability"
    ],
    "inlinks": [
      "data_quality"
    ]
  },
  {
    "category": "DE",
    "filename": "Primary Key",
    "sha": "dc2a02d0d707b375d3c72c6101536ac2c8f41992",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Primary%20Key.md",
    "text": "A primary key (PK) is a unique identifier for each record in a database table.\n\n- **Uniqueness**: No two records can have the same primary key value.\n- **Non-null**: A primary key cannot contain null values; every record must have a valid primary key.\n- **Immutability**: Ideally, the primary key should not change over time, as it serves as a stable reference for the record\n\nFor example, an ISBN serves as a primary key for books, uniquely identifying each book in the database",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "primary_key",
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ]
  },
  {
    "category": "DE",
    "filename": "Push-Down",
    "sha": "4ea3239ca09d40c4b2ab0c0b050186c12b5e1b0b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Push-Down.md",
    "text": "Query pushdown aims to execute as much work as possible in the source databases. \n\nPush-downs or query pushdowns push transformation logic to the source database. This reduces to store data physically and transfers them over the network. \n\nFor example, a [semantic layer](semantic%20layer.md) or [Data Virtualization](Data%20Virtualization.md) translates the transformation logic into [SQL](SQL.md) [[Querying|queries]] and sends the SQL queries to the database. The source database runs the SQL queries to process the transformations.\n\nPushdown optimization increases mapping performance when the source database can process transformation logic faster than the semantic layer itself.",
    "aliases": [
      "Query Pushdown"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "push-down",
    "outlinks": [
      "querying"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Pydantic",
    "sha": "b40f640c9f15daf478d0f288a66552b9b3a966a8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pydantic.md",
    "text": "Pydantic is a Python library used for [[Data Validation]] and settings management using Python type annotations.\n\nIt provides a way to define data models with type hints, and it automatically validates and parses input data to ensure it matches the specified types. \n\nPydantic is often used in applications where data integrity is crucial, such as in web APIs, configuration management, and data processing pipelines. It helps developers catch errors early by enforcing type constraints and providing clear error messages when data does not conform to the expected format.\n### What Pydantic Does:\n\n1. **[[Data Validation]] and Parsing**  \n    Pydantic takes raw input data (e.g., dictionaries, JSON) and validates it against the [[type checking]] types and constraints defined in a `BaseModel`. If the input data doesn't match the requirements (e.g., wrong type, missing fields, invalid values), it raises a `ValidationError`.\n    \n    Example:\n    \n    ```python\n    from pydantic import BaseModel\n    \n    class User(BaseModel):\n        id: int\n        name: str\n    user = User(id=\"1\", name=\"Alice\")  # Automatically parses \"1\" to integer.\n    ```\n    \n2. **Automatic Type Conversion**  \n    Pydantic can coerce compatible types into the expected type. For example, if a field expects an `int` but receives a string `\"123\"`, it will try to convert it to an integer.\n    \n3. **Error Messaging**  \n    If validation fails, Pydantic provides detailed error messages explaining what went wrong, making debugging easier.\n    \n4. **Nested and Complex Data Models**  \n    Pydantic supports nested models and complex data structures, enabling you to handle hierarchical data easily.\n    \n5. **Settings Management**  \n    Pydantic can load configuration from [[Environment Variables]] or other sources using its `BaseSettings` class, making it handy for managing application settings.\n    \n6. **Serialization and Deserialization**  \n    Pydantic models support converting data into [[Json]] or dictionaries, making it easy to work with web APIs or store validated data.\n\n### Key Advantages of Pydantic:\n\n1. **Type-Safe Programming**:  \n    By relying on Python’s type hints, Pydantic promotes better coding practices and helps prevent runtime errors.\n    \n2. **Ease of Use**:  \n    Pydantic abstracts a lot of the boilerplate code you'd write manually for validating and parsing data.\n    \n3. **Error Reporting**:  \n    Pydantic provides clear and structured error messages, making [[Debugging]] simpler.\n    \n4. **[[Interoperability]]**:  \n    It works well with libraries like [[FastAPI]], where it powers request/response validation and serialization.\n\n### Use Cases:\n\n1. **Web APIs**:  \n    Validating incoming HTTP requests and outgoing responses (e.g., with FastAPI).\n2. **Data Processing**:  \n    Ensuring raw input data from files or APIs meets requirements before processing.\n3. **Configuration Management**:  \n    Validating and loading application settings from environment variables or files.\n4. **Data Pipelines**:  \n    Verifying the integrity of data as it moves through pipeline stages.\n\n### Analogy to Summarize:\n\nThink of Pydantic as a **data traffic cop**. It stands at the intersection where raw data enters your application, ensuring that:\n\n- The data is well-formed.\n- It complies with rules you’ve set (type, format, constraints).\n- It’s transformed into the expected structure (if possible).\n\nBy using Pydantic, you focus on defining the rules, and it ensures the data fits them—saving you from writing repetitive validation code.\n\n### Is Pydantic [[Object-Oriented Programming]] (OOP)?\n\nWhile Pydantic uses classes and inheritance (features of OOP), it is **not purely OOP in intent or design**. Instead, it is:\n\n- **Data-centric**: Focused on defining and validating data structures rather than encapsulating behavior like traditional OOP.\n- **Declarative**: Pydantic models are [[Declarative Data Pipeline]] in nature. You define the \"shape\" of your data (fields and their types) and rely on Pydantic to handle validation, parsing, and serialization.\n\n#### Differences from Typical OOP:\n\n- **Behavior vs. Structure**:  \n    Traditional OOP often centers on defining behavior (methods) alongside data. Pydantic, on the other hand, prioritizes defining and validating data.\n- **State Management**:  \n    In OOP, objects encapsulate their state and methods for interacting with it. Pydantic models are more lightweight and focused on validation, not managing stateful objects.\n\n#### Similarities to OOP:\n\n- **Class-Based Models**:  \n    Pydantic models are Python classes, and you can use inheritance, encapsulation, and even add methods to your models.\n- **Reusability**:  \n    You can define base models and extend them, similar to class inheritance in OOP.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "python",
      "ML_Tools"
    ],
    "normalized_filename": "pydantic",
    "outlinks": [
      "declarative_data_pipeline",
      "environment_variables",
      "data_validation",
      "debugging",
      "type_checking",
      "interoperability",
      "fastapi",
      "json",
      "object-oriented_programming"
    ],
    "inlinks": [
      "data_validation",
      "fastapi",
      "maintainable_code",
      "pyright_vs_pydantic"
    ]
  },
  {
    "category": "DE",
    "filename": "Pyright vs Pydantic",
    "sha": "6c81bdb1dbb83b8456da271d564f0f4c8b5be156",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Pyright%20vs%20Pydantic.md",
    "text": "While [[Pyright]] and [[Pydantic]] serve different roles in Python development, they complement each other well. \n\nPyright helps ensure that the code adheres to ==type constraints before execution==, while Pydantic ensures that the ==data being processed== adheres to the expected types and formats during runtime. \n\n### Key Differences\n\n1. **Purpose**:\n   - **Pyright** is aimed at improving code quality through static analysis and type checking.\n   - **Pydantic** is focused on runtime [[Data Validation]], ensuring that the data conforms to specified types and constraints.\n\n2. **Functionality**:\n   - **Pyright** checks for type errors and enforces type hints during development, preventing potential issues before the code is executed.\n   - **Pydantic** validates and parses data at runtime, providing clear error messages when data does not conform to the expected format.\n\n3. **Use Cases**:\n   - **Pyright** is beneficial in any Python project where type safety is desired, especially in large codebases.\n   - **Pydantic** is particularly useful in applications that require data validation, such as web frameworks (e.g., [[FastAPI]]) and data processing pipelines.\n\n### Key Similarities\n\n4. **Type Annotations**:\n   - Both utilize Python's type hints to define and enforce types, promoting better coding practices and reducing runtime errors.\n\n5. **Error Handling**:\n   - Both tools provide mechanisms for error reporting, although they do so at different stages (compile-time for Pyright and runtime for Pydantic).\n\n6. **Improving Code Quality**:\n   - Both contribute to overall code quality and [[Maintainability]], albeit through different approaches—Pyright through static analysis and Pydantic through runtime validation.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "python"
    ],
    "normalized_filename": "pyright_vs_pydantic",
    "outlinks": [
      "data_validation",
      "maintainability",
      "fastapi",
      "pyright",
      "pydantic"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Querying Time Series",
    "sha": "c467e085e4fb0a4666e2b191097a55449c41345c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Querying%20Time%20Series.md",
    "text": "[[Querying|Query]] patterns for time series data often use SQL constructs:\n\n```\nSELECT ...      -- Can include aggregate functions (e.g., SUM, AVG)\nFROM ...\nWHERE ...       -- Often uses BETWEEN for time ranges\nORDER BY ...\n```\n\nCommon Query Patterns\n* Compare periods: Analyze changes between two time ranges.\n* Summarize windows: Compute aggregates for defined intervals (e.g., hourly, daily).\n* [[granularity]] plays an important role in both.\n\nWindowing Concepts\n\nSliding Window:\n  * A continuous set of rows grouped by a specific [[granularity]].\n  * The window slides by one row, creating overlapping windows.\n  * Useful for monitoring changes over time. Think moving averages.\n\nTumbling Window:\n  * Moves forward by the full window size (no overlap).\n  * Often used for logical groupings of time (e.g., full days, weeks).\n\nPerformance & Optimization\n* For older data or longer-range queries, use coarser granularity to improve performance.\n* Use `EXPLAIN` or `ANALYZE` to optimize [[SQL]] queries.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "querying_time_series",
    "outlinks": [
      "granularity",
      "querying",
      "sql"
    ],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "DE",
    "filename": "Query Optimisation",
    "sha": "bc4015462da84df62fb374e4cdfa0e4015c3807a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Query%20Optimisation.md",
    "text": "[[Querying]] can be optimised for time, ==space efficiency==, and concurrency of queries.\n \nOptimizing SQL [[Querying]]:\n- Timing queries\n- [[Database Index|Indexing]]\n- Managing [[Transaction]]\n- and vacuuming, and handling concurrency with transactions and locks ensures efficient and reliable database performance.\n\n### Timing [[Querying]]:\n\n- Use `.timer on` to measure query execution time and identify slow queries.\n\n### [[Database Index|Index]] Search\n\nCreating an index on specific columns can speed up searches:\n- A covering index includes all the columns required by a query, eliminating the need to access the table data.\n- Partial indexes cover a subset of rows, saving space while maintaining query performance for frequently accessed data i.e more likely to search movies that are recent.\n\nTrade-offs when using indexes\n- Indexes improve query speed but ==consume additional space== and can slow down data insertion and updates.\n\n### To remove redundancy use [[Transaction|Transactions]]\n\n[[Vacuum]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "querying"
    ],
    "normalized_filename": "query_optimisation",
    "outlinks": [
      "transaction",
      "querying",
      "database_index",
      "vacuum"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Querying",
    "sha": "fb417803ecc3662b80c26cff429edec9574659f6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Querying.md",
    "text": "Querying is the process of asking questions of data. Querying makes use of keys primary and foreign within tables.\n\nUseful Links\n- [CS50 SQL Course](https://cs50.harvard.edu/sql/2024/weeks/0/)\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Querying/Querying.ipynb\n\nSQL Commands and Examples\n- SELECT\n- LIMIT\n- ORDER\n- WHERE\n- NOT\n- LIKE\n- WITH\n- INSERT, UPDATE, or DELETE \n\nYou can have parameterised queries so that you can pass in variables to it:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Querying/Parameterised_Queries.ipynb\n\nRelated terms:\n- [[SQL Joins]]\n- [[SQL Injection]]:=Why we should not use f-strings in queries\n\nQuery Plan:\n- What is expected to happen to the query plan if there is [[Database Index|Indexing]]?\n- Use EXPLAIN or ANALYSE",
    "aliases": [
      "Queries",
      "Query"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "database",
      "exploration"
    ],
    "normalized_filename": "querying",
    "outlinks": [
      "sql_joins",
      "de_tools",
      "database_index",
      "sql_injection"
    ],
    "inlinks": [
      "columnar_storage",
      "common_table_expression",
      "cypher",
      "data_storage",
      "data_warehouse",
      "database_index",
      "database_techniques",
      "dimensional_modelling",
      "duckdb",
      "faiss",
      "file_management",
      "langchain",
      "microsoft_access",
      "neomodel",
      "olap",
      "push-down",
      "pyspark",
      "query_optimisation",
      "querying_time_series",
      "similarity_search",
      "snowflake",
      "soft_deletion",
      "sql",
      "sql_groupby",
      "sql_window_functions",
      "sqlalchemy",
      "sqlite",
      "transaction",
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Race Conditions",
    "sha": "2100194eb69a44a9836ea9dcef3f02c67cb9a3a8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Race%20Conditions.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "devops"
    ],
    "normalized_filename": "race_conditions",
    "outlinks": [],
    "inlinks": [
      "database_techniques",
      "processes_vs_threads"
    ]
  },
  {
    "category": "DE",
    "filename": "Relational Database",
    "sha": "6dc4ad66b06a7c2146d49ee54f0da83623db8451",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Relational%20Database.md",
    "text": "Has a [[Database Schema|schema]].\n\n[[Database]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "relational_database",
    "outlinks": [
      "database_schema",
      "database"
    ],
    "inlinks": [
      "data_engineering_tools",
      "data_lake",
      "database",
      "fabric",
      "structured_data"
    ]
  },
  {
    "category": "DE",
    "filename": "Relating Tables Together",
    "sha": "f9f9ec4046abe7f2250d9fd05e16166311f295c3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Relating%20Tables%20Together.md",
    "text": "Implementing these concepts, database tables can be effectively related, ensuring [[Data Integrity]], efficient retrieval, and easy maintenance.\n\nResources:\n- [LINK](https://cs50.harvard.edu/sql/2024/weeks/1/)\n### Notes on Relating Database Tables\n\n[[Primary Key]]\n\n[[Foreign Key]]\n\nOne-to-One Relationships:\n   - Each record in Table A relates to one record in Table B and vice versa.\n   - Example: Each employee has one unique profile.\n\nOne-to-Many Relationships:\n   - A single record in Table A can relate to multiple records in Table B.\n   - Example: One department has many employees.\n\n[[Many-to-Many Relationships]]\n\n[[Junction Tables]]\n   - Used to manage many-to-many relationships.\n   - Contains foreign keys from both tables it connects.\n   - Example: Enrollments table with StudentID and CourseID as foreign keys.\n\nReferential Integrity\n   - Ensures that relationships between tables remain consistent.\n   - Example: If an employee is assigned a department, the DepartmentID must exist in the Departments table.\n\nCascading Actions:\n   - Cascade Update: Updates related records automatically when a primary key is updated.\n   - Cascade Delete: Deletes related records automatically when a primary key is deleted.\n\n[[ER Diagrams]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "relating_tables_together",
    "outlinks": [
      "data_integrity",
      "primary_key",
      "er_diagrams",
      "foreign_key",
      "many-to-many_relationships",
      "junction_tables"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ]
  },
  {
    "category": "DE",
    "filename": "Row parameters in SQL",
    "sha": "6f70f1d3fc2f563d368cc67181bfb6ae707b506b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Row%20parameters%20in%20SQL.md",
    "text": "What are these",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "row_parameters_in_sql",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Row-based Storage",
    "sha": "aea20a901bf94c9ea56d9d59f18db1a48ca82105",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Row-based%20Storage.md",
    "text": "Data is stored in consecutive rows, allows [[CRUD]]\n\nRow-based storage is well-suited for transactional systems ([[OLTP]]) \n\nLess efficient than [[Columnar Storage]] in largedatasets.\n\nRow-based Storage Example (Transactional Workloads)**:\nFor the same table, if the goal is to efficiently handle transactions like inserting or updating an order, **row-based storage** organizes data row by row.\n\n| `order_id` | `customer_id` | `order_date` | `order_amount` |\n| ---------- | ------------- | ------------ | -------------- |\n| 1          | 101           | 2024-10-01   | $100           |\n| 2          | 102           | 2024-10-02   | $150           |\n| 3          | 103           | 2024-10-03   | $200           |\n\nIn **row-based storage**, entire records (rows) are stored together. For example:\n- Row 1: [1, 101, 2024-10-01, $100]\n\nWhen performing an **insert** or **update**, the entire row can be read and written back quickly, making this method ideal for transactional operations where complete records need to be processed together.\n\nUse case [[OLTP]]. For instance, inserting a new order or updating an existing one (like modifying `order_amount` or `customer_id`) is efficient because all the data for a single record is stored together in a row.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "row-based_storage",
    "outlinks": [
      "crud",
      "columnar_storage",
      "oltp"
    ],
    "inlinks": [
      "database_storage"
    ]
  },
  {
    "category": "DE",
    "filename": "SQL Groupby",
    "sha": "419f79d3e132485b39727cf8d98f3634af8c2419",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQL%20Groupby.md",
    "text": "The [[SQL]] `GROUP BY` clause is used to group rows that have the same values in specified columns into summary rows, like \"total sales per region\" or \"average age per department.\" \n\nIt is often used in conjunction with aggregate functions such as `COUNT()`, `SUM()`, `AVG()`, `MAX()`, and `MIN()` to perform calculations on each group.\n### Basic Syntax\n\n```sql\nSELECT column1, aggregate_function(column2)\nFROM table_name\nWHERE condition\nGROUP BY column1;\n```\n\n### Example Usage\n\nLet's say you have a table called `sales` with the following columns:\n\n- `id`: Unique identifier for each sale\n- `product`: The name of the product sold\n- `amount`: The sale amount\n- `region`: The region where the sale occurred\n\n#### 1. Count the Number of Sales per Product\n\nTo count how many sales were made for each product, you would use:\n\n```sql\nSELECT product, COUNT(*) AS total_sales\nFROM sales\nGROUP BY product;\n```\n\n#### 2. Calculate Total Sales Amount per Region\n\nTo calculate the total sales amount for each region, you would use:\n\n```sql\nSELECT region, SUM(amount) AS total_sales_amount\nFROM sales\nGROUP BY region;\n```\n\n### Using `HAVING` with `GROUP BY`\n\nYou can also filter the results of a `GROUP BY` [[Querying|query]] using the `HAVING` clause. This is useful when you want to filter groups based on aggregate values.\n\n#### Example: Filter Groups\n\nFor example, to find products with total sales greater than $1000:\n\n```sql\nSELECT product, SUM(amount) AS total_sales_amount\nFROM sales\nGROUP BY product\nHAVING SUM(amount) > 1000;\n```\n\n### Important Points\n\n- **Columns in SELECT**: When using `GROUP BY`, all columns in the `SELECT` statement must either be included in the `GROUP BY` clause or be used in an aggregate function.\n- **Order of Execution**: The `GROUP BY` clause is processed after the `WHERE` clause but before the `ORDER BY` clause in the SQL execution order.\n\n\n\n\n\n[[SQL Groupby]]\n   **Tags**:",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "SQL",
      "transformation"
    ],
    "normalized_filename": "sql_groupby",
    "outlinks": [
      "querying",
      "sql_groupby",
      "sql"
    ],
    "inlinks": [
      "sql_groupby"
    ]
  },
  {
    "category": "DE",
    "filename": "SMSS",
    "sha": "f037a689f2f89bab5d78ef85902218ae475aca1c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SMSS.md",
    "text": "microsoft [[SQL]] server management.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "smss",
    "outlinks": [
      "sql"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "SQL Injection",
    "sha": "277b6684f7386b68daf275823d868fc14cf77cf0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQL%20Injection.md",
    "text": "SQL injection is a code injection technique that targets applications using SQL databases. It occurs when a malicious user injects harmful SQL code into a query executing unintended commands, potentially compromising the [[Data Security]] of the database. \n\nNeed sanitized SQL queries, usee\n - Parameterized queries\n  - ORMs ([[Object Relational Mapper]]) that protect against manual query building.\n### How SQL Injection Works\n\nConsider a scenario where a website prompts users to log in with their username and password. The application might execute a query like this:\n\n```sql\nSELECT `id` FROM `users`\nWHERE `user` = 'Carter' AND `password` = 'password';\n```\n\nIf the user named Carter enters their credentials correctly, the query functions as intended. However, a malicious user could input a different string, such as:\n\n```\npassword' OR '1' = '1\n```\n\nThis alters the query to:\n\n```sql\nSELECT `id` FROM `users`\nWHERE `user` = 'Carter' AND `password` = 'password' OR '1' = '1';\n```\n\nAs a result, the attacker could gain unauthorized access to the database.\n\n### Example of Vulnerable Code\n\nThe following Python function demonstrates how SQL injection can occur due to unsafe query construction:\n\n```python\nimport sqlite3\n\ndef unsafe_query(user_input):\n    query = \"SELECT * FROM users WHERE name = '\" + user_input + \"'\"\n    conn = sqlite3.connect('example.db')\n    conn.execute(query)\n```\n\nIn this example, the `unsafe_query` function constructs SQL queries using string concatenation, making it vulnerable if user input is not properly sanitized.\n\n### Preventing SQL Injection\n\nTo mitigate the risk of SQL injection, it is essential to use prepared statements or parameterized queries. For example, consider an SQL injection attack that aims to display all user accounts from the `accounts` table:\n\n```sql\nSELECT * FROM `accounts`\nWHERE `id` = 1 UNION SELECT * FROM `accounts`;\n```\n\nUsing a prepared statement, we can safeguard against such attacks:\n\n```sql\nPREPARE `balance_check`\nFROM 'SELECT * FROM `accounts`\nWHERE `id` = ?';\n```\n\nIn this statement, the question mark acts as a placeholder for user input, preventing the execution of unintended SQL code.\n\n### Executing the Prepared Statement\n\nTo execute the prepared statement and check a user’s balance, we can set a variable for the user ID:\n\n```sql\nSET @id = 1;\nEXECUTE `balance_check` USING @id;\n```\n\nHere, the `SET` statement simulates obtaining the user’s ID through the application, with the `@` symbol denoting a variable in [[MySql]].\n\n### Testing with Malicious Input\n\nIf we attempt to run the same statements with a malicious ID:\n\n```sql\nSET @id = '1 UNION SELECT * FROM `accounts`';\nEXECUTE `balance_check` USING @id;\n```\n\nThe output will still reflect the balance of the user with ID 1, without exposing any additional data. This demonstrates that prepared statements effectively prevent SQL injection attacks.\n\n### Mitigation Strategies\n\n- **Use Parameterized Queries**: Always use parameterized queries or prepared statements to prevent SQL injection.\n- **Validate and Sanitize Inputs**: Ensure user inputs are validated and sanitized before being processed.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "security",
      "SQL"
    ],
    "normalized_filename": "sql_injection",
    "outlinks": [
      "mysql",
      "object_relational_mapper",
      "data_security"
    ],
    "inlinks": [
      "data_security",
      "input_is_not_properly_sanitized",
      "querying",
      "security_vulnerabilities",
      "sqlalchemy"
    ]
  },
  {
    "category": "DE",
    "filename": "SQL Joins",
    "sha": "0086d914f8290b02f31f8463e4de45f9c95b7027",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQL%20Joins.md",
    "text": "In [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb\n\n\n![[Pasted image 20250323083319.png|800]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "SQL"
    ],
    "normalized_filename": "sql_joins",
    "outlinks": [
      "pasted_image_20250323083319.png",
      "de_tools"
    ],
    "inlinks": [
      "database_techniques",
      "joining_datasets",
      "joining_time_series",
      "querying"
    ]
  },
  {
    "category": "DE",
    "filename": "SQLAlchemy vs. sqlite3",
    "sha": "3404930de1ddd72d81d40d8b352a8c98372236ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQLAlchemy%20vs.%20sqlite3.md",
    "text": "### SQLAlchemy vs. sqlite3: Which One Should You Use?\n\nThe choice between [[SQLAlchemy]] and [[SQLite]] depends on your specific needs. Here’s a comparison based on key factors:\n\n### 1. Abstraction and Ease of Use\n\n| Feature     | SQLAlchemy                                 | sqlite3                          |\n| ----------- | ------------------------------------------ | -------------------------------- |\n| Abstraction | High-level [[Object Relational Mapper|ORM]] (Object Relational Mapping) | Low-level, direct SQL execution  |\n| Ease of Use | Pythonic API for working with databases    | Requires writing raw SQL queries |\n| Best for    | Large projects, scalable applications      | Simple scripts, small projects   |\n\n-  Use SQLAlchemy if you want to work with database tables as Python objects (ORM).  \n- Use sqlite3 if you are comfortable writing SQL queries directly.\n\n### 2. Supported Databases\n\n|Feature|SQLAlchemy|sqlite3|\n|---|---|---|\n|Database Support|Works with MySQL, PostgreSQL, SQLite, MSSQL, etc.|Only works with SQLite|\n|Portability|Can switch databases easily|Tied to SQLite only|\n\n- Use SQLAlchemy if you need flexibility to work with different databases.  \n- Use sqlite3 if you are only working with SQLite.\n\n### 3. Performance and Scalability\n\n|Feature|SQLAlchemy|sqlite3|\n|---|---|---|\n|Performance|Slightly slower due to ORM overhead|Faster for simple queries|\n|Scalability|Supports connection pooling, transactions, and large-scale applications|Best for local, single-user applications|\n\n- Use SQLAlchemy for large applications with complex relationships.  \n- Use sqlite3 if you just need a simple, fast database for local use.\n\n### 4. Querying and Data Manipulation\n\n|Feature|SQLAlchemy|sqlite3|\n|---|---|---|\n|Querying|Can use both ORM and raw SQL queries|Only supports raw SQL queries|\n|Ease of Data Manipulation|Object-oriented approach (e.g., `session.add(obj)`)|SQL execution via `cursor.execute(query)`|\n\n- Use SQLAlchemy if you prefer writing queries in a Pythonic way (ORM).  \n- Use sqlite3 if you are fine with executing raw SQL statements.\n### 5. Transaction Handling\n\n|Feature|SQLAlchemy|sqlite3|\n|---|---|---|\n|Transaction Control|Automatic transaction management|Manual transaction handling (`conn.commit()`)|\n|Rollback Support|Easier and more reliable|Must be explicitly handled|\n\n- Use SQLAlchemy for better transaction control in complex applications.  \n- Use sqlite3 if you want manual control over transactions.\n\n### 6. Learning Curve\n\n|Feature|SQLAlchemy|sqlite3|\n|---|---|---|\n|Difficulty Level|Higher due to ORM concepts|Easier to get started|\n\n- Use sqlite3 if you want a simple database solution with SQL queries.  \n- Use SQLAlchemy if you are comfortable with an ORM and want a scalable approach.\n\n### When to Use SQLAlchemy?\n\n- You are building a large, scalable application.\n- You need database flexibility (MySQL, PostgreSQL, etc.).\n- You prefer Pythonic ORM instead of writing raw SQL.\n- You want better transaction handling and connection management.\n\n### When to Use sqlite3?\n\n- You need a lightweight, single-file database.\n- You are working on a small project or script.\n- You are comfortable writing raw SQL queries.\n- You do not need an ORM or multiple database support.\n\n### Final Recommendation\n\n- For simple SQLite-based projects: Use `sqlite3` (faster, simpler).\n- For larger applications needing scalability and [[Maintainability]]: Use `SQLAlchemy`.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "python",
      "SQL"
    ],
    "normalized_filename": "sqlalchemy_vs._sqlite3",
    "outlinks": [
      "sqlite",
      "object_relational_mapper",
      "maintainability",
      "sqlalchemy"
    ],
    "inlinks": [
      "sqlalchemy"
    ]
  },
  {
    "category": "DE",
    "filename": "SQL",
    "sha": "bddcb7f25682c0578d92080f3413d8e8dc522bb3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQL.md",
    "text": "Structured Query Language (SQL) is the standard language for interacting with relational databases, enabling efficient data [[Querying]] and manipulation. It serves as a common interface for [[Database]]s and data lakes.\n\nFeatures: \n  - Declarative language for storing and querying structured data.\n  - Transactional properties enhance speed and efficiency.\n  \n### Good Practices\n\nCapitalization: \n  - Use uppercase for SQL keywords for better readability.\n  - Use lowercase for table and column names.\n  \nQuotes:\n  - Use double quotes for SQL identifiers (table and column names).\n  - Use single quotes for string values.\n\n### Related terms\n\n[[Database Techniques]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "SQL"
    ],
    "normalized_filename": "sql",
    "outlinks": [
      "database",
      "querying",
      "database_techniques"
    ],
    "inlinks": [
      "data_engineering_tools",
      "dbt",
      "declarative_data_pipeline",
      "google_cloud_platform",
      "google_sheets",
      "graph_query_language",
      "how_is_schema_evolution_done_in_practice_with_sql",
      "pyspark",
      "querying_time_series",
      "smss",
      "snowflake",
      "sql_groupby",
      "sql_vs_nosql",
      "sqlalchemy",
      "sqlite",
      "unstructured_data"
    ]
  },
  {
    "category": "DE",
    "filename": "SQLAlchemy",
    "sha": "a83d3bef2fc7b0bff2451f5bd08a2d8af5aac596",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQLAlchemy.md",
    "text": "SQLAlchemy is a Python SQL toolkit and ==[[Object Relational Mapper]]== (ORM) that provides tools to interact with databases in a more Pythonic way. It allows you to work with relational databases such as MySQL, PostgreSQL, SQLite, and others without writing raw [[SQL]] queries manually.\n\nRelated:\n- [[SQLAlchemy vs. sqlite3]]\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLAlchemy/sql_alchemy.ipynb\n\n### Why Use SQLAlchemy?\n\n- Reduces SQL complexity: Write Python code instead of SQL queries.\n- Prevents [[SQL Injection]]: ORM prevents unsafe queries.\n- Improves [[Maintainability]]: Easier to refactor code.\n- Handles connection pooling: Manages database connections efficiently.\n- Works with Pandas: Can load and save data directly to databases.\n### Key Features of SQLAlchemy\n\n1. Database Connectivity\n    - Provides a unified interface to connect to different databases.\n      \n2. SQL Query Execution\n    - Allows execution of raw SQL queries using [[Pandas]]\n      \n3. ORM (Object Relational Mapping)\n    - Converts database tables into Python objects (classes).\n    - Eliminates the need to write SQL [[Querying|Queries]] manually.\n    - Example:\n        ```python\n        from sqlalchemy.orm import declarative_base\n        from sqlalchemy import Column, Integer, String\n        \n        Base = declarative_base()\n        \n        class Customer(Base):\n            __tablename__ = 'customers'\n            id = Column(Integer, primary_key=True)\n            name = Column(String)\n            phone_number = Column(String)\n        ```\n4. Transaction Management\n    - Provides robust control over commit and rollback operations.\n    - Ensures data integrity by handling failures safely.\n      \n5. Efficient Query Building\n    - Allows writing Pythonic queries instead of raw SQL.\n    - Example:\n        \n        ```python\n        from sqlalchemy.orm import sessionmaker\n        \n        Session = sessionmaker(bind=engine)\n        session = Session()\n        \n        customers = session.query(Customer).filter_by(name=\"John Doe\").all()\n        ```\n        \n6. Supports Multiple Databases\n    - Works with [[MySql]] ,[[PostgreSQL]], [[SQLite]], etc.\n    - Easily switch databases without changing the core logic.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "python",
      "SQL"
    ],
    "normalized_filename": "sqlalchemy",
    "outlinks": [
      "mysql",
      "object_relational_mapper",
      "sqlite",
      "sql",
      "sqlalchemy_vs._sqlite3",
      "querying",
      "postgresql",
      "maintainability",
      "pandas",
      "sql_injection",
      "de_tools"
    ],
    "inlinks": [
      "object_relational_mapper",
      "pyodbc",
      "sqlalchemy_vs._sqlite3"
    ]
  },
  {
    "category": "DE",
    "filename": "SQLite Studio",
    "sha": "181f8c7c024a0fcd606fb18a6a60143ddb75b43b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQLite%20Studio.md",
    "text": "An application to interact with [[SQLite]] databases.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "software",
      "tool"
    ],
    "normalized_filename": "sqlite_studio",
    "outlinks": [
      "sqlite"
    ],
    "inlinks": [
      "sqlite"
    ]
  },
  {
    "category": "DE",
    "filename": "Scalability",
    "sha": "95c0993353b55f4eed8b69d502591676a84e66ea",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Scalability.md",
    "text": "Scalability refers to the capability of a system, network, or process to handle a growing amount of work or its potential to accommodate growth.\n\n### Key Benefits of Scalability:\n\n- Performance Improvement: As demand increases, scalable systems can maintain or improve performance levels.\n- Cost Efficiency: Organizations can manage costs by scaling resources according to demand, avoiding over-provisioning.\n- Flexibility: Scalable systems can adapt to changing workloads and business needs, making it easier to accommodate growth.\n- Reliability: Distributing workloads across multiple nodes can enhance system reliability and reduce the risk of a single point of failure.\n\n### Vertical Scalability (Scaling Up):\n\nThis involves ==adding more resources to a single node== or server to increase its capacity. For example, upgrading a server's CPU, adding more RAM, or increasing storage space. \n\nVertical scaling can improve performance but ==has limitations,== as there is a maximum capacity that a single machine can reach.\n\n### Horizontal Scalability (Scaling Out):\n\nThis involves ==adding more nodes or servers== to a system to distribute the load. For example, adding more servers to a web application to handle increased traffic. \n\nHorizontal scaling allows for greater flexibility and can often be more cost-effective, as it enables the use of multiple lower-cost machines rather than relying on a single powerful machine.\n\n### Tags\n- **Tags**:",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "scalability",
    "outlinks": [],
    "inlinks": [
      "amazon_s3",
      "apache_kafka",
      "challenges_to_model_deployment",
      "classification",
      "data_ingestion",
      "databricks",
      "digital_transformation",
      "digital_twin",
      "distributed_computing",
      "event_driven",
      "machine_learning_algorithms",
      "model_deployment",
      "performance_dimensions",
      "publish_and_subscribe",
      "rest_api",
      "spreadsheets_vs_databases",
      "why_use_pyspark_in_databricks"
    ]
  },
  {
    "category": "DE",
    "filename": "SQLite",
    "sha": "30f8d13d0950f5ab2646f46213763e7b86160f4c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/SQLite.md",
    "text": "Lightweight [[Database Management System (DBMS)|DBMS]] used in various applications (phone apps, desktop apps, websites).\n\nNote [[SQLite Studio]] exists\n\nTo get in terminal enter: \n\nsqlite3 database.db\n\n\nRelated notes:\n- [[Querying]]\n- [[Concurrency]]\n- [[SQL]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "programming",
      "SQL"
    ],
    "normalized_filename": "sqlite",
    "outlinks": [
      "sql",
      "querying",
      "concurrency",
      "sqlite_studio",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "data_storage",
      "database_management_system_(dbms)",
      "duckdb_vs_sqlite",
      "file_management",
      "json_to_sqlite",
      "mysql",
      "pgadmin",
      "sqlalchemy",
      "sqlalchemy_vs._sqlite3",
      "sqlite_studio",
      "transaction",
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Scaling Server",
    "sha": "dbf3148b942b82726b48bd769e55569f75ea857d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Scaling%20Server.md",
    "text": "Scaling Server\n  - Horizontal Scaling: Adding more servers, preferred for scalability.\n  - Vertical Scaling: Adding more resources (memory, CPU) to existing servers.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "scaling_server",
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ]
  },
  {
    "category": "DE",
    "filename": "Schema Evolution",
    "sha": "2720b73ba20c71bdaba2ae73ceab8d1744c0c0a1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Schema%20Evolution.md",
    "text": "[[Database Schema|Schema]] Evolution means adding new columns without breaking anything or even enlarging some types. \n\nYou can even rename or reorder columns, although that might break backward compatibilities. Still, we can change one table, and the table format takes care of switching it on all distributed files. Best of all does not require rewrite of your table and underlying files.\n\n### How is schema evolution done in practice\n\nIn [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Writing/Schema_Evolution.ipynb\n\nSee also:\n- [[ACID Transaction]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "schema_evolution",
    "outlinks": [
      "database_schema",
      "de_tools",
      "acid_transaction"
    ],
    "inlinks": [
      "apache_iceberg",
      "data_lakehouse",
      "databricks",
      "delta_tables_in_databricks",
      "parquet"
    ]
  },
  {
    "category": "DE",
    "filename": "Search",
    "sha": "3274f7ab660d026debf3c27647f1cc741eb46587",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Search.md",
    "text": "Aims of any search:\n1. Speed\n2. Accuracy (Precision)\n3. Ease of use \n4. Comprehensiveness\n5. Freshness\n## How Search Algorithms Work (in Information Retrieval)\n\nWhen you type a query into a search engine (e.g., \"machine learning applications in healthcare\"), the system must:\n\n**Index documents**\n   * Before searching, all documents are processed into an **inverted index** (a mapping from terms → list of documents where they appear).\n   * Example:\n     * \"data\" → {doc1, doc3, doc7}\n     * \"learning\" → {doc2, doc3, doc4}\n\n**Retrieve candidate documents**\n   * Using the inverted index, the system finds documents containing at least one of the query terms.\n\n**Rank the documents**\n   * The system applies a **ranking function** (scoring model) to order documents by relevance to the query.\n   * Early methods: Boolean matching (exact matches).\n   * More advanced: Vector Space Models ([[TF-IDF]], [[BM25 (Best Match 25)]]), Neural embeddings, Transformers ([[BERT]]).\n\n**Return top results**\n   * Usually top-10 or top-100 documents are returned.\n\nIn short:\n* **Search algorithms**: retrieve and rank documents.\n* **BM25**: a probabilistic ranking function that balances **term frequency**, **term rarity**, and **document length**.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "mlprocess",
      "NLP"
    ],
    "normalized_filename": "search",
    "outlinks": [
      "bm25_(best_match_25)",
      "bert",
      "tf-idf"
    ],
    "inlinks": [
      "elasticsearch",
      "everything",
      "grep",
      "page_rank",
      "similarity_search",
      "tf-idf",
      "vector_database"
    ]
  },
  {
    "category": "DE",
    "filename": "Security Researcher",
    "sha": "9317a3e62eb903cda410d2eb812172b2249efed2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Security%20Researcher.md",
    "text": "Professionals who:\n  - Audit systems for vulnerabilities.\n  - Help design secure architectures.\n  - Conduct penetration tests (authorized attack simulations).\n  - Work in \"Red Teams\" (attack simulation) or \"Blue Teams\" (defense).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "security"
    ],
    "normalized_filename": "security_researcher",
    "outlinks": [],
    "inlinks": [
      "data_roles",
      "data_security"
    ]
  },
  {
    "category": "DE",
    "filename": "Security mitigation",
    "sha": "c7c9ac688eb593b678324ca22d23742fce7ffe29",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Security%20mitigation.md",
    "text": "### Secrets Management: Why Scan for Keys?\n\nPurpose of Secret Keys:\n  - Authenticate to APIs, databases, services (like AWS, GitHub, etc.).\n- Risks:\n  - Accidentally pushing keys into public repositories (e.g., GitHub) exposes you to massive risk.\n\nWhy scan?\n  - Tools like `truffleHog`, `git-secrets`, or GitHub's own secret scanning detect leaked keys early.\n\n### Bug Bounty Programs\n  - Some companies reward researchers who find exposed bugs and report them responsibly.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "security"
    ],
    "normalized_filename": "security_mitigation",
    "outlinks": [],
    "inlinks": [
      "data_security"
    ]
  },
  {
    "category": "DE",
    "filename": "Single Source of Truth",
    "sha": "54af616993f4610b9768a900e0f782d13c869c87",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Single%20Source%20of%20Truth.md",
    "text": "Sending data from across an enterprise into a centralized system such as a:\n\n- [[Database]]\n- [[Data Warehouse]]\n- [[Data Lakehouse]]\n- [[Data Lakehouse]]\n- [[master data management]]\n\nresults in a single unified location for accessing and analyzing all the information that is flowing through an organization.\n\n\n\n[[Single Source of Truth]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "storage"
    ],
    "normalized_filename": "single_source_of_truth",
    "outlinks": [
      "database",
      "data_lakehouse",
      "master_data_management",
      "single_source_of_truth",
      "data_warehouse"
    ],
    "inlinks": [
      "business_intelligence",
      "data_integration",
      "single_source_of_truth"
    ]
  },
  {
    "category": "DE",
    "filename": "Sklearn Pipiline",
    "sha": "f91e22b1e7cda21d81697533ec3fe0748eefd468",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Sklearn%20Pipiline.md",
    "text": "```python\n# Naivebayesfor email prediction\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('nb', MultinomialNB())\n])\nclf.fit(X_train, y_train)\nclf.score(X_test,y_test)\nclf.predict(user_input)\n```\n\nRelated:\n- [[Scikit-Learn]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "transformation"
    ],
    "normalized_filename": "sklearn_pipiline",
    "outlinks": [
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Slowly Changing Dimension",
    "sha": "28a7b76b60d2a8fb06b0150b88deb0464d1b241a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Slowly%20Changing%20Dimension.md",
    "text": "A Slowly Changing Dimension (SCD) is **a dimension that stores and manages both current and historical data over time in a [Data Warehouse](Data%20Warehouse.md)**.\n\nIt is considered and implemented as one of the most critical [[ETL]] tasks in tracking the history of dimension records.\n## How do you track slowly changing dimensions in a [[Database]]\n\nTake a customer dimension in a retail database.\n\nConsider a retail company that tracks customer information, including attributes such as name, address, and membership status. Over time, customers may change their addresses or upgrade their membership levels. \n#### Implementation of SCD\n\n- The original record for John Doe is retained with an end date to indicate that it is no longer current.\n- A new record is created for the updated information, allowing the company to maintain a history of changes over time.\n- This approach allows analysts to query the data and understand customer behavior and trends over time, which is essential for reporting and decision-making.\n\n1. **Current Data**: The current state of the customer dimension might look like this:\n\n| Customer ID | Name       | Address               | Membership Status |\n|-------------|------------|-----------------------|--------------------|\n| 1           | John Doe  | 123 Elm St, City A    | Gold               |\n| 2           | Jane Smith | 456 Oak St, City B    | Silver             \n\n1. **Change Occurs**: If John Doe moves to a new address and upgrades his membership to Platinum, the company needs to track this change.\n\n2. **Historical Data**: Using the SCD approach, the [[Dimension Table]] might be updated as follows:\n\n| Customer ID | Name       | Address               | Membership Status | Effective Date | End Date   |\n|-------------|------------|-----------------------|--------------------|----------------|------------|\n| 1           | John Doe  | 123 Elm St, City A    | Gold               | 2022-01-01     | 2023-10-01 |\n| 1           | John Doe  | 789 Pine St, City A   | Platinum           | 2023-10-01     | NULL       |\n| 2           | Jane Smith | 456 Oak St, City B    | Silver             | 2022-01-01     | NULL       |",
    "aliases": [
      "SCD"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "slowly_changing_dimension",
    "outlinks": [
      "database",
      "etl",
      "dimension_table"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Snowflake Schema",
    "sha": "5850076b23515dcb2382ff486db33cc304de6331",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Snowflake%20Schema.md",
    "text": "Snowflake Schema\n   - Description: A more [[Normalised Schema]] normalized form of a star schema where dimension tables are further broken down into additional tables.\n   - Advantages: Reduces data redundancy and can save storage space, but may be more complex to query.\n   - A variation of the [[Star Schema]], the snowflake schema normalizes dimension tables into multiple related tables. This can reduce data redundancy and improve data integrity but may complicate queries due to the additional joins required.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "orchestration"
    ],
    "normalized_filename": "snowflake_schema",
    "outlinks": [
      "star_schema",
      "normalised_schema"
    ],
    "inlinks": [
      "types_of_database_schema"
    ]
  },
  {
    "category": "DE",
    "filename": "Soft Deletion",
    "sha": "b52c377728288763616a43f3ff741437f46ea5d2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Soft%20Deletion.md",
    "text": "Soft deletion is a technique used in databases to ==mark records as deleted without physically removing them from the database==. \n\nThis approach is particularly useful in scenarios where [[Data Integrity]] and synchronization are important, such as during [[incremental synchronization]].\n\nWhen using [[incremental synchronization]] modes, fully deleted records from a source system are not replicated. To handle this, a field can be added to each record to indicate whether it should be treated as deleted. This allows the system to maintain a complete history of records while still functioning as if certain records are removed.\n\n## Implementation\n\nA common way to implement soft deletion is by adding a boolean flag, such as `is_deleted`, to the record [[Database Schema|schema]]. Here’s how it works:\n\n1. Flagging Records:\n   - When a record is \"deleted,\" the `is_deleted` flag is set to `true`.\n  \n1. Querying Data:\n   - All [[Querying|queries]] must be designed to exclude records where `is_deleted` is `true`. For example:\n     ```sql\n     SELECT  FROM table_name WHERE is_deleted = false;\n     ```\n\n1. Background Jobs:\n   - Periodically, background jobs can be executed to permanently remove records marked as deleted, if necessary, or to archive them for future reference.\n\n## Benefits\n\n- Data Integrity: Maintains a complete history of records, which can be useful for auditing and recovery.\n- Ease of Recovery: Records can be easily restored by simply resetting the `is_deleted` flag.\n- Synchronization: Facilitates incremental synchronization by ensuring that deleted records are still present in the database.\n\n## Considerations\n\n- Query Complexity: Requires careful query design to ensure that deleted records are consistently excluded.\n- Storage: Over time, soft-deleted records can accumulate, potentially leading to increased storage requirements.\n\n## Example of Soft Deletion\n\nLet's say we have a table named `users` that stores user information. We will add a boolean column called `is_deleted` to indicate whether a user is \"deleted.\"\n\nIn this example, we demonstrated how to implement soft deletion using a boolean flag in the `users` table. This approach allows for easy recovery of deleted records and maintains [[Data Integrity]] while facilitating incremental synchronization.\n\n#### Step 1: Modify the Table Structure\n\nFirst, we need to alter the `users` table to add the `is_deleted` column:\n\n```sql\nALTER TABLE users ADD COLUMN is_deleted BOOLEAN DEFAULT false;\n```\n\n#### Step 2: Soft Delete a User\n\nWhen a user wants to delete their account, instead of removing the record from the database, we update the `is_deleted` flag:\n\n```sql\nUPDATE users SET is_deleted = true WHERE user_id = 123;\n```\n\n#### Step 3: Querying Active Users\n\nTo retrieve a list of active users (those who are not deleted), we write our queries to exclude soft-deleted records:\n\n```sql\nSELECT  FROM users WHERE is_deleted = false;\n```\n\n#### Step 4: Restoring a Soft Deleted User\n\nIf a user decides to restore their account, we can simply set the `is_deleted` flag back to `false`:\n\n```sql\nUPDATE users SET is_deleted = false WHERE user_id = 123;\n```\n\n#### Step 5: Permanently Deleting Soft Deleted Users\n\nIf we want to permanently remove users who have been soft deleted for a certain period, we can run a background job to delete those records:\n\n```sql\nDELETE FROM users WHERE is_deleted = true AND deletion_date < NOW() - INTERVAL '30 days';\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "management"
    ],
    "normalized_filename": "soft_deletion",
    "outlinks": [
      "database_schema",
      "data_integrity",
      "incremental_synchronization",
      "querying"
    ],
    "inlinks": [
      "database_techniques",
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Software Design Patterns",
    "sha": "59c61cd8868eb87216c5eab6a0709cd49899ee6a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Software%20Design%20Patterns.md",
    "text": "[10 Design Patterns Explained in 10 Minutes](https://www.youtube.com/watch?v=tv-_1er1mWI)\n\n# Software Design Patterns\n\n## What Are Software Design Patterns?\n\nSoftware design patterns provide reusable solutions to common software design problems. They help standardize approaches, making code easier to understand, maintain, and extend. The influential book _Design Patterns_ by the \"Gang of Four\" categorizes design patterns into three types:\n\n- **Creational Patterns**: Handle object creation mechanisms.\n- **Structural Patterns**: Define how objects and components relate.\n- **Behavioral Patterns**: Govern object communication and workflows.\n\nUsing design patterns effectively can improve code quality, but excessive or incorrect use may introduce unnecessary complexity.\n\n## Key Software Design Patterns\n\n### Singleton Pattern\n\nThe **Singleton pattern** ensures that only one instance of a class exists and provides a global point of access.\n\nUse Cases: [[Database]] connections, Logging services, Global configurations\n#### Example (JavaScript):\n\n```javascript\nclass Singleton {\n  constructor() {\n    if (!Singleton.instance) {\n      Singleton.instance = this;\n    }\n    return Singleton.instance;\n  }\n}\nconst instance1 = new Singleton();\nconst instance2 = new Singleton();\nconsole.log(instance1 === instance2); // true\n```\n\n### Prototype Pattern\n\nThe **Prototype pattern** allows new objects to be created by cloning an existing object rather than instantiating a new class.\n\nUse Cases: Performance optimization, Object cloning\n\n#### Example (JavaScript):\n\n```javascript\nconst carPrototype = {\n  start: function () {\n    console.log(\"Engine started!\");\n  }\n};\nconst myCar = Object.create(carPrototype);\nmyCar.start(); // Engine started!\n```\n\n### Builder Pattern\n\nThe **Builder pattern** simplifies object creation when multiple configuration options exist.\n\nUse Cases: Constructing complex objects, UI component creation\n\n#### Example (JavaScript):\n\n```javascript\nclass CarBuilder {\n  constructor() {\n    this.car = {};\n  }\n  setColor(color) {\n    this.car.color = color;\n    return this;\n  }\n  setWheels(wheels) {\n    this.car.wheels = wheels;\n    return this;\n  }\n  build() {\n    return this.car;\n  }\n}\nconst myCar = new CarBuilder().setColor(\"red\").setWheels(4).build();\nconsole.log(myCar);\n```\n\n### Factory Pattern\n\nThe **Factory pattern** encapsulates object creation logic, making code more modular and easier to extend.\n\nUse Cases: Dependency injection, Platform-specific object creation\n\n#### Example (JavaScript):\n\n```javascript\nclass CarFactory {\n  static createCar(type) {\n    const carTypes = {\n      sedan: { type: \"sedan\", doors: 4 },\n      coupe: { type: \"coupe\", doors: 2 },\n    };\n    return carTypes[type] || null;\n  }\n}\nconst myCar = CarFactory.createCar(\"sedan\");\nconsole.log(myCar);\n```\n\n### Facade Pattern\n\nThe **Facade pattern** provides a simplified interface to a complex system.\n\nUse Cases: Simplifying APIs, Reducing dependencies\n\n#### Example (JavaScript):\n\n```javascript\nclass Computer {\n  start() { console.log(\"Computer starting...\"); }\n  shutdown() { console.log(\"Computer shutting down...\"); }\n}\nclass ComputerFacade {\n  constructor() {\n    this.computer = new Computer();\n  }\n  turnOn() {\n    this.computer.start();\n  }\n  turnOff() {\n    this.computer.shutdown();\n  }\n}\nconst myComputer = new ComputerFacade();\nmyComputer.turnOn();\n```\n\n### Proxy Pattern\n\nThe **Proxy pattern** acts as an intermediary to control access to an object.\n\nUse Cases: Lazy loading, [[Data Security]] proxies\n\n#### Example (JavaScript):\n\n```javascript\nclass RealImage {\n  constructor(filename) {\n    this.filename = filename;\n  }\n  display() {\n    console.log(\"Displaying \" + this.filename);\n  }\n}\nclass ProxyImage {\n  constructor(filename) {\n    this.realImage = null;\n    this.filename = filename;\n  }\n  display() {\n    if (!this.realImage) {\n      this.realImage = new RealImage(this.filename);\n    }\n    this.realImage.display();\n  }\n}\nconst image = new ProxyImage(\"test.jpg\");\nimage.display();\n```\n\n### Iterator Pattern\n\nThe **Iterator pattern** provides a way to access elements of a collection sequentially without exposing its internal structure.\n\nUse Cases: Collection traversal, Data processing\n\n#### Example (JavaScript):\n\n```javascript\nclass Iterator {\n  constructor(items) {\n    this.items = items;\n    this.index = 0;\n  }\n  next() {\n    return this.index < this.items.length ? { value: this.items[this.index++], done: false } : { done: true };\n  }\n}\nconst iterator = new Iterator([\"a\", \"b\", \"c\"]);\nconsole.log(iterator.next());\nconsole.log(iterator.next());\nconsole.log(iterator.next());\n```\n\n### Observer Pattern\n\nThe **Observer pattern** enables a one-to-many dependency between objects, ensuring changes to one object are reflected in its dependents.\n\nUse Cases: Event handling, Reactive programming\n\n#### Example (JavaScript):\n\n```javascript\nclass Subject {\n  constructor() {\n    this.observers = [];\n  }\n  subscribe(observer) {\n    this.observers.push(observer);\n  }\n  notify(data) {\n    this.observers.forEach(observer => observer.update(data));\n  }\n}\nclass Observer {\n  update(data) {\n    console.log(\"Received update: \" + data);\n  }\n}\nconst subject = new Subject();\nconst observer1 = new Observer();\nsubject.subscribe(observer1);\nsubject.notify(\"Hello World\");\n```\n\n### Mediator Pattern\n\nThe **Mediator pattern** centralizes communication between objects to reduce dependencies.\n\nUse Cases: Chat applications, Workflow coordination\n\n#### Example (JavaScript):\n\n```javascript\nclass Mediator {\n  constructor() {\n    this.participants = [];\n  }\n  register(participant) {\n    this.participants.push(participant);\n  }\n  send(message, sender) {\n    this.participants.forEach(participant => {\n      if (participant !== sender) {\n        participant.receive(message);\n      }\n    });\n  }\n}\nclass Participant {\n  constructor(name, mediator) {\n    this.name = name;\n    this.mediator = mediator;\n    mediator.register(this);\n  }\n  send(message) {\n    this.mediator.send(message, this);\n  }\n  receive(message) {\n    console.log(this.name + \" received: \" + message);\n  }\n}\nconst mediator = new Mediator();\nconst p1 = new Participant(\"Alice\", mediator);\np1.send(\"Hello\");\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "communication",
      "software"
    ],
    "normalized_filename": "software_design_patterns",
    "outlinks": [
      "database",
      "data_security"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "Spreadsheets vs Databases",
    "sha": "649d7149950f13e9565e5b8f24cddd1be129207d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Spreadsheets%20vs%20Databases.md",
    "text": "Compared to spreadsheets, databases offer:\n\n1. [[Scalability]]: Databases are designed to handle large volumes of data, making them suitable for applications with millions or even billions of records. In contrast, spreadsheets can become unwieldy and slow when dealing with large datasets.\n\n2. Update Frequency: Databases support real-time updates and continuous operations, allowing for dynamic [[Data Management]]. Spreadsheets, on the other hand, are more static and may require manual updates, which can lead to outdated information.\n\n3. Speed: Databases are optimized for quick access, retrieval, and manipulation of data. They can efficiently handle multiple concurrent users, making them ideal for environments where data is frequently accessed and modified. Spreadsheets can lag in performance under similar conditions.\n\n### Tags\n- **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "storage"
    ],
    "normalized_filename": "spreadsheets_vs_databases",
    "outlinks": [
      "scalability",
      "data_management"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ]
  },
  {
    "category": "DE",
    "filename": "Star Schema",
    "sha": "a07a7b2a775a9ed81641f6f8d826e03df10be084",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Star%20Schema.md",
    "text": "Star Schema\n   - This schema consists of a central fact table surrounded by dimension tables. The fact table contains quantitative data, while the dimension tables provide descriptive attributes. The star schema is easy to understand and query, making it popular for [[OLAP]] applications.\n   - Description: A simple and widely used form of dimensional modeling where a central fact table is connected to multiple dimension tables.\n   - Advantages: Easy to understand and query, with straightforward joins between fact and dimension tables.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "star_schema",
    "outlinks": [
      "olap"
    ],
    "inlinks": [
      "dimension_table",
      "dimensional_modelling",
      "snowflake_schema",
      "types_of_database_schema"
    ]
  },
  {
    "category": "CS",
    "filename": "Stored Procedures",
    "sha": "7b318717f59ddc6b121f950bd8e49e87470a11b5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Stored%20Procedures.md",
    "text": "Stored procedures are a way to automate SQL statements, allowing them to be executed repeatedly without rewriting the code.\n\n**Demonstration with the Boston MFA Database**  \n\nWe will use the Boston MFA database to illustrate stored procedures. Previously, we implemented a soft-delete feature for the `collections` table using views. Now, we will create a stored procedure to achieve similar functionality.\n\n1. **Select the Database**:\n   ```sql\n   USE `mfa`;\n   ```\n\n2. **Add a Deleted Column**:  \n   The `deleted` column needs to be added to the `collections` table to track soft deletions.\n   ```sql\n   ALTER TABLE `collections` \n   ADD COLUMN `deleted` TINYINT DEFAULT 0;\n   ```\n   The `TINYINT` type is appropriate since the column will only hold values of 0 or 1, with a default of 0 to retain all existing collections.\n\n3. **Change the Delimiter**:  \n   Before creating a stored procedure, change the delimiter to allow multiple statements.\n   ```sql\n   delimiter //\n   ```\n\n4. **Create the Stored Procedure**:  \n   Define the stored procedure to select current collections that are not marked as deleted.\n   ```sql\n   CREATE PROCEDURE `current_collection`()\n   BEGIN\n       SELECT `title`, `accession_number`, `acquired` \n       FROM `collections` \n       WHERE `deleted` = 0;\n   END//\n   ```\n\n5. **Reset the Delimiter**:  \n   After creating the procedure, reset the delimiter back to the default.\n   ```sql\n   delimiter ;\n   ```\n\n6. **Call the Stored Procedure**:  \n   Execute the procedure to see the current collections.\n   ```sql\n   CALL current_collection();\n   ```\n\n7. **Soft Delete an Item**:  \n   If we soft-delete an item, such as “Farmers working at dawn,” and call the procedure again, the deleted row will not appear in the output.\n   ```sql\n   UPDATE `collections` \n   SET `deleted` = 1 \n   WHERE `title` = 'Farmers working at dawn';\n   ```\n\n### Parameters in Stored Procedures\n\nStored procedures can accept parameters. For example, we can create a procedure to handle the sale of artwork.\n\n1. **Create the [[Transaction|Transactions]] Table**:\n   ```sql\n   CREATE TABLE `transactions` (\n       `id` INT AUTO_INCREMENT,\n       `title` VARCHAR(64) NOT NULL,\n       `action` ENUM('bought', 'sold') NOT NULL,\n       PRIMARY KEY(`id`)\n   );\n   ```\n\n2. **Create the Sell Procedure**:  \n   This procedure updates the `collections` table and logs the transaction.\n   ```sql\n   delimiter //\n   CREATE PROCEDURE `sell`(IN `sold_id` INT)\n   BEGIN\n       UPDATE `collections` SET `deleted` = 1 \n       WHERE `id` = `sold_id`;\n       INSERT INTO `transactions` (`title`, `action`)\n       VALUES ((SELECT `title` FROM `collections` WHERE `id` = `sold_id`), 'sold');\n   END//\n   delimiter ;\n   ```\n\n3. **Call the Sell Procedure**:  \n   To sell a specific item, call the procedure with the item's ID.\n   ```sql\n   CALL `sell`(2);\n   ```\n\n### Considerations\n\n- **Multiple Calls**:  \n  If the `sell` procedure is called with the same ID multiple times, it may lead to multiple entries in the `transactions` table. Logic can be added to prevent this.\n\n- **Programming Constructs**:  \n  Stored procedures can be enhanced with programming constructs available in MySQL, allowing for more complex logic.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "database"
    ],
    "normalized_filename": "stored_procedures",
    "outlinks": [
      "transaction"
    ],
    "inlinks": [
      "database_techniques"
    ]
  },
  {
    "category": "DE",
    "filename": "Structuring and organizing data",
    "sha": "32c2e440fd9cab972f467267aff3fc430797fc2d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Structuring%20and%20organizing%20data.md",
    "text": "Structuring and organizing data.\n\nIn [[DE_Tools]] see:\n\t- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/multi_index.ipynb\n\t- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb\n\nRelated terms:\n- [[Multi-level index]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "structuring_and_organizing_data",
    "outlinks": [
      "de_tools",
      "multi-level_index"
    ],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "DE",
    "filename": "Transaction",
    "sha": "02a56a7d749e2662f17cd424798166bbe862eed1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Transaction.md",
    "text": "Transactions are used for maintaining [[Data Integrity]] and should adhere to the [[ACID Transaction]].\n## Transaction Operations\n\n- Commit: Saves all changes made during the transaction.\n- Rollback: Reverts the database to its previous state if an error occurs during the transaction.\n\n## Concurrency and Transactions\n\n- Concurrency: Allows multiple [[Querying]] to run simultaneously, essential for high-traffic applications.\n- Race Conditions: Occur when concurrent transactions access and modify shared data, potentially causing inconsistencies.\n\n## Transaction Locks\n\nTo prevent ==race conditions,== transactions and locking mechanisms are employed to ensure that operations occur sequentially. Locks manage access to the database during [[Transaction|Transactions]]:\n\n- UNLOCK: Allows anyone to read or add data.\n- SHARED: Permits reading while allowing others to access the data.\n- LOCKED: Grants exclusive write access to ensure that no other transactions can interfere.\n\n### Types of Locks\n- Shared Locks: Used for read operations.\n- Exclusive Locks: Used for write operations.\n\n### [[granularity]] of Locks\n\n[[SQLite]] locks the entire database during exclusive transactions. While finer granularity (e.g., row-level locks) is possible in other database management systems ([[Database Management System (DBMS)|DBMS]]), SQLite's approach is simpler but can lead to contention in high-concurrency scenarios.\n\n### Timestamping\nUsing timestamping can help manage access to exclusive locks, allowing for more efficient handling of concurrent transactions ([[Concurrency]]).",
    "aliases": [
      "Transactions"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "transaction",
    "outlinks": [
      "transaction",
      "data_integrity",
      "sqlite",
      "querying",
      "concurrency",
      "acid_transaction",
      "granularity",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "acid_transaction",
      "grain",
      "granularity",
      "query_optimisation",
      "stored_procedures",
      "transaction"
    ]
  },
  {
    "category": "DE",
    "filename": "Turning a flat file into a database",
    "sha": "172aab3a5bacb44103228aa5306b4a39bb65d3a7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Turning%20a%20flat%20file%20into%20a%20database.md",
    "text": "## Summary:\n\n1. Read and Clean the Data: Load the data from the Excel sheet and clean it.\n2. Split the Data: Separate the data into two DataFrames, one for customers and one for orders.\n3. Create Tables: Create the SQLite tables with appropriate foreign key relationships.\n4. Insert Data: Insert the cleaned and separated data into the respective tables.\n5. Verify [[Foreign Key]]: Ensure that the foreign key relationships are valid.\n\n## Example Data Structure\n\n### Combined Excel Data (`Sheet1`):\n\n| order_id | order_date | customer_id | customer_name | contact_name | country | amount |\n|----------|------------|-------------|---------------|--------------|---------|--------|\n| 1        | 2024-01-15 | 101         | John Doe      | Jane Doe     | USA     | 100.50 |\n| 2        | 2024-02-20 | 102         | Alice Smith   | Bob Smith    | Canada  | 200.00 |\n| 3        | 2024-03-10 | 101         | John Doe      | Jane Doe     | USA     | 150.75 |\n| 4        | 2024-04-05 | 103         | Michael Brown | Sarah Brown  | UK      | 250.00 |\n\n## Steps to Process and Split Data\n\n### Step 1: Import Libraries and Read Data\n\n```python\nimport pandas as pd\nimport sqlite3\n\n# Example data for demonstration purposes\ndata = {\n    'order_id': [1, 2, 3, 4],\n    'order_date': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],\n    'customer_id': [101, 102, 101, 103],\n    'customer_name': ['John Doe', 'Alice Smith', 'John Doe', 'Michael Brown'],\n    'contact_name': ['Jane Doe', 'Bob Smith', 'Jane Doe', 'Sarah Brown'],\n    'country': ['USA', 'Canada', 'USA', 'UK'],\n    'amount': [100.50, 200.00, 150.75, 250.00]\n}\n\n# Create a DataFrame from the example data\ndf = pd.DataFrame(data)\n```\n\n### Step 2: Clean Data\n\n```python\n# Example cleaning function\ndef clean_data(df):\n    df.dropna(inplace=True)\n    df.columns = [col.strip().replace(\" \", \"_\").lower() for col in df.columns]\n    return df\n\n# Clean the data\ndf = clean_data(df)\n```\n\n### Step 3: Split Data into Customers and Orders\n\n```python\n# Extract unique customers\ncustomers_df = df[['customer_id', 'customer_name', 'contact_name', 'country']].drop_duplicates()\n\n# Extract orders\norders_df = df[['order_id', 'order_date', 'customer_id', 'amount']]\n```\n\n### Step 4: Create Tables with Foreign Keys in SQLite\n\n```python\n# Connect to SQLite database (or create it)\nconn = sqlite3.connect('data.db')\ncursor = conn.cursor()\n\n# Enable foreign key support\ncursor.execute(\"PRAGMA foreign_keys = ON\")\n\n# Create 'customers' table\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS customers (\n    customer_id INTEGER PRIMARY KEY,\n    customer_name TEXT NOT NULL,\n    contact_name TEXT,\n    country TEXT\n)\n''')\n\n# Create 'orders' table with a foreign key referencing 'customers'\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS orders (\n    order_id INTEGER PRIMARY KEY,\n    order_date TEXT,\n    customer_id INTEGER,\n    amount REAL,\n    FOREIGN KEY (customer_id) REFERENCES customers (customer_id)\n)\n''')\n\n# Commit changes\nconn.commit()\n```\n\n### Step 5: Insert Data into Tables\n\n```python\n# Insert data into 'customers' table\ncustomers_df.to_sql('customers', conn, if_exists='append', index=False)\n\n# Insert data into 'orders' table\norders_df.to_sql('orders', conn, if_exists='append', index=False)\n\n# Commit changes and close the connection\nconn.commit()\nconn.close()\n```\n\n### Verification: Ensure Foreign Key Relationships\n\n```python\nconn = sqlite3.connect('data.db')\ncursor = conn.cursor()\n\n# Query to check if all customer_id in orders table exist in customers table\ncursor.execute('''\nSELECT order_id\nFROM orders\nWHERE customer_id NOT IN (SELECT customer_id FROM customers)\n''')\n\ninvalid_orders = cursor.fetchall()\nconn.close()\n\nif invalid_orders:\n    print(\"Invalid foreign key references found:\", invalid_orders)\nelse:\n    print(\"All foreign key references are valid.\")\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "turning_a_flat_file_into_a_database",
    "outlinks": [
      "'order_id',_'order_date',_'customer_id',_'amount'",
      "'customer_id',_'customer_name',_'contact_name',_'country'",
      "foreign_key"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "melt"
    ]
  },
  {
    "category": "DE",
    "filename": "Types of Database Schema",
    "sha": "4da245309bd80c533e0698d4172e6f809b293e59",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Types%20of%20Database%20Schema.md",
    "text": "There are several types of database schemas commonly used in data warehousing and database design.\n\n[[Star Schema]]\n\n[[Snowflake Schema]]\n\nGalaxy Schema (or Fact Constellation Schema):\n   - This schema consists of multiple fact tables that share dimension tables. It is useful for complex data models that require analysis across different business processes. The galaxy schema allows for more flexibility in querying and reporting.\n\n[[Normalised Schema]]\n\nDenormalized Schema:\n   - A denormalized schema combines data from multiple tables into fewer tables to improve query performance. This approach is often used in data marts and data warehouses where read performance is prioritized over write performance.\n\nEntity-Relationship Model ([[ER Diagrams]])\n   - This is a conceptual schema that represents the data and its relationships in a graphical format. It is often used during the design phase of a database to visualize how entities (tables) relate to one another.\n\nColumnar Schema:\n   - In a [[Columnar Storage]] database, data is stored in columns rather than rows. This schema is optimized for read-heavy operations and analytical queries, making it suitable for data warehousing applications. Examples include Apache Cassandra and Google [[BigQuery]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "management"
    ],
    "normalized_filename": "types_of_database_schema",
    "outlinks": [
      "bigquery",
      "normalised_schema",
      "star_schema",
      "er_diagrams",
      "columnar_storage",
      "snowflake_schema"
    ],
    "inlinks": [
      "database_schema"
    ]
  },
  {
    "category": "DE",
    "filename": "Unix",
    "sha": "de6764a08f9e5eb28be72a6edb0a5c1add5bfc0c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Unix.md",
    "text": "Unix is a operating system.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "unix",
    "outlinks": [],
    "inlinks": [
      "cron_jobs",
      "windows_scheduled_tasks"
    ]
  },
  {
    "category": "DE",
    "filename": "Usability",
    "sha": "33c88c54d8d777597f5cc91524ba74fff1a1447e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Usability.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "term"
    ],
    "normalized_filename": "usability",
    "outlinks": [],
    "inlinks": [
      "batch_vs_powershell_scripts",
      "performance_dimensions"
    ]
  },
  {
    "category": "DE",
    "filename": "Vacuum",
    "sha": "1195ff3dcb439f75fbf44e016e288e06cab9d8a6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Vacuum.md",
    "text": "SQLite's \"VACUUM\" command reclaims unused space after data deletion, reducing database size.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "memory_management"
    ],
    "normalized_filename": "vacuum",
    "outlinks": [],
    "inlinks": [
      "database_techniques",
      "query_optimisation"
    ]
  },
  {
    "category": "DE",
    "filename": "Vector Database",
    "sha": "94f927072efc3c92e3f087505693f362a7c11263",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Vector%20Database.md",
    "text": "Vector databases are specialized systems designed to handle and manage [[Vector Embedding]]. As most real-world data is unstructured, such as text, images, and audio, vector databases play a role in organizing and querying this data effectively. Used because data is [[unstructured data]] i.e image\n\nRelated to:\n- [[FAISS]]\n### Key Features\n\n- Vector Embeddings: At the core, vector databases store embeddings generated by machine learning models. These embeddings transform complex data into fixed-size vectors that encapsulate semantic information.\n- [[Similarity Search]]: By leveraging the geometric properties of vector spaces, vector databases can quickly identify similar items. This is achieved by measuring distances (e.g., [[Cosine Similarity]], Euclidean distance) between vectors.\n- Indexing Methods: Various indexing techniques, such as HNSW (Hierarchical Navigable Small World) graphs, IVF (Inverted File), and PQ (Product Quantization), are employed to optimize [[Search]] speed and accuracy. Allows faster searching.\n\n### Querying Vectors\n\nTo query vectors, users typically specify a target vector and a similarity metric. The database then retrieves vectors that are closest to the target, based on the chosen metric. \n\n### Use Cases\n1. Long-term Memory for [[LLM]]: Vector databases can store vast amounts of contextual information, enhancing the memory and retrieval capabilities of large language models (LLMs). Implemented using [[Langchain]].\n2. Rank and Recommendation system using nearest neighbours.\n3. [[Semantic search]]: Unlike traditional keyword-based search, semantic search understands the context and meaning, providing more relevant results. This is particularly useful in natural language processing (NLP) applications.\n4. [[Similarity Search]]: Beyond text, vector databases support similarity searches for multimedia data, enabling applications in image recognition, audio analysis, and video retrieval.\n### Related Concepts\n- [[Vector Embedding]]: The process of converting data into vector form, capturing its semantic essence. A specific type of vector embedding used in NLP to represent words in a continuous vector space.\n- [[Semantic Relationships]]: A search technique that leverages the meaning and context of queries and data to deliver more relevant results.\n- [[Cosine Similarity]]\n### Options\nSeveral vector database solutions are available, each with unique features and optimizations:\n- Pincone: Known for its scalability and ease of integration with machine learning workflows.\n- Weaviate: Offers a semantic graph database with built-in vector search capabilities.\n- Chroma: Focuses on simplicity and performance for embedding-based applications.\n- Redis: Provides vector search capabilities through its modules, suitable for real-time applications.\n- Qdrant: Designed for high-performance vector search with a focus on scalability.\n- Milvus: An open-source solution optimized for handling large-scale vector data.\n- Vespa: Combines vector search with traditional search capabilities, ideal for complex applications.\n### Resources\n\n[Vector Databases simply explained! (Embeddings & Indexes)](https://www.youtube.com/watch?v=dN0lsF2cvm4&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "vector_database",
    "outlinks": [
      "vector_embedding",
      "langchain",
      "semantic_relationships",
      "llm",
      "semantic_search",
      "unstructured_data",
      "similarity_search",
      "cosine_similarity",
      "search",
      "faiss"
    ],
    "inlinks": [
      "relationships_in_memory"
    ]
  },
  {
    "category": "DE",
    "filename": "Vectorized Engine",
    "sha": "f44b0918d27ffebe3fedaef819c82a03846ce116",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Vectorized%20Engine.md",
    "text": "A modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of modern CPUs. \n\nVectorized engines, such as [[DuckDB]], process data in large blocks or batches using SIMD instructions, allowing for improved parallelism, cache locality, and reduced overhead compared to traditional row-at-a-time processing engines, using [[Columnar Storage]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "querying"
    ],
    "normalized_filename": "vectorized_engine",
    "outlinks": [
      "columnar_storage",
      "duckdb"
    ],
    "inlinks": [
      "database_storage"
    ]
  },
  {
    "category": "DE",
    "filename": "View Use Case",
    "sha": "cfa4f5c6da65efe3b56658a68758e823719b4ddc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/View%20Use%20Case.md",
    "text": "## View Use Case\n\n### Scenario\n\nA company wants to generate monthly performance reports for its employees. The performance data is spread across multiple tables, including `employees`, `departments`, and `performance_reviews`. ==Instead of writing complex queries== every time a report is needed, the company can create a view that simplifies data retrieval.\n\n### Step 1: Define the Tables\n\nAssume we have the following tables:\n\n- **employees**: Contains employee details.\n  - `employee_id`\n  - `name`\n  - `department_id`\n\n- **departments**: Contains department details.\n  - `department_id`\n  - `department_name`\n\n- **performance_reviews**: Contains performance review data.\n  - `review_id`\n  - `employee_id`\n  - `review_score`\n  - `review_date`\n\n### Step 2: Create a View\n\nTo simplify the reporting process, we create a view that joins these tables and aggregates the performance scores:\n\n```sql\nCREATE VIEW employee_performance AS\nSELECT \n    e.employee_id,\n    e.name,\n    d.department_name,\n    AVG(pr.review_score) AS average_score\nFROM \n    employees e\nJOIN \n    departments d ON e.department_id = d.department_id\nJOIN \n    performance_reviews pr ON e.employee_id = pr.employee_id\nGROUP BY \n    e.employee_id, e.name, d.department_name;\n```\n\n### Step 3: Query the View\n\nNow, whenever the HR department needs to generate a performance report, they can simply query the `employee_performance` view:\n\n```sql\nSELECT * FROM employee_performance WHERE average_score >= 4.0;\n```\n\nThis query retrieves all employees with an average performance score of 4.0 or higher, making it easy to identify top performers.\n\n### Benefits of Using Views in This Use Case\n\n1. **Simplification**: The view encapsulates complex joins and aggregations, allowing HR to retrieve performance data without needing to understand the underlying table structure.\n\n2. **Reusability**: The view can be reused for different reports, such as quarterly reviews or department-specific performance assessments.\n\n3. **[[Maintainability]]**: If the logic for calculating performance scores changes, the HR team only needs to update the view definition, not every individual query.\n\n4. **Data Consistency**: All reports generated from the view will be consistent, as they rely on the same underlying logic for calculating average scores.\n\n5. **[[Data Security]]**: If sensitive employee data needs to be protected, the view can be designed to exclude certain columns, ensuring that only necessary information is accessible.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "explainability",
      "transformation"
    ],
    "normalized_filename": "view_use_case",
    "outlinks": [
      "data_security",
      "maintainability"
    ],
    "inlinks": [
      "views"
    ]
  },
  {
    "category": "DE",
    "filename": "Views",
    "sha": "dd7aa0d26eaffef7f7f654a6708b1f99cb6f332f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Views.md",
    "text": "Views are virtual tables defined by SQL [[Querying|Query]] that ==simplify complex data representation.== They can remove unnecessary columns, aggregate results, partition data, and secure sensitive information.\n\nIn [[DE_Tools]] see:\nhttps://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Viewing/Viewing.ipynb\n\nBasic Usage:\n- Simplification\n- Aggregation (using GROUP)\n- [[Common Table Expression]]\n- Securing data: can give all values in a field the same value.\n\nAdvanced Usage\n- Temporary Views: Exist only for the ==duration of the database connection.==\n- [[Common Table Expression]]: Serve as temporary views for a single query.\n- [[Soft Deletion]]: Use views and triggers to mark records as deleted without physically removing them from the table.\n\nRelated topics:\n- [[View Use Case]]\n\n## Why Use Views?\n\n1. **Simplification and Abstraction**:\n   - Views encapsulate complex queries, allowing users to interact with data without needing to understand the underlying structure. This simplifies data retrieval by hiding complexity.\n\n2. **Security**:\n   - Views restrict access to specific data by granting users access to views instead of underlying tables, which can help protect sensitive information. Note: Access controls may vary by database system (e.g., not available in [[SQLite]]).\n\n3. **Reusability and [[Maintainability]]**:\n   - Define complex queries once in a view and reuse them across multiple applications, simplifying maintenance when logic changes.\n\n4. **Data Consistency and Integrity**:\n   - Ensure consistent data presentation across applications and users by encapsulating business logic for uniform calculations.\n\n5. **Performance Optimization**:\n   - While regular views do not inherently improve performance, materialized views can enhance performance by storing precomputed results.\n\n6. **Logical Data Independence**:\n   - Provide a layer of abstraction between physical data storage and access methods, allowing [[Database Schema|schema]] changes without affecting view users.\n\n7. **Aggregation and Partitioning**:\n   - Views can be used to calculate and store aggregated results (e.g., average ratings) and organize data by specific criteria (e.g., years or categories).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "views",
    "outlinks": [
      "common_table_expression",
      "sqlite",
      "view_use_case",
      "querying",
      "soft_deletion",
      "database_schema",
      "maintainability",
      "de_tools"
    ],
    "inlinks": [
      "common_table_expression",
      "joining_time_series"
    ]
  },
  {
    "category": "DE",
    "filename": "Why Use PySpark in Databricks",
    "sha": "5488319b5c010fd915daa27d29a3606542a19b4a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Why%20Use%20PySpark%20in%20Databricks.md",
    "text": "[[Databricks|Databricks]] & [[PySpark]]\n* **Managed Infrastructure:** Spark clusters are automatically handled.\n* **Integration:** Seamlessly connects with data sources (S3, Azure, Delta Lake).\n* **Collaboration:** Shared notebooks and versioned workflows.\n* **[[Scalability]]:** Run the same code on small or massive data volumes.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [],
    "normalized_filename": "why_use_pyspark_in_databricks",
    "outlinks": [
      "scalability",
      "pyspark",
      "databricks"
    ],
    "inlinks": [
      "pyspark"
    ]
  },
  {
    "category": "DE",
    "filename": "Windows Subsystem for Linux",
    "sha": "deb25684cb37a4f92c671465d082e50e39fd18b2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/Windows%20Subsystem%20for%20Linux.md",
    "text": "[[Windows Subsystem for Linux]] (WSL) is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows 11. It allows users to run a Linux environment directly on Windows without the need for a virtual machine or dual-boot setup. \n\nKey features of WSL include:\n\n1. **Integration with Windows**: Users can access files from both Windows and the [[Linux]] environment seamlessly.\n2. **Multiple Distributions**: WSL supports various Linux distributions, such as [[Ubuntu]], Debian, and Fedora, which can be installed from the Microsoft Store.\n3. **Command-Line Tools**: Users can run Linux command-line tools and applications directly in Windows, making it easier for developers to work in a familiar environment.\n4. **Performance**: WSL provides near-native performance for Linux applications, making it suitable for development and testing.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "windows_subsystem_for_linux",
    "outlinks": [
      "windows_subsystem_for_linux",
      "ubuntu",
      "linux"
    ],
    "inlinks": [
      "powershell_vs_bash",
      "windows_subsystem_for_linux"
    ]
  },
  {
    "category": "DE",
    "filename": "data lineage",
    "sha": "8e6767241c0a0d88c0f293542301e2fc63ac141e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/data%20lineage.md",
    "text": "Data lineage uncovers the [[Data Lifecycle Management]] life cycle of data. It aims to show the complete data flow from start to finish. \n\nData lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. \n\nThis includes all [Data Transformation](Data%20Transformation.md) (what changed and why).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "data_lineage",
    "outlinks": [
      "data_lifecycle_management"
    ],
    "inlinks": [
      "dbt",
      "declarative_data_pipeline",
      "model_observability"
    ]
  },
  {
    "category": "DE",
    "filename": "dbt 1",
    "sha": "a196dd9f014ca733a99b1dd363cd3eb7d18ff3fc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/dbt%201.md",
    "text": "Removing manual task compiling csv and jupyters to \n\nsingle command to streamline stuff\n\nsources\n\nmodels\n\ndocs\n\nmodels\n\ndeployments\n\n- dbt empowers data teams to leverage software engineering principles for transforming data.\n- The focus of this course is to build your analytics engineering mindset and dbt skills to give you more leverage in your work\n\n\ntests and docs\n\nwrite descriptions on the model \n\ncommands: \ndbt run\ndbt test\ndbt docs generate - what cols mean ect \n\nview lineage graph\n\nview dag here in docs\n\ndbt cloud \n\nrun on a scheudle \ndevelopmenet env\ndeployment\n\nrun jobs\n\ndbt run = build and test\n\ndbt? \n\n1) data sources - \n2) loaders\n3) data platform - databricks - dbt works here - curated data\n4) ml models, bi toosls\n\ndbt DAG for data lineage\n\ndbt cloud interface\n\ndbt is the T in the ELT \n\ndbt cloud IDE? \n\nDAG - flow of data from source\n\nsource table - \nmodels - build them \n\ndbt in vscodE?  \n\nuse YAMl to tell about tables",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "dbt_1",
    "outlinks": [],
    "inlinks": [
      "analytics_engineer"
    ]
  },
  {
    "category": "DE",
    "filename": "design pattern",
    "sha": "31a6e788843f4af933af3493c76df29ef021e7fa",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/design%20pattern.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "design_pattern",
    "outlinks": [],
    "inlinks": [
      "lambda_architecture"
    ]
  },
  {
    "category": "DE",
    "filename": "heterogeneous features",
    "sha": "dde518b78661bffff19bbd72186ad1dd73b88f0a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/heterogeneous%20features.md",
    "text": "## Description\n\nIn machine learning, heterogeneous features refer to a situation where the input data contains a variety of different types of features. Let's break it down:\n\n### 1. **Features:**\n   - Features are the individual measurable properties or characteristics of the data used for making predictions in a machine learning model.\n   - For example, in a dataset about houses, features could include the number of bedrooms, square footage, location, and whether it has a garden.\n\n### 2. **Homogeneous vs. Heterogeneous:**\n   - **Homogeneous Features:** In some datasets, all features are of the same type, such as numerical or categorical. For instance, a dataset containing only numerical features like age, income, and temperature is homogeneous.\n   - **Heterogeneous Features:** In contrast, heterogeneous features refer to datasets where features are of different types. This means the dataset may contain a mix of numerical, categorical, text, image, or other types of data.\n\n### 3. **Examples of Heterogeneous Features:**\n   - **Numerical Features:** Represented by continuous values like age, income, or temperature.\n   - **Categorical Features:** Represented by discrete values such as gender, city, or type of car.\n   - **Text Features:** Textual data like product descriptions, customer reviews, or email content.\n   - **Image Features:** Visual data represented by pixels in an image, used in tasks like image recognition or object detection.\n\n### 4. **Challenges and Considerations:**\n   - Handling heterogeneous features requires specialized techniques in [[Preprocessing]] and [[Model Building]].\n   - Different types of features may need different preprocessing steps, such as encoding categorical variables, scaling numerical features, or extracting features from text or images.\n   - Models need to be capable of handling diverse data types, either through feature engineering or using algorithms specifically designed for heterogeneous data.\n\n### 5. **Applications:**\n   - Heterogeneous features are common in many real-world applications, such as e-commerce (combining text descriptions with numerical features), healthcare (integrating medical records with images or text), and social media analysis (analyzing text, images, and user profiles).\n\n### 6. **Resources for Further Learning:**\n   - Feature Engineering for Machine Learning: [https://www.datacamp.com/community/tutorials/feature-engineering-kaggle](https://www.datacamp.com/community/tutorials/feature-engineering-kaggle)\n   - Handling Text Data in Machine Learning: [https://towardsdatascience.com/handling-text-data-in-machine-learning-projects-b52bbc9531d7](https://towardsdatascience.com/handling-text-data-in-machine-learning-projects-b52bbc9531d7)\n   - Image Feature Extraction Techniques: [https://towardsdatascience.com/image-feature-extraction-techniques-91e8625616f1](https://towardsdatascience.com/image-feature-extraction-techniques-91e8625616f1)\n\nUnderstanding how to work with heterogeneous features is essential for building effective machine learning models that can handle diverse types of data and extract meaningful insights from them.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning"
    ],
    "normalized_filename": "heterogeneous_features",
    "outlinks": [
      "preprocessing",
      "model_building"
    ],
    "inlinks": [
      "gradient_boosting"
    ]
  },
  {
    "category": "DE",
    "filename": "in-memory format",
    "sha": "318b3ad5449478128080d4017e7f2f371d71a66b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/in-memory%20format.md",
    "text": "The term \"in-memory format\" refers to the way data is stored and managed directly in a ==computer's RAM== ([[Random Access Memory]]) rather than on disk storage like a hard drive or SSD. This approach is used to optimize performance, as accessing data in RAM is significantly faster than accessing data on disk.\n\nIn-memory formats are often used in applications that require high-speed data processing, such as real-time analytics, caching systems, and certain types of databases (e.g., in-memory databases like Redis or SAP HANA). By keeping data in memory, these systems can reduce latency and improve throughput, enabling faster data retrieval and processing.\n\nIn-memory formats may involve ==specific data structures== or serialization methods that are optimized for quick access and manipulation in RAM. These formats are designed to make efficient use of memory resources while ensuring that data can be quickly read and written by the application.\n\nIn-memory formats are optimized to:\n- hit fast instruction sets \n- be cache friendly \n- be parallelizable\n\nFormats:\n- [Apache Arrow](term/apache%20arrow.md) \n- [Apache Spark](Apache%20Spark.md)\n- [NumPy](term/numpy.md)\n- [Pandas](term/pandas.md)\n\nThe opposed to in-memory formats are [Data Lake File Formats](Data%20Lake%20File%20Formats) which save space, be cross-language and serve as long-term storage.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "storage",
      "system"
    ],
    "normalized_filename": "in-memory_format",
    "outlinks": [
      "random_access_memory"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "incremental synchronization",
    "sha": "1e608f583424e67f0b3acb285a0081e29bb8f41d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/incremental%20synchronization.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "incremental_synchronization",
    "outlinks": [],
    "inlinks": [
      "soft_deletion"
    ]
  },
  {
    "category": "DE",
    "filename": "map reduce",
    "sha": "ce2425d0251164c862eaa59ebfabffa2d14be19b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/map%20reduce.md",
    "text": "MapReduce is a programming model and processing technique used for processing and generating large data sets with a parallel, distributed algorithm on a cluster. [[Distributed Computing]]\n\nIt is a core component of the Apache Hadoop [[Hadoop]] framework, which is designed to handle vast amounts of data across many servers. The MapReduce model simplifies data processing across large clusters by breaking down the task into two main functions: **Map** and **Reduce**.\n\nMapReduce is particularly effective for [[Batch Processing]] tasks where the data can be processed independently and aggregated later. However, it may not be the best choice for [[real-time processing]] or tasks that require low-latency responses, where other frameworks like [[Apache Spark]] might be more suitable.\n### Key Components of MapReduce\n\n1. **Map Function**:\n   - **Purpose**: To process and transform input data into a set of intermediate key-value pairs.\n   - **Functionality**: Each input data element is processed independently, and the output is a collection of key-value pairs.\n   - **Example**: In a word count application, the map function reads a document and emits each word as a key with a count of one as the value.\n\n2. **Shuffle and Sort**:\n   - **Purpose**: To organize the intermediate data by keys.\n   - **Functionality**: The framework sorts the output of the map function and groups all values associated with the same key together. This step is crucial for the reduce function to process data efficiently.\n\n3. **Reduce Function**:\n   - **Purpose**: To aggregate and summarize the intermediate data.\n   - **Functionality**: The reduce function takes the grouped key-value pairs and processes them to produce a smaller set of output values.\n   - **Example**: Continuing with the word count example, the reduce function sums up the counts for each word, resulting in the total count for each word across all documents.\n\n### Why MapReduce is Used\n\n- **Scalability**: MapReduce can process petabytes of data by distributing the workload across a large number of servers in a cluster.\n- **Fault Tolerance**: The framework automatically handles failures by reassigning tasks to other nodes, ensuring that the processing continues without data loss.\n- **Simplicity**: It abstracts the complexity of parallel processing, allowing developers to focus on the map and reduce logic without worrying about the underlying infrastructure.\n- **Flexibility**: MapReduce can be used for a wide range of applications, including data mining, log analysis, and machine learning, among others.\n- **Cost-Effectiveness**: By using commodity hardware and open-source software, organizations can process large data sets without significant investment in specialized hardware.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning"
    ],
    "normalized_filename": "map_reduce",
    "outlinks": [
      "apache_spark",
      "real-time_processing",
      "distributed_computing",
      "hadoop",
      "batch_processing"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "master data management",
    "sha": "7d3d5ca341004184e5ddb42aec258470336c2ce2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/master%20data%20management.md",
    "text": "Master data management is a method to ==centralize== master data.\n\nIt's the bridge between the business that maintain the data and know them best and the data folks, and it's a tool of choice. It helps with uniformity, accuracy, stewardship, semantic consistency, and accountability of mostly enterprise master data assets.\n\nMaster [[Data Management]](MDM) refers to the processes, technologies, and tools used to define, manage, and maintain an organization's critical data entities, such as customers, products, employees, suppliers, and locations, ==ensuring that this data is accurate, consistent, and up-to-date across all systems and departments.== The goal of MDM is to create a single, authoritative [[source of truth]] for master data, which is shared and synchronized across the organization to improve decision-making, reduce duplication, and maintain data integrity.\n\nMDM is especially important in large organizations where data is often siloed across various departments and systems, leading to ==inconsistencies, duplication, and errors.== By centralizing the management of key data, MDM helps improve operational efficiency, regulatory compliance, and the overall effectiveness of business processes.\n\nKey aspects of MDM include:\n\n1. [[Data Governance]]: Establishing policies, rules, and standards for how master data is managed, who is responsible for it, and how data quality is monitored.\n\n2. **Data Integration**: Consolidating and harmonizing data from various sources (e.g., databases, applications) to create a unified, consistent view of master data.\n\n3. [[Data Quality]]: Ensuring that the data is complete, accurate, valid, and consistent across the organization.\n\n4. **Data Stewardship**: Assigning roles and responsibilities for managing the master data and ensuring that it complies with the established governance policies.\n\n5. **Metadata Management**: Maintaining a consistent definition of data entities, relationships, and attributes, helping stakeholders understand the meaning and usage of the data.\n\n6. **Data Synchronization**: Ensuring that any updates or changes to master data in one system are reflected across all relevant systems.",
    "aliases": [
      "mdm"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "governance",
      "management",
      "storage"
    ],
    "normalized_filename": "master_data_management",
    "outlinks": [
      "source_of_truth",
      "data_governance",
      "data_management",
      "data_quality"
    ],
    "inlinks": [
      "data_management",
      "single_source_of_truth"
    ]
  },
  {
    "category": "DE",
    "filename": "neo4j",
    "sha": "eb76171473ac08df7eb1d2ae086921b52cc9392f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/neo4j.md",
    "text": "https://www.youtube.com/watch?v=IShRYPsmiR8\n\nRelated terms:\n- [[neomodel]]\n- [[GraphRAG]]\n- [[Cypher]]\n- [[Graph Query Language]]\n- [[GraphRAG|graph database]]\n\nNeo4j is a graph [[Database]]. Instead of storing data in tables (like SQL), it stores data as nodes (entities) and relationships (connections between entities). Instead of JOINs, Neo4j directly stores and indexes connections.\n\nWhen to use:  \n  - Complex relationships (social networks, fraud detection, recommendations).\n  - You need to traverse lots of relationships quickly.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "neo4j",
    "outlinks": [
      "neomodel",
      "graph_query_language",
      "database",
      "cypher",
      "graphrag"
    ],
    "inlinks": [
      "cypher",
      "graph_query_language",
      "graphrag",
      "neomodel",
      "text2cypher"
    ]
  },
  {
    "category": "DE",
    "filename": "pd.Grouper",
    "sha": "5cf76dab6b89623bb5495c844e97e0deb944a19f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/pd.Grouper.md",
    "text": "`pd.Grouper` is a utility in pandas used with `.groupby()` to flexibly group data by a specific column, often useful for time-based grouping, multi-index grouping, or applying custom frequency aggregation.\n\nSee:\n- [[Groupby]]\n- [[Multi-level index]]\n\n### Why Use `pd.Grouper`?\n- Allows more readable and declarative code when working with time-indexed data.\n- Supports multi-index groupings without restructuring your data.\n- Enables resampling-like grouping without setting the index.\n### Syntax\n```python\npd.Grouper(key=None, level=None, freq=None, axis=0, sort=False)\n```\n### Parameters\n- `key`: The column name to group by.\n- `level`: For MultiIndex, the level to group by.\n- `freq`: Used to group time-series data (e.g., `'D'` for daily, `'M'` for monthly).\n- `axis`: Default is 0 (rows).\n- `sort`: Whether to sort the result.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "pd.grouper",
    "outlinks": [
      "groupby",
      "multi-level_index"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "pgAdmin",
    "sha": "3bacc6210ff9412597d8f6aa8c0120769580dc3e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/pgAdmin.md",
    "text": "Think of pgAdmin as the “control panel” . It lets you operate PostgreSQL more easily.\n\n- Type: GUI administration tool for PostgreSQL.\n- Role: Provides a visual interface to interact with PostgreSQL databases without writing raw SQL for every task.\n- Nature: Desktop or web application that connects to a PostgreSQL server.\n- Usage:\n    - Browse databases, schemas, and tables in a tree view.\n    - Run SQL queries in a built-in editor.\n    - Manage users, roles, and permissions via forms.\n    - Create/alter database objects with point-and-click tools.\n- Interface: Click-based menus and dashboards, plus an SQL editor.\n\n\n- [ ] Explore by using a small dataset. What can you do with it, difference from [[SQLite]]\n#### Installation\n[How to set up a Postgres database on your Windows 10 PC](https://www.youtube.com/watch?v=4J0V3AaiOns)\n\nIn [[Tableau]] can connect to a database here.\n\nThere are plugins.\n\nSpatial objects?\n\nConnections to database tables: hosted\n#### Connecting\n- [[Adding a database to PostgreSQL]]\n\n#### PGAdmin\npgadmin tools: to check system information\n\n![[Pasted image 20250329081752.png]]\n\n\n\nNotes:\n\n[pgAdmin Tutorial - How to Use pgAdmin](https://www.youtube.com/@DatabaseStar)\n\nYou can generate [[ER Diagrams]] in pgadmin\n\nRelated:\n- [[PostgreSQL]]\n- [[Pgadmin Permissions on Windows]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      null
    ],
    "normalized_filename": "pgadmin",
    "outlinks": [
      "sqlite",
      "adding_a_database_to_postgresql",
      "postgresql",
      "er_diagrams",
      "pgadmin_permissions_on_windows",
      "tableau",
      "pasted_image_20250329081752.png"
    ],
    "inlinks": [
      "postgresql"
    ]
  },
  {
    "category": "DE",
    "filename": "pyodbc",
    "sha": "8c5ee339264902f2fc485a502530c06026f9baa0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/pyodbc.md",
    "text": "`pyodbc` is a Python library that provides an interface to connect to databases using the ODBC (Open Database Connectivity) standard.  \nIt allows Python applications to interact with relational databases such as SQL Server, PostgreSQL, MySQL, Oracle, and others, provided an ODBC driver is installed.\n\n### Why investigate?\n- Widely used for enterprise database connectivity.\n- Works across multiple database systems, making it portable.\n- Integrates with pandas for pulling data directly into DataFrames.\n- May be useful for projects requiring Microsoft SQL Server or legacy systems.\n### Key points to check\n- Required ODBC drivers on your system (depends on target database).\n- Alternatives like [[SQLAlchemy]].\n- Compatibility with Windows environments (relevant for corporate IT setups).\n### Example Usage\n```python\nimport pyodbc\nimport pandas as pd\n\n# Example connection to SQL Server\nconn = pyodbc.connect(\n    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n    \"SERVER=server_name;\"\n    \"DATABASE=database_name;\"\n    \"UID=user;\"\n    \"PWD=password\"\n)\n\n# Query\nquery = \"SELECT TOP 10 * FROM my_table\"\ndf = pd.read_sql(query, conn)\n\nconn.close()\n````\n\n### Next steps\n\n- Test connection to a local/remote SQL Server instance.\n- Benchmark performance against `sqlalchemy`.\n- Document common connection strings for databases you use.\n- Check driver installation steps on Windows vs Linux.",
    "date modified": "28-09-2025",
    "aliases": null,
    "title": "Investigate pyodbc",
    "tags": [
      "SQL",
      "python",
      "database",
      "collection"
    ],
    "normalized_filename": "pyodbc",
    "outlinks": [
      "sqlalchemy"
    ],
    "inlinks": [
      "database"
    ]
  },
  {
    "category": "DE",
    "filename": "reverse etl",
    "sha": "221de873276725456d4ae197e3a7f575e19900c5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/reverse%20etl.md",
    "text": "Reverse [[ETL]] is the flip side of the [ETL](ETL.md)/[ELT](term/elt.md). **With Reverse ETL, the data warehouse becomes the source rather than the destination**. Data is taken from the warehouse, transformed to match the destination's data formatting requirements, and loaded into an application – for example, a CRM like Salesforce – to enable action.\n\nIn a way, the Reverse ETL concept is not new to data engineers, who have been enabling data movement warehouses to business applications for a long time. \n\nAs [Maxime Beauchemin](term/maxime%20beauchemin.md) mentions in [his article](https://preset.io/blog/reshaping-data-engineering/), Reverse ETL “appears to be a modern new means of addressing a subset of what was formerly known as  [Master Data Management (MDM)](master%20data%20management.md).”\n\nRead more about in [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).",
    "aliases": [
      "Data Activation",
      "Operational Analytics"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "reverse_etl",
    "outlinks": [
      "etl"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "rollup",
    "sha": "bbef6848993b2999c2ba6a4b06178d529584259f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/rollup.md",
    "text": "Rollup refers to aggregating data to a higher level of [[granularity]], such as summarizing hourly data into daily totals.\n\n[[Database]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database"
    ],
    "normalized_filename": "rollup",
    "outlinks": [
      "granularity",
      "database"
    ],
    "inlinks": [
      "business_intelligence"
    ]
  },
  {
    "category": "DE",
    "filename": "semantic layer",
    "sha": "fd0664cef5d860103fa824a26cd7461fb7a4f0b4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/semantic%20layer.md",
    "text": "A [Semantic Layer](semantic%20layer.md) is much more flexible and makes the most sense on top of [transformed data](Data%20Transformation.md) in a [Data Warehouse](Data%20Warehouse.md).\n\nA semantic layer in the context of a data warehouse is an abstraction layer that sits between the raw data stored in the warehouse and the end users who need to access and analyze that data.\n\nIts primary purpose is to simplify complex data structures and present them in a more user-friendly and business-oriented way. This allows users to interact with the data without needing to understand the underlying complexities of the database schema or query languages.\n\nBridging the gap between complex data systems and business users, enabling more effective and efficient data-driven decision-making.\n\nAvoid extensive reshuffles or reprocesses of large amounts of data. \n\nThink of [OLAP](OLAP.md) cubes where you can dice-and-slice ad-hoc on significant amounts of data without storing them ahead of time\n\n### Key Features of a Semantic Layer\n\n1. Business-Friendly Terminology:\n   - Translates technical database terms into business-friendly language that is easier for non-technical users to understand.\n   - For example, instead of using column names like `cust_id` or `prod_sku`, the semantic layer might present them as \"Customer ID\" or \"Product SKU.\"\n\n2. Data Abstraction:\n   - Hides the complexity of the underlying data model, such as joins, table structures, and data transformations.\n   - Users can focus on business concepts rather than technical details.\n\n3. Consistent [[Metric]] and Calculations:\n   - Provides a centralized definition of key metrics and calculations, ensuring consistency across reports and analyses.\n   - For example, a metric like \"Total Revenue\" would be consistently calculated and presented, regardless of who is querying the data.\n\n4. Security and Access Control:\n   - Implements security rules and access controls to ensure that users only see data they are authorized to access.\n   - This can include row-level security, column-level security, and user-specific data views.\n\n5. Enhanced Query Performance:\n   - Optimizes queries by pre-aggregating data or using materialized views, reducing the load on the data warehouse and improving response times for users.\n\n### Benefits of a Semantic Layer\n\n- Ease of Use: Makes it easier for business users to access and analyze data without needing deep technical knowledge.\n- Faster Insights: Users can quickly generate reports and dashboards using familiar business terms and concepts.\n- Consistency: Ensures that all users are working with the same definitions and calculations, reducing discrepancies in reporting.\n- Scalability: Supports a wide range of analytical tools and applications, allowing organizations to scale their data analytics capabilities.\n\n### Implementation\n\nA semantic layer can be implemented using various tools and technologies, such as:\n\n- Business Intelligence (BI) Tools: Many BI platforms, like [[Tableau]], [[PowerBI]], and Looker, offer built-in semantic layer capabilities.\n- [[Data Virtualization]] Tools: Tools like Denodo or Dremio provide semantic layer functionality by creating virtual views of data.\n- Custom Solutions: Organizations can build custom semantic layers using middleware or data modeling tools.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "storage"
    ],
    "normalized_filename": "semantic_layer",
    "outlinks": [
      "powerbi",
      "data_virtualization",
      "tableau",
      "metric"
    ],
    "inlinks": [
      "granularity"
    ]
  },
  {
    "category": "DE",
    "filename": "storage layer object store",
    "sha": "a45e655310253cd433b348982edf4b16d072068b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/storage%20layer%20object%20store.md",
    "text": "A storage layer or object storage are services from the three big [[Cloud Providers]], \n\nAWS S3,[[S3 bucket]]\nAzure Blob Storage,\nand Google Cloud Storage. \n\nThe web user interface is easy to use. **Its features are very basic, where, in fact, these object stores store distributed files exceptionally well.** They are also highly configurable, with solid security and reliability built-in.",
    "aliases": [
      "Object Store"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "storage"
    ],
    "normalized_filename": "storage_layer_object_store",
    "outlinks": [
      "cloud_providers",
      "s3_bucket"
    ],
    "inlinks": []
  },
  {
    "category": "DE",
    "filename": "structured data",
    "sha": "821b84c803b85c8a092f6b461c69fa3d3fcbfeac",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/structured%20data.md",
    "text": "Structured data refers to data that has been formatted into a well-defined schema ([[Database Schema]]). An example would be data that is stored with precisely defined columns in a relational [[Database]] or excel spreadsheet. Examples of ==structured fields could be age, name, phone number, credit card numbers or address.== Storing data in a structured format allows it to be easily understood and queried by machines and with tools such as SQL.\n\n## Example of structure data\n\nBelow is an example of structured data as it would appear in a database:\n\n| | **age**| **name**| **phone**| \n|---------|-----|------|-----|\n|Record 1| 29 | Bob | 123-456 |\n|Record 2| 30 | Sue | 789-123 | \n\nIt may seem that all data is structured, but this is not always the case -- data can be unstructured, or semi-structured. The differences are best understood by example, and are discussed in the following sections. \n\n## Structured data vs. unstructured data\n\nStructured data can be contrasted with [unstructured data](unstructured%20data.md), which does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used by programs, and is difficult to analyze. Examples of unstructured data could be the contents of an email, contents of a word document, data from social media, photos, videos, survey results, etc.   \n\nAn simple example of unstructured data is a string that contains interesting information inside of it, but that has not been formatted into a well defined schema. An example is given below:\n\n|               |  **UnstructuredString**|\n|---------| -----------|\n|Record 1| \"Bob is 29\" |\n|Record 2| \"Mary just turned 30\"|\n\n## Structuring of unstructured data\n\nConverting unstructured data into structured data can be done during the [Data Transformation](Data%20Transformation.md) stage in an [ETL](ETL.md) or [ELT](term/elt.md) process.  \n\nFor example, in order to efficiently make use of the unstructured data given in the previous example, it may desirable to transform it into structured data such as the following:\n\n|               |  **name** | **age** |\n|---------| -----------|---- |\n|Record 1| \"Bob\" | 29 |\n|Record 2| \"Mary\"| 30 |\n\nStoring the data in a structured manner makes it much more efficient to query the data. For example, after structuring the data it is possible to easily and efficiently execute the following query on the structured data:\n  \n``` SQL\nSELECT * FROM X where Age=29\n```\n\nA query such as this would be expensive and/or more difficult to execute on unstructured data.\n\n## Structured data vs. semi-structured data\n\nStructured data can also be contrasted with [semi-structured data](term/semi-structured%20data.md), which lacks a rigid structure and does not conform directly to a data model. However, semi-structured data has tags and elements that describe the data. \n\nExamples of semi-structured data are [[Json]] or [[XML]] files. Semi-structured data often contains enough information that it can be relatively easily converted into structured data. \n\n==[structured data](term/structured%20data.md) refers to data that has been formatted into a well-defined schema==. An example would be data that is stored with precisely defined columns in a [[Relational Database]] or excel spreadsheet. Examples of structured fields could be age, name, phone number, credit card numbers or address.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "storage"
    ],
    "normalized_filename": "structured_data",
    "outlinks": [
      "xml",
      "database",
      "database_schema",
      "relational_database",
      "json"
    ],
    "inlinks": [
      "data_engineering_portal",
      "data_lake",
      "database",
      "named_entity_recognition",
      "semi-structured_data"
    ]
  },
  {
    "category": "DE",
    "filename": "unstructured data",
    "sha": "75c52915f1563570464bac7a47b8b9904fc53ff2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-engineering/unstructured%20data.md",
    "text": ">[!Important]\n> Unstructured data is data that does not conform to a data model and has no easily identifiable structure. \n\nUnstructured data cannot be easily used by programs, and is difficult to analyze. Examples of unstructured data could be the contents of an ==email, contents of a word document, data from social media, photos, videos, survey results==, etc.\n## An example of unstructured data\n\nAn simple example of unstructured data is a string that contains interesting information inside of it, but that has not been formatted into a well defined schema. An example is given below:\n\n|               |  **UnstructuredString**|\n|---------| -----------|\n|Record 1| \"Bob is 29\" |\n|Record 2| \"Mary just turned 30\"|\n\n## Unstructured vs structured data\n\nIn contrast with unstructured data, [structured data](term/structured%20data.md) refers to data that has been formatted into a well-defined schema. An example would be data that is stored with precisely defined columns in a relational database or excel spreadsheet. Examples of structured fields could be age, name, phone number, credit card numbers or address. Storing data in a structured format allows it to be easily understood and queried by machines and with tools such as  [[SQL]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "storage"
    ],
    "normalized_filename": "unstructured_data",
    "outlinks": [
      "sql"
    ],
    "inlinks": [
      "data_lake",
      "data_science",
      "vector_database"
    ]
  },
  {
    "category": "DS",
    "filename": "ACF Plots",
    "sha": "4442450e22d00f2b99fbd6149d87e4f5c4e9dc60",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/ACF%20Plots.md",
    "text": "An [[Autocorrelation]] Function (ACF) plot shows how each observation in a time series is correlated with its past values at different lags ([[Forecasting using Lags]]). It helps identify **trends**, **seasonality**, and whether a series is **stationary**.\n\n### **Understanding ACF Behavior**\n\n* In a [[Stationary Time Series]], autocorrelations **decay quickly toward zero** as lag increases. This indicates past values have decreasing influence on future values.\n* If autocorrelations remain **high across multiple lags**, this suggests:\n  * **Trend:** values persistently related over time ([[Trends in Time Series]]).\n  * **Seasonality:** repeating peaks at specific lag intervals ([[Seasonality in Time Series]]).\n\n**Key takeaway:** Slow decay or repeated high correlations indicate a **non-stationary** series.\n### **How to Interpret an ACF Plot**\n\n1. **Decay pattern:**\n   * Rapid decay → stationary series.\n   * Slow decay → non-stationary series.\n2. **Significant peaks:** Regular spikes at certain lags indicate seasonality.\n3. **Correlation magnitude:** High correlations at large lags suggest trend or long-term dependencies.\n\n### **Detailed Interpretation Guide**\n\n* **Significant spikes** outside confidence intervals → meaningful correlation at that lag.\n* **Gradual decay** → indicates an **autoregressive (AR)** process.\n* **Sharp cutoff** after a few lags → suggests a **moving average (MA)** process.\n* **Alternating signs** → may indicate oscillatory behavior.\n* **Slow decay over many lags** → possible non-stationarity or trend.\n* **Seasonal pattern** → repeated peaks at multiples of seasonal lag (e.g., lag 12 for monthly data).\n\n### **Additional Notes**\n\n* ACF includes *indirect effects*: e.g., correlation at lag 3 may result from correlations at lags 1 and 2.\n* Apply ACF to **stationary data**; otherwise correlations may be misleading.\n* Use ACF to determine **$q$** in an MA($q$) model.\n\n[[PACF Plots]]\n[[Forecasting with Autoregressive (AR) Models]]\n\n\n![[Pasted image 20250909163931.png]]",
    "aliases": [
      "ACF"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "acf_plots",
    "outlinks": [
      "forecasting_using_lags",
      "pasted_image_20250909163931.png",
      "stationary_time_series",
      "trends_in_time_series",
      "seasonality_in_time_series",
      "forecasting_with_autoregressive_(ar)_models",
      "autocorrelation",
      "pacf_plots"
    ],
    "inlinks": [
      "arima",
      "autocorrelation",
      "decomposition_in_time_series",
      "differencing_in_time_series",
      "evaluating_time_series_forecasts",
      "forecasting_with_autoregressive_(ar)_models",
      "pacf_plots",
      "sarima",
      "stationary_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "ADF Test",
    "sha": "f61a6f678665cb4d220aaa9b5395b56a2322266b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/ADF%20Test.md",
    "text": "### **Augmented Dickey-Fuller (ADF) Test**\n\nThe **ADF test** is a statistical procedure used to determine whether a time series is **stationary** or **non-stationary**. It is an extension of the Dickey-Fuller test that allows testing for [[Stationary Time Series|stationarity]] in series with more complex autocorrelation structures by including multiple lags.\n\n### **Hypotheses**\n\n1. **Null hypothesis ($H_0$):** The series is non-stationary (has a unit root).\n2. **Alternative hypothesis ($H_A$):** The series is stationary (no unit root).\n\n### **Key Concepts**\n\n* **Unit root:** Indicates that ([[Time Series Shocks]]) shocks to the time series have a **permanent effect** and the series is non-stationary.\n* **Integration:**\n  * If [[Differencing in Time Series|differencing]] a non-stationary series once produces a stationary series, it is said to be **integrated of order 1** ($I(1)$).\n  * Some processes may require differencing multiple times (order $d>1$).\n* **Mean reversion ([[Mean reverting]]):** A stationary series tends to revert around a constant mean (can be sinusoidal).\n\n### **Notes**\n\n* The ADF test generalizes the Dickey-Fuller test by testing multiple lags simultaneously.\n* Its conclusion focuses on the **absence of a unit root**, indicating stationarity.\n* Useful reference: [Dickey-Fuller Test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test).",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "analysis",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "adf_test",
    "outlinks": [
      "mean_reverting",
      "stationary_time_series",
      "differencing_in_time_series",
      "time_series_shocks"
    ],
    "inlinks": [
      "arima",
      "differencing_in_time_series",
      "forecasting_with_autoregressive_(ar)_models",
      "stationary_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "AI",
    "sha": "2016f5187b5ef6db35260084fc81df4401df4636",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/AI.md",
    "text": "A field that encompasses [[Machine Learning]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "ai",
    "outlinks": [
      "machine_learning"
    ],
    "inlinks": [
      "machine_learning"
    ]
  },
  {
    "category": "DS",
    "filename": "ARIMA vs Random Forest in Time Series",
    "sha": "53c0054dd5a4298e3bfbaeded57ab64b6ea65670",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/ARIMA%20vs%20Random%20Forest%20in%20Time%20Series.md",
    "text": "The difference between [[ARIMA]] and [[Random Forest]] in [[Time Series]] forecasting comes down to model type, assumptions, and how they handle data:\n\nIn short: ARIMA is a traditional statistical model suited for linear, [[Stationary Time Series]], while Random Forest is a flexible machine learning model that can handle non-linear patterns and many predictors.\n\n| Aspect                | ARIMA / SARIMA                                                                            | Random Forest                                                                             |\n| --------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n| Model type        | Parametric, statistical time series model                                                 | Non-parametric, ensemble machine learning model                                           |\n| Assumptions       | Assumes linear relationships; requires stationary data (constant mean/variance over time) | Makes no linearity or stationarity assumptions; learns patterns from features             |\n| Input             | Primarily past values (lags) of the series                                                | Can use lagged values, rolling statistics, external features, or any predictor            |\n| Captures patterns | Linear trends and seasonality                                                             | Non-linear patterns, interactions between features                                        |\n| Interpretability  | Relatively interpretable (coefficients have meaning)                                      | Less interpretable; [[Feature Importance]] can be analyzed                                    |\n| Forecast horizon  | Good for short- to medium-term forecasts if assumptions hold                              | Can perform well for complex patterns and longer horizons if features are engineered well |\n| Use case          | Classic [[Time Series Forecasting]] where data is seasonal/trended                            | When time series is influenced by multiple variables or shows non-linear behavior         |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "time_series"
    ],
    "normalized_filename": "arima_vs_random_forest_in_time_series",
    "outlinks": [
      "arima",
      "time_series_forecasting",
      "stationary_time_series",
      "random_forest",
      "time_series",
      "feature_importance"
    ],
    "inlinks": [
      "arima",
      "random_forest_for_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "ARIMA",
    "sha": "7ef6cc7781b507e2f8e7d561507a62d6ac31f453",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/ARIMA.md",
    "text": "ARIMA (AutoRegressive Integrated Moving Average) is a widely used method for [[Time Series Forecasting]] that models the autocorrelations within the data. It is particularly effective for datasets with trends or patterns that are non-seasonal. It is more **stochastic**, based on past correlations rather than explicit decomposition.\n\nWhen to use:\n - When shocks, noise structure, or lagged dependencies drive the series more than trend/seasonality.\n - Strong [[Autocorrelation]]\n* When seasonality changes over time (SARIMA can capture evolving seasonal relationships better than Holt-Winters).\n\nRequirement: [[Stationary Time Series]]\n### ARIMA Explained\n\nARIMA consists of three components:\n\n* AutoRegressive (AR): Uses past values to predict the current value.\n* Integrated (I): Applies differencing to make the series stationary. [[Differencing in Time Series]]\n* Moving Average (MA): Uses past forecast errors to improve predictions.\n\nThe main ARIMA parameters are:\n#### $p$ – Autoregressive (AR) Order\n\n* Represents the number of lagged observations included in the model.\n* Purpose: AR terms capture how past values influence the current value.\n* Example: $p=2$ -> the model uses the previous 2 time points to predict the current one.\n* How to pick $p$: Look at the Partial Autocorrelation Function ([[PACF Plots]]) plot:\n  * The lag after which PACF cuts off (drops to near zero) suggests the `p` value.\n  * Example: PACF is significant at lags 1 and 2, then drops -> `p = 2`.\n#### $d$ – Differencing Order\n\n* Represents the number of times the series is differenced ([[Differencing in Time Series]]) to achieve stationarity (removing trends).\n* Example: $d=1$ -> model uses $y_t - y_{t-1}$.\n* How to pick $d$:\n  * Visual inspection: Check if the time series has a trend or changing mean.\n  * [[ADF Test]] (Augmented Dickey-Fuller): If p-value > 0.05 -> series is non-stationary -> increase $d$.\n  * Rule of thumb: Usually $d ≤ 2$. Most series become stationary after 1–2 differences.\n#### $q$ – Moving Average (MA) Order\n\n* Represents the number of lagged forecast errors included in the model.\n* Purpose: MA terms capture short-term shocks ([[Time Series Shocks]]) or noise.\n* Example: $q=1$ -> model uses the previous time step’s error to adjust the prediction.\n* How to pick $q$: Look at the [[Autocorrelation]] Function ([[ACF Plots|ACF]]) plot:\n  * The lag after which ACF cuts off suggests the `q` value.\n  * Example: ACF is significant at lag 1 -> `q = 1`.\n\n### What ARIMA Does\n\n1. Checks for stationarity and applies differencing ($d$ times) if needed.\n2. Models the relationship between current values and:\n   * Past values (AR component)\n   * Past forecast errors (MA component)\n3. Fits parameters by minimizing a loss function (typically log-likelihood).\n4. Forecasts future values using the learned structure.\n### Fine-Tuning and Iterative Refinement\n\n* Test several ARIMA models around initial `(p,d,q)` estimates.\n* Evaluate using [[AIC in Model Evaluation]] BIC, or [[Cross Validation]] (lower values indicate a better model).\n* Check [[Residuals Analysis]]: They should resemble white noise (uncorrelated and zero mean).\n### Why ARIMA Isn’t Enough for Seasonal Data\n\nARIMA does not model repeating patterns (e.g., quarterly or monthly seasonality). For such cases, [[SARIMA]] is used.\n### Related Resources\n\nIn [[ML_Tools]] see:\n* [ARIMA Jupyter Notebook](https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/ARIMA.ipynb)\n* [[Forecasting_AutoArima.py]]\n* [[pmdarima]]\n* [[ARIMA vs Random Forest in Time Series]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "time_series",
      "ML_Tools"
    ],
    "normalized_filename": "arima",
    "outlinks": [
      "aic_in_model_evaluation",
      "time_series_forecasting",
      "residuals_analysis",
      "arima_vs_random_forest_in_time_series",
      "forecasting_autoarima.py",
      "stationary_time_series",
      "differencing_in_time_series",
      "cross_validation",
      "sarima",
      "adf_test",
      "time_series_shocks",
      "pmdarima",
      "autocorrelation",
      "ml_tools",
      "acf_plots",
      "pacf_plots"
    ],
    "inlinks": [
      "aic_in_model_evaluation",
      "arima_vs_random_forest_in_time_series",
      "autocorrelation",
      "autoregression",
      "differencing_in_time_series",
      "evolving_seasonality",
      "forecasting_autoarima.py",
      "holt-winters_vs_arima",
      "pmdarima",
      "random_forest_for_time_series",
      "sarima",
      "stationary_time_series",
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Additive vs Multiplicative Models Time Series",
    "sha": "1f350741888c8dbc1124c6b2ac4e82cf8b2a4248",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Additive%20vs%20Multiplicative%20Models%20Time%20Series.md",
    "text": "### Additive Model\n\nIn an **additive model**, the observed time series is expressed as the **sum of its components**:\n\n$Y_t = T_t + S_t + R_t$\n\nWhere:\n\n* $Y_t$: observed value at time $t$\n* $T_t$: trend component\n* $S_t$: seasonal component\n* $R_t$: residual (random noise)\n\nThis assumes each component contributes **independently and linearly** to the series.\n\nWe typically use an **additive model** when:\n\n* The **seasonal effect is stable over time** (e.g., summer–winter differences remain consistent).\n* The **trend does not distort or amplify seasonality**.\n* Seasonal fluctuations stay within a fixed range, even if minor noise exists.\n\nFor example, in daily minimum temperature data, the **seasonal swings remain steady** year after year. This stability indicates that the **additive model is appropriate**, since seasonality does not depend on the trend.\n### Multiplicative Model\n\nIn a **multiplicative model**, the components interact by scaling:\n\n$Y_t = T_t \\times S_t \\times R_t$\n\nWhere:\n\n* $Y_t$: observed value\n* $T_t$: trend\n* $S_t$: seasonal effect\n* $R_t$: residual\n\nThis model is suited for series where the **magnitude of seasonality changes with the trend**. For example, when seasonal peaks and troughs **grow larger as the trend increases**.\n\n### Choosing Between Models\n\n* **Additive model** ->stable seasonal effect, constant magnitude, independent of trend.\n* **Multiplicative model** -> seasonal effect scales with trend.\n\nThe choice can often be made by visually inspecting the decomposition plot. See [[Decomposition in Time Series]].\n\nExample:\n\n- **Additive**: seasonal effect is constant in magnitude (e.g., demand increases by +100 GWh every winter).\n- **Multiplicative**: seasonal effect scales with the level (e.g., winter demand is 20% higher than the baseline).  \n### Related\n\nSee [[Baseline Forecast]] for once the models type is selected.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "additive_vs_multiplicative_models_time_series",
    "outlinks": [
      "baseline_forecast",
      "decomposition_in_time_series"
    ],
    "inlinks": [
      "holt-winters_(exponential_smoothing)",
      "seasonality_in_time_series",
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Agent Exploration",
    "sha": "4015c66cfab7b15d4e374585b499a9c83be142f4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Agent%20Exploration.md",
    "text": "How an agent navigates its world determined by its policy",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "drafting"
    ],
    "normalized_filename": "agent_exploration",
    "outlinks": [],
    "inlinks": [
      "policy"
    ]
  },
  {
    "category": "DS",
    "filename": "Agentic Solutions",
    "sha": "2e6cb34bf0894d9f5579bc57ed866802aef626c2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Agentic%20Solutions.md",
    "text": "Agentic solutions leverage multiple autonomous agents (usually [[Small Language Models|SLM]]) to achieve goals collaboratively. These systems distribute tasks across agents that operate individually or collectively to solve complex problems.\n\nAgents can model specific business functions. Role clarity enhances the effectiveness of these systems.\n\nRelated terms:\n- [[GraphRAG]]\n- [[Agent-Based Modelling]]\n- [[Generative Agents - Interactive Simulacra of Human Behavior]]\n## Types of Agentic Solutions\n\nReactive Solutions (Ask Approach):  \n    Systems like chatbots and retrieval-augmented generation (RAG) tools respond to user queries.\n    \nAutonomous Solutions (Do Approach):  \n    Agents perform tasks proactively, e.g., drafting documents or scheduling meetings.\n## Business Process Integration\n\n### Workflow:\n1. Identify a business problem.\n2. Define personas (agents) required.\n3. Develop an agentic workflow.\n\n### Agentic Architectures:\n1. Vertical: Hierarchical structures for task delegation.\n2. Horizontal: Collaborative structures with high feedback loops.\n3. Mixed: Combines vertical delegation with horizontal collaboration.\n\nVertical Example: A primary agent delegates tasks to lower-level agents for execution.\n\n## The Orleans Framework\n\nA framework for building distributed applications in .NET.\n- Grains: Individual agents performing specific tasks.\n- Silos: Distributed nodes managing grains.\n- Clusters: Collections of silos for scalability.\n\n## Benefits of Using Agents\n\n1. Performance Gains: Task parallelization enhances throughput.\n2. Developer Abstraction: Modular design simplifies system understanding and debugging.\n3. Workflow Integration: Aligns AI agents with organizational processes.\n\n## Example Use Cases\n\n1. IT Helpdesk Agent: Automates troubleshooting and network access requests.\n2. Device Refresh Agent: Manages hardware upgrades and approvals.\n3. Lead Generation Agent: Identifies and researches potential leads.\n4. Budget Management Agent: Reviews financial data and aids in planning.\n5. Customer Support Agent: Triage support issues for faster resolution.\n6. Project Tracker Agent: Tracks project milestones and budget compliance.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "drafting"
    ],
    "normalized_filename": "agentic_solutions",
    "outlinks": [
      "small_language_models",
      "graphrag",
      "agent-based_modelling",
      "generative_agents_-_interactive_simulacra_of_human_behavior"
    ],
    "inlinks": [
      "agent-based_modelling",
      "langchain",
      "scaling_agentic_systems"
    ]
  },
  {
    "category": "DS",
    "filename": "Autocorrelation vs Autoregression",
    "sha": "374bb3602224456831d4f18b7830fea0dcbeb613",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Autocorrelation%20vs%20Autoregression.md",
    "text": "## Key Difference\n\n* **[[Autocorrelation]]**: A *diagnostic/statistical property* of the series.\n* **[[Autoregression|Autoregression]]**: A *model* that uses those properties for prediction.",
    "aliases": null,
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "time_series"
    ],
    "normalized_filename": "autocorrelation_vs_autoregression",
    "outlinks": [
      "autocorrelation",
      "autoregression"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Autocorrelation",
    "sha": "7c2816ee942505c00e707de243fc9754637c0baa",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Autocorrelation.md",
    "text": "**Autocorrelation** (also called serial [[Correlation]]) measures how a time series is related to a lagged version of itself. Helps identify repeating patterns, seasonality, or persistence in data.\n\nFormally, for a time series ${y_t}$, the autocorrelation at lag $k$ is:\n\n$$\n\\rho_k = \\frac{\\text{Cov}(y_t, y_{t-k})}{\\sqrt{\\text{Var}(y_t)\\,\\text{Var}(y_{t-k})}}\n$$\n\nIt takes values between $-1$ and $1$:\n\n* $\\rho_k > 0$: Positive correlation -> if $y_t$ is above its mean, $y_{t-k}$ tends to be above its mean too.\n* $\\rho_k < 0$: Negative correlation -> if $y_t$ is above its mean, $y_{t-k}$ tends to be below its mean.\n* $\\rho_k \\approx 0$: No linear relationship between $y_t$ and its past at lag $k$.\n\n### Intuition\n\nAutocorrelation tells you ==how predictable a series is from its past==:\n* Stock returns: low/no autocorrelation (mostly random).\n* Daily temperatures: high autocorrelation at lag 1 (yesterday’s temperature is a good predictor of today’s).\n* Strong seasonal effects: autocorrelation spikes at seasonal lags (e.g., 7 days for weekly seasonality).\n\n### Why it matters\n* Used to **detect [[Decomposition in Time Series]]** & [[Stationary Time Series]].\n* Helps decide AR and MA terms in [[ARIMA]].\n* Checked via **Autocorrelation Function (ACF)** and **Partial Autocorrelation Function (PACF)** plots ([[ACF Plots|ACF]], [[PACF Plots]]) to select lagged values.\n\n### Hurst exponents\n- The Hurst exponent ($H$) is a measure used to characterize the long-term memory of time series. \n- It helps to determine the presence of autocorrelation or persistence in the data. \n- The goal of the Hurst exponent is to provide us with a scalar value that will help us to identify whether a series is \n    - random walking,\n    - trending,\n    - [[Mean reverting]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "time_series"
    ],
    "normalized_filename": "autocorrelation",
    "outlinks": [
      "arima",
      "correlation",
      "decomposition_in_time_series",
      "stationary_time_series",
      "mean_reverting",
      "acf_plots",
      "pacf_plots"
    ],
    "inlinks": [
      "acf_plots",
      "arima",
      "autocorrelation_vs_autoregression",
      "feature_engineering_for_time_series",
      "holt-winters_vs_arima",
      "varmax"
    ]
  },
  {
    "category": "DS",
    "filename": "Autoregression",
    "sha": "4cb5c5ea8934e71e946169b0f58da905b16f761f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Autoregression.md",
    "text": "**Definition**: A time series model that predicts the current value as a linear combination of its past values.\n\n**Mathematical form (AR(p))**:\n\n  $$\n  X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\epsilon_t\n  $$\n\n  where $\\epsilon_t$ is white noise.\n* **Purpose**: Forecasting future values from historical dependencies.\n* **Use cases**:\n  * [[ARIMA]] modeling.\n  * Capturing short-term dependencies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "forecasting"
    ],
    "normalized_filename": "autoregression",
    "outlinks": [
      "arima"
    ],
    "inlinks": [
      "autocorrelation_vs_autoregression",
      "forecasting_with_autoregressive_(ar)_models"
    ]
  },
  {
    "category": "DS",
    "filename": "Baseline Forecast",
    "sha": "8070f7895c8db6e65d5ed8d1637b68b266124741",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Baseline%20Forecast.md",
    "text": "When working with [[Time Series]], it is useful to begin with **simple baseline models**. These provide a **benchmark** to compare more complex methods against.\n\nCommon baseline approaches include:\n\n* [[Naive Forecast]]\n* [[Seasonal Naive Forecast]]\n* [[Moving Average Forecast]]\n\nBaseline models are often **sufficient for many practical tasks**, and in some cases, they perform nearly as well as advanced models. At minimum, they set a **performance floor** that any sophisticated model should exceed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "baseline_forecast",
    "outlinks": [
      "naive_forecast",
      "time_series",
      "moving_average_forecast",
      "seasonal_naive_forecast"
    ],
    "inlinks": [
      "additive_vs_multiplicative_models_time_series",
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Basics of Time Series",
    "sha": "f981247d0e538636758fbecbdd01cbd833219919",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Basics%20of%20Time%20Series.md",
    "text": "Basics of [[Time Series]]\n Key Characteristics\n* Each record consists of timestamps with measurements.\n* Different time series can have different frequencies (e.g., hourly, daily).\n* Always check the unit of measurement for each field (e.g., cm, \\$, etc.).\n\nMetric types:\n  * Counter: Monotonically increases.\n  * Gauge: Can go up or down.\n  * Summary: Aggregate calculation over a period.\n\n Data Structure Considerations\n* Time series data is often stored in a long column format.\n* Partition datasets by period (e.g., monthly) for:\n\t- Easier period-based querying.\n\t- Computing aggregates.\n\t- Focusing on recent data efficiently.\n\nCommon Applications\n* Finance: Stock prices, interest rates, economic indicators.\n* Weather Forecasting: Temperature, precipitation, wind speed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "basics_of_time_series",
    "outlinks": [
      "time_series"
    ],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Batch gradient descent",
    "sha": "2e0afec6815bb01e2c97cbcf3e63b203ca78960c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Batch%20gradient%20descent.md",
    "text": "Batch Gradient Descent computes the gradient of the [[Cost Function]] ==using the entire training dataset== at each iteration before updating the [[Model Parameters]].\n\n#### Key Characteristics\n* Update Rule:\n  Parameters are updated once per [[Epoch]], after processing the entire dataset.\n* Objective:\n  Achieve accurate and stable updates, as the gradient is computed over all training examples.\n#### Pros:\n* Produces a stable convergence path.\n* Provides an accurate estimate of the gradient.\n\n#### Cons:\n* Computationally expensive for large datasets.\n* Requires the entire dataset to fit in memory, which may not be feasible for big data.\n* Slower to start learning compared to [[Stochastic Gradient Descent|SGD]] or Mini-Batch methods.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "batch_gradient_descent",
    "outlinks": [
      "cost_function",
      "epoch",
      "stochastic_gradient_descent",
      "model_parameters"
    ],
    "inlinks": [
      "gradient_descent"
    ]
  },
  {
    "category": "DS",
    "filename": "Bellman Equations",
    "sha": "3b9220a2b204d1e49774e535ae5574b2923b7c06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Bellman%20Equations.md",
    "text": "[[What are the Bellman equations that are used in RL?]]\n\nEquations here may not be accurate.\n\nIn reinforcement learning, Bellman's equations are fundamental to understanding how agents make decisions to maximize rewards over time. They are used to describe the relationship between the value of a state and the values of its successor states. There are two main types of Bellman's equations:\n\n1. Bellman Equation for State Value Function (V):\n   - This equation expresses the value of a state as the expected return starting from that state and following a particular policy. It is defined as:\n     $$\n     V(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s, a) [R(s, a, s') + \\gamma V(s')]\n     $$\n   - Here, \\(V(s)\\) is the value of state \\(s\\), \\(\\pi(a|s)\\) is the policy (probability of taking action \\(a\\) in state \\(s\\)), \\(P(s'|s, a)\\) is the transition probability to state \\(s'\\) from state \\(s\\) taking action \\(a\\), \\(R(s, a, s')\\) is the reward received, and \\(\\gamma\\) is the discount factor.\n\n2. Bellman Equation for Action Value Function (Q):\n   - This equation expresses the value of taking an action in a given state under a particular policy. It is defined as:\n     $$\n     Q(s, a) = \\sum_{s'} P(s'|s, a) [R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a')]\n     $$\n   - Here, \\(Q(s, a)\\) is the value of taking action \\(a\\) in state \\(s\\), and the other terms are similar to those in the state value function.\n\nBellman's equations are used in dynamic programming methods like Value Iteration and Policy Iteration to find optimal policies and value functions. They provide a recursive decomposition of the value functions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "modeling"
    ],
    "normalized_filename": "bellman_equations",
    "outlinks": [
      "what_are_the_bellman_equations_that_are_used_in_rl?"
    ],
    "inlinks": [
      "q-learning",
      "reinforcement_learning"
    ]
  },
  {
    "category": "DS",
    "filename": "Bias-Variance Trade Off",
    "sha": "1534a761b3b204067f5312b18382c240be605f8c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Bias-Variance%20Trade%20Off.md",
    "text": "The bias-variance trade-off describes the relationship between model complexity and performance. \n- High bias (underfitting) occurs when a model is too simple, leading to poor performance on both training and test data. \n- High variance (overfitting) happens when a model is overly complex, performing well on training data but poorly on unseen data.\n\nWays to Reduce Bias and Variance:\n- [[Regularisation]]\n- [[Boosting]]\n- [[Bagging]]\n\nRelated to [[Overfitting]]\n### [[Regularisation]]\n\nKey Takeaway:\n- Regularization trades variance for bias. The goal is to find the sweet spot that minimizes overall prediction error on unseen data.\n\nRegularization directly affects the bias–variance trade-off in machine learning by controlling model complexity.\n### Impact on Bias and Variance\n\n| Regularization                                              | Model Complexity      | Variance           | Bias           | Typical Effect                                                        |\n| ----------------------------------------------------------- | --------------------- | ------------------ | -------------- | --------------------------------------------------------------------- |\n| Increase regularization (e.g., higher $\\lambda$ in L1/L2) | Reduces flexibility   | Decreases variance | Increases bias | Model becomes simpler → less sensitive to training data, may underfit |\n| Decrease regularization (smaller $\\lambda$)               | Increases flexibility | Increases variance | Decreases bias | Model can fit training data better → may overfit                      |\n\n### Intuition\n\n* Variance: Measures sensitivity to small changes in the training data. High variance -> overfitting.\n* Bias: Measures error from oversimplifying the model. High bias ->underfitting.\n* Regularization: Adds a penalty for large weights-> discourages complex models -> reduces variance but increases bias.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "bias-variance_trade_off",
    "outlinks": [
      "regularisation",
      "bagging",
      "boosting",
      "overfitting"
    ],
    "inlinks": [
      "bias_in_ml",
      "variance_in_ml"
    ]
  },
  {
    "category": "DS",
    "filename": "Capability",
    "sha": "891898eb29645eac32bbab13a733fb1427a0d9fb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Capability.md",
    "text": "",
    "aliases": [
      "capable"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "term"
    ],
    "normalized_filename": "capability",
    "outlinks": [],
    "inlinks": [
      "batch_vs_powershell_scripts",
      "performance_dimensions"
    ]
  },
  {
    "category": "DS",
    "filename": "Choosing a Threshold",
    "sha": "a3fceacfe1b4650db72bea3c90e0d9cfafee02e7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Choosing%20a%20Threshold.md",
    "text": "The optimal threshold depends on the specific problem and the desired trade-off between different types of errors:\n\n1. Manual Selection: Based on domain expertise or prior knowledge, choose a threshold that seems reasonable.\n2. Receiver Operating Characteristic ([[ROC (Receiver Operating Characteristic)]]) Curve Analysis: Plot the true positive rate (TPR) against the false positive rate (FPR) for different threshold values. The optimal threshold often lies near the \"elbow\" of the ROC curve, where a small increase in FPR results in a significant increase in TPR.\n3. [[Precision-Recall Curve]] Analysis: Plot the precision against the recall for different threshold values. The optimal threshold often lies near the \"elbow\" of the precision-recall curve, where a small decrease in precision results in a significant increase in recall.\n4. [[Cost-Sensitive Analysis]]: Assign different costs to different types of errors (e.g., false positives vs. false negatives) and choose the threshold that minimizes the total cost.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "ml"
    ],
    "normalized_filename": "choosing_a_threshold",
    "outlinks": [
      "cost-sensitive_analysis",
      "roc_(receiver_operating_characteristic)",
      "precision-recall_curve"
    ],
    "inlinks": [
      "neural_network_classification"
    ]
  },
  {
    "category": "DS",
    "filename": "Choosing the Number of Clusters",
    "sha": "037c982d00eea9da6cc0da1ca258497687151179",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Choosing%20the%20Number%20of%20Clusters.md",
    "text": "The optimal number of clusters ([[Clustering]]) depends on the data and the desired level of [[granularity]]. Here are some common approaches:\n\n1. Elbow Method: [[WCSS and elbow method]]: Plot the within-cluster sum of squares (WCSS) as a function of the number of clusters. The optimal number of clusters is often the point where the WCSS starts to decrease slowly.\n   \n2. [[Silhouette Analysis]]: Calculate the silhouette coefficient for each data point, which measures how similar a data point is to its own cluster compared to other clusters. The optimal number of clusters 1 is often the one that maximizes the average silhouette coefficient.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "preprocessing"
    ],
    "normalized_filename": "choosing_the_number_of_clusters",
    "outlinks": [
      "granularity",
      "wcss_and_elbow_method",
      "silhouette_analysis",
      "clustering"
    ],
    "inlinks": [
      "neural_network_classification"
    ]
  },
  {
    "category": "DS",
    "filename": "Clustermap",
    "sha": "642f2fa60fb15afac22db8b640142595ac529275",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Clustermap.md",
    "text": "Related to:\n- [[Preprocessing|Preprocess]]\n- Purpose: Identify which features are most similar using [[Dendrograms]].\n- Visualization: Regions of color show clustering, similar to a heatmap.\n- Functionality: Performs clustering on both rows and columns.\n\nRequirements: Input should be numerical; data needs to be scaled.\n  ```python\n  import seaborn as sns\n  sns.clustermap(x_scaled, cmap='mako', standard_scale=0)  # 0 for rows, 1 for columns\n  ```\n## Resources\n- [Video Explanation](https://youtu.be/crQkHHhY7aY?t=149)\n- [Seaborn Clustermap Documentation](https://seaborn.pydata.org/generated/seaborn.clustermap.html)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "preprocessing"
    ],
    "normalized_filename": "clustermap",
    "outlinks": [
      "preprocessing",
      "dendrograms"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Correlated Time Series",
    "sha": "877ed7d268af34ec9877f6d23fe049f393f5c8fc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Correlated%20Time%20Series.md",
    "text": "To study the correlation among a ==group== of univariate time series, the goal is to understand how they co-move over time — whether their patterns are synchronous, lagged, or independent.\n\nCorrelation captures linear co-movement on a common timeline. For misaligned, nonlinear, or lagged dependencies, use DTW or cross-correlation instead.\n\nSteps:\n\n1. Align Time Frames – Resample or interpolate to ensure all series share a common time index.\n   \n2. [[Z-Normalisation]] or [[Standardisation]] – Convert each series to zero mean and unit variance to remove scale effects:\n   \n3. Optionally Detrend or Difference – Remove long-term trends or seasonality to focus on short-term co-movement.\n   \n4. Compute Correlations\n   * Use `df.corr()` for standard Pearson correlations.\n   * Compute rolling correlations to study how relationships change over time.\n     \n1. Visualize\n   * [[Heatmap]] for pairwise correlation matrix.\n   * Network graph or dendrogram for clustering similar series.",
    "aliases": [],
    "date modified": "5-10-2025",
    "tags": [
      "analysis",
      "ml_process",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "correlated_time_series",
    "outlinks": [
      "z-normalisation",
      "heatmap",
      "standardisation"
    ],
    "inlinks": [
      "dynamic_time_warping"
    ]
  },
  {
    "category": "DS",
    "filename": "Covariance Structures",
    "sha": "38ae7cd6cbe40171a9360a6e8d5cfd394cf2994a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Covariance%20Structures.md",
    "text": "A covariance structure in general refers to the way variability and relationships between variables (or dimensions) are modeled and described in a dataset. It specifies how the data points are distributed in space, particularly focusing on the relationships between variables and their individual variances.\n\n### Key Components of Covariance Structure\n\nCovariance Matrix:\n- A mathematical representation of the [[Covariance]] structure for multiple variables. It shows the [[Variance]] of each variable along the diagonal and the covariances between variables off the diagonal.\n- For a dataset with $p$ variables: \n$$\n\\Sigma = \\begin{bmatrix} \n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\dots & \\text{Cov}(X_1, X_p) \\\\ \n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\dots & \\text{Cov}(X_2, X_p) \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\dots & \\text{Var}(X_p) \n\\end{bmatrix}\n$$\n\n### What Does Covariance Structure Describe?\n\nThe covariance structure describes:\n\n1. Shape of Data [[Distributions|Distribution]]\n    - The spread and orientation of data in multi-dimensional space.\n    - Example: Circular, elliptical, or elongated distributions.\n\n2. Relationships Between Variables:\n    - Whether variables are positively, negatively, or not correlated.\n\n3. Dimensional Dependencies:\n    - If some variables are strongly related, the structure will capture these dependencies.\n\n### Why Covariance Structure Is Important\n\n1. In [[Statistics]]:\n    - It is crucial for multivariate statistical methods like principal component analysis ([[Principal Component Analysis|PCA]]), [[Factor Analysis]], and regression.\n    - Helps in understanding how features interact and in reducing dimensionality.\n\n2. In Machine Learning:\n    - [[Clustering]] algorithms like [[Gaussian Mixture Models]] (GMMs) rely on covariance structure to fit the data.\n    - Determines the flexibility of models in adapting to real-world data distributions.\n\n3. In [[Data Analysis]]:\n    - Covariance structure reveals patterns and dependencies in the data that might not be apparent from simple univariate analyses.\n\n### Real-Life Example of Covariance Structure\n\nImagine a dataset of height ($X$) and weight ($Y$) for a group of individuals:\n\n- Variance in height shows how spread out people's heights are.\n- Variance in weight shows how spread out weights are.\n- Covariance between height and weight shows whether taller people tend to weigh more (positive covariance).\n\nIf plotted, the covariance structure would determine whether the data points form:\n- A circular cluster (if height and weight are unrelated).\n- An elongated cluster (if taller people tend to weigh more).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "covariance_structures",
    "outlinks": [
      "clustering",
      "data_analysis",
      "statistics",
      "principal_component_analysis",
      "distributions",
      "variance",
      "covariance",
      "gaussian_mixture_models",
      "factor_analysis"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "kmeans_vs_gmm"
    ]
  },
  {
    "category": "DS",
    "filename": "Cross Validation",
    "sha": "77e2dcb00ae633a7c1e93548645fa0d8c74d41e2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Cross%20Validation.md",
    "text": "Cross-validation is a statistical technique used in machine learning to ==assess how well a model will generalize== to an independent dataset. It is a crucial step in the model-building process because it helps ensure that the model is not [[Overfitting]] or underfitting the training data.\n\n- Cross-validation is a technique used in machine learning and statistics to evaluate the performance ([[Model Optimisation]]) of a predictive model.\n- It provides a robust evaluation by splitting the training data into smaller chunks and training the model multiple times.\n- K-Fold Cross-Validation: Involves dividing the dataset into (k) equal-sized subsets (called \"folds\") and using each fold as a validation set once, while the remaining (k-1) folds are used for training.\n- The model's performance is averaged across all (k) folds to provide a more robust estimate of its generalization performance.\n### Common Variations\n\n- K-Fold Cross-Validation: The most common method, where the data is split into (k) folds and the model is trained (k) times, each time using a different fold as the validation set.\n- Stratified K-Fold: Ensures each fold has a similar proportion of class labels, important for imbalanced datasets.\n- Repeated K-Fold: Repeats the process multiple times with different random splits for more robust results.\n- Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest serve as the training set.\n\n### How Cross-Validation Fits into Building a Machine Learning Model\n\n1. [[Model Evaluation]]: Used to evaluate the performance of different models or algorithms to choose the best one.\n2. [[Hyperparameter]] Tuning: Provides a reliable performance metric for each set of hyperparameters.\n3. [[Model Validation]]: Ensures consistent performance across different subsets of data.\n4. [[Bias in ML]] tradeoff: Helps in understanding the tradeoff between bias and variance, guiding the choice of model complexity.\n\nAdvantages:\n- Reduced Bias: Offers a more reliable performance estimate compared to using a single validation set.\n- Efficient Data Use: All data is used for both training and validation.\n- Prevents Overfitting: By evaluating on multiple folds, it can detect if the model is overfitting to the training data.\n### Choosing (k)\n\n- Common values: 5 or 10\n- Higher (k) leads to more accurate estimates but increases computation time.\n- Consider dataset size and complexity when choosing (k).\n\n### Cross-Validation Strategy in [[Time Series]]\n\nAll notebooks use cross-validation based on `TimeSeriesSplit` to ensure proper evaluation of performance with no [[Data Leakage]]. This method ensures that training and test data are split while maintaining the chronological order of the data.\n\n### Related\n\n[[Learning Curve]]\n\n[[Cross Validation]]\nBoth (sklearn)[`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) and [`RepeatedStratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html) can be very effective when used on classification problems with a severe [[Imbalanced Datasets]]. They both _stratify_ the sampling by the class label; that is, they split the dataset in such a way that preserves approximately the same class distribution (i.e., the same percentage of samples of each class) in each subset/fold as in the original dataset. However, a single run of `StratifiedKFold` might result in a noisy estimate of the model's performance, as different splits of the data might result in very different results. That is where `RepeatedStratifiedKFold` comes into play.\n\n`RepeatedStratifiedKFold` allows improving the estimated performance of a machine learning model, by simply repeating the [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) procedure multiple times (according to the `n_repeats` value), and reporting the _mean_ result across all folds from all runs. This _mean_ result is expected to be a more accurate estimate of the model's performance",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#evaluation"
    ],
    "type": null,
    "normalized_filename": "cross_validation",
    "outlinks": [
      "hyperparameter",
      "model_optimisation",
      "cross_validation",
      "model_validation",
      "time_series",
      "bias_in_ml",
      "learning_curve",
      "model_evaluation",
      "data_leakage",
      "overfitting",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "arima",
      "cross_validation",
      "decision_tree",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "hyperparameter_tuning",
      "model_evaluation",
      "model_optimisation",
      "model_random_states",
      "model_selection",
      "overfitting",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "resampling",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "DS",
    "filename": "DS & ML Portal",
    "sha": "0b780d80e784b122718a5a675100e5b8e79cb8c8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/DS%20&%20ML%20Portal.md",
    "text": "### Machine Learning Fundamentals\n\n- [[ML_Tools]]\n- [[Supervised Learning]]\n- [[Unsupervised Learning]]\n- [[Reinforcement learning]]\n- [[Deep Learning]]\n\n### Model Training and Optimisation\n\n- [[Learning Rate]]\n- [[Overfitting]]\n- [[Regularisation]]\n- [[Hyperparameter]]\n- [[Hyperparameter Tuning]]\n- [[Model Optimisation]]\n- [[Model Selection]]\n- [[vanishing and exploding gradients problem]]\n\n### Feature Engineering and Data Handling\n\n- [[Feature Selection]]\n- [[Feature Engineering]]\n- [[Imbalanced Datasets]]\n- [[uncategorised/Outliers]]\n- [[Anomaly Detection]]\n- [[Multicollinearity]]\n- [[Dimensionality Reduction]]\n- [[Clustering]]\n### Machine Learning Models\n\nClassification Models\n\n- [[Classification]]\n- [[Binary Classification]]\n- [[Support Vector Machines]]\n- [[Decision Tree]]\n- [[Random Forest]]\n- [[K-nearest neighbours]]\n- [[Logistic Regression]]\n\nRegression Models\n\n- [[Regression]]\n- [[Linear Regression]]\n\nBoosting and Optimisation\n\n- [[Gradient Descent]]\n- [[Gradient Boosting]]\n- [[XGBoost]]\n\n### Deep Learning and Neural Networks\n \n- [[BERT]]\n- [[LSTM]]\n- [[Recurrent Neural Networks]]\n- [[Transformer]]\n- [[Attention mechanism]]\n- [[Neural network]]\n\n### Model Evaluation and Metrics\n\n- [[Cost Function]]\n- [[Loss function]]\n- [[Cross Entropy]]\n- [[Evaluation Metrics]]\n- [[Model Evaluation]]\n- [[Accuracy]]\n- [[Precision]]\n- [[Recall]]\n\n### Algorithms and Frameworks\n\n- [[Machine Learning Algorithms]]\n- [[Optimisation techniques]]\n- [[Optimisation function]]\n- [[Model Ensemble]]\n- [[Batch Processing]]\n- [[Apache Spark]]\n- [[Scikit-Learn]]\n\n### Statistical and Data Analysis Concepts\n\n- [[Distributions]]\n- [[Statistics]]\n- [[Correlation]]\n- [[Data Analysis]]\n- [[Data Quality]]\n- [[Principal Component Analysis]]\n\n### Misc\n\n- [[Interpretability]]\n- [[RAG]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "ds_&_ml_portal",
    "outlinks": [
      "lstm",
      "regularisation",
      "linear_regression",
      "model_optimisation",
      "optimisation_techniques",
      "recurrent_neural_networks",
      "deep_learning",
      "hyperparameter_tuning",
      "uncategorised/outliers",
      "dimensionality_reduction",
      "xgboost",
      "apache_spark",
      "principal_component_analysis",
      "scikit-learn",
      "hyperparameter",
      "optimisation_function",
      "binary_classification",
      "clustering",
      "gradient_boosting",
      "loss_function",
      "statistics",
      "anomaly_detection",
      "multicollinearity",
      "imbalanced_datasets",
      "feature_selection",
      "overfitting",
      "support_vector_machines",
      "k-nearest_neighbours",
      "precision",
      "classification",
      "batch_processing",
      "gradient_descent",
      "interpretability",
      "rag",
      "regression",
      "correlation",
      "attention_mechanism",
      "logistic_regression",
      "recall",
      "machine_learning_algorithms",
      "decision_tree",
      "cost_function",
      "data_quality",
      "neural_network",
      "cross_entropy",
      "model_ensemble",
      "bert",
      "supervised_learning",
      "reinforcement_learning",
      "accuracy",
      "model_selection",
      "transformer",
      "unsupervised_learning",
      "data_analysis",
      "feature_engineering",
      "random_forest",
      "distributions",
      "vanishing_and_exploding_gradients_problem",
      "model_evaluation",
      "learning_rate",
      "ml_tools",
      "evaluation_metrics"
    ],
    "inlinks": [
      "machine_learning_operations",
      "orthogonalization"
    ]
  },
  {
    "category": "DS",
    "filename": "Data Assessment",
    "sha": "24d1135b07257f14a286965e93310fa35fd0b7e7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Assessment.md",
    "text": "Purpose\n\n* Evaluate the suitability of data for predictive modeling.\n* Plan for [[Data Preparation]] rather than communicate results externally.\n\nKey Characteristics\n\n* Internal Focus: Analyses are intended for internal understanding and improvement of the data.\n* Data Visualization: Graphics are used to explore limitations and dynamics, not for presenting findings externally.\n\nReference\n\n* Process outlined in [[Data Mining - CRISP]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "process"
    ],
    "normalized_filename": "data_assessment",
    "outlinks": [
      "data_mining_-_crisp",
      "data_preparation"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Data Collection",
    "sha": "8b0c1a94fa5b232ba8106f5b511f1c0077ec56ba",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Collection.md",
    "text": "Determine the [[Data Quality]] and quantity of data required and get it from [[Data Sources]].\n\nTypes of data:\n- [[Imbalanced Datasets]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "process"
    ],
    "normalized_filename": "data_collection",
    "outlinks": [
      "imbalanced_datasets",
      "data_quality",
      "data_sources"
    ],
    "inlinks": [
      "business_observability",
      "business_understanding",
      "data_analyst",
      "data_hierarchy_of_needs",
      "digital_transformation",
      "preprocessing"
    ]
  },
  {
    "category": "DS",
    "filename": "Data Mining - CRISP",
    "sha": "bc9f423116a16c9c694ba5dad1c0ac102577967e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Mining%20-%20CRISP.md",
    "text": "CRISP-DM stands for Cross-Industry Standard Process for [[Data Mining]], a widely adopted framework that provides a structured approach to planning, organizing, and conducting data mining projects. CRISP-DM provides a flexible, iterative process that allows for refinement at any stage, making it adaptable to various industries and types of data mining projects.\n\n### Process\n\n[[Data Understanding]]\n   \n[[Data Understanding]]\n   \n[[Data Preparation]]\n\n[[Data Modeling]]\n   \n[[Data Evaluation]]\n   \n[[Data Deployment]]\n\n\n```mermaid\ngraph TD\n    A[Business Understanding] --> B[Data Understanding]\n    B --> C[Data Preparation]\n    C --> D[Modeling]\n    D --> E[Evaluation]\n    E --> F[Deployment]\n    F --> A\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "process"
    ],
    "normalized_filename": "data_mining_-_crisp",
    "outlinks": [
      "data_understanding",
      "data_preparation",
      "data_deployment",
      "data_modeling",
      "data_evaluation",
      "data_mining"
    ],
    "inlinks": [
      "data_assessment",
      "data_mining",
      "data_understanding",
      "eda"
    ]
  },
  {
    "category": "DS",
    "filename": "Data Preparation",
    "sha": "d15ac372f637045c0294c94872cda11e2aeefbfc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Preparation.md",
    "text": "Purpose\n\n* Transform raw data into a clean, integrated, and analysable form suitable for modeling and deployment.\n* Ensure that the dataset is free from missing or irrelevant values, aligned with business objectives, and structured for the unit of analysis (e.g., per person, per transaction).\n* Bridge the gap between [[Data Understanding]] and modeling.\n\nEnd Goals\n\n* No unaddressed missing data.\n* All fields useful, relevant, and reliable.\n* Dataset in the correct form for deployment.\n* Comprehensive representation of the business problem, not limited to chosen KPIs or incentives.\n\nKey Considerations\n\n* Unit of analysis: For transactional data, aggregate variables so one row represents the entity being analyzed.\n* Stakeholder input: Collaborate with SMEs and business teams to ensure alignment with problem objectives.\n* Metrics & incentives: Verify that chosen features address business problems, not only organizational KPIs.\n\nCore Tasks\n\n1. Data Integration: Combine data from multiple sources, addressing missingness and inconsistencies.\n2. Data Selection: Decide which variables and records to include or exclude.\n3. Data Quality Verification: Reassess missing data, duplicates, and anomalies.\n4. Feature Engineering: Construct new variables and transformations from existing data.\n5. Feature Selection: Identify the most relevant features for modeling.\n\nProcess Characteristics\n\n* Iterative: Data understanding and data preparation overlap, feeding back into each other.\n* Checklist-driven: Preparation should follow a plan built during [[Data Understanding]].\n\nFormal Documentation\n\n* Documentation is essential for reproducibility, progress tracking, and deployment readiness.\n* Reports generated in this phase form part of the CRISP-DM cycle and provide long-term value.\n\nReports to Produce\n\n* Initial Data Collection Report: Data sources, acquisition methods, extraction details, problems encountered.\n* Data Description Report: Field definitions, metadata, and integration implications.\n* Data Exploration Report: Document patterns, anomalies, and SME discussions.\n* Data Quality Report: Missing data treatment, imputation methods, and feature construction records.\n\nOutcome\n\n* A finalized dataset ready for modeling, supported by clear documentation of sources, transformations, and decisions.\n* Traceability of how features, imputations, and integrations were performed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "data_preparation",
    "outlinks": [
      "data_understanding"
    ],
    "inlinks": [
      "anomaly_detection",
      "data_assessment",
      "data_deployment",
      "data_mining_-_crisp",
      "data_understanding"
    ]
  },
  {
    "category": "DS",
    "filename": "Data Science",
    "sha": "0b8c2d0a7698472433edf182e62a6fb6501413d8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Science.md",
    "text": "Data science combines math models and buisness logic.\n\nA field that uses the [[Scientific Method]], algorithms, and systems to ==extract knowledge== and insights from structured and [[unstructured data]]. It combines techniques from [[Statistics]], [[Computer Science]], and domain expertise to analyze and interpret complex data sets, enabling informed decision-making and predictive modeling.\n\nResources:\n- https://scikit-learn.org/stable/auto_examples/index.html\n\nAt the business level a data scientist should be:\n- advocating for business problems,\n- pragmatic, \n- apply across business, \n- should be able to translate communication from top level to bottom.\n\nRelated:\n- [[Scaling Data Science Capability]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "data_science",
    "outlinks": [
      "scaling_data_science_capability",
      "computer_science",
      "unstructured_data",
      "statistics",
      "scientific_method"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Data Scientist",
    "sha": "a93d94651f68b805c57b207ed27ad606fcbb3e85",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Scientist.md",
    "text": "Data Scientist\n  - Utilizes [[business intelligence]] (BI) tools to analyze data.\n  - Works with data lakes to extract insights.\n  - Develops and deploys production Machine Learning (ML) models for predictions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "role"
    ],
    "normalized_filename": "data_scientist",
    "outlinks": [
      "business_intelligence"
    ],
    "inlinks": [
      "data_roles"
    ]
  },
  {
    "category": "DS",
    "filename": "Data Understanding",
    "sha": "6e307b3a004a26515850834ad041e287e1dc50a2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Data%20Understanding.md",
    "text": "Purpose\n* Collect, explore, and understand the data.\n* Ensure [[Data Quality]] before moving to [[Data Preparation]].\n* Gain insights relevant to the problem and identify potential hidden patterns.\n* Part of the [[Data Mining - CRISP]] process; crucial for guiding later modeling steps.\n\n\nKey Activities\n* Describe the data: Document sources, formats, variables,[[Data Dictionary]], and basic structure.\n* Explore the data: Summarize distributions, check for [[storage/utils/file_getter/selected_files/Outliers|anomalies]], and visualize relationships.\n* Verify quality: Assess missing values, duplicates, inconsistencies, and measurement errors.\n* Refine scope: Decide what data is relevant, discard unnecessary variables.\n\nExploration Guidelines\n\n* Identify the target variable (if supervised).\n* Group features to make exploration manageable.\n* Use methods such as [[Decision Tree]] analysis for investigating relationships and [[Feature Selection]].\n\nOutcome\n* A clear understanding of what data is available, its limitations, and its suitability for the problem.\n* A foundation for [[Data Preparation]] and subsequent modeling.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "data_understanding",
    "outlinks": [
      "data_preparation",
      "data_dictionary",
      "storage/utils/file_getter/selected_files/outliers",
      "data_mining_-_crisp",
      "decision_tree",
      "feature_selection",
      "data_quality"
    ],
    "inlinks": [
      "data_deployment",
      "data_mining_-_crisp",
      "data_preparation",
      "working_with_smes"
    ]
  },
  {
    "category": "DS",
    "filename": "Datasets",
    "sha": "1214da8a4b918a14246f165140127095eb86bb66",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Datasets.md",
    "text": "This note collects notes on datasets that are good examples for exploring various concepts.\n\nUse UCI Machine Learning repository. \n## Heart Failure Prediction Dataset\n- **Link**: [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n- **Useful for**: Exploring predictive modeling in healthcare.\n\n## Time Series Exploration\n- **Description**: There is a dataset with seasonality, bikes, which can be used to explore [[Time Series]] concepts.\n\n## Numenta Anomaly Benchmark (NAB)\n- **Link**: [Numenta Anomaly Benchmark (NAB)](https://github.com/numenta/NAB?ref=hackernoon.com)\n- **Columns**: timestamp, value\n- **Description**: NAB is used to evaluate and compare the performance of different anomaly detection algorithms on a diverse set of time series data. It includes real-world and artificial time series data covering domains such as finance, transportation, and environmental monitoring.\n\n## U.S. Census Bureau's International Data Base (IDB)\n- **Link**: [International Data Base (IDB)](https://www.census.gov/data-tools/demo/idb?ref=hackernoon.com)\n- **Useful for**: Researchers, policymakers, and businesses studying population dynamics, forecasting future population growth, monitoring economic development, and comparing demographic and economic characteristics of different countries.\n\n## Wikipedia Web Traffic Time Series Dataset\n- **Link**: [Wikipedia Web Traffic Time Series Dataset](https://www.kaggle.com/code/muonneutrino/wikipedia-traffic-data-exploration/data?ref=hackernoon.com)\n- **Useful for**: Examining the dynamics of website traffic, understanding interactions with Wikipedia, and identifying patterns and trends in online behavior. It can be used to compare traffic across languages, analyze the popularity of articles, and track the evolution of articles over time.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "exploration"
    ],
    "normalized_filename": "datasets",
    "outlinks": [
      "time_series"
    ],
    "inlinks": [
      "forecasting_exponential_smoothing.py",
      "handling_different_distributions",
      "mnist",
      "sklearn_datasets",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "DS",
    "filename": "Decomposition in Time Series",
    "sha": "d0c2016b8f633f9b4b82de836db60eb51231f16e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Decomposition%20in%20Time%20Series.md",
    "text": "When analyzing a [[Time Series]], we often use [[Decomposition in Time Series]] to separate it into three main components:\n* [[Trends in Time Series]] – long-term direction in the data\n* [[Seasonality in Time Series]] – repeating patterns at fixed intervals\n* [[Residuals Analysis]] – the unexplained variation after removing trend and seasonality\n\nA common method for decomposition is [[STL Decomposition]], which flexibly extracts trend and seasonal components.\n\n### Why Decompose?\n\n* Helps identify stable trend and seasonal structures.\n* Makes it easier to test whether the series is a [[Stationary Time Series]] (constant mean and variance).\n* Prepares the data for forecasting models.\n\n### Handling Trend and Seasonality\n\nTo model effectively, you often need to **make the data stationary**:\n* Remove trends\n* Remove seasonality\n\nThis can be checked using:\n* Plots of the series\n* [[ACF Plots]] and [[PACF Plots]]\n* Statistical tests (e.g., ADF test)\n\n### Practical Considerations\n\n* **Check stationarity** before modeling [[Stationary Time Series]].\n* **Split into components** (trend, seasonality, residuals) where helpful.\n* **Avoid over-differencing** — too much differencing removes useful information.\n* **Model choices**:\n  * Classical approaches: [[SARIMA]], [[Exponential Smoothing]]\n  * Decomposition + ML: [[Random Forest for Time Series]], Gradient Boosting\n\nResource:\n[Understanding Stationarity in Time Series](https://towardsdatascience.com/time-series-forecasting-made-simple-part-4-1-understanding-stationarity-in-a-time-series/)\n\n### Image\n\n![[Pasted image 20250909160252.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "decomposition_in_time_series",
    "outlinks": [
      "residuals_analysis",
      "decomposition_in_time_series",
      "stationary_time_series",
      "random_forest_for_time_series",
      "trends_in_time_series",
      "pasted_image_20250909160252.png",
      "time_series",
      "seasonality_in_time_series",
      "stl_decomposition",
      "sarima",
      "acf_plots",
      "exponential_smoothing",
      "pacf_plots"
    ],
    "inlinks": [
      "additive_vs_multiplicative_models_time_series",
      "autocorrelation",
      "decomposition_in_time_series",
      "pacf_plots",
      "seasonality_in_time_series",
      "stationary_time_series",
      "stl_decomposition",
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Differencing in Time Series",
    "sha": "1a0b41f30982e990f834bc20dec2968e4ae98fc8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Differencing%20in%20Time%20Series.md",
    "text": "### Why Do Differencing in Time Series?\n\n**Differencing** is a technique used to make a non-stationary time series **stationary** ([[Stationary Time Series]]) by removing trends and, if needed, seasonal effects.\n\n- **First-order differencing:**\n    - Subtracts each observation from its previous value.\n    - Captures the **change from one time step to the next**.\n    - Helps remove **linear trends** in the series.\n    \n- **Higher-order differencing:**\n    - If first-order differencing is not enough, you can apply **second-order differencing**:\n    - Captures changes in the rate of change, useful for stronger trends.\n    \n- **Seasonal differencing:**\n    - If seasonal patterns remain after first-order differencing, perform **seasonal differencing**\n    \n- **Why it matters:**\n    - Differencing stabilizes the mean of the series.\n    - Makes the series suitable for models like [[ARIMA]] and [[SARIMA]], which assume stationarity.\n    \n- **Next steps:**\n    - After differencing, check stationarity with statistical tests such as [[ADF Test]] and [[KPSS Test]].\n    - Visual inspection and [[ACF Plots]] can also confirm whether trends or seasonality have been removed.",
    "aliases": [
      "Differencing"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "differencing_in_time_series",
    "outlinks": [
      "arima",
      "stationary_time_series",
      "sarima",
      "adf_test",
      "kpss_test",
      "acf_plots"
    ],
    "inlinks": [
      "adf_test",
      "arima",
      "forecasting_with_autoregressive_(ar)_models",
      "sarima",
      "stationary_time_series",
      "trends_in_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Dynamic Time Warping",
    "sha": "1eaa5777c297d5f5867efc2f19700c66530186f0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Dynamic%20Time%20Warping.md",
    "text": "Dynamic Time Warping (DTW) is a method for measuring ==similarity== between time series that may vary in speed, phase, or length. DTW seeks for the *temporal alignment* (a matching between time indexes of the two time series) that minimizes Euclidean distance between the aligned series.\n\nThere are different variations and extensions.\n\n### Conceptual Overview\n\nDynamic Time Warping computes an *optimal alignment* between two time series by stretching or compressing segments along the time axis to minimize overall distance.\n\nUnlike Euclidean distance (which assumes perfect time alignment), DTW allows local warping — making it robust to ==temporal distortions== such as shifts, delays, or different speeds of progression.\n\nFormally, for two sequences $X = (x_1, \\ldots, x_n)$ and $Y = (y_1, \\ldots, y_m)$, DTW finds a warping path $W = (w_1, \\ldots, w_k)$ minimizing\n\n$$\nDTW(X, Y) = \\min_W \\sum_{(i,j) \\in W} d(x_i, y_j)\n$$\n\nsubject to continuity, monotonicity, and boundary conditions.\n\nDTW will change if curves are stretched or squeezes. It is invariant to translation.\n\nEuclidean distance is not suitable for this.\n\n### Common Use Cases\n\nClustering Time Series\n* DTW is often used as a distance metric in clustering algorithms such as $k$-means (with DTW distance) or hierarchical clustering.\n* It groups together time series with similar *shape* or *evolution pattern*, even if they are temporally misaligned.\n\nTime Series Classification\n* Used with $k$-Nearest Neighbours (k-NN) classifiers:\n* Each new sequence is classified by finding the training series with the smallest DTW distance.\n* Especially common when you have small datasets or irregular time axes.\n\nAnomaly Detection\n* DTW can detect deviations from normal temporal patterns.\n* Comparing a new sequence against a reference template: a high DTW distance indicates an anomaly.\n\nPattern Matching / Template Matching\n* DTW is used to locate ==subsequences== that match a reference pattern within a larger time series.\n* The technique can handle stretching or compression of the pattern.\n\nForecast Evaluation / Model Comparison\n* Used to assess the *shape similarity* between predicted and observed series.\n* More robust than RMSE or MAE when there are temporal shifts between predicted and actual peaks.\n\n### When to Use DTW\n\n| Condition                                       | DTW Appropriate?             |\n| ----------------------------------------------- | ---------------------------- |\n| Series differ in time length                    | ✅                            |\n| Series have similar shapes but different timing | ✅                            |\n| Series are strictly aligned (same timestamps)   | ❌ (correlation is simpler)   |\n| Very large datasets (millions of series)        | ⚠️ Computationally expensive |\n### Clustering \n\n- Cluster using [[Dendrograms]] (find number of clusters) or [[K-nearest neighbours|KNN]]\n- Use Time Series Cluster Kernel TCK\n- Use [[Dimensionality Reduction]] to visualise.\n\n### Related:\n- [[Correlated Time Series]]\n\n![[Pasted image 20251005094835.png]]",
    "aliases": [
      "DWT"
    ],
    "date modified": "5-10-2025",
    "tags": [
      "anomaly_detection",
      "clustering",
      "evaluation",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "dynamic_time_warping",
    "outlinks": [
      "pasted_image_20251005094835.png",
      "correlated_time_series",
      "dendrograms",
      "dimensionality_reduction",
      "k-nearest_neighbours"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Evaluating Time Series Forecasts",
    "sha": "61d7fcbb84c11f8a04398ea99c19930cb3789057",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Evaluating%20Time%20Series%20Forecasts.md",
    "text": "Evaluating a **time series forecast** involves measuring how well your predictions match the actual observed data. There are several approaches and metrics, depending on the goal of your analysis.\n\n## Point Forecast Accuracy Metrics\n\nThese metrics compare the predicted values directly with the observed values:\n\n- [[Mean Absolute Error]] (MAE): Measures the average magnitude of errors without considering their direction. It is simple and easy to interpret.  \n- [[Mean Squared Error|MSE]]: Squares the errors to penalize larger deviations more heavily.  \n- [[Root Mean Squared Error|RMSE]]: Square root of MSE; in the same units as the data and emphasizes large errors.  \n- [[Mean Absolute Percentage Error|MAPE]]: Expresses errors as a percentage of the actual values, useful for understanding relative accuracy.  \n\nThese metrics provide a quick sense of overall accuracy, but they do not capture patterns in the errors.\n## Residual Analysis\n\nResiduals are the differences between the ==actual values and the predicted values==: Analyzing residuals helps identify whether the model has left any systematic patterns unexplained:\n\n- Plot **residuals** over time to check for trends or cycles.  \n- Residuals should ideally be **uncorrelated** and **approximately normally distributed**.  \n- Tools for residual analysis:  \n  - **ACF of residuals**: Detects any remaining autocorrelation in the errors [[ACF Plots|ACF]].  \n  - **Histogram or Q-Q plot**: Checks whether residuals are roughly normally distributed [[Q-Q Plot]].  \n## [[Out-of-sample rolling forecast evaluation]]\n\nThis approach tests how well the model performs on **data that was not available during training**:\n\n- **Train-test split**: Train the model on historical data, forecast future points, and compare with actual outcomes.  \n- **Rolling / walk-forward validation**: Refit the model as new data becomes available, forecast ahead, and record errors. This approach shows **how forecast accuracy evolves over time** and across different forecast horizons. Rolling evaluation mimics real-world forecasting conditions and gives a realistic estimate of forecast reliability.\n\n## Business / Decision-Oriented Evaluation\n\nAccuracy metrics alone are not always sufficient; forecasts should be evaluated in the context of their **practical impact**: [[Time Series Forecasts in Business]]:\n\n- Does the forecast support decision-making?  \n- Are critical thresholds or targets met?  \n- For example, in energy demand forecasting, errors may translate into over- or under-supply penalties, which are more meaningful than purely numerical accuracy.  \n\nEvaluating forecasts from a business perspective ensures that models are not only statistically sound but also operationally useful.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "evaluation",
      "time_series"
    ],
    "normalized_filename": "evaluating_time_series_forecasts",
    "outlinks": [
      "mean_absolute_error",
      "q-q_plot",
      "time_series_forecasts_in_business",
      "mean_absolute_percentage_error",
      "mean_squared_error",
      "out-of-sample_rolling_forecast_evaluation",
      "acf_plots",
      "root_mean_squared_error"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Evolving Seasonality",
    "sha": "cf5fb5ecec8bb272aff5462ee54722b4026901e4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Evolving%20Seasonality.md",
    "text": "Use [[ARIMA]]/ [[SARIMA]]\n\n[[Seasonality in Time Series]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "evolving_seasonality",
    "outlinks": [
      "arima",
      "seasonality_in_time_series",
      "sarima"
    ],
    "inlinks": [
      "holt-winters_(exponential_smoothing)",
      "sarima"
    ]
  },
  {
    "category": "DS",
    "filename": "F-statistic",
    "sha": "bb9046dc1101eeae16598916689e5d4b8eeea995",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/F-statistic.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "f-statistic",
    "outlinks": [],
    "inlinks": [
      "anova",
      "f-regression"
    ]
  },
  {
    "category": "DS",
    "filename": "Feature Engineering",
    "sha": "1dbb40229e8dfbc5a8d6b470ced305de61377b48",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Feature%20Engineering.md",
    "text": "Its the term given to the iterative process of building good features for a better model. Its the process that makes relevant features (using formulas and relations between others). \n \nWe use it when we have a refined and optimised model.\n\nWhat does it involve\n- Create new features from existing ones (e.g., ratios, interactions).\n- Transform features to better capture non-linear relationships.\n- [[Dimensionality Reduction]] if necessary.\n\nThe main techniques of feature engineering:\n- are selection (picking subset), \n- learning (picking the best), \n- extraction and combination(combining).\n\nExample:\nPredicting house prices. Raw features might be square footage, number of bedrooms, and location. Feature engineering could involve: Combining square footage and bedrooms into a \"living space\" feature.\n\n**Example**:\n- Decompose datetime information into separate features for date and time to capture their respective predictive powers.\n\n\n![[C1_W2_Lab07_FeatureEngLecture.png]]\n\nInteraction terms: [[Feature Engineering]]:\n- Definition and Purpose: Interaction terms are new features created by combining existing ones to capture the interaction effects between them, improving model ==accuracy==.\n- Common Methods: Multiplication (e.g., square footage * number of bedrooms) and division (e.g., price per square foot) are common ways to create interaction terms.\n- Benefits: They help uncover ==complex patterns, tackle non-linearities==, and enhance the model's ability to learn how features influence each other.\n- Application: Use domain knowledge to identify meaningful interactions and start with simple, pairwise interactions to avoid overfitting.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#optimisation",
      "process"
    ],
    "type": null,
    "normalized_filename": "feature_engineering",
    "outlinks": [
      "feature_engineering",
      "c1_w2_lab07_featureenglecture.png",
      "dimensionality_reduction"
    ],
    "inlinks": [
      "automated_feature_creation",
      "class_separability",
      "ds_&_ml_portal",
      "eda",
      "encoding_categorical_variables",
      "feature_engineering",
      "feature_engineering_for_time_series",
      "machine_learning_operations",
      "model_optimisation",
      "preprocessing",
      "random_forest_for_time_series",
      "regression",
      "time_series_python_packages"
    ]
  },
  {
    "category": "DS",
    "filename": "Feature Scaling",
    "sha": "f55502be03914a2e8421d907fbb7c3cb70e9bd01",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Feature%20Scaling.md",
    "text": "Used in preparing data for machine learning models. \n\nFeature Scaling is a [[Preprocessing]] step in machine learning that involves adjusting the range and distribution of feature values. \n\nThis ensures that all features contribute equally to the model's performance, especially when they are measured on different scales, which is particularly important for distance-based algorithms, [[Principal Component Analysis]], and optimization techniques like [[Gradient Descent]]. \n\nBy using methods like normalization and standardization, you can enhance the performance and accuracy of your models.\n\nSee sklearn.preprocessing\n\nExamples of algorithms not affected by feature scaling are [[Naive Bayes Classifier]], [[Decision Tree]], and [[Linear Discriminant Analysis]].\n### Why Use Feature Scaling?\nFeature scaling is important for several reasons:\n\n1. Distance-Based Algorithms: Algorithms like k-nearest neighbors (KNN) rely on distance measures (e.g., Euclidean distance). If features are on different scales, those with larger magnitudes will disproportionately influence the distance calculations. Scaling ensures that all features weigh equally.\n\n2. Principal Component Analysis (PCA): PCA aims to identify the directions (principal components) that maximize variance in the data. If features have high magnitudes, they will dominate the variance calculation, skewing the results. Scaling helps to mitigate this issue.\n\n3. Gradient Descent Optimization: In optimization algorithms like gradient descent, features with larger ranges can cause inefficient convergence. Scaling ensures that all features are on a similar scale, allowing for faster and more stable convergence to the optimal solution.\n\n[[Feature Scaling]] is useful for models that use distances like [[Support Vector Machines|SVM]] and [[K-means]]\n### When Scaling Is Unnecessary\n\n1. Tree-based Algorithms:\n    - Algorithms like [[Decision Tree]], [[Random Forest]], and Gradient Boosted Trees are invariant to feature scaling because they split data based on thresholds, not distances.\n    - Example: Splits are determined by feature values, not their magnitude.\n      \n2. Data with Uniform Scales:\n    - If all features have the same range or are already normalized (e.g., percentages), scaling may not be required.\n\n### Common Scaling Methods\n- [[Normalisation]]\n- [[Standardisation]]\n\nMin-Max Scaling: Scales features to a fixed range (e.g., $[0, 1]$), preserving relative distances.\n### Example of Scaling\nHere’s how you can scale a DataFrame using the `scale` function from `sklearn`:\n\n```python\nfrom sklearn import preprocessing\ndf_scaled = preprocessing.scale(df)  # Scales each variable (column) with respect to itself\n```\n\nThis returns an array where each feature is standardized.\n\n### Note:\n- Scaling is done when one feature is at a significantly different scale.\n- For each data point, subtract the mean and divide by the range (max-min).\n\n![[Pasted image 20241224083928.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "preprocessing"
    ],
    "normalized_filename": "feature_scaling",
    "outlinks": [
      "gradient_descent",
      "pasted_image_20241224083928.png",
      "normalisation",
      "preprocessing",
      "naive_bayes_classifier",
      "feature_scaling",
      "standardisation",
      "k-means",
      "random_forest",
      "principal_component_analysis",
      "linear_discriminant_analysis",
      "decision_tree",
      "support_vector_machines"
    ],
    "inlinks": [
      "clustering",
      "feature_scaling",
      "preprocessing"
    ]
  },
  {
    "category": "DS",
    "filename": "Feature Selection vs Feature Importance",
    "sha": "4fc841165fa562c8e1a1bcfcb48297e0662505d9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Feature%20Selection%20vs%20Feature%20Importance.md",
    "text": "- [[Feature Selection]] is about choosing which features to include in the model ==before training==, aiming to improve model performance and efficiency.\n- [[Feature Importance]] is about understanding the role and ==impact== of each feature ==after the model has been trained,== providing insights into the model's decision-making process.\n\nUse for [[Interpretability]] of the model, but they are applied at different stages and serve different purposes.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "selection"
    ],
    "normalized_filename": "feature_selection_vs_feature_importance",
    "outlinks": [
      "feature_selection",
      "interpretability",
      "feature_importance"
    ],
    "inlinks": [
      "feature_selection"
    ]
  },
  {
    "category": "DS",
    "filename": "Forecasting using Lags",
    "sha": "2a4d36ca2948438986ca782c365175e42179a6d5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Forecasting%20using%20Lags.md",
    "text": "A **lag value** is simply the value of a series at a previous time step, used as a feature for the current prediction. For example, at time $t$, the feature $y_{t-1}$ is the lagged value from the previous step.\n\nLags capture short-term dependencies in a series and are especially useful for models such as [[Random Forest for Time Series]].\n\n### Why use lags?\n\n* They help the model learn relationships from recent history.\n* Useful when modeling [[Seasonality in Time Series]], since repeating patterns often depend on past values.\n* Performance can be compared by looking at [[Evaluation Metrics]] with and without lag features.\n\n### Limitations of Lags\n\nWhile lags can boost short-horizon forecasts, they can cause problems in **longer-term forecasting**:\n\n* At some point, the model no longer has real lag values to use as inputs—it must rely on its own **predicted values**.\n* This creates **error propagation**: each prediction carries uncertainty, which compounds with each additional step.\n\nExample:\n* At time $t$, $y_{t-1}=5$, model predicts $y_t=6$.\n* At $t+1$, the model must use **6 (a prediction)** as input, leading to more uncertainty.\n* If it predicts $8$ at $t+1$, the forecast for $t+2$ is based on $8$ (itself dependent on $6$), compounding the error.\n\nThis recursive process means performance usually **degrades with horizon length**.\n\n### Beyond Lags\n\nTo reduce over-reliance on lagged features, it’s often useful to add **other numerical or categorical predictors**, such as:\n\n* Calendar features (holiday flags, weekday vs weekend, month/season indicators).\n* External variables (e.g., temperature, promotions, economic indicators).\n\nThese features can help the model generalize better and reduce error accumulation over time.\n\nResources:\n* [How to Forecast Time Series Using Lags](https://towardsdatascience.com/how-to-forecast-time-series-using-lags-5876e3f7f473/)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "forecasting_using_lags",
    "outlinks": [
      "seasonality_in_time_series",
      "random_forest_for_time_series",
      "evaluation_metrics"
    ],
    "inlinks": [
      "acf_plots",
      "time_series_forecasting"
    ]
  },
  {
    "category": null,
    "filename": "Forecasting with Autoregressive (AR) Models",
    "sha": "9453c39edd57bfca0572f4aaf15a0ef972527e04",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Forecasting%20with%20Autoregressive%20(AR)%20Models.md",
    "text": "Predict future values using past observations ([[Autoregression]]), assuming linear dependence on previous time steps.\n\n**Procedure**\n\n1. **Ensure [[Stationary Time Series|Stationarity]]**\n\n   * Remove trend and seasonality via *[[Differencing in Time Series|differencing]]* or *decomposition*.\n   * Test for stationarity with the **[[ADF test]]**.\n\n2. **Model Identification**\n\n   * Use **PACF** to determine AR order $p$.\n   * Fit an **AR($p$)** model:\n     $$X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\epsilon_t$$\n\n3. **Model Fitting & Diagnostics**\n\n   * Estimate coefficients via Maximum Likelihood or Least Squares.\n   * Check residuals: should resemble **white noise** (no autocorrelation).\n\n4. **Forecasting**\n\n   * Generate forecasts $\\hat{X}_{t+h}$ recursively using the fitted coefficients.\n   * For differenced data, **reconstruct** forecasts by reversing the differencing operation.\n\n5. **Interpretation**\n\n   * AR models describe *persistence* or *memory* in data.\n   * A slow decay in [[ACF Plots|ACF]] or significant PACF lags indicates how many past values drive predictions.",
    "aliases": [],
    "date modified": "14-10-2025",
    "tags": [
      "classifer",
      "ml_process",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "forecasting_with_autoregressive_(ar)_models",
    "outlinks": [
      "autoregression",
      "stationary_time_series",
      "differencing_in_time_series",
      "adf_test",
      "acf_plots"
    ],
    "inlinks": [
      "acf_plots"
    ]
  },
  {
    "category": "DS",
    "filename": "Forward Propagation",
    "sha": "9e4f7ff3cede4a57b7497879742bead7b809fca8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Forward%20Propagation.md",
    "text": ">[!Summary]  \n> Forward propagation is the process by which input data moves through a neural network, layer by layer, to produce an output. During this process, each layer’s weights and biases are applied to the input data, and an activation function is used to transform the data at each layer. \n> \n> Mathematically, for each layer, the input $x$ is transformed into an output $y$ through the equation $y = f(Wx + b)$, where $W$ represents the weights, $b$ is the bias, and $f$ is the activation function (e.g., [[Relu]], sigmoid). The output from one layer becomes the input to the next, and this continues until the final layer produces the predicted output. \n> \n> This process does not involve learning; it only ==computes the prediction based on current weights.==\n\n>[!Breakdown]  \n> Key Components:  \n> - Input data: Initial values fed into the network.  \n> - Weights ($W$) and biases ($b$): Parameters adjusted during training.  \n> - Activation function: Non-linear transformation, e.g., ReLU or sigmoid.  \n> - Output: Prediction made by the network.\nM\n>[!important]  \n> - Forward propagation calculates predictions ==by applying current model parameters== to inputs.  \n> - It is the first step before backpropagation, where the error is used to adjust weights.\n\n>[!attention]  \n> - Forward propagation does not involve ==learning or updating weights.==  \n> - The accuracy of forward propagation depends entirely on the current values of weights and biases.\n\n>[!Example]  \n> In a simple neural network with one hidden layer, forward propagation can be described as:  \n> $$ z_1 = W_1 x + b_1 $$  \n> $$ a_1 = \\text{ReLU}(z_1) $$  \n> $$ z_2 = W_2 a_1 + b_2 $$  \n> $$ y = \\text{sigmoid}(z_2) $$  \n> Here, $x$ is the input, and $y$ is the output prediction.\n\n>[!Follow up questions]  \n> - How does the choice of [[Activation Function]] impact the forward propagation process?  \n> - In deep networks, how can [[vanishing and exploding gradients problem]] during forward propagation affect training?\n\n>[!Related Topics]  \n> - [[Backpropagation]] in neural networks  \n> - Activation functions in deep learning",
    "aliases": [
      "feedforward pass",
      "forward pass"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "statistics"
    ],
    "normalized_filename": "forward_propagation",
    "outlinks": [
      "relu",
      "activation_function",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation"
    ],
    "inlinks": [
      "feed_forward_neural_network",
      "fitting_weights_and_biases_of_a_neural_network"
    ]
  },
  {
    "category": "DS",
    "filename": "Gaussian Mixture Models",
    "sha": "dfb1734a905ed61af2cf29c65ec440d9fd3cede4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Gaussian%20Mixture%20Models.md",
    "text": "Gaussian Mixture Models (GMMs) represent data as a mixture of multiple Gaussian [[Distributions]], with each cluster corresponding to a different Gaussian component. GMMs are more effective than [[K-means]] because they consider the distributions of the data rather than relying solely on distance metrics.\n\nSoft [[Clustering]] technique.\n\n[[Kmeans vs GMM]]\n\nGMMs can have difference [[Covariance Structures]]\n## Key Concepts\n\n- **Gaussian Components**: Each Gaussian distribution is characterized by its mean and [[Covariance]].\n- **Likelihood**: The likelihood of a data point belonging to a cluster is given by the formula:\n  $$\n  P(X | C_k) = \\pi_k \\cdot \\mathcal{N}(X | \\mu_k, \\Sigma_k)\n  $$\n  where $P(X | C_k)$ is the probability of data point $X$ given cluster $C_k$, $\\pi_k$ is the prior probability of cluster $C_k$, and $\\mathcal{N}$ is the Gaussian distribution.\n- **Expectation-Maximization (EM) Algorithm**: GMMs utilize the EM algorithm to iteratively optimize the parameters of the Gaussian components.\n\n## Advantages of GMMs\n\n- **Complex Data Distributions**: GMMs can capture complex data distributions, unlike [[K-means]], which only considers distance metrics.\n- **Probabilistic Framework**: GMMs provide a probabilistic framework for clustering, allowing for soft assignments of data points to clusters.\n- **Modeling Elliptical Clusters**: The use of covariance matrices enables GMMs to model elliptical clusters, enhancing clustering performance.\n\n## Applications\n\n- **[[Anomaly Detection]]**: GMMs are widely used in various applications, including anomaly detection.\n## Important Considerations\n\n- **Covariance Types**: The choice of covariance types (full, tied, diagonal, spherical) can significantly impact the performance of GMMs.\n\n## Follow-up Questions\n\n- How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency?\n- What are the implications of choosing different covariance types in GMMs?\n\n![[Pasted image 20250126135722.png|500]]",
    "aliases": [
      "GMM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "gaussian_mixture_models",
    "outlinks": [
      "covariance_structures",
      "clustering",
      "kmeans_vs_gmm",
      "anomaly_detection",
      "k-means",
      "distributions",
      "covariance",
      "pasted_image_20250126135722.png"
    ],
    "inlinks": [
      "anomaly_detection",
      "clustering",
      "covariance",
      "covariance_structures",
      "kmeans_vs_gmm",
      "machine_learning_algorithms"
    ]
  },
  {
    "category": "DS",
    "filename": "Gitlab",
    "sha": "47d7ed25d8ebdf024043f8ae53c245a38a1af13d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Gitlab.md",
    "text": "[GitLab CI CD Tutorial for Beginners Crash Course](https://www.youtube.com/watch?v=qP8kir2GUgo)\n\n  - Provides managed runners to execute [[CI-CD]] pipelines.\n  - Integrates with version control systems to automate the CI/CD process.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "gitlab",
    "outlinks": [
      "ci-cd"
    ],
    "inlinks": [
      "ci-cd",
      "gitlab-ci.yml"
    ]
  },
  {
    "category": "DS",
    "filename": "Gompertz Model",
    "sha": "f956183ded3ed4f8b2df2824eb8d779ef5321b72",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Gompertz%20Model.md",
    "text": "## Gompertz Model\n\n**Equation:**  \n\n$$\ny(t) = K \\cdot e^{-e^{-g(t-t_0)}}\n$$\n\n- **K:** Saturation level  \n- **g:** Growth rate coefficient  \n- **$t_0$:** Timing parameter (controls where curve inflects)\n\n**Characteristics:**\n\n- Asymmetric S-curve: **slow start, fast middle growth, then gradual approach to K**  \n- Often fits biological or human adoption data better than logistic  \n- Early growth is slower, late growth is smoother\n\n**Use Cases:**\n\n- When growth starts slowly and accelerates later  \n- Customer adoption with cautious early adopters and gradual ramp-up\n\nRelated:\n- [[Customer Growth Modeling]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "gompertz_model",
    "outlinks": [
      "customer_growth_modeling"
    ],
    "inlinks": [
      "customer_growth_modeling"
    ]
  },
  {
    "category": "DS",
    "filename": "Good Enough Principle in Data Projects",
    "sha": "3f1dc0f533fbb3e1cd3eb2dd77f53009cf2061b1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Good%20Enough%20Principle%20in%20Data%20Projects.md",
    "text": "Overview\n* Based on the 80/20 principle: roughly 20% of effort delivers 80% of value.\n* Focus on achieving practical, usable outcomes rather than perfect solutions.\n* Avoid analysis paralysis and unnecessary complexity.\n\nKey Principles\n* Time and effort tracking: balance marginal gains vs cost of complexity.\n* Good enough: deliver usable results rather than perfect models.\n* Iterative improvement: refine solutions based on feedback and operational realities.\n\nUnderstand Project Goals\n* Clearly define what success looks like before starting.\n* Include measurable outcomes and customer agreement.\n* Use a meeting with stakeholders to align on success criteria.\n\n\nUnderstand Customer Needs\n* Practical implementation of results is critical.\n* Understand user skills and context (who will use the model, reports, or analysis).\n* Deployment considerations: cloud, local, or hybrid environments.\n\n\nPerformance Considerations\n* Performance Dimensions: time, cost, code quality, stakeholder satisfaction.\n* Track time and effort, considering diminishing returns and model complexity.\n* Focus on core, fundamental features for the MVP.\n\n\nMinimum Viable Product (MVP)\n* Identify the simplest solution that provides value.\n* Helps with deployment, operational hurdles, and validating assumptions.\n* Allows small wins and prevents scope creep.\n* Iterative approach: test, gather feedback, and refine.\n\nCommon reasons DS projects fail:\n* Fuzzy goals: unclear or unmeasurable success metrics.\n* Poor project management: unrealistic timelines or milestones.\n* Insufficient resources: software, skills, or expertise gaps.\n* Lack of senior management support or incentives.\n* Over-complexity: chasing marginal gains at high cost.\n\nAlso see:\n- [[Data Sources]]\n- [[Communication with Stakeholders]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "good_enough_principle_in_data_projects",
    "outlinks": [
      "communication_with_stakeholders",
      "data_sources"
    ],
    "inlinks": [
      "project_management_portal"
    ]
  },
  {
    "category": "DS",
    "filename": "GraphRAG",
    "sha": "11eceaa476f39f776d15cecd2a6b117e4ca8d5b0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/GraphRAG.md",
    "text": "[[GraphRAG]] is a [[RAG]] framework that utilizes [[Knowledge Graph]]s to enhance information retrieval and processing. A significant aspect of this framework is the use of large language models (LLMs) for [[Named Entity Recognition]] (NER) within [[neo4j]].\n\n### Related Terms\n- [[How to search within a graph]]\n- **[[Text2Cypher]]**: This feature allows users to interact with the graph in a user-friendly manner, converting natural language queries into Cypher queries.\n- How to move datasets into a graph database.\n- Graphrag patterns.\n- The role of [[Interpretability]] in understanding graph-based retrieval[[GraphRAG]]AG]]\n\n### Implementation\n\nUse of knowledge graphs. This specific graph is called a \"Lexical Graph with Extracted Entities\".\n[LinkedIn Post](https://www.linkedin.com/posts/rani-baghezza-69b154b8_thats-why-im-bullish-on-knowledge-graphs-activity-7287474722039033857-BXyN?utm_source=share&utm_medium=member_desktop)\n\nIn [[ML_Tools]] see: [[Wikipedia_API.py]]\n### Resources\n\n- [GraphRAG Site](https://graphrag.com/concepts/intro-to-graphrag/)\n- [Neo4j: Building Better GenAI: Your Intro to RAG & Graphs](https://www.youtube.com/watch?v=OuyTENdRcNs)\n\nLink: https://www.youtube.com/watch?v=knDDGYHnnSI",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "drafting"
    ],
    "normalized_filename": "graphrag",
    "outlinks": [
      "neo4j",
      "text2cypher",
      "interpretability",
      "wikipedia_api.py",
      "how_to_search_within_a_graph",
      "knowledge_graph",
      "graphrag",
      "ml_tools",
      "rag",
      "named_entity_recognition"
    ],
    "inlinks": [
      "agentic_solutions",
      "graphrag",
      "how_to_search_within_a_graph",
      "knowledge_graph",
      "neo4j",
      "neomodel",
      "relationships_in_memory",
      "text2cypher"
    ]
  },
  {
    "category": "DS",
    "filename": "Handling Missing Data",
    "sha": "50ce5f21b57243ac85233ba4292aab8565abe1d7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Handling%20Missing%20Data.md",
    "text": "Missing data can provide insights into the data collection process. It's important to determine whether the missing data is randomly distributed or specific to certain features. Filling in data is a type of [[Data Transformation]].\n\nIn [[DE_Tools]]see: \n- [[Handling_Missing_Data_Basic.ipynb]]\n- [[Handling_Missing_Data.ipynb]]\n\nResources:\n- https://scikit-learn.org/stable/modules/impute.html\n## Identifying Missing Data\nHow do you find which features have the most missing data?\n\nTo find where missing values (NA) are located in your dataset, use the following commands:\n```python\ndf.isnull().sum()\ndf.isna().sum()\ndf[df.columns[df.isnull().sum() > 0].tolist()].info()\n```\n## Treating Missing Values ([[Imputation Techniques]])\n\nThere are two main strategies for handling missing values: removing them or replacing them.\n\nRemove Missing Values:\n  - `dropna`: Drops rows with missing values.\n  - `df.dropna(inplace=True)`: Drops rows with NA values and updates the DataFrame in place.\n  - `df.reset_index(inplace=True, drop=True)`: Resets the index after dropping rows.\n\nReplace Missing Values:\n  - `fillna`: Fills missing values with specified values.\n\t- Example: `df['var1'] = df['var1'].fillna(df['var1'].mean())` fills missing values in `var1` with the column's average.  \n  - `isnull`: Checks for missing values.\n  - `df.reindex`: Reindexes the DataFrame.\n  - Imputation methods for filling in missing data so that it has a higher likelyhood of being true.\n\nRelated:\n- [[Missing Data]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing",
      "transformation"
    ],
    "normalized_filename": "handling_missing_data",
    "outlinks": [
      "imputation_techniques",
      "data_transformation",
      "missing_data",
      "handling_missing_data_basic.ipynb",
      "de_tools",
      "handling_missing_data.ipynb"
    ],
    "inlinks": [
      "data_cleansing",
      "missing_data",
      "outliers",
      "pandas",
      "xgboost"
    ]
  },
  {
    "category": "DS",
    "filename": "Holt-Winters (Exponential Smoothing)",
    "sha": "355fbd8c9a678ca4f3f1faa026c7120b465893e1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Holt-Winters%20(Exponential%20Smoothing).md",
    "text": "The **Holt-Winters model** extends Holt’s linear trend model by adding a **seasonality component**, making it suitable for time series with both trend and repeating seasonal patterns. It decomposes the series into three components:\n\n- **Level ($L_t$):** the baseline value at time $t$  \n- **Trend ($T_t$):** the rate of change at time $t$  \n- **Seasonality ($S_t$):** the repeating seasonal pattern  \n\nThe forecast is generated by combining these components. For example, the additive seasonal forecast equation is:\n\n$$\n\\hat{X}_{t+h} = L_t + hT_t + S_{t+h-m}\n$$`\n\nwhere $m$ is the seasonal period. Holt-Winters is [[Interpretability|interpretable]], as each component has a clear meaning, and is controlled by three smoothing parameters (typically optimized automatically):\n\n- $\\alpha$ - level smoothing  \n- $\\beta$ - trend smoothing  \n- $\\gamma$ - seasonality smoothing  \n\nTwo seasonal variants exist:[[Additive vs Multiplicative Models Time Series]]\n- **Additive:** seasonal fluctuations remain constant in magnitude  \n- **Multiplicative:** seasonal fluctuations scale proportionally with the series level  \n\n## Benefits\n\n- Highly **interpretable**: each component (level, trend, seasonality) is meaningful  \n- Quick and **robust** for medium-term forecasts  \n- Works well when historical patterns are reasonably stable  \n## Assumptions\n\n1. **Decomposable structure**  \n   - The time series can be represented as **level + trend + seasonality**  \n\n2. **Stationary seasonality**  \n   - Seasonal patterns are consistent over time  \n   - Additive: constant magnitude  \n   - Multiplicative: scales with series level  \n\n1. **No structural breaks** - [[Time Series Shocks]]\n   - The model assumes no abrupt, permanent shifts in the trend or seasonality  \n\n4. **Exponential decay of influence**  \n   - Recent observations carry more weight than older ones  \n\n5. **Random errors**  \n   - Forecast errors are assumed to be white noise, without autocorrelation  \n\n> **In short:** Holt-Winters works best with a stable trend and seasonal pattern, without major shocks or evolving seasonality.\n\n## Limitations\n- Struggles with sudden shocks or structural breaks (e.g., COVID-19 demand changes)  \n- Extrapolates past trends; long-term forecasts can become unrealistic  \n- Cannot adapt quickly to ([[Evolving Seasonality]])\n- Does not explicitly model autocorrelation in residuals\n## Handling [[Time Series Shocks]]\n\nHolt-Winters adapts gradually due to exponential smoothing. For sudden changes:\n- Re-estimate parameters using post-shock data  \n- Use hybrid models (e.g., Holt-Winters + regression with external variables)  \n- Consider models that explicitly handle **regime changes**  \n### Related\n- [[Holt-Winters vs ARIMA]]  \n- [[Time Series Shocks]]  \n- [[Exponential Smoothing]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "forecasting",
      "ml",
      "model",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "holt-winters_(exponential_smoothing)",
    "outlinks": [
      "evolving_seasonality",
      "time_series_shocks",
      "exponential_smoothing",
      "interpretability",
      "additive_vs_multiplicative_models_time_series",
      "holt-winters_vs_arima"
    ],
    "inlinks": [
      "exponential_smoothing",
      "holt-winters_vs_arima",
      "out-of-sample_rolling_forecast_evaluation"
    ]
  },
  {
    "category": "DS",
    "filename": "Holt-Winters vs ARIMA",
    "sha": "dceac0129dff148e21e2c439f13c9c63371d9ade",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Holt-Winters%20vs%20ARIMA.md",
    "text": "**Summary**\n- [[Holt-Winters (Exponential Smoothing)]] is best when data has a **clear trend and stable seasonality**, and interpretability is important.  \n- **[[ARIMA]]** is best when data can be differenced to stationarity and is driven primarily by **autocorrelation patterns** rather than stable seasonal components.  \n- Both methods can complement each other, and in practice, it is often useful to compare forecasts from both to validate results.\n\n**Holt-Winters (Exponential Smoothing)** decomposes a time series into **level, trend, and seasonality**, then applies smoothing to capture patterns. It is component-driven and highly interpretable, making it easy to explain to stakeholders.\n\n**ARIMA (Autoregressive Integrated Moving Average)** models the series using **autoregressive (AR)** and **moving average (MA)** terms, often after differencing to remove trend or seasonality. It is stochastic, relying on past correlations rather than explicit decomposition.\n\n> In short: Holt-Winters is component-based, while ARIMA is autocorrelation-based.\n\nHolt-Winters is particularly useful when data shows a **clear trend and stable seasonality**, and when interpretability matters. It does not require strict stationarity, and tuning is straightforward, often limited to smoothing parameters (α, β, γ) that can be optimized automatically.\n\nARIMA, on the other hand, is most effective when the series exhibits **strong autocorrelation** without clear seasonality, or when shocks and lagged dependencies drive the dynamics. Seasonal ARIMA (SARIMA) can handle evolving seasonal patterns better than Holt-Winters.\n\nThese methods can also be combined. For example, one can fit Holt-Winters to capture trend and seasonality, then apply ARIMA to the **[[Residuals Analysis]]** to model autocorrelation that Holt-Winters does not explain. Alternatively, SARIMA provides a statistically rigorous approach that integrates both seasonal and autocorrelation modeling. \n\n# Holt-Winters vs ARIMA: Assumptions\n\n\n| Aspect                         | Holt-Winters (Exponential Smoothing)                                        | ARIMA (Autoregressive Integrated Moving Average)                                          |\n| ------------------------------ | --------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n| **Data structure**             | Assumes the series can be decomposed into **level, trend, and seasonality** | Assumes the series can be modeled as a **linear function of past values and past errors** |\n| [[Trends in Time Series]]      | Explicitly modeled via Holt’s extension                                     | Must be removed via differencing (d)                                                      |\n| [[Seasonality in Time Series]] | Assumes **fixed, stable seasonal pattern** (additive or multiplicative)     | SARIMA handles seasonality, but it must be specified and may evolve                       |\n| [[Stationary Time Series]]     | Not required; works on raw data with trend/seasonality                      | Required; differencing and transformations often needed                                   |\n| **[[Autocorrelation]]**        | Not explicitly modeled; captured indirectly through smoothing               | Explicitly modeled (AR terms for past values, MA terms for past errors)                   |\n| [[Time Series Shocks]]         | Adapts gradually; slow to adjust after abrupt changes                       | Can adapt if differencing captures regime shifts, but may struggle with abrupt shocks     |\n| **[[Interpretability]]         | Very interpretable: forecast = level + trend + seasonality                  | Less intuitive: parameters (p, d, q) are statistical rather than directly interpretable   |\n| **Forecast horizon**           | Strong for **short- to medium-term** forecasts if structure remains stable  | Works for short-term; long-term forecasts often converge to the mean                      |\n| **Parameter tuning**           | Few parameters (α, β, γ), optimized automatically                           | More complex: (p, d, q) and seasonal (P, D, Q, m), chosen via ACF/PACF + AIC/BIC          |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "forecasting",
      "ml",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "holt-winters_vs_arima",
    "outlinks": [
      "arima",
      "residuals_analysis",
      "stationary_time_series",
      "holt-winters_(exponential_smoothing)",
      "trends_in_time_series",
      "seasonality_in_time_series",
      "time_series_shocks",
      "autocorrelation",
      "interpretability"
    ],
    "inlinks": [
      "holt-winters_(exponential_smoothing)"
    ]
  },
  {
    "category": "DS",
    "filename": "Holt’s Linear Trend Model (Double Exponential Smoothing)",
    "sha": "973a480b12bb032ae954f98724ea2693d409fbb2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Holt%E2%80%99s%20Linear%20Trend%20Model%20(Double%20Exponential%20Smoothing).md",
    "text": "Double Exponential Smoothing extends [[Simple Exponential Smoothing (SES)]] by introducing a **trend component** ([[Trends in Time Series]]), allowing it to capture both the *level* and the *trend* of a time series.\n\nIt is governed by two smoothing parameters:\n\n* $\\alpha$ : level smoothing\n* $\\beta$ : trend smoothing\n\n**Update equations:**\n\n* Level: $L_t = \\alpha X_t + (1 - \\alpha)(L_{t-1} + T_{t-1})$\n* Trend: $T_t = \\beta (L_t - L_{t-1}) + (1 - \\beta) T_{t-1}$\n* Forecast: $\\hat{X}_{t+h} = L_t + hT_t$\n\nThis method is especially useful for datasets exhibiting consistent upward or downward trends.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "forecasting",
      "ml",
      "time_series"
    ],
    "normalized_filename": "holt’s_linear_trend_model_(double_exponential_smoothing)",
    "outlinks": [
      "trends_in_time_series",
      "simple_exponential_smoothing_(ses)"
    ],
    "inlinks": [
      "exponential_smoothing"
    ]
  },
  {
    "category": "DS",
    "filename": "Imbalanced Datasets",
    "sha": "d3275a0e201bf848b1d3256737a28a60dba2a28a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Imbalanced%20Datasets.md",
    "text": "Handling imbalanced datasets to ensure robustness of models is a common challenge in machine learning, particularly in classification tasks where one class significantly outnumbers the other(s). \n\nIn [[Classification]] tasks, an imbalanced dataset can lead to a model that ==performs well on the majority class but poorly on the minority class==. This is because the model may learn to predict the majority class more often due to its prevalence. \n\nFor [[Regression]] tasks, handling outliers or data skewness might be necessary.\n### Examples\n\nConsider a scenario where you have an imbalanced dataset of resumes, with a majority of male resumes and a minority of female resumes. You want to build a model to predict gender based on resume features.\n## Strategies to address imbalances\n### Data-Level Approaches\n\nResampling Techniques:\n  - Oversampling: Increase the number of instances in the minority class by duplicating existing samples or generating new ones using techniques like [[SMOTE (Synthetic Minority Over-sampling Technique)]].\n  - Undersampling: Reduce the number of instances in the majority class by randomly removing samples. This can help balance the dataset but may lead to loss of important information.\n  - Data Augmentation: Apply transformations to existing data to create new samples, which is particularly useful in image data. Techniques include rotation, flipping, scaling, and cropping.\n\n### Algorithm-Level Approaches\n\n- [[Cost-Sensitive Analysis]] / Cost-Sensitive Learning: Modify the learning algorithm to give more importance to the minority class. This can be done by assigning higher misclassification costs to the minority class during training.1. You have a perfectly balanced dataset but still experience poor classification accuracy. Why might the class separability be the issue?\n- Ensemble Methods: [[Bagging]] and Boosting: Use ensemble techniques like [[Random Forest]] or AdaBoost, which can be adapted to handle class imbalance by adjusting the sample weights or using balanced bootstrap samples.\n\n### Evaluation Metrics & Others\n\n- [[Model Evaluation]]/[[Evaluation Metrics]]Use Appropriate Metrics: Instead of accuracy, use metrics that are more informative for [[Imbalanced Datasets]], such as precision, recall, F1-score, and the area under the ROC curve (AUC-ROC).\n- [[Anomaly Detection]] Models: Treat the minority class as anomalies and use [[Anomaly Detection]] techniques to identify them.\n- [[Transfer Learning]]: Use pre-trained models that have learned features from a balanced dataset, which can be fine-tuned on the imbalanced dataset.",
    "aliases": [
      "Class Imbalance"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "data_quality",
      "exploration",
      "ML_Tools"
    ],
    "normalized_filename": "imbalanced_datasets",
    "outlinks": [
      "regression",
      "cost-sensitive_analysis",
      "transfer_learning",
      "smote_(synthetic_minority_over-sampling_technique)",
      "bagging",
      "anomaly_detection",
      "random_forest",
      "model_evaluation",
      "imbalanced_datasets",
      "evaluation_metrics",
      "classification"
    ],
    "inlinks": [
      "accuracy",
      "class_separability",
      "cross_validation",
      "data_collection",
      "data_selection_in_ml",
      "determining_threshold_values",
      "ds_&_ml_portal",
      "eda",
      "gini_impurity_vs_cross_entropy",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "imbalanced_datasets",
      "imbalanced_datasets_smote.py",
      "metric",
      "neural_network_classification",
      "precision-recall_curve",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "DS",
    "filename": "Interpolation",
    "sha": "62181b2cae2b88df9301aa2741c7a2d3779c8841",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Interpolation.md",
    "text": "Interpolation is a method used to **estimate unknown values between known data points**. In data analysis, it is often applied when you have missing values in a dataset or want to generate smoother curves for visualization.\n\nFormally, if you have data points $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, interpolation constructs a function $f(x)$ such that $f(x_i) = y_i$ for all $i$, and then you can use $f(x)$ to estimate $y$ at values of $x$ that are not in the original data.\n\nInterpolation is useful for **filling missing data, smoothing curves, and resampling** datasets, but one should avoid extrapolating too far beyond the known points, as it can be unreliable.\n\n\nCommon types of interpolation:\n\n1. **Linear interpolation**: Connects two adjacent points with a straight line. Simple and fast.\n   $y = y_1 + \\frac{(x - x_1)(y_2 - y_1)}{x_2 - x_1}$\n\n2. **Polynomial interpolation**: Fits a single polynomial through all points. Can be accurate for few points but unstable for many points.\n\n3. **Spline interpolation**: Fits piecewise polynomials (usually cubic) between points, ensuring smoothness at joins. Often used in time series and graphics.\n\n4. **Nearest-neighbor interpolation**: Assigns the value of the nearest known point. Simple but can produce jumps.\n\n5. **Time-series specific methods**: Forward-fill, backward-fill, or more advanced methods like linear or spline interpolation along the time axis.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "time_series"
    ],
    "normalized_filename": "interpolation",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Intervention Analysis",
    "sha": "bb9046dc1101eeae16598916689e5d4b8eeea995",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Intervention%20Analysis.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "intervention_analysis",
    "outlinks": [],
    "inlinks": [
      "stationary_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Joining Time Series",
    "sha": "0cb64e49acc19f05f872035d72b5b454dd69957e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Joining%20Time%20Series.md",
    "text": "When working with multiple time series, joining them typically involves aligning records by timestamp. This can serve two purposes:\n\n* Combine metrics: Add additional measurements from different sources.\n* Extend time span: Append data to create a longer series.\n\nKey Consideration: [[granularity]]\n* The time granularity of each series matters when joining.\n* If one series has hourly data and another has daily data, you must decide how to aggregate or resample before the join.\n\n\nClock Skew\n* Clock skew occurs when timestamps do not perfectly align across sources.\n* A common solution is to truncate or round timestamps to the desired precision (e.g., nearest minute, hour).\n\nRelated Analysis Patterns\n* [[Views]]\n* [[SQL Joins]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "joining_time_series",
    "outlinks": [
      "granularity",
      "sql_joins",
      "views"
    ],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Kernel Machines",
    "sha": "f71f85e804d4dd71057ece49a005a9f90d3c9881",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Kernel%20Machines.md",
    "text": "A kernel machine is a class of [[Algorithms]] that use a ([[Kernelling]]) kernel function to implicitly map data into a higher-dimensional feature space, without explicitly computing the mapping. This is useful for handling nonlinear decision boundaries.\n\nThe most famous kernel machine is the Support Vector Machine ([[Support Vector Machines]]).\n\nOther examples: Kernel Ridge Regression, Kernel PCA.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      null
    ],
    "normalized_filename": "kernel_machines",
    "outlinks": [
      "support_vector_machines",
      "kernelling",
      "algorithms"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "KPSS Test",
    "sha": "bb9046dc1101eeae16598916689e5d4b8eeea995",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/KPSS%20Test.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "kpss_test",
    "outlinks": [],
    "inlinks": [
      "differencing_in_time_series",
      "stationary_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "LSTM in Time Series",
    "sha": "a6f07fc0525a03b6e6fcc6294b3dc4f2695ec7c5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/LSTM%20in%20Time%20Series.md",
    "text": "Learning: \"Use classical methods first\"\n\nResource: https://towardsdatascience.com/exploring-the-lstm-neural-network-model-for-time-series-8b7685aa8cf/\n\n[[LSTM]]\n[[Time Series Forecasting]]\n\nLSTMs are one of the state-of-the-art models\n\nUses [[Tensorflow]]\n\nRelated:\n- [[Adam Optimizer]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "lstm_in_time_series",
    "outlinks": [
      "lstm",
      "tensorflow",
      "adam_optimizer",
      "time_series_forecasting"
    ],
    "inlinks": [
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Latency",
    "sha": "d4071fc8d885db4bcd7c044dc8faf9aaf20fb495",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Latency.md",
    "text": "[[Performance Dimensions]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "term"
    ],
    "normalized_filename": "latency",
    "outlinks": [
      "performance_dimensions"
    ],
    "inlinks": [
      "alternatives_to_batch_processing",
      "challenges_to_model_deployment",
      "data_ingestion",
      "distributed_computing",
      "memory_caching",
      "performance_dimensions"
    ]
  },
  {
    "category": "DS",
    "filename": "MNIST",
    "sha": "ad7a2f53b87f478a7e197fbec9597ac1ea6b6656",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/MNIST.md",
    "text": "[[Datasets]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "exploration"
    ],
    "normalized_filename": "mnist",
    "outlinks": [
      "datasets"
    ],
    "inlinks": [
      "batch_normalisation",
      "neural_network_in_practice",
      "neural_scaling_laws"
    ]
  },
  {
    "category": "DS",
    "filename": "Logistic Model Curve",
    "sha": "3b9dadca7ccbbbdb5a951e3eab0b25c92c7cfdf6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Logistic%20Model%20Curve.md",
    "text": "## Logistic Model Curve\n\n**Equation:**  \n\n$$\ny(t) = \\frac{K}{1 + e^{-r(t-t_0)}}\n$$\n\n- **K:** Carrying capacity (maximum possible value)  \n- **r:** Growth rate  \n- **$t_0$:** Inflection point (time at which growth is fastest)\n\n**Characteristics:**\n\n- Symmetric S-curve around the inflection point  \n- Growth slows as it approaches K due to saturation  \n- Easy to interpret: **K directly gives maximum potential size**\n\n**Use Cases:**\n\n- When saturation is **symmetric** around the midpoint  \n- Customer adoption where growth initially accelerates, then decelerates evenly\n\nRelated:\n- [[Customer Growth Modeling]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "logistic_model_curve",
    "outlinks": [
      "customer_growth_modeling"
    ],
    "inlinks": [
      "customer_growth_modeling"
    ]
  },
  {
    "category": "DS",
    "filename": "Normalisation",
    "sha": "cd0b762a590d0f68bacf4e1475fa10981473dfd0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Normalisation.md",
    "text": "Standardizing data distributions for consistency. \n\nIn ML:\n- [[Z-Normalisation]]\n- [[Standardisation]]\n- [[Normalisation vs Standardisation]]\n- [[Batch Normalisation]]\n\nIn [[Data Engineering]]:\n- [[Normalisation of data]]\n- [[Normalised Schema]]\n- [[How to normalise a merged table]]\n\nIn [[NLP]]:\n- [[Normalisation of Text]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "normalisation",
    "outlinks": [
      "data_engineering",
      "z-normalisation",
      "normalisation_vs_standardisation",
      "normalised_schema",
      "standardisation",
      "how_to_normalise_a_merged_table",
      "normalisation_of_text",
      "batch_normalisation",
      "normalisation_of_data",
      "nlp"
    ],
    "inlinks": [
      "anomaly_detection",
      "feature_scaling",
      "melt",
      "neural_network",
      "normalisation_vs_standardisation",
      "z-score"
    ]
  },
  {
    "category": "DS",
    "filename": "Mean Absolute Percentage Error",
    "sha": "39dbd61aab135f53305110b7b91f0130ad143682",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Mean%20Absolute%20Percentage%20Error.md",
    "text": "MAPE is a metric for evaluating forecasting or regression models. It measures the **average absolute error between predicted and actual values**, expressed as a **percentage of the actual values**. This makes it intuitive for communicating model accuracy to stakeholders or management. Lower values indicate better model performance.\n\n**Formula:**\n\n$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$\n\n**Notes:**\n\n* Useful for comparing errors across different scales.\n* Sensitive to very small actual values, which can inflate the percentage error.\n\nRelated:\n- [[Regression Metrics]]\n- [[Mean Absolute Error]]\n\n### MAPE vs RMSE\n\nFor an initial forecast model, use MAPE over RMSE because it provides a relative measure of error that's easy to interpret. By expressing errors as a percentage of actual values, MAPE is scale-independent and gives a intuitive sense of how far off forecasts are.\n\nMAPE also treats all periods more evenly, while [[Root Mean Squared Error]] gives extra weight to large errors, which can skew evaluation if outliers or volatile periods exist.\n\nMAPE provides a simple, easy-to-read overview of forecast accuracy on test data and helps guide improvements before moving on to more sensitive metrics like RMSE or MAE.",
    "aliases": [
      "MAPE"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "mean_absolute_percentage_error",
    "outlinks": [
      "root_mean_squared_error",
      "mean_absolute_error",
      "regression_metrics"
    ],
    "inlinks": [
      "evaluating_time_series_forecasts",
      "regression_metrics",
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Out-of-sample rolling forecast evaluation",
    "sha": "73b4c1cfaecf71ebab31c8327c4c58c5418d9a2e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Out-of-sample%20rolling%20forecast%20evaluation.md",
    "text": "Out-of-sample rolling forecast evaluation (also called *walk-forward validation*) is a way of testing time series models by simulating real-world forecasting conditions. At each step, the model is trained only on the data available up to that point, then asked to predict future values. These forecasts are then compared against the actual observed outcomes.  \n\n## Why It Matters\n- Forecasting isn’t about explaining the past, it’s about predicting the future.  \n- Out-of-sample testing mimics the **real-world process** of making forecasts without knowledge of future values.  \n- Provides a realistic measure of model reliability across different forecast horizons.  \n## Key Insights\n- **Short-term forecasts (1–2 steps ahead):** Usually more accurate and reliable.  \n- **Medium/long-term forecasts (3+ steps ahead):** Errors often grow as uncertainty compounds.  \n- Helps quantify **how far ahead forecasts remain trustworthy** before performance breaks down.  \n\n## Practical Use\n- In operational contexts (e.g., [[Energy Demand Forecasting]]), this method tells you:  \n  - *How much confidence can I place in a 1-month ahead forecast?*  \n  - *Does accuracy degrade too quickly at 3 or 6 months ahead?*  \n- Supports better decision-making by aligning planning horizons with forecast reliability.  \n\n## Related\n-  [[Train-test splits for time series]]\n- [[Confidence Interval]]  \n- [[Holt-Winters (Exponential Smoothing)]]\n- [[Model Evaluation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "out-of-sample_rolling_forecast_evaluation",
    "outlinks": [
      "energy_demand_forecasting",
      "train-test_splits_for_time_series",
      "holt-winters_(exponential_smoothing)",
      "confidence_interval",
      "model_evaluation"
    ],
    "inlinks": [
      "evaluating_time_series_forecasts"
    ]
  },
  {
    "category": "DS",
    "filename": "PACF Plots",
    "sha": "c6bb8a2216f382ae8b82b9d87195dc7608143f1f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/PACF%20Plots.md",
    "text": "The Partial Autocorrelation Function (PACF) plot shows how a time series is correlated with its **lagged values**, ==while **controlling for correlations at shorter lags**.== For instance, in a series of air passengers, PACF reveals how much each observation is directly influenced by past values and identifies how far back statistically significant correlations exist.\n\nPACF differs from the [[ACF Plots]] in that it **isolates direct correlations**, removing the effects of intermediate lags. Examining **both ACF and PACF together** provides a more complete view of the time series structure.\n\n### **Purpose**\n\nThe PACF is used to:\n\n* Identify **==direct== relationships** between a time series and its lagged values.\n* Determine the **order $p$** for an **autoregressive (AR) model**.\n* Complement the ACF in diagnosing stationarity, seasonality, and the underlying process type (AR, MA, ARMA).\n\n### **How to Interpret a PACF Plot**\n\n* **Significant spike at lag $k$** → indicates the direct influence of $X_{t-k}$ on $X_t$.\n* **Sharp cutoff after lag $p$** → suggests an **AR($p$) process**.\n* **Gradual decay** → may reflect a moving average (MA) or mixed ARMA process.\n* **Few isolated spikes** → implies only short-term dependencies.\n\n### **Additional Notes**\n\n* The **first significant lag** in PACF often determines the **AR order $p$**.\n* PACF complements ACF — together they guide **model identification for ARIMA**.\n* If **both ACF and PACF decay slowly**, the series is likely non-stationary.\n\n[[Decomposition in Time Series]]\n[[ACF Plots]]\n\n\n![[Pasted image 20250909160420.png]]\n\nDataset\n\n![[Pasted image 20250909160435.png | 500]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "pacf_plots",
    "outlinks": [
      "pasted_image_20250909160435.png",
      "acf_plots",
      "decomposition_in_time_series",
      "pasted_image_20250909160420.png"
    ],
    "inlinks": [
      "acf_plots",
      "arima",
      "autocorrelation",
      "decomposition_in_time_series",
      "sarima"
    ]
  },
  {
    "category": "DS",
    "filename": "Performance Dimensions",
    "sha": "3d3f0a124e631522afd2ba6dfe70afb75e1afb40",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Performance%20Dimensions.md",
    "text": "Efficiency & Performance  \n- [[Cost-efficiency]]: Ensuring that the solutions used are cost-effective and provide value for money.  \n- [[Speed]]: The ability to process and analyze data quickly to meet business needs.  \n- [[Performance]]: Optimizing query performance by organizing data in a way that supports efficient retrieval.\n\n Flexibility & Scalability  \n- [[Flexibility]]: The capability to adapt to changing requirements and data sources.  \n- [[Scalability]]: Ensuring the system can handle increasing volumes of data without performance degradation. Can handle large volumes of data, making it suitable for enterprise-level data warehousing.  \n- [[Reusability]]: Designing components that can be reused across different projects or processes.  \n- [[Latency]]: Minimizing delays in data processing and retrieval.  \n\n Usability & Accessibility  \n- [[Simplicity]]: Keeping the system easy to manage and understand, reducing complexity.  \n- [[Usability]]: Providing a clear and intuitive structure that is easy for business users to understand and navigate.  \n- [[Accessibility]]: Ensuring that data is accessible when needed by authorized users.\n- [[Capability]]\n- [[Durability]]\n\n Data Quality & Integrity  \n- [[Data Integrity]]: Ensuring data accuracy and consistency.  \n- [[Data Quality]]: Maintaining high-quality data for reliable insights.  \n- [[Availability]]: Ensuring that data is available when required by authorized users.  \n\n [[Interoperability]]  \n- [[Interoperability]]: Ensuring that different systems and tools can work together seamlessly.  \n- [[Compatibility]]: Ensuring data is compatible with different systems.\n- [[portability]]\n\n\n[[Maintainability]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "portal",
      "term"
    ],
    "normalized_filename": "performance_dimensions",
    "outlinks": [
      "availability",
      "usability",
      "accessibility",
      "compatibility",
      "interoperability",
      "speed",
      "data_integrity",
      "performance",
      "durability",
      "cost-efficiency",
      "flexibility",
      "simplicity",
      "capability",
      "data_quality",
      "latency",
      "portability",
      "reusability",
      "scalability",
      "maintainability"
    ],
    "inlinks": [
      "data_lifecycle_management",
      "data_principles",
      "dimensional_modelling",
      "latency"
    ]
  },
  {
    "category": "DS",
    "filename": "Properties of Time Series Models",
    "sha": "315aac8f2026d7da8cc279a72b1c48c29635ef78",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Properties%20of%20Time%20Series%20Models.md",
    "text": "Properties of time series\n- Mean Reversion\n- Presence of Unit Root, autoregressive term indicating nonstationarity.\n- Hurst Component: ($H$) is a measure used to characterize the long-term memory of time series. \n### Mean reversion VS stationarity\n\nTesting for mean reversion and testing for stationarity are related but distinct concepts in time series analysis.\n\n**Key Differences**:\n- Mean reversion testing is focused on whether a time series will return to a specific level (the mean).\n- Stationarity testing checks if the overall statistical properties of the series remain consistent over time.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "properties_of_time_series_models",
    "outlinks": [],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Prophet",
    "sha": "c4f8d329fdba2a3208c6623ae4962247f98ceb92",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Prophet.md",
    "text": "**Prophet** is a [[Time Series Forecasting]] method developed by [[facebook]] for modeling time series data with **trend**, **seasonality**, and **holiday effects**. It is designed to handle business time series with strong seasonal patterns and missing data.\n\n### How it Works\n\nProphet models a time series as an additive combination of components:\n\n$$y(t) = g(t) + s(t) + h(t) + \\epsilon_t$$\n\nwhere:\n\n* $g(t)$ - trend component (piecewise linear or logistic growth)\n* $s(t)$ - seasonal component (modeled with Fourier series)\n* $h(t)$ - holiday effects\n* $\\epsilon_t$ - error term (irregular noise)\n\nIt fits these components using [[curve fitting]] and regression, rather than classical statistical assumptions (e.g., stationarity).\n### Benefits\n\n* **Interpretable:** Components (trend, seasonality, holidays) are explicitly modeled and easy to visualize.\n* **Robust:** Handles missing data, outliers, and irregular time series.\n* **Flexible:** Captures both additive and multiplicative seasonal effects.\n* **Automatic:** Minimal tuning required - works well with default settings.\n* **Scalable:** Efficient for large datasets and multiple time series.",
    "aliases": [],
    "date modified": "16-10-2025",
    "tags": [
      "analysis",
      "forecasting",
      "ml_process",
      "time_series",
      "ML_Tools"
    ],
    "normalized_filename": "prophet",
    "outlinks": [
      "curve_fitting",
      "time_series_forecasting",
      "facebook"
    ],
    "inlinks": [
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Random Forest Regression",
    "sha": "83a3b98be2ba13833c22ca01c14863a380d1313b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Random%20Forest%20Regression.md",
    "text": "Random Forest Regression: Like random forests for classification, random forest regression combines multiple regression trees to improve prediction accuracy.\n\nSee:\n- [[Random Forest]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "regressor"
    ],
    "normalized_filename": "random_forest_regression",
    "outlinks": [
      "random_forest"
    ],
    "inlinks": [
      "machine_learning_algorithms",
      "random_forest"
    ]
  },
  {
    "category": "DS",
    "filename": "Residuals Analysis",
    "sha": "36ca5bac19e29966e8a5e92df8337730fe0c11bd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Residuals%20Analysis.md",
    "text": "Residual analysis is the process of examining the difference between observed values and the values predicted by a model. These differences, known as residuals, indicate what the model has *not* explained. Studying residuals helps determine whether the model is appropriate and whether there are patterns left to model.\n\n## What are residuals?\n\nFor a model prediction $\\hat{y}_t$, the residual is:\n\n$e_t = y_t - \\hat{y}_t$\n\nResiduals represent unexplained variation in the data. If the model is suitable, this unexplained component should resemble random noise.\n## Why residual analysis matters\n\nResidual analysis provides diagnostic information about model fit. It can reveal:\n\n* Missed structure in the data (trend, seasonality, cycles).\n* Remaining autocorrelation or lagged effects.\n* Nonlinear relationships not captured by the model.\n* Variance changes (heteroskedasticity).\n* Deviations from normality that may affect inference.\n* Outliers or one-off events influencing the model.\n\nResiduals that show patterns suggest the model can be improved.\n\n## What good residuals look like\n\nA well-specified model produces residuals with these properties:\n\n* Mean close to $0$.\n* No visible trend or seasonality.\n* Constant variance through time.\n* No significant autocorrelation.\n* Roughly Gaussian distribution.\n\nThis behaviour resembles **white noise**, indicating the model has extracted all systematic structure from the data.\n## Common diagnostic tools\n\n### Plots\n\n* **Residual time plot:** should show no structure or drift.\n* **ACF/PACF:** should have no significant autocorrelation.\n* **Histogram/Density plot:** should appear symmetric and bell-shaped.\n* **Q-Q plot:** points should align with the $45^\\circ$ reference line.\n* **Residuals vs fitted values:** should show no patterns or funnels.\n\n### Statistical tests\n\n* **Normality tests:** Jarque–Bera, Shapiro–Wilk.\n* **Autocorrelation tests:** Ljung–Box or Box–Pierce.\n* **Heteroskedasticity tests:** Breusch–Pagan or ARCH tests.\n## When residuals indicate problems\n\nPatterns in residuals often reveal specific issues:\n\n* **Trend in residuals:** the model has missed an underlying trend.\n* **Seasonal patterns:** missing seasonal components.\n* **Clusters of volatility:** heteroskedasticity; consider GARCH-type models.\n* **Significant autocorrelation:** the model is under-differenced or missing lag structure.\n* **Skewness or heavy tails:** distributional assumptions may not hold.\n\nThese signals suggest that model refinement, transformation, or a different modelling approach may be needed.",
    "aliases": [],
    "date modified": "9-11-2025",
    "tags": [
      "analysis",
      "ml_process",
      "statistics"
    ],
    "normalized_filename": "residuals_analysis",
    "outlinks": [],
    "inlinks": [
      "arima",
      "decomposition_in_time_series",
      "holt-winters_vs_arima"
    ]
  },
  {
    "category": "DS",
    "filename": "Rolling Mean vs Cumulative Mean",
    "sha": "0f796aa5cd6d1572d652f694b651f1070608c496",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Rolling%20Mean%20vs%20Cumulative%20Mean.md",
    "text": "The rolling mean and cumulative mean are both methods for smoothing or summarising data over time, but they differ in *scope* and *window size*.\n### Rolling Mean (Moving Average)\n\n* Definition:  The *rolling mean* calculates the average of a fixed-size window that moves across the dataset.\n* Purpose:  Captures *local trends* and smooths short-term fluctuations.\n* Example:\n\n  ```\n  Data: [2, 4, 6, 8, 10]\n  Window = 3\n  Rolling mean = [NaN, NaN, 4, 6, 8]\n  ```\n\n  (Each value is the mean of the previous 3 observations.)\n\n* Use Cases:\n\n  * Smoothing noisy data.\n  * Detecting local trends or seasonal effects.\n  * Feature creation in time series models.\n\n### Cumulative Mean (Expanding Mean)\n\n* Definition:  The *cumulative mean* computes the average from the start of the dataset up to the current point.\n\n* Purpose:\n  Shows the *long-term average* as more data becomes available.\n\n* Example:\n  ```\n  Data: [2, 4, 6, 8, 10]\n  Cumulative mean = [2, 3, 4, 5, 6]\n  ```\n\n  (Each value is the mean of all previous observations up to that point.)\n\nUse Cases:\n  * Monitoring convergence of a process.\n  * Observing long-term stability.\n  * Evaluating cumulative performance metrics.\n### Comparison Summary\n\n| Feature          | Rolling Mean                    | Cumulative Mean             |\n| ---------------- | ------------------------------- | --------------------------- |\n| Window Size  | Fixed (e.g., last 7 days)       | Expanding (start → current) |\n| Focus        | Local trends                    | Global, long-term trend     |\n| Sensitivity  | Reacts to short-term changes    | Smoothed by all past values |\n| Example Tool | `Series.rolling(window).mean()` | `Series.expanding().mean()` |",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "analysis",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "rolling_mean_vs_cumulative_mean",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "SHapley Additive exPlanations",
    "sha": "21f80f8010a38e3ea4d64abf700c301be3b958cc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/SHapley%20Additive%20exPlanations.md",
    "text": "SHAP provides a unified approach to measure [[Feature Importance]] by computing the contribution of each feature to each prediction, based on game theory.\n\n### Key Points\n\n- **Purpose**: SHAP provides consistent and locally accurate explanations by assigning each feature ==an importance value based== on Shapley values from cooperative game theory.\n\n- **How it Works**: \n  - It calculates how each feature contributes to the model's output by comparing predictions with and without the feature, across various feature value combinations.\n\n- **Use Cases**: Suitable for complex models like neural networks, random forests, or gradient boosting machines, where internal logic is difficult to understand.\n\n- **Advantage**: \n  - Provides global explanations (model-wide feature importance) and local explanations (individual prediction reasons).\n\n- **Scenario**: \n  - A financial institution uses a black-box XGBoost model to predict whether a loan applicant should be approved. The model takes several factors into account, such as credit score, income, employment history, and outstanding debts.\n  - **SHAP Explanation**: For a specific loan rejection case, SHAP values reveal that the applicant’s high debt-to-income ratio and short employment history contributed the most to the rejection decision. These factors had the highest negative SHAP values for this prediction, providing actionable insights to both the applicant and the loan officers.\n\n### Example Code\n\nTo compute SHAP values, you can use the SHAP library to interpret feature importance for any machine learning model:\n\n```python\nimport shap\n\n# Explain the model's predictions using SHAP\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Plot the summary plot of feature importance\nshap.summary_plot(shap_values, X_test)\n```",
    "aliases": [
      "SHAP"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "modeling"
    ],
    "normalized_filename": "shapley_additive_explanations",
    "outlinks": [
      "feature_importance"
    ],
    "inlinks": [
      "feature_importance",
      "model-agnostic_feature_importance",
      "model_interpretability",
      "use_of_rnns_in_energy_sector"
    ]
  },
  {
    "category": "DS",
    "filename": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "sha": "3c1c0e3544a14e2ff001b6168cc847e18db8b8eb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/SMOTE%20(Synthetic%20Minority%20Over-sampling%20Technique).md",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\n\nSee: [[Resampling]]\n\nGenerate synthetic samples for the minority class by interpolating between existing samples.\n\nSMOTE: This technique generates synthetic samples for the minority class (female resumes) by creating new instances that are interpolations of existing ones.",
    "aliases": [
      "SMOTE"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "smote_(synthetic_minority_over-sampling_technique)",
    "outlinks": [
      "resampling"
    ],
    "inlinks": [
      "imbalanced_datasets",
      "imbalanced_datasets_smote.py"
    ]
  },
  {
    "category": "DS",
    "filename": "STL Decomposition",
    "sha": "dd787183bf426799bb00c534b493686c1f091a97",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/STL%20Decomposition.md",
    "text": "When trend and seasonal patterns in a time series are messy or evolving, we use STL to extract them.\n\nTrends may change gradually, and seasonal behaviours can vary year to year. This is why we need a more adaptable approach than classical decomposition.\n\n```python\nfrom statsmodels.tsa.seasonal import STL\n```\n\nSeasonal-Trend decomposition using LOESS\n* Unlike classical decomposition, ==STL allows seasonal patterns to change gradually.==\n* Better suited for real-world, non-stationary data.\n* Produces adaptive seasonality and cleaner residuals compared to classical methods.\n* Useful when you want to deseasonalize but the seasonality is not straightforward.\n\nKey ideas:\n* STL identifies one-off events (e.g., COVID dip) as irregular, not trend or seasonality.\n* LOESS (Locally Estimated Scatterplot Smoothing) underpins STL, refining trend and seasonal estimates iteratively.\n* Initial estimates use moving averages (trend via 12-month MA, seasonality via grouped monthly averages).\n* Refinement ensures the seasonal component is centered (mean = 0 per cycle), preventing contamination of the trend.\n\nTakeaway:\nSTL is more flexible and robust than classical decomposition, making it a preferred choice when patterns evolve over time.\n\nRelated:\n- [[Decomposition in Time Series]]\n\nResources:\n- https://towardsdatascience.com/time-series-forecasting-made-simple-part-3-1-stl-decomposition-understanding-initial-trend-and-seasonality-prior-to-loess-smoothing/\n\nClassical\n\n![[Pasted image 20250909171538.png]]\n\nWith STL\n\n![[Pasted image 20250909171641.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "stl_decomposition",
    "outlinks": [
      "pasted_image_20250909171538.png",
      "pasted_image_20250909171641.png",
      "decomposition_in_time_series"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "decomposition_in_time_series",
      "seasonality_in_time_series",
      "stationary_time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Scatter Plots",
    "sha": "fac43968fa9fd4a078b862d7c91e4c5f7d3ea214",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Scatter%20Plots.md",
    "text": "Use hue for a 3rd (discrete or continuous) variable in 2d plots.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "scatter_plots",
    "outlinks": [],
    "inlinks": [
      "anomaly_detection",
      "data_visualisation",
      "feature_selection"
    ]
  },
  {
    "category": "DS",
    "filename": "Scientific Method",
    "sha": "291d30f187a0104aeb34664f8bd0b587728c2153",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Scientific%20Method.md",
    "text": "### Step 1: Start with Data\n\n- **Collect Data**: Gather all relevant data sources that might be useful for your analysis.\n- **Understand Data**: Familiarize yourself with the data types, structures, and any existing metadata.\n- **Clean Data**: Perform data cleaning to handle missing values, outliers, and inconsistencies.\n\n### Step 2: Develop Intuitions\n\n- **Explore Data**: Use exploratory [[Data Analysis]] ([[EDA]]) techniques to visualize and summarize the data.\n- **Identify Patterns**: Look for trends, correlations, and anomalies that might inform your understanding.\n- **Ask Preliminary Questions**: Consider what initial questions the data might help answer.\n### Step 3: Formulate Your Question\n\n- [[Problem Definition]]: Clearly articulate the problem you are trying to solve.\n- **Set Objectives**: Determine what you aim to achieve with your analysis.\n- **Consider Stakeholders**: Ensure the question aligns with business goals and stakeholder interests.\n\n### Step 4: Validate the Question\n\n- **Test Feasibility**: Use the current data to assess whether the question is answerable.\n- **Iterate**: Refine the question based on initial findings and feedback.\n- **Formulate Hypothesis**: Develop a testable hypothesis that can guide your analysis.\n\n### Step 5: Create a Testing Framework\n\n- **Design Experiments**: Plan how you will test your hypothesis, including control and experimental groups if applicable.\n- **Select Methods**: Choose appropriate statistical or machine learning methods for analysis.\n- **Prepare Tools**: Set up the necessary tools and environments for running experiments.\n\n### Step 6: Analyze Results\n\n- **Run Experiments**: Execute your tests and collect results.\n- **Interpret Data**: Use quantitative metrics to analyze the outcomes.\n- **Draw Insights**: Identify key insights and patterns that answer your question.\n\n### Step 7: Assess Impact\n\n- **Define Success Metrics**: Determine how you will measure success (e.g., accuracy, ROI, user engagement).\n- **Evaluate Impact**: Assess the potential impact of your solution on the business.\n- **Communicate Findings**: Present your results and recommendations to stakeholders.\n\n### Additional Considerations\n\n- **Iterative Process**: Be prepared to revisit and refine each step as new insights emerge.\n- **[[Documentation]]**: Keep thorough documentation of your process, findings, and decisions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "scientific_method",
    "outlinks": [
      "documentation",
      "eda",
      "data_analysis",
      "problem_definition"
    ],
    "inlinks": [
      "data_science",
      "knowledge_work",
      "thinking_systems"
    ]
  },
  {
    "category": "DS",
    "filename": "Scipy",
    "sha": "343eaf519942510befce64e1d1b309bc4bb235da",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Scipy.md",
    "text": "Python package.\n## expit\n\nThe function `expit` from `scipy.special` is a numerically stable implementation of the sigmoid function, `expit` handles these cases safely and efficiently using internal tricks like: Clipping, Log-exp identities, SIMD-optimized instructions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "normalized_filename": "scipy",
    "outlinks": [],
    "inlinks": [
      "scikit-learn"
    ]
  },
  {
    "category": "DS",
    "filename": "Seasonality in Time Series",
    "sha": "8d274fc5c005eda53aef5d10916d18aa1b64f9c0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Seasonality%20in%20Time%20Series.md",
    "text": "Seasonality refers to patterns that repeat at fixed intervals (daily, weekly, monthly, or yearly).\nExample: higher sales of cold drinks during summer.\n#### Ways to Handle Seasonality\n\n1. Seasonal Differencing\n   Remove seasonal effects by subtracting the value from the same season in a previous cycle:\n\n   $y'_t = y_t - y_{t-s}$\n\n   where $s$ is the seasonal period (e.g., $s = 12$ for monthly data with yearly seasonality).\n\n2. Break the series into trend (T), seasonal (S), and residual (R) components:\n\t1. [[Decomposition in Time Series]]\n\t2. [[Additive vs Multiplicative Models Time Series]]\n\t3. [[STL Decomposition]]\n\n3. Seasonal Dummy Variables\n   Encode seasonality as categorical indicators (e.g., month or quarter dummies) to use as features in machine learning models.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "seasonality_in_time_series",
    "outlinks": [
      "stl_decomposition",
      "additive_vs_multiplicative_models_time_series",
      "decomposition_in_time_series"
    ],
    "inlinks": [
      "acf_plots",
      "decomposition_in_time_series",
      "evolving_seasonality",
      "forecasting_using_lags",
      "holt-winters_vs_arima"
    ]
  },
  {
    "category": "DS",
    "filename": "Seasonal Naive Forecast",
    "sha": "bfb24d82a74f18c9542ac35f66203eefb0eb54e5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Seasonal%20Naive%20Forecast.md",
    "text": "Seasonal Naive Forecast\nAssumes the value will repeat from the same point last season (e.g., last week or last month).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "seasonal_naive_forecast",
    "outlinks": [],
    "inlinks": [
      "baseline_forecast"
    ]
  },
  {
    "category": "DS",
    "filename": "Shot Learning",
    "sha": "4862ce299b594e20199ded247d2df54dec92c378",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Shot%20Learning.md",
    "text": "Zero-shot learning (ZSL) is a machine learning approach where a model correctly predicts classes it has **never seen during training**, using **semantic knowledge** (e.g., descriptions, attributes, embeddings) to transfer understanding from known classes to unseen ones.\n\nExample: A model trained on animals (dog, cat) can identify a zebra by leveraging textual descriptions like “striped horse-like animal.”",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      null
    ],
    "normalized_filename": "shot_learning",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "Silhouette Analysis",
    "sha": "7cff4ea0f46d365418769562689abc7ae529509c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Silhouette%20Analysis.md",
    "text": "Silhouette analysis is a technique used to evaluate the quality of clustering results. It provides a measure of how similar an object is to its own cluster compared to other clusters. This analysis helps in determining the appropriateness of the number of clusters and the consistency within clusters.\n\nOverall, silhouette analysis is a valuable tool for assessing the quality of [[Clustering]] results and making informed decisions about the number of clusters and the clustering algorithm's effectiveness.\n\n[Sklearn link](https://scikit-learn.org/1.5/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n### Key Concepts:\n\n **Silhouette Score:** For each data point, the silhouette score is calculated using the following formula:\n \n  $s(i) = \\frac{b(i)  a(i)}{\\max(a(i), b(i))}$\n  \n  where:\n   $a(i)$ is the average distance between the data point $i$ and all other points in the same cluster.\n   $b(i)$ is the average distance between the data point $i$ and all points in the nearest cluster (the cluster with the smallest average distance to $i$).\n\n **Interpretation of Silhouette Scores:**\n   - A silhouette score ranges from -1 to 1.\n   - A score close to 1 indicates that the data point is ==well clustered== and far from neighboring clusters.\n   - A score close to 0 indicates that the data point is on or very close to the decision boundary between two neighboring clusters.\n   - A negative score indicates that the data point might have been assigned to the wrong cluster.\n\nExploratory Questions:\n- How does silhouette score behave in high-dimensional embedding spaces?\n- Can it be trusted when clusters have unequal sizes or densities?\n\n### Silhouette Plot:\n\n A silhouette plot displays the silhouette scores of all data points in a dataset. It provides a visual representation of how well each data point lies within its cluster.\n \n The plot is divided into regions, each corresponding to a cluster, and the width of each region represents the average silhouette score of the points in that cluster.\n\nGood\n \n![[Pasted image 20241231172403.png]]\n\nBad\n\n![[Pasted image 20241231172459.png]]\n\n### Applications:\n\n **Determining the Optimal Number of Clusters:** By calculating the average silhouette score for different numbers of clusters, one can identify the number of clusters that results in the highest average silhouette score, indicating the best clustering structure.\n \n **Cluster Validation:** Silhouette analysis helps in validating the consistency within clusters and identifying potential misclassifications.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "clustering"
    ],
    "normalized_filename": "silhouette_analysis",
    "outlinks": [
      "pasted_image_20241231172403.png",
      "clustering",
      "pasted_image_20241231172459.png"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters",
      "cluster_density",
      "cluster_seperation"
    ]
  },
  {
    "category": "DS",
    "filename": "Simple Exponential Smoothing (SES)",
    "sha": "88b9601c3d719f83b65cc289b4f2d9347f6728ef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Simple%20Exponential%20Smoothing%20(SES).md",
    "text": "Simple Exponential Smoothing is a forecasting method designed for time series ==without trend or seasonality==. It applies a single smoothing parameter $\\alpha$ to update the series’ **level**, producing forecasts that are constant over time. The forecasts adapt gradually to changes in the average level but do not account for upward or downward trends.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "simple_exponential_smoothing_(ses)",
    "outlinks": [],
    "inlinks": [
      "exponential_smoothing",
      "holt’s_linear_trend_model_(double_exponential_smoothing)"
    ]
  },
  {
    "category": "DS",
    "filename": "SparseCategorialCrossentropy or CategoricalCrossEntropy",
    "sha": "e6d6d0b28c333ff7b024c303d40eb5301a6e8445",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/SparseCategorialCrossentropy%20or%20CategoricalCrossEntropy.md",
    "text": "To understand the differences and use cases for `SparseCategoricalCrossentropy` and `CategoricalCrossentropy` in [[Tensorflow]], let's break down each one:\n\n### CategoricalCrossentropy\n\n- **Use Case**: This [[Loss function]] is used when you have one-hot encoded labels. [[One-hot encoding]] means that each label is represented as a vector with a length equal to the number of classes, where the correct class is marked with a 1 and all other classes are marked with 0s.\n- **Example**: If you have three classes, a label might look like `[0, 1, 0]` for class 2.\n- **Functionality**: It calculates the [[Cross Entropy]] loss between the true labels and the predicted probabilities.\n\n### SparseCategoricalCrossentropy\n\n- **Use Case**: This loss function is used when your labels are integers instead of one-hot encoded vectors. Each label is represented by a single integer corresponding to the correct class.\n- **Example**: If you have three classes, a label might simply be `1` for class 2.\n- **Functionality**: It also calculates the cross-entropy loss but expects the labels to be in integer form, which can be more memory efficient.\n\n### Key Differences\n\n- **Input Format**: The main difference is the format of the labels. `CategoricalCrossentropy` requires one-hot encoded labels, while `SparseCategoricalCrossentropy` works with integer labels.\n- **Efficiency**: `SparseCategoricalCrossentropy` can be more efficient in terms of memory and computation, especially when dealing with a large number of classes.\n\n### When to Use Which\n\n- Use `CategoricalCrossentropy` if your labels are already one-hot encoded or if you prefer to work with one-hot encoded labels for any specific reason.\n- Use `SparseCategoricalCrossentropy` if your labels are integers, which is often the case when labels are directly loaded from datasets.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "deep_learning"
    ],
    "normalized_filename": "sparsecategorialcrossentropy_or_categoricalcrossentropy",
    "outlinks": [
      "tensorflow",
      "cross_entropy",
      "one-hot_encoding",
      "loss_function"
    ],
    "inlinks": [
      "neural_network_in_practice"
    ]
  },
  {
    "category": "DS",
    "filename": "Stacking",
    "sha": "1d0c27e158819ce83184d1e1b1e56afb39befaff",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Stacking.md",
    "text": "Stacking in a [[Model Ensemble]] combines predictions of multiple base models ==by training a meta-model== on the outputs of the base models.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "stacking",
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": [
      "model_ensemble"
    ]
  },
  {
    "category": "DS",
    "filename": "Stationary Time Series",
    "sha": "0498e34a0129783cc6fcca4018db6536d12f42b2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Stationary%20Time%20Series.md",
    "text": "A stationary time series is one whose statistical properties do not change over time. Many classical time series models (e.g., [[ARIMA]], [[SARIMA]]) assume stationarity. Non-stationary data can lead to misleading results expecially in [[Time Series Forecasting]]. \n\nThis means the process has the same behavior regardless of when you observe it. Checking a series’ stationarity is important because most time series methods do not model non-stationary data effectively. \"Non-stationary\" is a term that means the trend in the data is not mean-reverting - it continues steadily upwards or downwards throughout the series’ timespan.\n\nFormally, a time series ${y_t}$ is stationary if:\n* The mean is constant: $E[y_t] = \\mu$ for all $t$.\n\t* ![[Pasted image 20250909163151.png]]\n* The variance is constant: $\\text{Var}(y_t) = \\sigma^2$ for all $t$ (the spread of data). If the time series goes up and down by similar amounts throughout the series, then it is said to have Constant Variance. More spread\n\t* ![[Pasted image 20250909163410.png]]\n* The autocovariance depends only on the lag $k$, not on the specific time $t$:\n  $\\text{Cov}(y_t, y_{t+k}) = \\gamma_k$. If the relationship between values depends only on the gap between them, regardless of when they occur, then there is Constant Autocovariance.\n\n### Types of stationarity\n\n1. Strict stationarity: The entire distribution of the process is invariant to shifts in time (all moments remain constant).\n2. Weak (or covariance) stationarity: Only the first two moments (mean, variance, covariance) are invariant. This weaker definition is often sufficient for models like [[ARIMA]].\n\n### Examples\n\nStationary: White noise (mean = 0, variance constant).\n\nNon-stationary:\n  * Series with a trend (e.g., increasing sales over time).\n  * Series with changing variance (e.g., volatility clustering in finance).\n  * Series with seasonality (patterns repeating over time).\n\n### Trnasfomration to get Stationarity\n\nCommon [[Data Transformation]] to achieve stationarity include:\n- [[Differencing in Time Series]]\n* [[STL Decomposition]]\n* [[Log transformation]] (to stabilize variance)\n* Detrending or deseasonalising\n\n### Tests for Stationarity\n\nPractical checks:\n- An easy way to check for constant mean and variance is to chop up the data into separate chunks.\n- Then, one calculates statistics for each chunk, and compare them. \n- Large deviations in either the mean or the variance among chunks might indicate that the time series is nonstationary.\n\n[[Statistical Tests]]:\n- [[ADF Test]] & [[KPSS Test]]: Augmented Dickey-Fuller test give statistical justification to what our eyes see. If the the p-value is not less than 0.05, we must assume the series is non-stationary.\n- Visual inspection with [[STL Decomposition]]\n- [[ACF Plots]]\n\n### Related\n\n- [[Decomposition in Time Series]]\n- One time events (Interventions) be removed (think Covids impact of stock data). See [[Intervention Analysis]].\n- [[Decomposition in Time Series]]\n\nResources:\n- https://towardsdatascience.com/time-series-forecasting-made-simple-part-4-1-understanding-stationarity-in-a-time-series/",
    "aliases": [
      "stationarity"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "probability",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "stationary_time_series",
    "outlinks": [
      "arima",
      "time_series_forecasting",
      "decomposition_in_time_series",
      "data_transformation",
      "differencing_in_time_series",
      "stl_decomposition",
      "sarima",
      "statistical_tests",
      "adf_test",
      "kpss_test",
      "log_transformation",
      "pasted_image_20250909163410.png",
      "pasted_image_20250909163151.png",
      "intervention_analysis",
      "acf_plots"
    ],
    "inlinks": [
      "acf_plots",
      "adf_test",
      "arima",
      "arima_vs_random_forest_in_time_series",
      "autocorrelation",
      "decomposition_in_time_series",
      "differencing_in_time_series",
      "feature_engineering_for_time_series",
      "forecasting_with_autoregressive_(ar)_models",
      "holt-winters_vs_arima",
      "time_series",
      "time_series_forecasting"
    ]
  },
  {
    "category": "DS",
    "filename": "Time Series Forecasting",
    "sha": "2a6826bf51e50d747c7da33a43f943d1210aa1e5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Time%20Series%20Forecasting.md",
    "text": "When working with [[Time Series]] data, the main goal is often predicting future values based on historical patterns. Can be done:\n\n- Fast: [[Moving Average Forecast]]\n- Cheap: [[Exponential Smoothing]]\n- Good: Think [[SARIMA]]\n\nRequires (seasonality/trends can be handled separately):\n- [[Stationary Time Series]]: because the patterns remain consistent over time (Non stationary makes pattern detection difficult).\n\nMay use:\n- [[Forecasting using Lags]]\n\n#### Time Series Statistical Methods\n\nTraditional models that explicitly capture trend, seasonality, and autocorrelation in time series data.\n\nClassical Methods: Usually Beats SoTA methods\n- [[Baseline Forecast]]\n- [[Moving Average Forecast]]\n- [[Exponential Smoothing]]\n- [[ARIMA]] – Autoregressive Integrated Moving Average.\n- [[SARIMA]] – Seasonal ARIMA for multiple seasonal patterns.\n- [[Prophet]] – Handles seasonality and holidays well.\n- [[Multiple Linear Regression]]\n- [[Regression Metrics]]\n- [[Mean Absolute Percentage Error]]\n\nImplementations:\n- [[Forecasting_Baseline.py]] – Naive or simple baseline forecasting.\n- [[Forecasting_AutoArima.py]] – Automated ARIMA model selection.\n- See: [[Forecasting_Exponential_Smoothing.py]]\n#### Time Series Machine Learning Methods\n\nModern approaches that use feature-based forecasting or global models across multiple series. These methods require [[Feature Engineering for Time Series]], such as lag features, rolling windows, and time-based features (hour, day, week). Possible [[Data Leakage]] if not setup correctly.\n\nML/State of the Art Methods:\n- [[LSTM in Time Series]]\n\nExamples:\n- [[Random Forest]] for time series (global approach)\n- [[XGBoost]]\n- [[LightGBM]]\n\n#### Time Series Model Selection & Evaluation\n\nTo know if a forecasting model is good:\n- Use proper [[Evaluation Metrics]] such as MAE, RMSE, MAPE.\n- Apply time series cross-validation (rolling or expanding windows). See https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n- Time series cross validation techniques like Nested Cross Validation, Time Series Split Cross Validation, Blocked Cross Validation\n\nResources:\n[Time Series Forecasting Guide](https://simrenbasra.github.io/simys-blog/2024/09/19/timeseries_part2.html)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series",
      "ML_Tools"
    ],
    "normalized_filename": "time_series_forecasting",
    "outlinks": [
      "feature_engineering_for_time_series",
      "forecasting_using_lags",
      "lightgbm",
      "forecasting_autoarima.py",
      "moving_average_forecast",
      "xgboost",
      "multiple_linear_regression",
      "prophet",
      "forecasting_baseline.py",
      "regression_metrics",
      "baseline_forecast",
      "sarima",
      "lstm_in_time_series",
      "mean_absolute_percentage_error",
      "data_leakage",
      "exponential_smoothing",
      "forecasting_exponential_smoothing.py",
      "arima",
      "stationary_time_series",
      "time_series",
      "random_forest",
      "evaluation_metrics"
    ],
    "inlinks": [
      "arima",
      "arima_vs_random_forest_in_time_series",
      "data_analysis",
      "exponential_smoothing",
      "forecasting_exponential_smoothing.py",
      "google_sheets",
      "lstm",
      "lstm_in_time_series",
      "prediction_intervals_vs_confidence_interval",
      "prophet",
      "pycaret",
      "random_forest",
      "random_forest_for_time_series",
      "stationary_time_series",
      "time_series",
      "time_series_forecasts_in_business",
      "use_cases_for_a_simple_neural_network_like",
      "use_of_rnns_in_energy_sector"
    ]
  },
  {
    "category": "DS",
    "filename": "Time Series Forecasts in Business",
    "sha": "e3d6f9f65d3f7acdc9141e570014af82ea318ed7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Time%20Series%20Forecasts%20in%20Business.md",
    "text": "**What are you looking for in business when doing [[Time Series Forecasting]]?**\n\n* **Short-term planning:**\n  * Allocate resources efficiently (staff, inventory, production capacity).\n  * Optimize customer service and operational schedules.\n  * Plan promotions, logistics, and supply chain needs based on near-term demand.\n\n* **Long-term planning:**\n  * Guide strategic decisions, such as market expansion or capital investment.\n  * Forecast revenue growth or customer base changes—but with caution, as uncertainty increases over time.\n  * Incorporate risk management and scenario analysis for external factors (e.g., economic changes, competitor actions, seasonality).\n\n* **Decision support:**\n  * Identify trends, seasonality, and anomalies to inform business strategy.\n  * Detect early warning signs of declining demand or unexpected surges.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "time_series"
    ],
    "normalized_filename": "time_series_forecasts_in_business",
    "outlinks": [
      "time_series_forecasting"
    ],
    "inlinks": [
      "evaluating_time_series_forecasts"
    ]
  },
  {
    "category": "DS",
    "filename": "Time Series Shocks",
    "sha": "1892e3d277474160835d82383d93a33fd3381a0e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Time%20Series%20Shocks.md",
    "text": "Think Covid's impact on the stock market.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "time_series_shocks",
    "outlinks": [],
    "inlinks": [
      "adf_test",
      "arima",
      "holt-winters_(exponential_smoothing)",
      "holt-winters_vs_arima"
    ]
  },
  {
    "category": "DS",
    "filename": "Time Series Learning Resources",
    "sha": "4df28fa93b84d3456dc87b84be7a544d91ec2a5f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Time%20Series%20Learning%20Resources.md",
    "text": "### Courses\n* **Intel course on time series** — [Link](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/course-time-series-analysis.html)\n\n### Books & References\n* **Collection of links** for books, articles, frameworks, etc. on time series — [Link](https://github.com/ElizaLo/Time-Series)\n* **Book on Time Series Analysis (University of California)** — [Link](https://stats.libretexts.org/Bookshelves/Advanced_Statistics/Time_Series_Analysis_%28Aue%29)\n\n### Blog Posts\n* **Neptune blog** (inspired by the Intel course) — [Link](https://neptune.ai/blog/time-series-forecasting#:~:text=Pseudo%2Dadditive%20models%20combine%20the,related%20to%20the%20multiplicative%20model)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "time_series_learning_resources",
    "outlinks": [],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "DS",
    "filename": "Time Series",
    "sha": "1ae2c84b5375cbbf30cef664a683f056b0c7a2f6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Time%20Series.md",
    "text": "Time series data is a sequence of data points collected or recorded at successive points in time, typically at uniform intervals. Its key characteristic is the ==temporal ordering of observations,== which makes it essential for analyzing trends, patterns, and changes over time.\n\nInterest:\n* [[Time Series Forecasting]]\n* [[Anomaly Detection in Time Series]]\n* [[Decomposition in Time Series]]\n* [[Stationary Time Series]]\n- [[Feature Engineering for Time Series]]\n- [[MLOPS for Time Series]]\n- [[Additive vs Multiplicative Models Time Series]]\n- [[Growth Models in Time Series]]\n- [[Properties of Time Series Models]]\n- [[Clustering Time Series]]\n\nSetups:\n* [[Querying Time Series]]\n* [[Joining Time Series]]\n* [[Time Series Python Packages]]\n\nOther:\n* [[Basics of Time Series]]\n* [[Time Series Learning Resources]]\n\nRelevant Resources\n* [Rob J Hyndman Forecasting: Principles and Practice](https://otexts.com/fpp3/index.html)\n* [Time series analysis with Python](https://filippomb.github.io/python-time-series-handbook/notebooks/00/intro.html)\n* [mlcourse.ai](https://mlcourse.ai/book/topic09/topic09_intro.html)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "portal",
      "time_series",
      "ML_Tools",
      "algorithm"
    ],
    "normalized_filename": "time_series",
    "outlinks": [
      "feature_engineering_for_time_series",
      "joining_time_series",
      "time_series_forecasting",
      "growth_models_in_time_series",
      "decomposition_in_time_series",
      "basics_of_time_series",
      "stationary_time_series",
      "anomaly_detection_in_time_series",
      "time_series_python_packages",
      "time_series_learning_resources",
      "querying_time_series",
      "clustering_time_series",
      "additive_vs_multiplicative_models_time_series",
      "properties_of_time_series_models",
      "mlops_for_time_series"
    ],
    "inlinks": [
      "altair",
      "anomaly_detection_in_time_series",
      "arima_vs_random_forest_in_time_series",
      "baseline_forecast",
      "basics_of_time_series",
      "cross_validation",
      "datasets",
      "decomposition_in_time_series",
      "duckdb",
      "recurrent_neural_networks",
      "time_series_forecasting",
      "varmax"
    ]
  },
  {
    "category": "DS",
    "filename": "Trends in Time Series",
    "sha": "ed2f07a07faedfdae48f514316e529c16848373f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/Trends%20in%20Time%20Series.md",
    "text": "Trends are the overall direction your data is moving in over time  -  going up, down or staying flat.  \n* [[Differencing in Time Series]]: Subtract the previous value from the current value:\n  $$y'_t = y_t - y_{t-1}$$\n  Removes linear trends. Can apply higher-order differencing if needed.\n* Detrending with regression: Fit a line or curve to the data and use residuals.\n* Transformation: [[Log transformation]] or square root to stabilize growth rates.\n\nIn [[Exponential Smoothing]] can add a dampening term to. Which applies a factor (0 < $\\phi$< 1) that slows down the trend over time:\n- Damping is useful to avoid overly optimistic or pessimistic forecasts. Without damping, trends extend linearly into the future, which can be unrealistic (e.g., unchecked growth). A damped trend gradually flattens, reflecting real-world limits (like market saturation). This is especially relevant for the customer growth series.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "trends_in_time_series",
    "outlinks": [
      "log_transformation",
      "differencing_in_time_series",
      "exponential_smoothing"
    ],
    "inlinks": [
      "acf_plots",
      "decomposition_in_time_series",
      "holt-winters_vs_arima",
      "holt’s_linear_trend_model_(double_exponential_smoothing)"
    ]
  },
  {
    "category": "DS",
    "filename": "how do you do the data selection",
    "sha": "4570d756d38c1c8027cd8d0827b8c0296d677104",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/how%20do%20you%20do%20the%20data%20selection.md",
    "text": "When you sample a dataset, [[how do you do the data selection]]? [[Data Selection]]\nA: By randomly sampling, by time period (use a feature)..",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "selection"
    ],
    "normalized_filename": "how_do_you_do_the_data_selection",
    "outlinks": [
      "data_selection",
      "how_do_you_do_the_data_selection"
    ],
    "inlinks": [
      "how_do_you_do_the_data_selection"
    ]
  },
  {
    "category": "DS",
    "filename": "pmdarima",
    "sha": "354e4869cdd4b06edcc64d0d01ab9aa40f8aec32",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/pmdarima.md",
    "text": "Helps find [[Model Parameters]] for [[ARIMA]] models\n\n[[Forecasting_AutoArima.py]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "pmdarima",
    "outlinks": [
      "arima",
      "forecasting_autoarima.py",
      "model_parameters"
    ],
    "inlinks": [
      "arima",
      "time_series_python_packages"
    ]
  },
  {
    "category": "DS",
    "filename": "sklearn datasets",
    "sha": "ad1f6bbab049cb3a423c98187042e3eb629cc4d5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/sklearn%20datasets.md",
    "text": "[[Scikit-Learn]]\n[[Datasets]]\n\nmake a dataframe by \n\n```python\nds = datasets.load_dataset()\ndf = pd.DataFrame(ds.data,columns=ds.feature_names)\ndf.head()\n#add target column\ndf['target'] = ds.target\n```\n\nSklearn Datasets are a collection of datasets used \nfor testing machine learning algorithms.\n\nwith each dataset, we have keys:\n- data: np array \n- target\n- feature_names\n- DESCR\n- images\n- column names: textual ordered column names\n- ect",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration"
    ],
    "normalized_filename": "sklearn_datasets",
    "outlinks": [
      "datasets",
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "DS",
    "filename": "stack memory",
    "sha": "d1e72e7c7d2abf921520a356e0caf9900fe2c0cd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/data-science/stack%20memory.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "memory_management"
    ],
    "normalized_filename": "stack_memory",
    "outlinks": [],
    "inlinks": [
      "heap_memory"
    ]
  },
  {
    "category": "DL",
    "filename": "Deep Learning",
    "sha": "bb997242a6875347e692d68e7137e4bc43d55a3e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Deep%20Learning.md",
    "text": "Deep learning is a subset of machine learning that uses neural networks to process large-scale data for tasks like image and speech recognition, natural language processing, and recommendation systems. \n \n A neural network consists of layers of nodes where each node performs weighted sums of its inputs, applies activation functions like [[Relu]] or sigmoid, and produces an output. \n \n [[Backpropagation]] is the primary algorithm for training neural networks by minimizing error through [[Gradient Descent]]. Regularization techniques, such as dropout, prevent overfitting. \n \n Popular frameworks like [[PyTorch]] and [[Tensorflow]] facilitate deep learning model development.\n\nQuestions:\n- [[What is the role of gradient-based optimization in training deep learning models. ]]\n- [[Explain different gradient descent algorithms, their advantages, and limitations.]]\n\nAreas of Deep Learning:\n- [[LLM]]\n- [[Neural network|Neural Network]]\n\nFollow up questions\n - How does the choice of activation function affect the performance of deep learning models across different tasks?\n - What are the trade-offs between different gradient descent algorithms (e.g., [[Stochastic Gradient Descent|SGD]] vs. Adam) in training neural networks? See [[Optimisation techniques]].\n\nRelated Topics\n - [[Transfer Learning]]: Applying pre-trained models to new tasks.",
    "aliases": [
      "DL"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "deep_learning",
    "outlinks": [
      "gradient_descent",
      "relu",
      "llm",
      "transfer_learning",
      "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.",
      "optimisation_techniques",
      "tensorflow",
      "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models.",
      "pytorch",
      "backpropagation",
      "stochastic_gradient_descent",
      "neural_network"
    ],
    "inlinks": [
      "adam_optimizer",
      "convolutional_neural_networks",
      "cuda",
      "ds_&_ml_portal",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "model_parameters",
      "neural_network",
      "neural_network_classification",
      "optimising_neural_networks",
      "pytorch"
    ]
  },
  {
    "category": "DL",
    "filename": "Convolutional Neural Networks",
    "sha": "d4d21d1120ecad8b1e87512fed300736604ef006",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Convolutional%20Neural%20Networks.md",
    "text": "Convolutional networks, or CNNs, are specialized [[Deep Learning]] architectures designed for processing data with grid-like structures, such as images. \n\nThey use convolutional layers with learnable filters to extract spatial features from the input data. The convolutional operation involves sliding these filters across the input, performing element-wise multiplications and summations to create feature maps. \n\nCNNs are particularly effective for image classification, object detection, and image segmentation tasks.\n\nPrimarily used in image recognition and processing tasks. CNNs use convolutional layers to automatically detect spatial patterns in images, like edges and textures.\n\nPooling:\n\nThe idea of pooling in convolutional neural networks is to do two things:\n- Reduce the number of parameters in your network (pooling is also called “down-sampling” for this reason)\n- To make feature detection ([[Feature Extraction]]) more robust by making it more impervious to scale and orientation changes\n- shrink multiple data to single points.\n\n![[Pasted image 20241006124829.png|500]]\n\n![[Pasted image 20241006124735.png|500]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "convolutional_neural_networks",
    "outlinks": [
      "deep_learning",
      "pasted_image_20241006124735.png",
      "feature_extraction",
      "pasted_image_20241006124829.png"
    ],
    "inlinks": [
      "types_of_neural_networks"
    ]
  },
  {
    "category": "DL",
    "filename": "LSTM",
    "sha": "7e7c071abaef15f09edb1d8d8c104cb7f78d8299",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/LSTM.md",
    "text": "# What is LSTM\n\nLSTM (Long Short-Term Memory) networks are a specialized type of Recurrent Neural Network (RNN) designed to overcome the [[vanishing and exploding gradients problem]] that affects traditional [[Recurrent Neural Networks]]. \n\nLSTMs address this challenge through their unique architecture.\n\nUsed for tasks that require the retention of information over time, and problems involving ==sequential data.== \n\nThe key strength of LSTMs is their ability to manage ==long-term dependencies== using their ==gating mechanisms==.\n### Key Components of LSTM Networks:\n\nResources: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n$x_t$ input, $h_t$ output, cell state $C_t$, conveyer belt\n\n![[Pasted image 20241015211424.png|500]]\n\nMemory Cell:\n   - The core of an LSTM network is the memory cell, which maintains information over long time intervals. This cell helps store, forget, or pass on information from previous time steps.\n\n==Gates==:\n   - Input Gate: Controls how much of the input should be allowed into the memory cell.\n   - Forget Gate: Determines which information should be discarded from the memory cell.\n   - Output Gate: Controls what part of the cell's memory should be output as the hidden state for the current time step.\n\nThese gates are regulated by ==sigmoid== activation, which output values between 0 and 1, acting like a filter to determine the amount of information that should pass through. This gate mechanism allows the LSTM network to maintain a balance between retaining relevant data and discarding unnecessary information over time.\n# Why is LSTM less favourable over using transformers\n\n>[!Summary]  \n> Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network ([[Recurrent Neural Networks]]), are less favorable than [[Transformer]] for many modern tasks, especially in Natural Language Processing ([[NLP]]). LSTMs process sequences of data one step at a time, making them inherently sequential and difficult to parallelize. Transformers, on the other hand, leverage a self-attention mechanism that allows them to process entire sequences simultaneously, leading to faster training and the ability to capture long-range dependencies more effectively. \n> \n> Mathematically, LSTM’s sequential nature leads to slower computations, while the Transformer’s attention mechanism computes relationships between all tokens in a sequence, allowing better scalability and performance for tasks like translation, summarization, and language modeling.\n\n>[!Breakdown]  \n> Key Components:  \n> - Sequential Processing in LSTM: Each time step relies on the previous one, creating a bottleneck for long sequences.  \n> - Self-Attention Mechanism in Transformers: Allows simultaneous processing of all elements in a sequence.  \n> - Parallelization: Transformers leverage parallel computing more effectively due to non-sequential data processing.  \n> - Positional Encoding: Used by Transformers to retain the order of the sequence, overcoming the need for explicit recurrence.\n\n>[!important]  \n> - LSTMs are slower in training due to their sequential nature, as calculations depend on previous states.  \n> - Transformers efficiently handle long-range dependencies using self-attention, which calculates the relationships between tokens directly without needing previous time steps.\n\n>[!attention]  \n> - LSTMs suffer from vanishing/exploding gradient issues, especially in long sequences, limiting their effectiveness for long-term dependencies.  \n> - Transformers require more data and computational power to train, which can be a limitation in resource-constrained environments.\n\n>[!Example]  \n> In a language translation task, LSTMs process words sequentially, making them less efficient in handling long sentences. In contrast, a Transformer can analyze the entire sentence at once, using self-attention to determine relationships between all words, leading to faster and more accurate translations.\n\n>[!Follow up questions]  \n> - How does the attention mechanism in Transformers improve the model's ability to capture long-range dependencies compared to LSTM’s ==cell structure?==  \n> - In what cases might LSTM still be a better option over Transformers, despite their limitations?\n\n>[!Related Topics]  \n> - [[Attention mechanism]] in deep learning  \n> - [[BERT]] (Bidirectional Encoder Representations from Transformers)  \n> - [[GRU]] (Gated Recurrent Unit) as an alternative to LSTM\n\n\n### Example Workflow in Python using Keras:\n\nIn this example, we define a simple LSTM model in [[Keras]] for a [[Time Series Forecasting]] task. The model processes sequences with 1000 time steps, and the LSTM layer has 50 units, followed by a fully connected (Dense) layer for the final prediction.\n\n```python\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\n# Sample data: time series with 1000 timesteps and 1 feature\ntime_steps = 1000\nfeatures = 1\nX_train = np.random.rand(1000, time_steps, features)\ny_train = np.random.rand(1000)\n\n# Define LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='tanh', return_sequences=False, input_shape=(time_steps, features)))\nmodel.add(Dense(1))  # Output layer for regression tasks\n\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=64)\n```\n\n# notes\n\n[[LSTM]]\nHow to implement [[LSTM]] with [[PyTorch]]?\nhttps://lightning.ai/lightning-ai/studios/statquest-long-short-term-memory-lstm-with-pytorch-lightning?view=public&section=all\nwithout lightning - there is a script there\n\n[LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)",
    "aliases": [
      "LSTM vs. Transformer",
      "RNN vs. Transformer"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "time_series"
    ],
    "normalized_filename": "lstm",
    "outlinks": [
      "lstm",
      "bert",
      "time_series_forecasting",
      "attention_mechanism",
      "transformer",
      "recurrent_neural_networks",
      "gru",
      "vanishing_and_exploding_gradients_problem",
      "pasted_image_20241015211424.png",
      "nlp",
      "pytorch",
      "keras"
    ],
    "inlinks": [
      "ai_engineer",
      "anomaly_detection_in_time_series",
      "attention_mechanism",
      "ds_&_ml_portal",
      "lstm",
      "lstm_in_time_series",
      "named_entity_recognition",
      "recurrent_neural_networks",
      "transformers_vs_rnns"
    ]
  },
  {
    "category": "DL",
    "filename": "How is reinforcement learning being combined with deep learning",
    "sha": "1d0c06760d9d33565c185482ed5d89345316f835",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/How%20is%20reinforcement%20learning%20being%20combined%20with%20deep%20learning.md",
    "text": "The sources touch upon reinforcement learning as an area beyond the scope of their discussion. However, the combination of [[Reinforcement learning]] with [[Deep Learning]] has shown remarkable results in recent years, particularly in areas like game playing and robotics. \n\nExploring the potential of this combination in other domains and developing new algorithms that effectively integrate deep learning representations with reinforcement learning principles could lead to significant advancements in artificial intelligence.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "how_is_reinforcement_learning_being_combined_with_deep_learning",
    "outlinks": [
      "deep_learning",
      "reinforcement_learning"
    ],
    "inlinks": []
  },
  {
    "category": "DL",
    "filename": "Policy",
    "sha": "0b56cf0e0e00a0232172c240c09d0e87968ed9ef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Policy.md",
    "text": "In [[Reinforcement learning]] (RL), a **policy** is a strategy or a rule that defines the actions an agent takes in a given state to achieve its goals. It essentially ==maps states of the environment to actions that the agent should take when in those states.==\n\nPolicies are used in RL as they determine the behavior of the agent. \n\nThe goal of many RL algorithms is to find an optimal policy that maximizes the cumulative reward the agent receives over time. This involves balancing exploration (trying new actions) and exploitation (using known actions that yield high rewards).\n\n### Key Concepts\n\n**On-Policy vs. Off-Policy**:\n  - **On-Policy**: The agent learns the value of the policy it is currently following. An example is the [[Sarsa]] algorithm, which updates its policy based on the actual actions taken by the agent.\n  - **Off-Policy**: The agent learns the value of the optimal policy, regardless of the actions it actually takes. [[Q-Learning]] is an example of an off-policy algorithm, as it updates its policy based on the best possible action in the next state, not necessarily the action taken.\n\n**Conservatism**:\n  - Some policies, like those in SARSA, are more conservative in their updates. This means they are more cautious and adapt to uncertainties in the environment, making them suitable for environments where [[Agent Exploration]] and [[exploitation]] need to be balanced carefully.\n\n### Example of a Policy\n\nConsider a simple grid world where an agent can move up, down, left, or right to reach a goal. A policy in this context could be a set of rules that tells the agent to always move towards the goal if it is visible, or to explore randomly if the goal is not visible.\n\nFor instance, in a 3x3 grid where the goal is at position (2, 2), a simple policy might be:\n\n- If the agent is at (0, 0), move right.\n- If the agent is at (0, 1), move right.\n- If the agent is at (0, 2), move down.\n- Continue this pattern until the agent reaches the goal at (2, 2).\n\nThis policy can be represented as a table or a function that maps each state (grid position) to an action (move direction).\n\n|State (Position)|Action|\n|---|---|\n|(0, 0)|Move Right|\n|(0, 1)|Move Right|\n|(0, 2)|Move Down|\n|(1, 0)|Move Right|\n|(1, 1)|Move Right|\n|(1, 2)|Move Down|\n|(2, 0)|Move Right|\n|(2, 1)|Move Right|\n|(2, 2)|Goal Reached|",
    "aliases": [
      "policies"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "policy",
    "outlinks": [
      "q-learning",
      "reinforcement_learning",
      "agent_exploration",
      "exploitation",
      "sarsa"
    ],
    "inlinks": [
      "agent-based_modelling",
      "q-learning",
      "reinforcement_learning"
    ]
  },
  {
    "category": "DL",
    "filename": "Multi-Agent Reinforcement Learning",
    "sha": "13ca94153a38d1f47793b40b178be37281eb16db",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Multi-Agent%20Reinforcement%20Learning.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "agents",
      "deep_learning"
    ],
    "normalized_filename": "multi-agent_reinforcement_learning",
    "outlinks": [],
    "inlinks": [
      "reinforcement_learning"
    ]
  },
  {
    "category": "DL",
    "filename": "Sarsa",
    "sha": "25fa60166c1ee8586fe638b83fbe2541cf5435b9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Sarsa.md",
    "text": "SARSA stands for State-Action-Reward-State-Action\n\nSARSA is another value-based [[Reinforcement learning]] algorithm, differing from Q-learning in that it updates the Q-values based on the action actually taken by the policy.\n\n**SARSA update rule:**\n\n$$\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n$$\n\n**Explanation:**\n\n- **$Q(s_t, a_t)$**: The Q-value of the current state $s_t$ and action $a_t$.\n- **$\\alpha$**: The learning rate, determining how much new information overrides old information.\n- **$r_{t+1}$**: The reward received after taking action $a_t$ from state $s_t$.\n- **$\\gamma$**: The discount factor, balancing immediate and future rewards.\n- **$Q(s_{t+1}, a_{t+1})$**: The Q-value for the next state $s_{t+1}$ and the action $a_{t+1}$ actually taken according to the policy.\n\n**Notes**:\n\n- SARSA’s on-policy nature ensures that it learns a policy that aligns with its exploration strategy, leading to more stable behavior in environments with randomness or noise.\n- The learning process may be slower compared to Q-learning, but it can be more robust in environments where the agent’s behavior is expected to align closely with the policy it follows.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "sarsa",
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "policy",
      "reinforcement_learning"
    ]
  },
  {
    "category": "DL",
    "filename": "Relu",
    "sha": "4519c022a58e7193a3a70ee47b887b58164e1760",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/deep-learning/Relu.md",
    "text": "How does the ReLU [[Activation Function]] help overcome the vanishing gradient problem?\n\nThe rectified linear unit (ReLU) activation function helps overcome the vanishing gradient problem by avoiding ==gradient saturation==. ReLU replaces negative values with zero, ensuring that the gradients flowing backward remain non-zero and do not vanish. This promotes better gradient flow and enables effective learning in deep neural networks.\n[[vanishing and exploding gradients problem]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "relu",
    "outlinks": [
      "activation_function",
      "vanishing_and_exploding_gradients_problem"
    ],
    "inlinks": [
      "activation_function",
      "deep_learning",
      "fitting_weights_and_biases_of_a_neural_network",
      "forward_propagation",
      "initialization_methods",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "devops",
    "filename": ".md",
    "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/.md",
    "text": "",
    "normalized_filename": ".md",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "AB testing",
    "sha": "4ec44b48d78ca6fa62c4d6911e598f395d4506d6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/AB%20testing.md",
    "text": "A/B [[Testing]] is a method of performance testing two versions of a product like an app.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "ab_testing",
    "outlinks": [
      "testing"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "API",
    "sha": "6a7e16c9499303628a47cce33232645bce1337de",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/API.md",
    "text": "An API (Application Programming Interfaces) allows one system (client) to ==request specific actions from another system== (server).\n\nUsing a predefined set of rules and ==protocols==. \n\nGood API documentation is necessary for developers to integrate and use APIs effectively.\n#### Resources:\n- [Link](https://www.youtube.com/watch?v=yBZO5Rb4ibo)\n- [[REST API]]\n- [[FastAPI]]\n#### API Principles\n\n1. **Controlled Access**: APIs provide access to certain parts of a system while keeping the core functionalities secure.\n2. **System Independence**: APIs function independently of changes in the underlying system.\n3. **Simplicity**: APIs are designed to be ==user-friendly== and come with comprehensive documentation to guide developers.\n#### Implementation\n\nIn [[ML_Tools]] see: [[Wikipedia_API.py]]\n\n#### Example:\n\nFor instance, a weather app querying a weather API to fetch the current weather conditions involves sending a structured request and receiving a response.\n\nTypes of API Connections:\n1. **Web APIs**: These facilitate communication between web clients (browsers or apps) and servers. For example, online shopping apps use APIs to process transactions on remote servers.\n2. **Database APIs**: These allow applications to interact with databases, ensuring data is accessed and manipulated efficiently.\n3. **Device APIs**: When apps like Instagram or WhatsApp request access to your phone's camera or microphone, they use device APIs.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "api",
    "outlinks": [
      "rest_api",
      "ml_tools",
      "fastapi",
      "wikipedia_api.py"
    ],
    "inlinks": [
      "api_driven_microservices",
      "batch_vs_powershell_scripts",
      "data_contract",
      "data_ingestion",
      "generators_in_python",
      "model_deployment",
      "neomodel",
      "normalisation_of_data",
      "postman",
      "rest_api",
      "streamlit"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "API Driven Microservices",
    "sha": "14d5d65d6de8376807cce0edf47c5e7724df7532",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/API%20Driven%20Microservices.md",
    "text": "API-driven microservices refer to a [[software architecture]] approach where [[microservices]] communicate with each other and with external systems primarily through well-defined [[API]] (Application Programming Interfaces). \n\nThis architecture is designed to enhance modularity, scalability, and flexibility by breaking down an application into smaller, independent services that can be developed, deployed, and scaled independently.\n\nAPI-driven microservices architecture is particularly beneficial for large, complex applications that require frequent updates and scaling. It allows organizations to innovate faster, improve fault isolation, and better align development efforts with business needs. However, it also introduces complexity in terms of service orchestration, data consistency, and network communication, which must be carefully managed.\n\nKey characteristics of API-driven microservices include:\n\n1. **Decoupled Services**: Each microservice is a separate, self-contained unit that performs a specific business function. Services are loosely coupled, meaning changes to one service do not directly impact others.\n\n2. **API Communication**: Microservices interact with each other and with external clients through APIs. These APIs are typically RESTful, but they can also use other protocols like gRPC, GraphQL, or messaging systems like Kafka.\n\n3. **Independent Deployment**: Each microservice can be developed, tested, deployed, and scaled independently of the others. This allows for more agile development and continuous deployment practices.\n\n4. **Technology Agnostic**: Different microservices can be built using different technologies or programming languages, as long as they adhere to the agreed-upon API contracts.\n\n5. **Scalability and Resilience**: Microservices can be scaled independently based on demand. If one service fails, it does not necessarily bring down the entire system, enhancing resilience.\n\n6. **Focused Functionality**: Each microservice is designed to handle a specific business capability, making it easier to understand, develop, and maintain.\n\n7. **API Gateway**: Often, an API gateway is used to manage and route requests to the appropriate microservices. It can also handle cross-cutting concerns like authentication, logging, and rate limiting.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "software"
    ],
    "normalized_filename": "api_driven_microservices",
    "outlinks": [
      "software_architecture",
      "microservices",
      "api"
    ],
    "inlinks": [
      "event_driven_events"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Alternatives to Batch Processing",
    "sha": "bdadf590d4a6ff5ed2c2420a90c94e347f212e8a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Alternatives%20to%20Batch%20Processing.md",
    "text": "If you’re working with a streaming dataset ([[Data Streaming]]), why might [[Batch Processing]] not be suitable, and what alternatives would you consider?  \n\n**[[Latency]]**: Batch processing involves collecting data over a period and processing it in large chunks. This can introduce significant delays, making it unsuitable for applications that require real-time or near-real-time insights.\n\n**Timeliness**: Streaming datasets often require immediate processing to respond to events as they occur. Batch processing cannot meet the demand for timely [[Data Analysis]].\n\n**Data Freshness**: In streaming scenarios, data is continuously generated, and waiting for a batch interval can result in outdated information being analyzed.\n\n### Alternatives to Batch Processing\n\n1. **Stream Processing**: This approach processes data in real-time as it arrives. Tools like [[Apache Kafka]], [[Apache Flink]], and [[Apache Spark Streaming]] are designed for handling streaming data efficiently.\n\n2. **Event-Driven Architectures**: Implementing an [[Event-Driven Architecture]] allows systems to react to data changes or events immediately, ensuring timely processing and response.\n\n3. **Micro-batching**: This technique processes small batches of data at very short intervals, striking a balance between batch and stream processing. Tools like [[Apache Spark Streaming]] can utilize micro-batching to handle streaming data more effectively.\n\n4. **Complex Event Processing (CEP)**: CEP systems analyze and process streams of events in real-time, allowing for the detection of patterns and trends as they happen.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "alternatives_to_batch_processing",
    "outlinks": [
      "event-driven_architecture",
      "apache_spark_streaming",
      "apache_kafka",
      "data_analysis",
      "apache_flink",
      "data_streaming",
      "batch_processing",
      "latency"
    ],
    "inlinks": [
      "data_streaming"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Amazon S3",
    "sha": "427acb2523b7a7636be357313ee1e170b689fe41",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Amazon%20S3.md",
    "text": "## Amazon S3 buckets (onedrive essentially)\n\nAmazon Simple Storage Service (S3) is a versatile ==object storage solution== known for its [[Scalability]], data availability, security, and performance.\n\nStands for Simple Storage Service.\n\n**What is Amazon S3?**\n\nIt's an object storage service, allowing you to upload and store various types of objects, including images, text, videos, and more. S3's structure resembles that of a typical file system, with folders, subfolders, and files. Notably, it's extremely cost-effective, with storage costs starting at only 0.023 cents per GB, and it offers high durability with data replicated across three availability zones.\n\n**Why is Amazon S3 Useful?**\n\n1. **Cost-Effective Storage**: S3 provides cheap, reliable storage for objects of any type.\n2. **Low Latency, High Throughput**: It offers fast access to your data, making it suitable for hosting static websites.\n3. **Integration with AWS Services**: S3 can be integrated with other AWS services like SNS, SQS, and Lambda for powerful event-driven applications.\n4. **Lifecycle Management**: S3 offers lifecycle management to automatically transition data to lower-cost storage tiers based on access patterns.\n\n**Step-by-Step Walkthrough in the AWS Console**\n\n1. **Creating a Bucket**: Start by navigating to the AWS Management Console and selecting S3. Create a bucket with a unique name and choose the region closest to your application.\n2. **Uploading a File**: Once the bucket is created, upload a file using the console. You can add metadata and set permissions as needed.\n3. **Exploring Settings**: Dive into bucket settings to configure options like versioning, logging, encryption, and lifecycle management. Block public access to ensure [[Data Security]].\n\n**Additional Features**\n\n1. **Transfer Acceleration**: Accelerate data transfer to and from S3 using optimized network paths.\n2. **Events**: Configure event notifications to trigger actions in response to S3 events like object creation or deletion.\n3. **Requester Pays**: Enable Requester Pays to allow bucket owners to pass on data transfer costs to requesters.",
    "aliases": [
      "S3"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "storage"
    ],
    "normalized_filename": "amazon_s3",
    "outlinks": [
      "scalability",
      "data_security"
    ],
    "inlinks": [
      "apache_iceberg",
      "aws_lambda",
      "data_engineering_tools",
      "data_storage",
      "distributed_computing",
      "model_deployment_using_pycaret"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Apache Airflow",
    "sha": "f8fc9c8910e293744c47249a2141e06cf518c52d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Apache%20Airflow.md",
    "text": "Schedular think CROM jobs with python.\n\nApache Nifi may be better.\n\n[Airflow](https://airflow.apache.org/) is a [data orchestrator](term/data%20orchestrator.md) and the first that made task scheduling popular with [Python](term/python.md). \n\nAirflow programmatically author, schedule, and monitor workflows. It follows the [imperative](term/imperative.md) paradigm of schedule as *how* a DAG [[Directed Acyclic Graph (DAG)]] is run has to be defined within the Airflow jobs. Airflow calls its *Workflow as code* with the main characteristics\n- **Dynamic**: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation.\n- **Extensible**: The Airflow framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment.\n- **Flexible**: Workflow parameterization is built-in leveraging the [Jinja Templating](term/jinja%20template.md) engine.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "software"
    ],
    "normalized_filename": "apache_airflow",
    "outlinks": [
      "directed_acyclic_graph_(dag)"
    ],
    "inlinks": [
      "data_engineering",
      "data_management",
      "directed_acyclic_graph_(dag)",
      "etl"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Apache Kafka",
    "sha": "9ff8efe4d72b81deb087dc39218885d20d82bc45",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Apache%20Kafka.md",
    "text": "Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and data streaming applications. It is designed to handle high-throughput, fault-tolerant, and scalable messaging. \n\n### Features\n\nImmutable commit log: Kafka maintains an append-only log of messages, which ensures [[Data Integrity]] and replicability.\n\nKafka allows applications to [[Publish and Subscribe]] to streams of records, similar to a message queue or enterprise messaging system.\n\nDurability and Reliability: Kafka stores streams of records in a fault-tolerant way, ensuring data durability and reliability. It replicates data across multiple nodes to prevent data loss.\n\n[[Scalability]]: Kafka is designed to scale horizontally by adding more brokers (servers) to the cluster, which can handle increased load and data volume.\n\nKafka is optimized for high throughput, making it suitable for processing large volumes of data in real-time.\n\nData in Kafka is organized into topics, which are further divided into partitions. Each partition is an ordered, immutable sequence of records that is continually appended to.\n\nProducers are applications that publish data to Kafka topics, while consumers are applications that subscribe to topics and process the data.\n\nKafka is commonly used for log aggregation, real-time analytics, [[Data Integration]], stream processing, and building event-driven architectures.\n\nKafka integrates ([[Data Integration]]) well with various data processing frameworks like Apache Spark, Apache Flink, and Apache Storm, as well as with databases and other [[Data Storage]] systems.",
    "aliases": [
      "Kafka"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "software"
    ],
    "normalized_filename": "apache_kafka",
    "outlinks": [
      "data_integrity",
      "publish_and_subscribe",
      "scalability",
      "data_integration",
      "data_storage"
    ],
    "inlinks": [
      "alternatives_to_batch_processing",
      "data_engineering_tools",
      "data_streaming",
      "publish_and_subscribe"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Apache Spark",
    "sha": "9c9bb5e22e09a9e850c5a684aeeac3fdc4073667",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Apache%20Spark.md",
    "text": "Apache Spark is an open-source multi-language engine for executing [Data Engineer](Data%20Engineer.md) and [Machine Learning](Machine%20Learning.md) on single-node machines or clusters. It's optimized for large-scale data processing.\n\nSpark runs well with [Kubernetes](term/kubernetes.md).\n\nSpark is a highly popular framework for large-scale data processing. It allows [[Data Engineer]] to process massive datasets in memory, which makes it faster than traditional disk-based approaches. Spark is versatile, supporting batch processing, real-time data streaming, machine learning, and graph processing.",
    "aliases": [
      "Spark"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "apache_spark",
    "outlinks": [
      "data_engineer"
    ],
    "inlinks": [
      "apache_iceberg",
      "batch_processing",
      "big_data",
      "databricks",
      "databricks_vs_snowflake",
      "delta_tables_in_databricks",
      "distributed_computing",
      "ds_&_ml_portal",
      "hadoop",
      "map_reduce",
      "parquet",
      "pyspark",
      "scala"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Bash",
    "sha": "ea32e6188bcae0de63e5d43333c24c1a5552db4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Bash.md",
    "text": "#### Automation Scripts\n\nIn [[ML_Tools]], see: [[Bash_folder]]\n#### **Basic Commands**\n\n1. **Show Current Directory**: \n   ```bash\n   pwd\n   ```\n2. **Display Contents of a Text File**: \n   ```bash\n   cat filename.txt\n   ```\n3. **Search for a Word in a File**: \n   ```bash\n   [[grep]] \"word\" filename.txt\n   ```\n4. **Replace Text in a File (Output Only)**: \n   ```bash\n   sed 's/old/new/g' filename.txt\n   ```\n\n### Writing and Running a Bash Script\n\n1. **Create a Script**: \n   ```bash\n   nano hello.sh\n   ```\n   Add:\n   ```bash\n   #!/bin/bash\n   echo \"Hello, $(whoami)! Welcome to Bash scripting!\"\n   ```\n   Save and exit: **Ctrl + O**, **Enter**, **Ctrl + X**.\n\n2. **Make the Script Executable**: \n   ```bash\n   chmod +x hello.sh\n   ```\n\n3. **Run the Script**: \n   ```bash\n   ./hello.sh\n   ```\n\n### Useful Bash Automation Tips\n\n- **Clear Screen**: \n   ```bash\n   clear\n   ```\n- **Keyboard Shortcut**: **Ctrl + L**.\n- **Clear Screen and Command History**: \n   ```bash\n   clear && history -c\n   ```\n- **Reset Terminal**: \n   ```bash\n   reset\n   ```\n\n### Managing Command History\n\n1. **Clear Current Session’s History**: \n   ```bash\n   history -c\n   ```\n2. **Save History to a Custom File**: \n   ```bash\n   history > my_session_history.txt\n   ```\n3. **Clear and Remove Saved History**: \n   ```bash\n   history -c\n   > ~/.bash_history\n   ```\n4. **Start a Fresh Bash Session**: \n   ```bash\n   exec bash\n   ```\n\n#### **Example: Conditional Execution**\n```bash\nif [ -f filename.txt ]; then\n  echo \"File exists.\"\nelse\n  echo \"File does not exist.\"\nfi\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "bash",
    "outlinks": [
      "bash_folder",
      "ml_tools",
      "grep"
    ],
    "inlinks": [
      "command_line",
      "command_prompt",
      "powershell_vs_bash"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Batch Processing",
    "sha": "c5c26b5b811f2156b213dbac816a29dd4f95a1fb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Batch%20Processing.md",
    "text": "**Batch Processing** is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them together in a single batch.\n\n[[Apache Spark]] is the leading technology for batch processing, offering scalable and distributed data processing. It can handle unmanageable data sizes by using parallelism and [[Distributed Computing]]\n\nA key concept in batch processing is **MapReduce**:\n  - **Map**: Splits the data into smaller, manageable pieces for parallel processing.\n  - **Reduce**: Aggregates the processed data results from the individual tasks.\n  - **Order**: The order of Map and Reduce steps is flexible; the primary focus is on splitting and then aggregating data.\n\nBatch processing is widely supported by cloud infrastructures like **Amazon EMR** and **[[categories/devops/Databricks]]**, which provide scalable environments for running batch jobs.\n\n\n\n\n[[Batch Processing]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "process",
      "system"
    ],
    "normalized_filename": "batch_processing",
    "outlinks": [
      "categories/devops/databricks",
      "apache_spark",
      "distributed_computing",
      "batch_processing"
    ],
    "inlinks": [
      "alternatives_to_batch_processing",
      "batch_processing",
      "data_streaming",
      "ds_&_ml_portal",
      "hadoop",
      "lambda_architecture",
      "map_reduce",
      "multiprocessing",
      "publish_and_subscribe",
      "spacy"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Batch vs PowerShell scripts",
    "sha": "22c0af89a3a4f42c8491d5ace38cd7b56091cbd7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Batch%20vs%20PowerShell%20scripts.md",
    "text": "That depends on what “better” means in your context - whether you’re optimising for \n- [[Capability]]\n- [[Usability]]\n- [[Interoperability]]\n\n| Criterion            | [[bat]] (Batch) Scripts                                                | [[PowerShell]] Scripts                                                                   |\n| -------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n| -Language Age-       | 1980s DOS heritage; limited modern support                             | Introduced 2006; actively developed                                                      |\n| -Syntax-             | Simple, terse, but inconsistent and hard to read for complex logic     | Rich, consistent syntax; closer to modern programming languages                          |\n| -Capabilities-       | Primarily for file operations, launching programs, simple control flow | Full .NET access, objects rather than just text, strong error handling                   |\n| -Integration-        | Works out-of-the-box on any Windows version, including old ones        | Available on modern Windows; PowerShell Core runs cross-platform (Windows, Linux, macOS) |\n| -Error Handling-     | Rudimentary; mostly relies on `%ERRORLEVEL%`                           | Try/Catch/Finally, structured exception handling                                         |\n| -Data Handling-      | Text-based; everything is a string                                     | Object-based; can work directly with rich data types                                     |\n| -Portability-        | Only works well in Windows [[Command Prompt\\|CMD]] environments        | PowerShell Core (pwsh) works on multiple OSs                                             |\n| -Learning Curve-     | Low for simple tasks, steep for complex ones                           | Moderate; more concepts to learn but more powerful                                       |\n| -Community & Future- | Legacy use; unlikely to gain new features                              | Actively developed, especially in PowerShell 7+                                          |\n\nIn practice\n\nUse -Batch- when:\n  * You need a quick, lightweight script on an old or minimal Windows system.\n  * Your task is trivial (launching executables, simple file moves).\n    \nUse -PowerShell- when:\n  * You need robust automation, data manipulation, or [[API]] interaction.\n  * You want maintainable, future-proof scripts.\n  * You’re working cross-platform.\n\n-Rule of thumb:-\nFor anything beyond simple file and process commands, PowerShell is objectively more [[Capability|capable]] and [[Maintainability|maintainable]]. Batch is essentially “legacy” - still useful for tiny jobs, but not a good choice for long-term automation.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "automation",
      "system"
    ],
    "normalized_filename": "batch_vs_powershell_scripts",
    "outlinks": [
      "usability",
      "api",
      "command_prompt\\",
      "bat",
      "maintainability",
      "interoperability",
      "capability",
      "powershell"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "CI-CD",
    "sha": "fa26033bd9113f0f149904607ae1c3992d8a1e0e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/CI-CD.md",
    "text": "**CI/CD** stands for **[[Continuous Integration]]** and **[[Continuous Delivery/Deployment]]**. It is a set of practices aimed at streamlining and accelerating the [[Software Development Life Cycle]]. The main goals of CI/CD are to improve software quality, reduce integration issues, and deliver updates to users more frequently and reliably.\n\nTools and Technologies\n- [[Gitlab]]\n- [[Docker]]\n- [[Continuous Delivery/Deployment]]\n- [[Continuous Integration]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "ci-cd",
    "outlinks": [
      "gitlab",
      "continuous_integration",
      "software_development_life_cycle",
      "continuous_delivery/deployment",
      "docker"
    ],
    "inlinks": [
      "devops",
      "gitlab"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Catalogs, Schemas, and Tables in Databricks",
    "sha": "7569ed0d8e661fad4d3c7df575671249aa7c6bdc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Catalogs,%20Schemas,%20and%20Tables%20in%20Databricks.md",
    "text": "**Concept:**\nDatabricks organizes data using a **three-level namespace**: `catalog.schema.table`\n\n**Structure:**\n\n* **Catalog:** Top-level container (e.g., `example`).\n* **Schema:** Logical grouping of related tables (e.g., `databricks`).\n* **Table:** Dataset stored in Delta format (e.g., `databricks_test`).\n\n**Example Commands:**\n\n```python\nspark.sql(\"CREATE CATALOG IF NOT EXISTS example\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS example.databricks\")\ndf.write.mode(\"overwrite\").saveAsTable(\"example.databricks.databricks_test\")\n```\n\n**Benefits:**\n\n* Clear separation of data domains.\n* Simplified access control and governance.\n* Consistent naming and d[[Catalogs, Schemas, and Tables in Databricks]]- [[Delta Tables and Catalogs]]\n\nIn [[Databricks]], all managed data assets - including [[Delta Tables in Databricks]] are organized hierarchically within the **Unity Catalog** system.\n\n### **Hierarchy**\n\n```\ncatalog.schema.table\n```\n\n* **Catalog:** Top-level container (e.g., `main`, `example`, `samples`).\n* **Schema:** Logical grouping of related tables within a catalog.\n* **Table:** Actual data entity (e.g., Delta table, view, or external table).\n\nExample:\n\n```sql\nSELECT * FROM example.sales.customers;\n```\n\nHere:\n\n* `example` = catalog\n* `sales` = schema\n* `customers` = Delta Table\n### **How Delta Tables Fit In**\n\nA **Delta Table** can be:\n**Managed (inside Databricks)** — stored within a specific catalog and schema.\n   * Databricks manages both the metadata and physical files.\n   * Example:\n\n     ```python\n     df.write.format(\"delta\").saveAsTable(\"example.sales.transactions\")\n     ```\n### **Why Catalogs Matter**\n\nCatalogs in Databricks:\n* Provide **governance** via Unity Catalog (access control, lineage, auditing).\n* Enable **data discovery** across teams.\n* Help enforce consistent naming conventions and permissions.\n\n### **Typical Structure**\n\n| Level           | Example Name   | Description                                      |\n| --------------- | -------------- | ------------------------------------------------ |\n| **Catalog**     | `example`      | Project or business domain                       |\n| **Schema**      | `sales`        | Logical grouping (e.g., transactions, customers) |\n| **Delta Table** | `transactions` | The actual dataset stored in Delta format        |",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "data_governance",
      "data_management",
      "data_storage",
      "SQL",
      "database"
    ],
    "normalized_filename": "catalogs,_schemas,_and_tables_in_databricks",
    "outlinks": [
      "delta_tables_in_databricks",
      "delta_tables_and_catalogs",
      "databricks",
      "catalogs,_schemas,_and_tables_in_databricks"
    ],
    "inlinks": [
      "catalogs,_schemas,_and_tables_in_databricks",
      "databricks",
      "delta_tables_in_databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Click",
    "sha": "5af63d6170d65875fed044c8af3dbe74fd7be0e4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Click.md",
    "text": "Python Click, or \"[[Command Line]] Interface Creation Kit,\" is a library for building command-line interfaces (CLIs). It supports arbitrary nesting of commands, automatic help page generation, and lazy loading of subcommands. \n## Installation\n\nTo install Click, use pip:\n\n```sh\npip install click\n```\n\n## Creating a Command Group\n\nClick uses groups to organize related commands. A group serves as a container for multiple commands.\n\n```python\nimport click\nimport json\n\n@click.group(\"cli\")\n@click.pass_context\n@click.argument(\"document\")\ndef cli(ctx, document):\n    \"\"\"An example CLI for interfacing with a document\"\"\"\n    with open(document) as _stream:\n        _dict = json.load(_stream)\n    ctx.obj = _dict\n\ndef main():\n    cli(prog_name=\"cli\")\n\nif __name__ == '__main__':\n    main()\n```\n\nRunning `python script.py --help` generates an automatic help page.\n\n## Adding Commands\n\nCommands can be added to a Click group using the `@<group>.command` decorator.\n\n### Checking Context Object\n\n```python\nimport pprint\n\n@cli.command(\"check_context_object\")\n@click.pass_context\ndef check_context(ctx):\n    pprint.pprint(type(ctx.obj))\n```\n\n### Custom Pass Decorator\n\nA pass decorator allows passing specific objects through context.\n\n```python\npass_dict = click.make_pass_decorator(dict)\n```\n\n### Retrieving Keys from Context\n\n```python\n@cli.command(\"get_keys\")\n@pass_dict\ndef get_keys(_dict):\n    keys = list(_dict.keys())\n    click.secho(\"The keys in our dictionary are\", fg=\"green\")\n    click.echo(click.style(str(keys), fg=\"blue\"))\n```\n\n### Retrieving a Specific Key\n\n```python\n@cli.command(\"get_key\")\n@click.argument(\"key\")\n@click.pass_context\ndef get_key(ctx, key):\n    if key in ctx.obj:\n        pprint.pprint(ctx.obj[key])\n    else:\n        click.echo(f\"Key '{key}' not found in document.\", err=True)\n```\n\n### Arbitrary Nesting of Commands\n\n```python\n@cli.command(\"get_summary\")\n@click.pass_context\ndef get_summary(ctx):\n    ctx.invoke(get_key, key=\"summary\")\n```\n\n## Adding Optional Parameters\n\nOptional parameters can be defined using the `@click.option` decorator.\n\n```python\n@cli.command(\"get_results\")\n@click.option(\"-d\", \"--download\", is_flag=True, help=\"Download the result to a JSON file\")\n@click.option(\"-k\", \"--key\", help=\"Specify a key from the results\")\n@click.pass_context\ndef get_results(ctx, download, key):\n    results = ctx.obj.get('results', [])\n    if key:\n        results = {key: sum(entry.get(key, 0) for entry in results)}\n    if download:\n        filename = f\"{key or 'results'}.json\"\n        with open(filename, 'w') as w:\n            json.dump(results, w)\n        click.echo(f\"File saved to {filename}\")\n    else:\n        pprint.pprint(results)\n```\n\n## Using `@click.pass_obj`\n\n`@click.pass_obj` passes only `ctx.obj` instead of the full context.\n\n```python\n@cli.command(\"get_text\")\n@click.option(\"-s\", \"--sentences\", is_flag=True, help=\"Return sentences\")\n@click.option(\"-p\", \"--paragraphs\", is_flag=True, help=\"Return paragraphs\")\n@click.option(\"-d\", \"--download\", is_flag=True, help=\"Download as JSON file\")\n@click.pass_obj\ndef get_text(_dict, sentences, paragraphs, download):\n    results = _dict.get('results', [])\n    text = {} if paragraphs else {'text': ''}\n    for idx, entry in enumerate(results):\n        if paragraphs:\n            text[idx] = entry.get('text', '')\n        else:\n            text['text'] += entry.get('text', '')\n    if sentences:\n        text = {i: s for i, s in enumerate(text.get('text', '').split('.')) if s}\n    pprint.pprint(text)\n    if download:\n        filename = \"paragraphs.json\" if paragraphs else \"text.json\"\n        with open(filename, 'w') as w:\n            json.dump(text, w)\n        click.echo(f\"File saved to {filename}\")\n```\n\n## Handling User Input\n\nClick provides `@click.prompt` to interact with users.\n\n```python\n@cli.command(\"prompt_user\")\n@click.pass_context\ndef prompt_user(ctx):\n    name = click.prompt(\"Enter your name\")\n    age = click.prompt(\"Enter your age\", type=int)\n    click.echo(f\"Hello {name}, you are {age} years old!\")\n```\n\n## Handling Confirmation\n\nUse `@click.confirm` to get user confirmation before proceeding.\n\n```python\n@cli.command(\"confirm_action\")\n@click.pass_context\ndef confirm_action(ctx):\n    if click.confirm(\"Do you want to proceed?\"):\n        click.echo(\"Proceeding with action...\")\n    else:\n        click.echo(\"Action canceled.\")\n```\n\n## Conclusion\n\nPython Click simplifies CLI creation with its decorators and built-in features like automatic help generation and context passing. This guide provides a foundation for building more advanced command-line applications. Additionally, handling user input and confirmations enhances the interactivity of CLI applications.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "ML_Tools"
    ],
    "normalized_filename": "click",
    "outlinks": [
      "command_line"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Clustering_Dashboard.py",
    "sha": "37ba04e82087a13e82c9c2435afec55b5f1d763c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Clustering_Dashboard.py.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Clustering_Dashboard.py",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet"
    ],
    "normalized_filename": "clustering_dashboard.py",
    "outlinks": [],
    "inlinks": [
      "dash"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Code Diagrams",
    "sha": "313023ab5ba25015460b4a89cce9de13ffba98e6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Code%20Diagrams.md",
    "text": "Class Diagrams\n- Show the hierarchy and relationships of classes. See: [[Classes]] (Object-Oriented).\n\nArchitecture Diagrams\n- Depict the overall structure of the system.\n- Show how main components fit together and interact.\n\nSee: [[Architecture Diagram]]\n- Sequence Diagrams\n- Show how components interact over time.\n\nUseful for visualising the flow of communication between parts of the system.\n\nSee: [[Sequence diagram]]\n\nTools\n- Diagrams can be produced in [[Mermaid]].\n\nRelated\n[[Documentation & Meetings]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "documentation"
    ],
    "normalized_filename": "code_diagrams",
    "outlinks": [
      "classes",
      "sequence_diagram",
      "mermaid",
      "documentation_&_meetings",
      "architecture_diagram"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Command Line",
    "sha": "533a8acc7b66fd5392cb54aa0528c33091ebf3b8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Command%20Line.md",
    "text": "The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts, and perform various tasks.\n\n[[PowerShell]]\n\n[[Powershell vs Bash]]\n\n[[Bash]]\n\n[[Command Prompt]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "command_line",
    "outlinks": [
      "command_prompt",
      "powershell",
      "bash",
      "powershell_vs_bash"
    ],
    "inlinks": [
      "click",
      "justfile",
      "powershell",
      "terminal_commands",
      "windows_scheduled_tasks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Continuous Delivery - Deployment",
    "sha": "9fcded934a49d6b112a50e058dc72fd21365568b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Continuous%20Delivery%20-%20Deployment.md",
    "text": "Continuous Delivery\n   - Ensures that code changes are automatically prepared for a release to production.\n   - Builds, tests, and releases are automated, but the deployment is manual.\n\nContinuous Deployment:\n   - Extends continuous delivery by automating the deployment process.\n   - Every change that passes the automated tests is deployed to production automatically.\n\nA continuous integration and continuous deployment (CI/CD) pipeline is **a series of steps that must be performed in order to deliver a new version of software**\n\nRelated:\n- [[Continuous Integration|CI]]\n- [[Model Deployment]]",
    "aliases": [
      "CD"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "continuous_delivery_-_deployment",
    "outlinks": [
      "continuous_integration",
      "model_deployment"
    ],
    "inlinks": [
      "github_actions",
      "machine_learning_operations"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Continuous Integration",
    "sha": "e81b381a72e1268118283ea42a00642bd614f65d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Continuous%20Integration.md",
    "text": "- Developers frequently integrate code into a shared repository.\n   - Automated builds and tests are run to detect issues early.\n   - Encourages smaller, more manageable code changes.",
    "aliases": [
      "CI"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "continuous_integration",
    "outlinks": [],
    "inlinks": [
      "ci-cd",
      "continuous_delivery_-_deployment",
      "data_engineer",
      "github_actions",
      "gitlab-ci.yml",
      "machine_learning_operations",
      "testing"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Cron jobs",
    "sha": "b37bbc6273220f4b2015c99ca2a3f6324a91c4c3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Cron%20jobs.md",
    "text": "A **cron job** is a scheduled task that runs automatically at specified intervals on [[Unix]]-like systems.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "cron_jobs",
    "outlinks": [
      "unix"
    ],
    "inlinks": [
      "github_actions",
      "similarity_search",
      "windows_scheduled_tasks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Data Ingestion",
    "sha": "28346943fe851599b114241f836d88bf159b5e8c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Data%20Ingestion.md",
    "text": "Data ingestion is the process of collecting and importing raw data from various sources ([[Database]], [[API]], [[Data Streaming]] services) into a system for processing and analysis, and can be performed in batch and realtime ingestion. The goal is to gather raw data that can be processed and analysed.\n\nUsed for building [[Data Pipeline]]\n\nChallenges\n- [[Data Quality]]: Ensuring that the ingested data is accurate, complete, and consistent.\n- [[Scalability]]: Handling large volumes of data efficiently as the data sources grow.\n- [[Latency]]: Minimizing the delay between data generation and processing, especially in real-time scenarios.\n\nUse Cases:\n- Data ingestion is used in various applications, including: [[business intelligence]], [[Machine Learning]]\n\nRelated to:\n- [[Data Engineering Tools]]\n# What is involved:\n\n`df = pd.read_csv('Categorical.csv')`\n\n- Gather relevant data from appropriate sources, addressing any quality or privacy concerns.\n\n```python\n## Get textbook data using for example:\n\nimport re\ndef read_file(filename):\n    with open(filename, \"r\", encoding='UTF-8') as file:\n        contents = file.read().replace('\\n\\n',' ').replace('[edit]', '').replace('\\ufeff', '').replace('\\n', ' ').replace('\\u3000', ' ')\n    return contents\ntext = read_file('Data various/Monte_Cristo.txt')\n\ntext_start = [m.start() for m in re.finditer('VOLUME ONE', text)]\ntext_end = [m.start() for m in re.finditer('End of Project Gutenberg', text)]\ntext = text[text_start[1]:text_end[0]]\n```\n\nHow would you approach a colleague who is hesitant to share their data?\n- explain the purpose and benefits\n- ensure confidentiality (GDPR) with data masking.\n- and finding common ground to address any concerns or objections.\n- build trust.\n- make agreements of terms of use/ownership/document the data accessing process.\n\nHow would you go about obtaining the necessary permissions for a dataset?\n- establishing clear communication channels within the organsisation.\n- obtaining necessary approvals\n- emphasizing the value of collaboration.\n\nHow would you gather sensitive data?\n- Get consent. Ensure anonymity (follow regulations)\n\nHow to you ensure data is unbiased and representative.\n- Stratified sampling, (group then randomly sample).\n- Examine the data sources.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "preprocessing"
    ],
    "type": "process",
    "normalized_filename": "data_ingestion",
    "outlinks": [
      "data_streaming",
      "database",
      "data_pipeline",
      "machine_learning",
      "api",
      "scalability",
      "data_engineering_tools",
      "data_quality",
      "latency",
      "business_intelligence"
    ],
    "inlinks": [
      "data_engineering_tools",
      "data_lakehouse",
      "data_lifecycle_management",
      "data_pipeline",
      "data_pipeline_to_data_products",
      "data_storage",
      "data_warehouse",
      "powerquery"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Data Orchestration",
    "sha": "d935a610f8a41dd30164f6178ca6c4800c58a818",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Data%20Orchestration.md",
    "text": "Data orchestration refers to the process of managing and coordinating the flow of data across various systems and environments, particularly in complex and heterogeneous cloud settings. A Data Orchestrator is responsible for modeling dependencies between different tasks, ensuring that data is processed and moved efficiently from one stage to another. It integrates with both legacy systems and modern cloud-based tools, as well as data lakes and data warehouses.\n\nThe orchestration process involves invoking computations, such as executing business logic in languages like SQL and Python, and applying machine learning models at the appropriate times. These actions can be triggered based on time schedules or custom-defined logic. Essentially, data orchestration ensures that all components of a data workflow are executed in the correct order and at the right time, facilitating seamless data integration and processing across diverse systems.\n\n```dataview\nLIST\nfrom #data_orchestration\nsort file.name asc\n```\n\n## A Data Orchestrator\n\nA Data Orchestrator models dependencies between different tasks in [complex heterogeneous cloud environments](https://mattturck.com/data2021/) end-to-end. It handles integrations with legacy systems, new cloud-based tools, and your data lakes and data warehouses. It invokes [computation](https://en.wikipedia.org/wiki/Orchestration_(computing)), such as wrangling your business logic in [SQL](SQL.md) and [Python](term/python.md) and applying ML models at the right time based on a time-based trigger or by custom-defined logic.\n\nMore Insights in [Data Orchestration Trends: The Shift from Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).\n\n[[Data Pipeline to Data Products]]",
    "aliases": [
      "#orchestration"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "data_orchestration",
    "outlinks": [
      "data_pipeline_to_data_products"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Data Pipeline to Data Products",
    "sha": "6524221af877bada83bf41e12e0590f1a14ef31e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Data%20Pipeline%20to%20Data%20Products.md",
    "text": "The journey from [[Data Pipeline]] to [[Data Product]] involves transforming raw data into valuable insights or applications that can be used to drive business decisions. This process typically includes several stages, each with its own set of tasks and objectives.\n\nRead more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).\n### Workflow\n\n1. Define Objectives:\n   - Understand the business goals and what insights or products are needed.\n\n1. Design the Pipeline:\n   - Plan the architecture and select appropriate tools for each stage of the pipeline.\n\n1. Implement and Test:\n   - Build the pipeline, ensuring data flows smoothly from ingestion to product delivery.\n   - Test for accuracy, performance, and reliability.\n\n1. Deploy and Monitor:\n   - Deploy the pipeline in a production environment.\n   - Continuously monitor for performance and make adjustments as needed.\n\n1. Iterate and Improve:\n   - Gather feedback and refine the pipeline and products to better meet business needs.\n### Example\n\nImagine a retail company wants to create a recommendation system for its online store:\n\n1. [[Data Ingestion]]: Collect customer browsing and purchase data from the website.\n2. Data Processing: Clean and transform the data to identify patterns in customer behavior.\n3. Data Storage: Store the processed data in a data warehouse for easy access.\n4. Data Analysis: Use machine learning algorithms to analyze the data and generate recommendations.\n5. Data Visualization: Create dashboards to visualize customer trends and recommendation performance.\n6. Data Products: Deploy the recommendation system on the website to enhance customer experience.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "data_pipeline_to_data_products",
    "outlinks": [
      "data_ingestion",
      "data_product",
      "data_pipeline"
    ],
    "inlinks": [
      "data_orchestration",
      "data_pipeline"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Data Pipeline",
    "sha": "af8a643c02fe7dece17161d41857d8de172499cc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Data%20Pipeline.md",
    "text": "A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can be stored, analyzed, and used to generate insights. \n\nIt ensures that data flows smoothly and efficiently through different stages, maintaining data quality and [[Data Integrity]].\n\nBy implementing a data pipeline, organizations can automate data workflows, reduce manual effort, and ensure timely and accurate data delivery for decision-making.\n### Workflow\n\n1. [[Data Ingestion]]\n2. [[Data Transformation]]\n3. [[Data Storage]]\n4. [[Preprocessing|Data Preprocessing]]\n5. [[Data Management]]\n#### Other steps:\n\nDesign:\n   - Define the objectives and requirements of the data pipeline.\n   - Choose appropriate tools and technologies.\n\nDevelopment:\n   - Build the pipeline components and integrate them into a cohesive system.\n\nTesting:\n   - Validate the pipeline to ensure data accuracy and performance.\n\nDeployment:\n   - Deploy the pipeline in a production environment.\n\nMonitoring and Maintenance:\n   - Continuously monitor the pipeline and make necessary adjustments to improve performance and reliability.\n\n### Related Notes\n\n- [[Data Pipeline to Data Products]]",
    "aliases": [
      "ETL Pipeline"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "process"
    ],
    "normalized_filename": "data_pipeline",
    "outlinks": [
      "data_integrity",
      "data_ingestion",
      "data_pipeline_to_data_products",
      "data_transformation",
      "preprocessing",
      "data_management",
      "data_storage"
    ],
    "inlinks": [
      "data_engineer",
      "data_engineering",
      "data_ingestion",
      "data_management",
      "data_pipeline_to_data_products",
      "github_actions",
      "spacy"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Data Streaming",
    "sha": "46a957648ab07b26084cc2d49a08043e1bb22841",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Data%20Streaming.md",
    "text": "Data Streaming is used for real-time data processing, allowing continuous flow and processing of data as it arrives. This is different from [[Batch Processing]], which handles data in chunks.\n\nThe key to data streaming is the [[Publish and Subscribe]]\n  \n[[Apache Kafka]]\n\nExample:\n  - Companies like Netflix use Kafka to handle billions of messages daily, powering real-time recommendations, analytics, and user activity tracking.\n\n[[Alternatives to Batch Processing]]\n\n\n\n[[Data Streaming]]\n   **Tags**:",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "data_streaming",
    "outlinks": [
      "publish_and_subscribe",
      "alternatives_to_batch_processing",
      "apache_kafka",
      "data_streaming",
      "batch_processing"
    ],
    "inlinks": [
      "alternatives_to_batch_processing",
      "data_ingestion",
      "data_streaming",
      "distributed_computing",
      "generators_in_python",
      "lambda_architecture",
      "publish_and_subscribe"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Databricks & dbt",
    "sha": "74893b113e61a016e7a648c15d5b394c593644eb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Databricks%20&%20dbt.md",
    "text": "## How [[dbt]] and [[Databricks]] work together\n\nWhen using dbt with Databricks, dbt becomes the transformation orchestration layer on top of the Databricks SQL engine (or Spark).\n\nYou connect dbt to Databricks using a dbt adapter (e.g., `dbt-databricks`). Then dbt compiles your SQL models into Databricks-compatible SQL and executes them there.\n\n### The typical flow:\n\n1. Data ingestion: Data lands in Databricks (raw zone) via ETL tools (e.g., Fivetran, Airbyte).\n2. Transformation: dbt connects to Databricks, and executes transformations defined in dbt models — creating clean “silver” and “gold” tables.\n3. Analytics/ML: Cleaned and modeled data is then used within Databricks notebooks, dashboards, or ML pipelines.\n\n## Why you’d use dbt inside Databricks\n\nUsing dbt in Databricks gives you:\n\n* Modular, version-controlled SQL transformations\n* Data quality tests (e.g., assert non-null keys, unique constraints)\n* Dependency management between models\n* Documentation and lineage auto-generated from your models\n* CI/CD integration for deployment and testing\n\nIn contrast, Databricks alone gives you:\n\n* Raw compute and storage\n* Execution of arbitrary data workloads\n* Broader support for Python/ML pipelines\n\nSo dbt adds governance, structure, and maintainability to your SQL transformations in Databricks.\n\n### Analogy\n\n| Concept          | Databricks                         | dbt                                     |\n| ---------------- | ---------------------------------- | --------------------------------------- |\n| Primary function | Compute & storage (data lakehouse) | Transform & model data                  |\n| Language focus   | SQL, Python, Scala, R              | SQL                                     |\n| Execution        | Runs Spark jobs / SQL queries      | Orchestrates SQL models                 |\n| Purpose          | Platform for all data workloads    | Framework for analytics transformations |\n| Works with       | Delta tables, Unity Catalog        | Databricks SQL endpoint or cluster      |\n\n### Example\n\nSuppose you have a raw table in Databricks:\n`raw.sales_transactions`\n\nIn dbt, you would define a model like this:\n\n```sql\n-- models/sales_summary.sql\nSELECT \n  customer_id,\n  SUM(amount) AS total_spent,\n  COUNT(*) AS transactions\nFROM {{ ref('raw_sales_transactions') }}\nGROUP BY customer_id\n```\n\ndbt compiles this to Databricks SQL and materializes it as a Delta table in your catalog (e.g., `analytics.sales_summary`).",
    "aliases": [],
    "date modified": "22-10-2025",
    "tags": [
      "data_integration",
      "data_modeling",
      "data_pipeline",
      "governance"
    ],
    "normalized_filename": "databricks_&_dbt",
    "outlinks": [
      "databricks",
      "dbt"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Databricks vs Snowflake",
    "sha": "7126e19e74370113f7564d5c4a5ee1d95b2d5276",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Databricks%20vs%20Snowflake.md",
    "text": "Comparison between **[[categories/devops/Databricks]]** and **[[Snowflake]]**:\n\n- **Databricks** is a platform that emphasizes collaborative data science and engineering through interactive notebooks, making it suitable for advanced analytics and machine learning applications.\n- **Snowflake**, on the other hand, focuses on [[Data Warehouse]] and offers a robust SQL interface for analytics, making it a preferred choice for organizations prioritizing data storage and reporting capabilities.\n\n| Feature                      | **Databricks**                                                                                                                     | **Snowflake**                                                                                                      |\n| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n| **Primary Functionality**    | Unified ==analytics platform== for big data processing and machine learning.                                                       | Cloud-based data warehousing and analytics platform.                                                               |\n| **Data Processing**          | Built on [[Apache Spark]], optimized for large-scale data processing and machine learning workflows.                               | Uses its own SQL-based engine for data warehousing; excels in querying structured data.                            |\n| **Collaboration**            | Emphasizes collaboration through **notebooks** (e.g., Jupyter `.ipynb` files) that allow for interactive data analysis and coding. | Provides features for data sharing and collaboration but lacks the notebook interface.                             |\n| **Data Structure**           | Supports both structured and unstructured data, integrating seamlessly with data lakes (e.g., Delta Lake).                         | Primarily designed for structured data and semi-structured data (like JSON) stored in tables.                      |\n| **Scalability**              | Uses clusters to scale up compute resources dynamically; suitable for big data workloads.                                          | Offers automatic scaling of compute and storage resources, focusing on cost-effective scaling.                     |\n| **Machine Learning Support** | Integrated support for ML libraries (e.g., MLlib, MLflow) to build and deploy machine learning models.                             | Limited built-in support for machine learning, primarily used for data storage and querying.                       |\n| **Query Language**           | Supports multiple programming languages (Python, R, Scala, SQL) within notebooks.                                                  | Primarily uses SQL for querying data, providing a familiar interface for data analysts.                            |\n| **Deployment**               | Available on major cloud platforms (AWS, Azure, [[Google Cloud Platform|GCP]]); allows for more customization and flexibility in deployment.                 | Also cloud-native, designed for seamless deployment in the cloud, with less emphasis on infrastructure management. |\n| **Use Cases**                | Ideal for big data analytics, data engineering, and data science projects requiring complex processing.                            | Best suited for traditional data warehousing, business intelligence, and analytics use cases.                      |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cloud"
    ],
    "normalized_filename": "databricks_vs_snowflake",
    "outlinks": [
      "categories/devops/databricks",
      "snowflake",
      "apache_spark",
      "data_warehouse",
      "google_cloud_platform"
    ],
    "inlinks": [
      "snowflake"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Databricks",
    "sha": "17d6c26daf4146dc3ce2e5c59589249877b6cbd4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Databricks.md",
    "text": "Databricks is a cloud-based data analytics and engineering platform built on top of [[Apache Spark]]. It provides a workspace for [[Data Engineer]]s, data scientists, and analysts to collaborate on [[Big Data]] and machine learning projects.\n\nDatabricks simplifies large-scale data processing and enables real-time analytics by combining compute [[scalability]], Delta Lake reliability, and integrated ML tooling.\n### Platform Integration\n\n* Cloud Compatibility: Supports all major providers - AWS, Azure, and [[Google Cloud Platform|GCP]].\n* Technology Stack: Combines the functionality of:\n  * [[Apache Spark]] for distributed data processing\n  * [[Delta Lake]] for ACID-compliant storage and versioning\n  * [[MLflow]] for machine learning lifecycle management\n* Data [[Data Lakehouse|Lakehouse]] Architecture: Unifies [[Data Warehouse|data warehouse]] performance with [[Data Lake|data lake]] scalability.\n\n### Core Components\n\n1. **Clusters**: Provide the distributed compute power for running Spark jobs.\n2. **Workspaces**: Shared environments where teams collaborate using notebooks, libraries, and jobs.\n3. **Notebooks**: Interactive development interfaces supporting Python, SQL, R, and [[Scala]]. They can be scheduled as jobs for production workflows.\n4. [[Catalogs, Schemas, and Tables in Databricks]]: Hierarchical namespaces used to organize and govern data.\n5. [[Delta Tables in Databricks|Delta Table]]: Core storage abstraction providing ACID transactions, [[schema evolution]], and time travel capabilities.\n\n### Scalability and Reliability\n\nDatabricks inherits scalability from the [[Hadoop]] ecosystem but offers significant improvements through:\n\n* Elastic clusters that auto-scale resources based on workload demand.\n* Fault tolerance for resilient job execution.\n* Delta Lake for consistent and recoverable data storage.\n\n### Typical Workflow\n\n1. Data Ingestion: Connect to external sources (e.g., APIs, databases, or Google Sheets).\n2. Transformation: Clean and prepare data using Spark DataFrames.\n3. Storage: Persist processed data into managed Delta tables.\n4. Analysis & Visualization: Query with SQL or connect to BI tools for reporting.\n5. Productionization: Convert notebooks into automated jobs for repeatable workflows.\n### Related:\n- [[Spark DataFrames in Databricks]]\n- [[Overwriting and Refreshing Tables in Databricks]]\n- [[Delta Tables in Databricks]]\n- [[Catalogs, Schemas, and Tables in Databricks]]\n\n## [[Databricks]]\n\n**Databricks** is a **data platform** built on **Apache Spark**, designed for:\n\n* Large-scale **data processing** (batch + streaming)\n* **Data engineering**, **machine learning**, and **analytics**\n* Managing data in a **Lakehouse architecture** (unifying data lakes and warehouses)\n* Storing data in **Delta tables** within a **catalog** (Unity Catalog)\n\nIt’s both a **processing engine** and a **collaborative workspace** for Python, SQL, R, and Scala.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "cloud",
      "data_engineering",
      "data_management",
      "data_pipeline",
      "software"
    ],
    "normalized_filename": "databricks",
    "outlinks": [
      "schema_evolution",
      "delta_tables_in_databricks",
      "data_engineer",
      "scala",
      "spark_dataframes_in_databricks",
      "catalogs,_schemas,_and_tables_in_databricks",
      "scalability",
      "overwriting_and_refreshing_tables_in_databricks",
      "apache_spark",
      "mlflow",
      "data_lakehouse",
      "hadoop",
      "data_lake",
      "delta_lake",
      "databricks",
      "big_data",
      "data_warehouse",
      "google_cloud_platform"
    ],
    "inlinks": [
      "big_data",
      "catalogs,_schemas,_and_tables_in_databricks",
      "databricks",
      "databricks_&_dbt",
      "delta_tables_in_databricks",
      "github_actions",
      "loading_google_sheets_into_databricks",
      "pyspark",
      "why_use_pyspark_in_databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Debugging",
    "sha": "d460109e29a27a5fdbf9746419413e04e805810f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Debugging.md",
    "text": "Debugging is the process of identifying, analyzing, and resolving bugs or defects in software while [[Testing]]. It is a critical part of the [[Software Development Life Cycle]], ensuring that applications function correctly and efficiently. Debugging involves several techniques and tools to pinpoint the source of errors and fix them.\n\nIn [[ML_Tools]] see:\n- [[Debugging.py]]\n- [[Testing_unittest.py]]\n- [[Testing_Pytest.py]]\n#### Key Concepts in Debugging\n\n1. [[Types of Computational Bugs]]: Understanding the types of bugs, such as cumulative rounding errors, integer overflow, and race conditions, is essential for effective debugging.\n\n2. **How to Manage/View Bugs**:\n   \n   - **Console Log/Dir**: Use console logging to output variable values and program states to the console, helping to trace the flow of execution.\n    \n   - **Availability in VSCode**: Visual Studio Code provides powerful debugging features like breakpoints, watch expressions, and call stacks to help developers inspect and modify code execution.\n     \n   - **Sample Script**: Creating a minimal script that reproduces the bug can simplify the debugging process by isolating the problem.\n     \n   - **Logging in Python**: Python's logging module allows developers to record events, errors, and informational messages, which can be crucial for diagnosing issues.\n     \n   - **Run and Debug**: Step through code execution using debugging tools to observe the program's behavior and identify where it deviates from expected results.\n     \n   - **Log Point/Break Point**: Set breakpoints to pause execution at specific lines of code, allowing inspection of variables and program state at that moment.\n\n#### Solution Attempts\n\n1. **Reproduce the Bug**: Simplifying the code to reproduce the bug helps in understanding its cause and facilitates easier sharing with others for collaborative debugging. Sharing on platforms like [[StackBiz]] can help others contribute to the solution.\n\n2. **Automated Testing**: Implementing automated tests ensures that code changes do not introduce new bugs and that existing functionality remains intact.\n\n3. **Test-Driven Development (TDD)**: Writing tests before the actual code helps define expected behavior and ensures that the code meets these expectations.\n\n4. **Static Analysis**: Tools like [[TypeScript]] and ESLint analyze code for potential errors without executing it, helping to catch issues early in the development process.\n\n#### Debugging Tools and Techniques\n\n- **Integrated Development Environments (IDEs)**: IDEs like Visual Studio Code, IntelliJ IDEA, and Eclipse offer built-in debugging tools that streamline the debugging process.\n  \n- **Version Control Systems**: Tools like [[Git]] allow developers to track changes and revert to previous versions if a bug is introduced.\n  \n- **Profilers**: These tools analyze program performance and help identify bottlenecks or inefficient code paths.\n  \n- **Memory Analyzers**: Tools like Valgrind help detect memory leaks and other memory-related issues.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "ML_Tools"
    ],
    "normalized_filename": "debugging",
    "outlinks": [
      "testing_pytest.py",
      "testing_unittest.py",
      "git",
      "debugging.py",
      "types_of_computational_bugs",
      "typescript",
      "software_development_life_cycle",
      "stackbiz",
      "testing",
      "ml_tools"
    ],
    "inlinks": [
      "generators_in_python",
      "grep",
      "pydantic",
      "pyright",
      "types_of_computational_bugs"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Declarative Data Pipeline",
    "sha": "f8c3eb4c48347ce9f49dbca48264428c5bc35f4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Declarative%20Data%20Pipeline.md",
    "text": "In a **declarative data pipeline**, the focus is on *what* needs to be achieved, not *how* it should be executed. You define the desired outcome or the data products, and the system takes care of the underlying execution details, such as the order in which tasks are performed. This is in contrast to an **imperative** pipeline, where the developer explicitly specifies the steps and the order in which they should be executed. Here's a breakdown of the key aspects:\n\n### **Declarative Programming**:\n- Focuses on *==what==* needs to be done.\n- Describes the desired state or result without dictating the control flow or step-by-step process.\n- In a data pipeline context, a declarative approach might involve specifying the desired data products and letting the system optimize how and when different parts of the pipeline are executed.\n- Example: [[SQL]] is often considered declarative because you specify the result you want (e.g., the output of a query) without explicitly stating the steps for how the database engine should retrieve it.\n\n### **Imperative Programming**:\n- Focuses on *==how==* tasks should be done.\n- Specifies the ==control flow== explicitly, dictating the exact steps to be performed and the order of operations.\n- In a data pipeline, this would involve writing scripts that detail each step in the transformation and loading process in the sequence they must be executed.\n- Example: A series of Python scripts that process data in a specific sequence.\n\n### **Advantages of Declarative Pipelines**:\n1. **Easier to Debug**: Since the desired state is clearly defined, it is easier to identify discrepancies between the intended outcome and the current state. This can help pinpoint issues in the pipeline.\n   \n2. **Automation**: Declarative systems often enable better automation since the system has the flexibility to determine the most efficient way to achieve the defined goals.\n\n3. **Simplicity and Intent**: Declarative approaches focus on the *==intent==* of the program, making it easier for others to understand what the program is supposed to do without having to dive into implementation details. \n\n4. **Reactivity**: The pipeline can automatically adjust when inputs or dependencies change. For example, if certain data dependencies change, the system can rerun the necessary parts of the pipeline to maintain consistency.\n\n### **Example in Data Engineering**:\n\nA declarative approach to data engineering would involve **Functional Data Engineering** principles. This involves treating data as immutable and focusing on defining the desired transformations and outputs in a declarative manner. Instead of writing imperative scripts for each data transformation step, you'd define the desired outputs, and the system would optimize the execution.\n\n### **Use Cases**:\nDeclarative pipelines are particularly useful in [[data lineage]], **[[Data Observability]]**, and [[Data Quality]] monitoring**. By defining *what* data products should exist and what their properties should be, it's easier to track changes and ensure the consistency and quality of data. It also makes systems more resilient to changes, as the declarative nature enables the system to adjust the execution order or method dynamically, based on current conditions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "process"
    ],
    "normalized_filename": "declarative_data_pipeline",
    "outlinks": [
      "data_lineage",
      "data_quality",
      "data_observability",
      "sql"
    ],
    "inlinks": [
      "altair",
      "dagster",
      "pydantic"
    ]
  },
  {
    "category": null,
    "filename": "Delta Tables in Databricks",
    "sha": "55d7471d8c8ef499380f5c3f77b56fb06b9ba6ea",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Delta%20Tables%20in%20Databricks.md",
    "text": "A Delta Table is a storage format used in [[Databricks]] and [[Apache Spark]] that brings reliability, consistency, and performance to big data storage. It extends the traditional [[Parquet]] format with ACID transactions, schema enforcement, and time travel capabilities.\n\nA Delta Table stores data in the Delta Lake format, which sits on top of a data lake (e.g., [[Azure Data Lake]], [[AWS S3]], or GCP Storage). It allows users to read, write, and modify large-scale datasets safely — similar to how a relational [[database]] table works.\n### Key Features\n\n[[ACID Transaction]]\n   * Ensures *atomicity, consistency, isolation, and durability* for all operations.\n   * Prevents data corruption during concurrent writes or failures.\n\n[[Schema Evolution]] and Enforcement\n   * Rejects writes that don’t match the table schema.\n   * Optionally allows automatic schema evolution when new columns are added.\n\nTime Travel\n   * Enables querying historical versions of data using a timestamp or version number.\n   * Useful for debugging, audits, or reproducing past results.\n\nUpserts and Deletes (MERGE INTO)\n   * Supports `UPDATE`, `DELETE`, and `MERGE` operations natively.\n   * Simplifies data correction and change data capture (CDC) workflows.\n\nOptimized Reads and Writes\n   * Uses file-level metadata tracking and data skipping for efficient queries.\n\n### Delta Table Structure\n\nA Delta Table is stored as a directory containing:\n* Data files (in Parquet format).\n* A _delta_log/ directory that maintains the transaction history as JSON files.\n### Example Usage\n\n```python\n# Create Delta table from a DataFrame\ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"example.sales_transactions\")\n\n# Read Delta table\ndf = spark.read.format(\"delta\").table(\"example.sales_transactions\")\n\n# Query historical data (time travel)\ndf_old = spark.read.format(\"delta\").option(\"versionAsOf\", 3).table(\"example.sales_transactions\")\n```\n\n### Advantages Over [[Parquet]]\n\n| Feature           | Parquet     | Delta Table   |\n| ----------------- | ----------- | ------------- |\n| ACID Transactions | ❌           | ✅             |\n| Schema Evolution  | ⚠️ (manual) | ✅ (automatic) |\n| Time Travel       | ❌           | ✅             |\n| Updates/Deletes   | ❌           | ✅             |\n| Data Skipping     | ⚠️          | ✅             |\n\n### Related:\n- [[Catalogs, Schemas, and Tables in Databricks]]",
    "aliases": [
      "Delta Table"
    ],
    "date modified": "19-10-2025",
    "tags": [
      "data_engineering",
      "data_management",
      "databricks"
    ],
    "normalized_filename": "delta_tables_in_databricks",
    "outlinks": [
      "schema_evolution",
      "parquet",
      "database",
      "acid_transaction",
      "azure_data_lake",
      "catalogs,_schemas,_and_tables_in_databricks",
      "apache_spark",
      "databricks",
      "aws_s3"
    ],
    "inlinks": [
      "catalogs,_schemas,_and_tables_in_databricks",
      "databricks",
      "loading_google_sheets_into_databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "DevOps",
    "sha": "b32181e93ade8f9ac8fa58b36b173253ee1e56da",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/DevOps.md",
    "text": "DevOps refers to practices for collaboration and automation between [[Devops Portal]] (Dev) and IT operations (Ops) teams, aiming for faster, more reliable software delivery.\n\n**Integration**: It integrates the work of software development and operations teams by fostering a culture of collaboration and shared responsibility.\n\n**Principles**: \n- Emerges from Agile principles.\n- Emphasizes collaboration between development and operations teams.\n\n**Approach**: \n- Utilizes continuous integration and continuous delivery ([[CI-CD]]) to ensure frequent code changes and quick feedback loops.\n- Enables rapid and reliable updates with high levels of automation and efficiency.\n\n**Goals**: \n - Ensures existing processes are optimized and streamlined.\n\n Related to:\n- [[DataOps]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "devops",
    "outlinks": [
      "dataops",
      "devops_portal",
      "ci-cd"
    ],
    "inlinks": [
      "devops_portal",
      "machine_learning_operations",
      "software_development_life_cycle"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Devops Portal",
    "sha": "ca68c2a29941792de05dbd48dfa6dcdf53a5cda8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Devops%20Portal.md",
    "text": "Tools:\n- [[tool.uv]]\n- [[tool.ruff]]\n\nFile types:\n- [[Justfile]]\n- [[TOML]]\n- [[Makefile]]\n- [[Json]]\n\nPractices:\n- [[Testing]]\n- [[Documentation & Meetings]]\n\nRelated to:\n- [[DevOps]]\n\n```dv\nTABLE file.name AS \"Note\", length(file.inlinks) AS \"Backlinks\"\nFROM \"\"\nSORT length(file.inlinks) DESC\nLIMIT 100\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "devops_portal",
    "outlinks": [
      "json",
      "justfile",
      "tool.ruff",
      "toml",
      "tool.uv",
      "testing",
      "makefile",
      "documentation_&_meetings",
      "devops"
    ],
    "inlinks": [
      "devops",
      "security_vulnerabilities"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Digital Transformation",
    "sha": "785b57c6b509384629366b21a8f8799d0e1f830d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Digital%20Transformation.md",
    "text": "Digital transformation should not only address today’s inefficiencies, but also prepare digital assets and infrastructure for future automation, analytics, and growth. This requires designing systems with future tracking and scalability in mind.\n\n==Digital transformation starts with data centralisation==\n\nTo digitally transform your department, you need a structured and strategic approach that addresses both technological and organisational change. This includes understanding current practices, engaging stakeholders, aligning tools to business goals, and fostering a culture of continuous improvement.\n\nDigital transformation is not just a technological initiative—it is a [[Change Management|change program]].\n\nKey benefits include:\n\n- Saving time and money.\n- Increasing capacity and responsiveness.\n- Systematically improving business processes.\n\nAreas typically improved:\n\n- Reporting and financial operations.\n- Data quality and backend systems.\n- Customer service and engagement.\n\nCommon issues to address:\n\n- Siloed systems.\n- Lack of KPIs.\n- Multiple uncoordinated spreadsheets.\n- Ad hoc reporting with no repeatable process.\n- Poor documentation and low system transparency.\n- Missed insights due to fragmented or low-quality data.\n- Reactive rather than proactive operations.\n\n### [[Data Audit]]\n\n- Understand Department Processes: Identify current workflows, systems, and technologies being used.\n\n- Identify Pain Points: Gather employee feedback on inefficiencies, bottlenecks, and challenges.\n\n- [[Data Collection]] and Analysis: Assess how existing data is being used—or underused—in decision-making.\n\n- Conduct a Company-Wide Data Audit: Document where data is stored, in what formats, and how accessible it is.\n\n- Assess [[Data Quality]] and Debt: Evaluate areas with poor data quality or inconsistent data practices; establish strategies to manage and reduce data debt.\n\n### Define Clear Objectives\n\n- Set Strategic Goals: Align digital transformation efforts with organisational priorities such as operational efficiency, improved service delivery, or enhanced analytics.\n\n- Define Quantifiable Metrics: Identify KPIs (e.g. reduced processing time, increased customer satisfaction, higher data accuracy) to track transformation success.\n\n- Client-Focused Goals: Ensure the goals align with what the client needs—better understanding of the network, clearer billing, actionable insights, and planning tools.\n\n### Engage Stakeholders\n\n- Leadership Buy-In: Secure executive support and ownership of the transformation strategy.\n\n- Employee Involvement: Engage staff to understand operational realities and gain their input and support.\n\n- Stakeholder Mapping: Identify key decision-makers, influencers, and change agents across the department.\n\n### Evaluate and Select Technologies\n\n- Research Tools: Identify relevant platforms such as analytics tools, automation systems, and cloud services.\n\n- Pilot Testing: Run small-scale pilots to validate value and feasibility before large-scale implementation.\n\n- Integration and [[Interoperability]]: Ensure selected tools integrate with existing systems and datasets.\n\n### Build a Transformation Roadmap\n\n- Prioritise Initiatives: Focus on high-impact areas, such as automating manual tasks or centralising data.\n\n- Create a Timeline: Outline phased implementation steps, with milestones and review points.\n\n- Allocate Resources: Assign teams, budgets, and tools required to execute the roadmap.\n\n### [[Change Management]] and Training\n\n- Manage Resistance: Communicate benefits clearly to reduce fear and resistance to change.\n\n- Training and Support: Provide comprehensive training and continuous support for new systems and practices.\n\n- Foster a Digital Culture: Encourage experimentation, innovation, and learning across the organisation.\n\n### Implement New Technologies and Processes\n\n- Phased Rollout: Deploy technologies in manageable phases to allow iteration and adaptation.\n\n- Monitor and Iterate: Continuously evaluate implementation progress and adjust based on feedback.\n\n### Measure and Optimise\n\n- Track Success Metrics: Use pre-defined KPIs to evaluate outcomes.\n\n- Continuous Improvement: Review workflows and tools regularly to refine and optimise.\n\n### Ensure Long-Term Sustainability\n\n- Encourage Innovation: Support ongoing adoption of new tools and practices.\n\n- Monitor Trends: Stay informed of new technologies, regulatory changes, and best practices.\n\n- Revisit Goals: Periodically assess organisational objectives and adjust digital strategies accordingly.\n\n### Example Focus Areas\n\n- Automation: Use RPA and AI to reduce manual work and errors.\n\n- Data-Driven Decision Making: Enable insight through analytics and dashboards.\n\n- Cloud Adoption: Improve [[Scalability]], collaboration, and accessibility with cloud platforms.\n\n- Collaboration Tools: Enhance teamwork and communication using platforms like Microsoft Teams or Slack.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "governance"
    ],
    "normalized_filename": "digital_transformation",
    "outlinks": [
      "data_audit",
      "scalability",
      "interoperability",
      "data_quality",
      "change_management",
      "data_collection"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Docker Image",
    "sha": "9ede73785979f676e4862f1a82046b264251c412",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Docker%20Image.md",
    "text": "A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. Docker images are used to create Docker containers, which are instances of these images running in an isolated environment.\n\nDocker images are used for:\n\n1. **Consistency**: They ensure that software runs the same way regardless of where it is deployed, whether on a developer's laptop, a test server, or in production.\n\n2. **Portability**: Docker images can be easily shared and moved across different environments, making it easier to deploy applications.\n\n3. **Version Control**: Images can be versioned, allowing developers to track changes and roll back to previous versions if necessary.\n\n4. **Isolation**: Each Docker container runs in its own isolated environment, which helps in avoiding conflicts between different applications or services running on the same host.\n\n5. **Scalability**: Docker images can be used to quickly scale applications by running multiple containers from the same image.\n### Example: Running a Simple Web Server (dont do unless required)\n\n1. **Install Docker**: First, ensure Docker is installed on your machine. You can download it from the [Docker website](https://www.docker.com/products/docker-desktop).\n\n2. **Pull a Docker Image**: Use a pre-built Docker image from Docker Hub. For this example, we'll use the official Nginx image, which is a popular web server.\n\n   ```bash\n   docker pull nginx\n   ```\n\n3. **Run a Docker Container**: Start a container using the Nginx image. This will run the web server.\n\n   ```bash\n   docker run --name my-nginx -d -p 8080:80 nginx\n   ```\n\n   - `--name my-nginx`: Names the container \"my-nginx\".\n   - `-d`: Runs the container in detached mode (in the background).\n   - `-p 8080:80`: Maps port 80 in the container to port 8080 on your host machine.\n\n4. **Access the Web Server**: Open a web browser and go to `http://localhost:8080`. You should see the default Nginx welcome page, indicating that the web server is running inside the Docker container.\n\n5. **List Running Containers**: Check which containers are running.\n\n   ```bash\n   docker ps\n   ```\n\n6. **Stop the Container**: When you're done, you can stop the container.\n\n   ```bash\n   docker stop my-nginx\n   ```\n\n7. **Remove the Container**: If you no longer need the container, you can remove it.\n\n   ```bash\n   docker rm my-nginx\n   ```\n\n### Key Features Demonstrated:\n\n- **Portability**: The Nginx server runs the same way on any machine with Docker installed.\n- **Isolation**: The web server runs in its own environment, separate from other applications.\n- **Ease of Use**: Starting and stopping services is straightforward with simple commands.\n- **Resource Efficiency**: Docker containers are lightweight compared to virtual machines.\n\nThis example gives you a taste of how Docker can be used to quickly deploy and manage applications. As you become more familiar with Docker, you can explore building your own images, managing multi-container applications with Docker Compose, and more.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "docker_image",
    "outlinks": [],
    "inlinks": [
      "docker"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Docker",
    "sha": "1279ed7e995e6e620d4a181ccbe3fa31cfe85d75",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Docker.md",
    "text": "Utilizes Docker images [[Docker Image]] to set up containers for consistent development and testing environments.\n\nContainers can include necessary dependencies like Python and pip.\n\nTutorial:\n- [The Only Docker Tutorial You Need To Get Started](https://www.youtube.com/watch?v=DQdB7wFEygo)\n\nDocker Volumes - storing data\n\nDocker Compose\n\nMulti use containers\n\n`docker init` command\n- generated Dockerfile - image\n- compose.yaml\n- .dockerignore",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "software"
    ],
    "normalized_filename": "docker",
    "outlinks": [
      "docker_image"
    ],
    "inlinks": [
      "ci-cd",
      "github_actions"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Elastic Net",
    "sha": "7a8ce61e58fb41bdb9346468061f0eb22866ae36",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Elastic%20Net.md",
    "text": "This method combines both L1 ([[L1 Regularisation]]) and L2 ([[Ridge]]) regularization by adding both absolute and squared penalties to the loss function. It strikes a balance between Ridge and Lasso.\n\nIt is particularly useful when you have high-dimensional datasets with highly correlated features.\n\nThe Elastic Net loss function is:\n\n    $$\\text{Loss} = \\text{MSE} + \\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2$$\n    \nwhere $\\lambda_1$ controls the L1 regularization and $\\lambda_2$ controls the L2 regularization.\n\n#### Code\n\n```python\nfrom sklearn.linear_model import ElasticNet\n\n# Initialize an Elastic Net model\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls the L1/L2 mix\nmodel.fit(X_train, y_train)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet"
    ],
    "normalized_filename": "elastic_net",
    "outlinks": [
      "l1_regularisation",
      "ridge"
    ],
    "inlinks": [
      "embedded_methods"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Event Driven Events",
    "sha": "e14823fca2c8c9ca2b078d6e152aa3e4e42b5df0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Event%20Driven%20Events.md",
    "text": "Events can be stored in a [[Data Lake]] and analysed to find patterns/predictions.  \n\n[[Event Driven Microservices]] allow for [[Business observability]]\n\n[[Monolith Architecture]]\n\n[[Event Driven Microservices]]\n\n[[API Driven Microservices]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "event_driven_events",
    "outlinks": [
      "api_driven_microservices",
      "business_observability",
      "event_driven_microservices",
      "monolith_architecture",
      "data_lake"
    ],
    "inlinks": [
      "aws_lambda",
      "event_driven",
      "event_driven_microservices"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Environment Variables",
    "sha": "f04b288ef495f31d318d487c8b2ae5ab81fb5036",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Environment%20Variables.md",
    "text": "Solution 1: Set Environment Variables Permanently (Recommended)\nThis ensures that environment variables persist across sessions.\n\nOn Windows (Permanent)\nOpen Control Panel → System → Advanced system settings → Environment Variables.\n\nUnder System Variables, click New.\n\nVariable Name: PG_USER\n\nVariable Value: postgres\n\nClick New again.\n\nVariable Name: PG_PASSWORD\n\nVariable Value: your_password\n\nClick OK and restart your computer.\n\nOnce restarted, Jupyter Notebook should be able to access the variables.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "system"
    ],
    "normalized_filename": "environment_variables",
    "outlinks": [],
    "inlinks": [
      "bat",
      "pydantic"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Epub",
    "sha": "4a12e7adb93723084de6f2f8128b89a3fa3dce28",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Epub.md",
    "text": "An EPUB (short for *electronic publication*) file is a widely used open eBook format that is designed for reflowable content, meaning it can adapt its layout to fit various screen sizes-unlike PDFs, which preserve a fixed layout.\n\n### Key Features of EPUB\n- Reflowable Text: The content adjusts to screen size, font preferences, and orientation. This is ideal for smartphones, tablets, and e-readers like Kobo or Apple Books.\n- [[html]] + CSS Based: Internally, an EPUB file is a compressed archive (`.zip`) that contains HTML files, images, stylesheets, metadata, and a manifest.\n- Navigation: It supports table of contents, internal links, and chapters for easy navigation.\n- Supports Rich Media: EPUB 3 can include audio, video, interactive elements, and MathML.\n\n### How EPUB Shows “Pages”\n\nEPUB doesn't have fixed \"pages\" like PDF. Instead:\n\n- The reading software (like Apple Books, Calibre, or Kobo) dynamically splits content into pages based on screen size, font size, and user settings.\n- Pages can vary in number depending on:\n  - Device screen resolution\n  - Font size or style\n  - Margin settings\n\nBecause of this, you can't refer to a fixed page number universally across devices.\n### EPUB vs PDF\n\n| Feature                | EPUB                                 | PDF                                |\n|------------------------|--------------------------------------|------------------------------------|\n| Layout                 | Reflowable                           | Fixed                              |\n| Usability on small screens | Excellent                         | Poor                               |\n| Internal format        | HTML + CSS + XML                     | PostScript-based (binary)          |\n| Navigation             | Flexible (TOC, links, metadata)      | Static (can have TOC, but fixed)   |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "epub",
    "outlinks": [
      "html"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Event Driven",
    "sha": "c3cdf66a8c34b05d053accf41d53c2d3dbbcbe03",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Event%20Driven.md",
    "text": "Event-driven refers to a ==programming paradigm== or architectural style where the flow of the program is determined by events—changes in state or conditions that trigger specific actions or responses. \n\nIn this model, components of a system communicate through events, which can be generated by user interactions, system changes, or external sources.\n\n### Key Concepts of Event-Driven Architecture:\n\n1. Events: An event is a significant change in state or an occurrence that can trigger a response. For example, a user clicking a button, a file being uploaded, or a sensor detecting a change in temperature.\n\n2. Event Producers: These are components or services that generate events. For instance, a web application might produce events when users perform actions like signing up or making a purchase.\n\n3. Event Consumers: These are components or services that listen for and respond to events. They take action based on the events they receive, such as updating a database or sending a notification.\n\n4. Event Channels: These are the pathways through which events are transmitted from producers to consumers. This can include message queues, event buses, or streaming platforms.\n\n5. Loose Coupling: In an event-driven system, components are often loosely coupled, meaning that producers and consumers do not need to know about each other directly. This allows for greater flexibility and scalability.\n\nBenefits of Event-Driven Architecture:\n- [[Scalability]]\n- Responsiveness\n- Flexibility\n\n### Related topics\n\n- [[Event Driven Events]]\n- [[Event-Driven Architecture]]\n- [[Event Driven Microservices]]\n- **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "event_driven",
    "outlinks": [
      "scalability",
      "event-driven_architecture",
      "event_driven_events",
      "event_driven_microservices"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Excel vs Google Sheets",
    "sha": "f4d944e6ee0862a8262364a086cb91ef73cb0c07",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Excel%20vs%20Google%20Sheets.md",
    "text": "[[Excel]]\n[[Google Sheets]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "tool"
    ],
    "normalized_filename": "excel_vs_google_sheets",
    "outlinks": [
      "excel",
      "google_sheets"
    ],
    "inlinks": [
      "google_sheets"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Everything",
    "sha": "4b774a8c4b3253a6b2c94c34fe6f07a3a9e192c0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Everything.md",
    "text": "Software to [[Search]] through a computer effectively.\n\n- [ ] Can we search with descriptions ? \n\n## Tips\n\nuse \\ to match in paths i.e \\playground \n\ncan copy file \n\nnew window crl+ n\n\nUse | to get or search \n\nsearch syntax",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "everything",
    "outlinks": [
      "search"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Excel pivot table",
    "sha": "8af4f510bc025464c12f38c9c0edc8e856911464",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Excel%20pivot%20table.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "tool",
      "transformation"
    ],
    "normalized_filename": "excel_pivot_table",
    "outlinks": [],
    "inlinks": [
      "excel",
      "olap"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Excel",
    "sha": "6a1620be0d55e18d4171cbc78edc76dadec712a6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Excel.md",
    "text": "Data Modeling and Integration\n* Power Pivot: Enables creation of advanced data models and supports large datasets with fast calculations using DAX.\n* Data Model: Integrates multiple tables into a single model for use with PivotTables and Power Pivot.\n* Data Connections: Links external workbooks or databases so updates reflect dynamically when source data changes.\n* Related: See also [[Powerquery]] for [[ETL]] capabilities.\n\nLookup and Retrieval\n* VLOOKUP: Finds a value in the first column of a range and returns a value from a specified column in the same row.\n* INDEX-MATCH / INDEX-MATCH-MATCH: More flexible alternatives to VLOOKUP, allowing lookups by row and column positions.\n\nAnalysis and Forecasting\n* [[Excel pivot table]]: Summarizes, groups, and aggregates large datasets for interactive analysis.\n* Forecast Sheet: Generates time series forecasts using historical data.\n* What-If Analysis: Includes tools like Scenario Manager, Goal Seek, and Data Tables to explore how input changes affect outputs.\n* Consolidate: Aggregates data from multiple sheets or ranges into a single summary.\n\nFormula Tools and Validation\n* Evaluate Formula: Walks through a formula step-by-step to debug or understand calculations.\n* [[Data Validation]]: Restricts input values, enables drop-down lists, and enforces data entry rules.\n\nData Cleaning and Preparation\n* Text to Columns: Splits text from one column into multiple columns based on delimiters or fixed width.\n\nData model",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "file_type",
      "software"
    ],
    "normalized_filename": "excel",
    "outlinks": [
      "etl",
      "powerquery",
      "excel_pivot_table",
      "data_validation"
    ],
    "inlinks": [
      "excel_vs_google_sheets",
      "olap",
      "powerquery"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Firebase",
    "sha": "34a24e951473faa00b8e39c3a8461c4348d66cd5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Firebase.md",
    "text": "Googles version of [[AWS]]\n\n[Setup basics](https://www.youtube.com/watch?v=XC4Y1KLNLzI&list=WL&index=6)\n\nProject idea: Set up a basic emailer app.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "frontend"
    ],
    "normalized_filename": "firebase",
    "outlinks": [
      "aws"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "FastAPI",
    "sha": "e092a92cbaff0be57b27c501e0a437ccbd81bb65",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/FastAPI.md",
    "text": "**FastAPI** is a modern web framework for building APIs with Python. It is designed to be fast and easy to use, leveraging Python's type hints to provide features like:\n\n1. Automatic generation of OpenAPI documentation.\n2. Input data validation based on Python's type annotations.\n3. Asynchronous request handling with native support for `asyncio`.\n4. High performance, as it is built on Starlette and [[Pydantic]].\n### Key Features\n\n- **Automatic validation:** Based on type hints and Pydantic models.\n- **Interactive API docs:** Automatically generated Swagger UI and ReDoc.\n- **Asynchronous support:** Full support for async functions.\n- **Dependency injection:** Built-in support for dependencies.\n\n### How to Run\n\n==In [[ML_Tools]] see: [[FastAPI_Example.py]]== <- see this\n\n1. Save the script as `main.py`.\n2. Install FastAPI and Uvicorn:\n    `pip install fastapi uvicorn`\n    \n3. Run the server:  \n\t```cmd\n\tren FastAPI_Example.py main.py # possibly change to\n\tuvicorn FastAPI_Example:app --reload\n\t```\n    \n4. Open the browser and navigate to:\n    - **API documentation (Swagger UI):** `http://127.0.0.1:8000/docs`\n    - **ReDoc documentation:** `http://127.0.0.1:8000/redoc`",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "frontend"
    ],
    "normalized_filename": "fastapi",
    "outlinks": [
      "fastapi_example.py",
      "ml_tools",
      "pydantic"
    ],
    "inlinks": [
      "api",
      "model_deployment",
      "pydantic",
      "pyright_vs_pydantic"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "GIS",
    "sha": "58cf7972265ecfdc422b875fdb085023b7262a42",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/GIS.md",
    "text": "Geographic information system.\n\nFile formats: \n\nThe Web Map Tile Service (WMTS) and Web Feature Server (WFS) are both specifications used in the field of Geographic Information Systems (GIS) to serve different types of geographic data over the web. The primary differences between them lie in the type of data they serve and how they serve it.\n\n\t[[Web Map Tile Service (WMTS)]]\n\t[[Web Feature Server (WFS)]]\n\t[[Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)]]\n\n[[shapefile]]\n\nThere are free GIS software",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type",
      "software"
    ],
    "normalized_filename": "gis",
    "outlinks": [
      "web_map_tile_service_(wmts)",
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "web_feature_server_(wfs)",
      "shapefile"
    ],
    "inlinks": [
      "shapefile",
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "GPT",
    "sha": "bbecf37ff24fc14aad7a7dd33cd9e7135949ba3f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/GPT.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "agents",
      "cloud",
      "tool"
    ],
    "normalized_filename": "gpt",
    "outlinks": [],
    "inlinks": [
      "self-attention"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Git",
    "sha": "d8659400b978cd6453abcf3c763fd134a6402c96",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Git.md",
    "text": "tags:\n  - software\n\nDo git bash here.\n\ngit status\n\ngit add . (adds all)\n\ngit status\n\ngit commit -m \"\"\n\ngit push\n\n## Notes\n\nhttps://www.youtube.com/watch?v=xnR0dlOqNVE\n\n[Git Fork vs. Git Clone](https://www.youtube.com/watch?v=6YQxkxw8nhE)\n\n[[How to do git commit messages properly]]\n\n## Examples\n\n\n# Git: Common Issues and Fixes\n\nGit can be frustrating, especially when things go wrong. This guide provides practical solutions to common Git mistakes, explained in simple terms.\n\nhttps://ohshitgit.com/\n\n### how to remove something from a git history, if i forgot to add it to the gitignore, but now have\n\n2. Remove the file from the Git index\nThis tells Git to stop tracking the file.\n\ngit rm --cached path/to/file\nFor a folder:\ngit rm -r --cached path/to/folder\n\n3. Commit this change\nThis saves the removal from the index.\n\nbash\nCopy code\ngit commit -m \"Stop tracking path/to/file and add to .gitignore\"\n\n\n## Undoing Mistakes\n\n### I messed up badly! Can I go back in time?\n\nYes! Use Git’s reflog to find a previous state:\n\n```bash\ngit reflog\n# Find the index of the state before things broke\ngit reset HEAD@{index}\n```\n\n_This is useful for recovering deleted commits, undoing bad merges, or rolling back to a working state._\n\n## Commit Fixes\n\n### I committed but forgot a small change!\n\n```bash\n# Make the change\ngit add .\ngit commit --amend --no-edit\n```\n\n⚠ Warning: Never amend a commit that has already been pushed!\n\n### I need to change the last commit message!\n\n```bash\ngit commit --amend\n```\n\nThis will open an editor where you can modify the commit message.\n\n\n\n## 🔀 Branching Issues\n\n### I committed to `master` but wanted a new branch!\n\n```bash\n# Create a new branch from the current state\ngit branch new-branch\n# Remove the commit from master\ngit reset HEAD~ --hard\ngit checkout new-branch\n```\n\n⚠ Warning: If you’ve already pushed the commit, additional steps are needed.\n\n### I committed to the wrong branch!\n\n```bash\n# Undo the last commit but keep the changes\ngit reset HEAD~ --soft\ngit stash\ngit checkout correct-branch\ngit stash pop\ngit add .\ngit commit -m \"Moved commit to correct branch\"\n```\n\nAlternative:\n\n```bash\ngit checkout correct-branch\ngit cherry-pick master  # Moves last commit to correct branch\ngit checkout master\ngit reset HEAD~ --hard  # Removes the commit from master\n```\n\n\n\n## 🔍 Diff and Reset\n\n### I ran `git diff`, but it showed nothing!\n\nIf your changes are staged, use:\n\n```bash\ngit diff --staged\n```\n\nThis shows differences between the last commit and staged files.\n\n### I need to undo a commit from 5 commits ago!\n\n```bash\ngit log  # Find the commit hash\ngit revert [commit-hash]\n```\n\nThis creates a new commit that undoes the changes.\n\n\n\n## 🗑️ Undoing Changes\n\n### I need to undo changes to a file!\n\n```bash\ngit log  # Find a commit before the changes\ngit checkout [commit-hash] -- path/to/file\ngit commit -m \"Reverted file to previous version\"\n```\n\n### I want to reset my repo to match the remote!\n\n⚠ _Destructive action—this cannot be undone!_\n\n```bash\ngit fetch origin\ngit checkout master\ngit reset --hard origin/master\ngit clean -d --force  # Removes untracked files\n```\n\n\n\n## 🤯 Last Resort\n\nIf everything is completely broken, nuke the repo and reclone:\n\n```bash\ncd ..\nsudo rm -r repo-folder\ngit clone https://github.com/user/repo.git\ncd repo-folder\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "git",
    "outlinks": [
      "how_to_do_git_commit_messages_properly"
    ],
    "inlinks": [
      "debugging",
      "how_to_do_git_commit_messages_properly",
      "powershell_vs_bash"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Github Gists",
    "sha": "cc77a1b61478cf73fba7af6e0d7dcbce4cd323f3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Github%20Gists.md",
    "text": "**Purpose:**\nGitHub Gists are a way to **share code snippets, configuration files, or small text documents** easily. They provide a lightweight alternative to a full repository for storing and collaborating on small pieces of content.\n\n**Summary:**\nGitHub Gists provide a **fast, shareable, and versioned way** to manage small files, scripts, or snippets outside of a full repository while still benefiting from GitHub’s versioning and collaboration features.\n\n**Key Points:**\n\n* **Lightweight code sharing:** Store single files or a few small files without creating a full repo.\n* **Version control:** Every update to a Gist is versioned, so you can track changes over time.\n* **Public or secret:**\n  * Public Gists are searchable and viewable by anyone.\n  * Secret Gists are unlisted (not searchable) but still accessible via URL.\n* **Collaboration:** Others can **fork or comment** on your Gist.\n* **Integration with GitHub API:** You can programmatically create, update, or fetch Gists.\n* **Embedding:** Gists can be embedded in blogs, documentation, or other websites.\n\n**Common Use Cases:**\n* Sharing code snippets or scripts with others.\n* Maintaining small configuration files, templates, or notes.\n* Storing single-file tools for personal use (e.g., a resume JSON, scripts).\n* Quick experiments without creating a full repository.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "github_gists",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Github Actions",
    "sha": "10912068170b5add793bb1dc48733ade6eb1cc32",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Github%20Actions.md",
    "text": "GitHub Actions provides an automation layer for software and data workflows. It enables version-controlled CI/CD, scheduled analytics jobs, repository management, and integrations with external platforms. All defined declaratively within a repository using YAML files within a repository’s `.github/workflows` directory.\n\n### Key Capabilities\n\n#### [[Continuous Integration]] ([[Continuous Integration|CI]])\n\nAutomatically build and test code whenever changes are pushed or pull requests are opened.\n\n* Run unit, integration, and linting tests.\n* Validate data transformations or model training scripts.\n\n```yaml\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run tests\n        run: pytest\n```\n\n#### Continuous Deployment ([[Continuous Delivery - Deployment|CD]])\n\nDeploy validated code automatically to production, staging, or documentation environments.\n\n* Deploy to cloud platforms (AWS, [[Azure]], [[Google Cloud Platform|GCP]]).\n* Publish Python packages to PyPI or Docker images to [[Docker]] Hub.\n* Deploy static websites or documentation to GitHub Pages.\n#### Data and Analytics Automation\n\nGitHub Actions can serve as part of a lightweight [[data pipeline]].\n\n* Trigger scripts to run ETL or [[EDA]] tasks on schedule.\n* Automate report or dashboard generation.\n* Commit analysis results (e.g., CSVs, Markdown summaries, charts).\n#### Documentation and Reporting\n\nGenerate and maintain [[documentation]] automatically.\n\n* Build and deploy technical docs using tools like Sphinx or MkDocs.\n* Generate changelogs and release notes.\n* Auto-update README or markdown tables with metadata or stats.\n\n#### Scheduled or Event-Driven Tasks\n\nDefine workflows that run on a fixed schedule (like [[Cron jobs]]) or in response to repository events.\n\n* Perform weekly data checks or backups.\n* Refresh external data connections.\n* Update [[dashboards]] periodically.\n#### Integration with External Systems\n\nGitHub Actions can trigger and interact with external services or APIs.\n\n* Notify Slack or Teams on build status.\n* Trigger workflows in [[Databricks]], [[dbt]] Cloud, or Airflow.\n* Fetch or push data via REST APIs for external [[reporting]].",
    "aliases": [],
    "date modified": "2-11-2025",
    "tags": [
      "data_governance",
      "data_integration",
      "data_pipeline",
      "devops",
      "software"
    ],
    "normalized_filename": "github_actions",
    "outlinks": [
      "reporting",
      "continuous_integration",
      "data_pipeline",
      "dbt",
      "documentation",
      "eda",
      "cron_jobs",
      "databricks",
      "azure",
      "continuous_delivery_-_deployment",
      "google_cloud_platform",
      "dashboards",
      "docker"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Global Interpreter Lock",
    "sha": "c658204fd2f94bcd882396d585d16d9b3be894dd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Global%20Interpreter%20Lock.md",
    "text": "In CPython, the Global Interpreter Lock allows only one thread to execute Python bytecode at a time. \n\nThis makes [[Multithreading]] inefficient for CPU-bound tasks, but still useful for I/O-bound tasks. \n\n[[Multiprocessing]] avoids this limitation entirely by running each process with its own interpreter and GIL.",
    "aliases": [
      "GIL"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "system"
    ],
    "normalized_filename": "global_interpreter_lock",
    "outlinks": [
      "multithreading",
      "multiprocessing"
    ],
    "inlinks": [
      "multiprocessing",
      "multiprocessing_vs_multithreading"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Google Colab",
    "sha": "a3dd6727cbbdba9ff69fe51319685f3efd87471d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Google%20Colab.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "cloud",
      "communication"
    ],
    "normalized_filename": "google_colab",
    "outlinks": [],
    "inlinks": [
      "binder",
      "ipynb"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Google Cloud Platform",
    "sha": "de93f5df6b721566d39cc21c44c97b51036b8e7d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Google%20Cloud%20Platform.md",
    "text": "Google Cloud Platform is a suite of cloud computing services offered by Google. It provides a range of services including computing, storage, and application development that run on Google hardware.\n\nResources:\n [Introduction to Google Cloud](https://www.youtube.com/watch?v=IeMYQqJeK4)\n### Compute Engine\n\n Description: GCP's Infrastructure as a Service (IaaS) offering, allowing users to run virtual machines on Google's infrastructure.\n \n Features:\n   Custom Machine Types: Create VMs with custom configurations.\n   Preemptible VMs: Costeffective, shortlived instances for batch jobs and faulttolerant workloads.\n   Sustained Use Discounts: Automatic discounts for prolonged usage.\n   Persecond Billing: Charges calculated per second for cost savings.\n   \n Use Cases: Suitable for web hosting, data processing, and largescale applications.\n Integration: Works seamlessly with other GCP services like Google [[kubernetes]] Engine, Cloud Storage, and [[BigQuery]].\n\n### Bigtable\n A scalable [[NoSQL]] database service for large analytical and operational workloads.\n\n### App Engine\n A platform for building scalable web applications and mobile backends.\n\n### [[BigQuery]]\n A fullymanaged, serverless data warehouse for largescale data analytics.\n\n### Cloud Storage\n Object storage service for storing and accessing data on Google's infrastructure.\n\n### Cloud [[SQL]]\n Managed relational database service for [[MySql]], PostgreSQL, and SQL Server.\n\n### CI/CD\n Tools and services for continuous integration and continuous delivery.\n\n### [[uncategorised/Firebase]]\n A platform for building mobile and web applications with realtime databases, authentication, and more.\n\n## Notes:\n Consider setting up a personal GCP example for handson experience.\n Explore the generic repository for additional resources and examples.",
    "aliases": [
      "GCP"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cloud"
    ],
    "normalized_filename": "google_cloud_platform",
    "outlinks": [
      "mysql",
      "kubernetes",
      "sql",
      "bigquery",
      "uncategorised/firebase",
      "nosql"
    ],
    "inlinks": [
      "databricks",
      "databricks_vs_snowflake",
      "github_actions",
      "model_deployment_using_pycaret"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Google My Maps Data Extraction",
    "sha": "fa37f43b20a844447da80ba4384953f29803c1f8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Google%20My%20Maps%20Data%20Extraction.md",
    "text": "### Summary:\n\nThis guide covers the key workflows and tools for managing and processing location data in Google Sheets and Google My Maps. Suppose we have marks on Google My Maps. In order to extract the location of markers to a google sheet.\n\n1. **Export Marker Data** from Google My Maps as KML/CSV.\n2. Convert KML to CSV\n3. **Use Apps Script in Google Sheets** to extract data like addresses or postal codes from coordinates.\n\n### **Extract Data from Google My Maps**  \n   - **Export Custom Markers:**\n     1. Open Google My Maps.\n     2. Use the menu (three dots) to select **Export to KML**.\n     3. The exported file will contain marker names, descriptions.\n\n### Extract KML data to google sheets\n   \n- Rename KML file to XML.\n- Open XML in excel.\n- Extract marker and coordinate data.\n- Paste data into google sheets.\n\n### **Extracting Information from Coordinates in Google Sheets**  \n\nUse [[**Google Apps Script**]] to extract additional information like addresses or postal codes from geographic coordinates.\n\n- **Get Address from Coordinates**:\n  ```javascript\n  function getAddress(lat, lng) {\n    var response = Maps.newGeocoder().reverseGeocode(lat, lng);\n    var result = response.results[0];\n    if (result) {\n      return result.formatted_address;\n    } else {\n      return 'No address found';\n    }\n  }\n  ```\n- **Get Postal Code from Coordinates**:\n  ```javascript\n  function getPostalCode(lat, lng) {\n    var response = Maps.newGeocoder().reverseGeocode(lat, lng);\n    var result = response.results[0];\n    if (result) {\n      for (var i = 0; i < result.address_components.length; i++) {\n        var component = result.address_components[i];\n        if (component.types.indexOf('postal_code') !== -1) {\n          return component.long_name;\n        }\n      }\n      return 'Postal code not found';\n    } else {\n      return 'No results found';\n    }\n  }\n  ```\n- Use `getAddress()` with `getPostalCode()`.\n- =getPostalCode(56.033139, -3.4182519)",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "google_my_maps_data_extraction",
    "outlinks": [
      "**google_apps_script**"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Google Sheets",
    "sha": "0fa59c5d22a02745d85fda50e9a028fb79493604",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Google%20Sheets.md",
    "text": "Useful functions:\n- QUERY: Simple [[SQL]] commands such as [[Groupby]]\n- ARRAYFORMULA\n- Indirect\n- Xlookup\n- AI function\n- Sumproduct i.e. $\\sum_{i,j}\\alpha_i \\beta_j$\n- SWITCH: Like a nested if statements\n- Percentile: \n- Importrange:\n- Unique: removes duplicates\n- Forecast : [[Time Series Forecasting]]\n\nAccessing google sheets from a script:\nhttps://www.youtube.com/watch?v=zCEJurLGFRk\n\nRelated:\n- [[Excel vs Google Sheets]]\n- [[Google Sheet Pivots Table]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "software"
    ],
    "normalized_filename": "google_sheets",
    "outlinks": [
      "time_series_forecasting",
      "sql",
      "excel_vs_google_sheets",
      "groupby",
      "google_sheet_pivots_table"
    ],
    "inlinks": [
      "1-to-1's_with_a_line_manager",
      "excel_vs_google_sheets",
      "loading_google_sheets_into_databricks",
      "looker_studio"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Gradio",
    "sha": "db30775d4b69f6fe1b480ba83a8737beeba46a2d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Gradio.md",
    "text": "Gradio is an open-source platform that simplifies the process of ==creating user interfaces== for machine learning models. \n\nIt allows users to quickly build interactive demos and applications for their models without extensive front-end development knowledge. \n\nMain uses:\n\n- **Interactive Interfaces**: Gradio provides a simple way to create web-based interfaces where users can interact with machine learning models by uploading files, entering text, or adjusting sliders.\n- **Rapid Prototyping**: It enables quick prototyping and sharing of machine learning models, making it easier to demonstrate model capabilities to stakeholders or gather user feedback.\n- **Ease of Integration**: Gradio can be easily integrated with popular machine learning frameworks like TensorFlow, PyTorch, and Hugging Face Transformers, allowing seamless deployment of models.\n\n### Related content\n\n[Video Link](https://www.youtube.com/watch?v=eE7CamOE-PA&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl&index=2)\nhttps://www.gradio.app/\n\n[[1-Overview]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "software"
    ],
    "normalized_filename": "gradio",
    "outlinks": [
      "1-overview"
    ],
    "inlinks": [
      "model_deployment"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Hadoop",
    "sha": "45ea60f0bfca5cf93679b11f191d12ff26bc7e18",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Hadoop.md",
    "text": "Hadoop provides the backbone for distributed storage and computation. It uses HDFS (Hadoop Distributed File System) to split large datasets across clusters of servers, while MapReduce enables parallel processing. It’s well-suited for [[Batch Processing]]asks, though newer tools like [[Apache Spark|Spark]] often outperform Hadoop in terms of speed and ease of use.\n\n1. **Architecture**:\n   - **Open-Source Framework**: Hadoop is an open-source framework for distributed storage and processing of large datasets using clusters of commodity hardware.\n   - **Distributed File System**: The Hadoop Distributed File System (HDFS) stores data across multiple machines, providing high throughput access to data.\n   - **MapReduce**: Originally designed for [[Batch Processing]] using the MapReduce programming model, though newer frameworks like Apache Spark are often used now.\n\n2. **Data Storage**:\n   - **Unstructured, Semi-Structured, and Structured Data**: Hadoop can handle a wide variety of data formats, including unstructured, semi-structured, and structured data.\n   - **Scalable Storage**: HDFS can store vast amounts of data by adding more nodes to the cluster.\n\n3. **Management**:\n   - **Complex Management**: Requires more administrative effort to manage and maintain the infrastructure, including handling failures, load balancing, and tuning.\n\n4. **Performance**:\n   - **[[Batch Processing]]**: Hadoop is optimized for batch processing of large datasets, though it can be less efficient for real-time processing compared to other systems.\n   - **Latency**: Higher latency for query processing compared to [[Snowflake]], particularly for complex analytical queries.\n\n5. **Use Cases**:\n   - **[[Big Data]] Processing**: Ideal for large-scale data processing tasks, including ETL (Extract, Transform, Load), data mining, and large-scale machine learning.\n   - **[[Data Lake]]**: Commonly used as a data lake to store vast amounts of raw data.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "hadoop",
    "outlinks": [
      "big_data",
      "snowflake",
      "apache_spark",
      "data_lake",
      "batch_processing"
    ],
    "inlinks": [
      "big_data",
      "databricks",
      "distributed_computing",
      "map_reduce",
      "parquet",
      "snowflake_vs_hadoop"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Grep",
    "sha": "cc8c7a3c4b27150e298ca7dd3df1b9509f14ced3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Grep.md",
    "text": "In [[DE_Tools]] see:\nhttps://github.com/rhyslwells/DE_Tools/blob/main/DE_Tools/Explorations/Other/Terminal/Grep\n\n#### Example of what Grep can do \n\n 1. [[Search]] for a pattern in files\n\n```bash\ngrep \"error\" logfile.txt\n```\n\n* Searches for the word `\"error\"` in `logfile.txt`.\n* Useful for quickly checking logs or documents.\n\n 2. Recursive search in directories\n\n```bash\ngrep -r \"TODO\" ./src\n```\n\n* Recursively searches all files in the `./src` directory for `\"TODO\"`.\n* Useful for finding code comments, notes, or markers across a codebase.\n\n 3. Display line numbers of matches\n\n```bash\ngrep -n \"fail\" test_results.txt\n```\n\n* Shows line numbers where `\"fail\"` appears.\n* Helpful for [[Debugging]] or jumping to the right place in a large file.\n\n 4. Show only matching part of the line\n\n```bash\ngrep -o \"http[s]\\?://[a-zA-Z0-9./?=_-]*\" webpage.html\n```\n\n* `-o` prints *only* the matching substring (e.g., all URLs from HTML).\n* Good for extracting emails, URLs, etc.\n\n 5. Search multiple files and include filenames\n\n```bash\ngrep \"password\" *.conf\n```\n\n* Searches all `.conf` files and prints matching lines with their filenames.\n* Useful for auditing sensitive configurations.\n\n#### Flags\n\nUseful for finding where a function is used in a code base.\n\n|Flag|Description|\n|---|---|\n|`-w`|Match whole words only (ignores substrings).|\n|`-i`|Perform a case-insensitive match.|\n|`-n`|Show line numbers of matching lines.|\n|`-r`|Recursively search subdirectories.|\n|`-l`|List only filenames that contain matches (no line content shown).|\n|`-c`|Count matches per file, instead of showing them.|\n|`-A N`|Show N lines after each match.|\n|`-B N`|Show N lines before each match.|\n|`-C N`|Show N lines before and after each match (context lines).|\n|`-P`|Use Perl-compatible regular expressions (PCRE).|\n|`-v`|Invert match: show lines that do not match the pattern.|\n|`-o`|Print only the matching part of the line (not the entire line).|\n|`--color`|Highlight matched strings with color (if supported by terminal).|\n|`--include`|Include only files that match the given glob pattern (used with `-r`).|\n|`--exclude`|Exclude files that match the given pattern from recursive searches.|",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "grep",
    "outlinks": [
      "debugging",
      "de_tools",
      "search"
    ],
    "inlinks": [
      "bash"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Hugging Face",
    "sha": "1c8f9d1d34acf6aeb345cdb3e71d4195f2121d5d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Hugging%20Face.md",
    "text": "Hugging Face is open-source platform known for its contributions to natural language processing (NLP) and machine learning. \n\nIt provides a library called [[Transformer]], which includes pre-trained models for tasks such as text classification, translation, summarization, and question answering. \n\nHugging Face is widely used for:\n\n- **Access to Pre-trained Models**: Offers a vast collection of state-of-the-art models that can be easily fine-tuned for specific NLP tasks.\n- **Ease of Use**: Simplifies the implementation of complex NLP models with user-friendly APIs.\n- **Community and Collaboration**: Hosts a vibrant community where researchers and developers share models and datasets, fostering collaboration and innovation in AI.\n- Datasets: It also have csv, [[Parquet]],ect training sets, with links to associated papers. See: https://huggingface.co/datasets\n\n### Resources:\n- [Getting Started With Hugging Face in 15 Minutes | Transformers, Pipeline, Tokenizer, Models](chat.openai.com/c/4e1f8036-5b3e-4372-9e0c-e8a7c0c15dfd)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "software"
    ],
    "normalized_filename": "hugging_face",
    "outlinks": [
      "parquet",
      "transformer"
    ],
    "inlinks": [
      "comparing_llms",
      "tokenisation",
      "transfer_learning"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Json to SQLite",
    "sha": "6cab396db5edeb0152f0407100fa11ec4437eb1c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Json%20to%20SQLite.md",
    "text": "In [[DE_Tools]] see:\n- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/JSON\n\nRelated:\n- [[Json]]\n- [[SQLite]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "json_to_sqlite",
    "outlinks": [
      "sqlite",
      "json",
      "de_tools"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Json",
    "sha": "965f5362c4abe03888300e4940f9516c8da3c0cd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Json.md",
    "text": "Stands for [javascript object notation](https://www.json.org/json-en.html)\n- records separated by commas\n- keys & strings wrapped by double quotes\n- good choice for data transport\n\n\nJSON data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet. The Raw JSON stored by Airbyte during ELT is an example of semi-structured data. This looks as follows:  \n\n|               |  **\\_airbyte_data**|\n|---------| -----------|\n|Record 1| \\\"{'id': 1, 'name': 'Mary X'}\\\" |\n|Record 2| \\\"{'id': 2, 'name': 'John D'}\\\"|\n\n```JSON\n{\n  \"json\": [\n    \"rigid\",\n    \"better for data interchange\"\n  ],\n  \"yaml\": [\n    \"slim and flexible\",\n    \"better for configuration\"\n  ],\n  \"object\": {\n    \"key\": \"value\",\n    \"array\": [\n      {\n        \"null_value\": null\n      },\n      {\n        \"boolean\": true\n      },\n      {\n        \"integer\": 1\n      },\n      {\n        \"alias\": \"aliases are like variables\"\n      },\n      {\n        \"alias\": \"aliases are like variables\"\n      }\n    ]\n  },\n  \"paragraph\": \"Blank lines denote\\nparagraph breaks\\n\",\n  \"content\": \"Or we\\ncan auto\\nconvert line breaks\\nto save space\",\n  \"alias\": {\n    \"bar\": \"baz\"\n  },\n  \"alias_reuse\": {\n    \"bar\": \"baz\"\n  }\n}\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "json",
    "outlinks": [],
    "inlinks": [
      "altair",
      "devops_portal",
      "ipynb",
      "json_to_sqlite",
      "multi-level_index",
      "normalisation_of_data",
      "pydantic",
      "rest_api",
      "semi-structured_data",
      "structured_data",
      "why_json_is_better_than_pickle_for_untrusted_data",
      "yaml"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Justfile",
    "sha": "f26ac23869117d573eebc01046f0ca44ed0b55ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Justfile.md",
    "text": "Justfile is a command runner designed to streamline workflows by allowing users to define simple, reusable commands for common tasks. \n\nThis approach minimizes the cognitive load associated with memorizing long command sequences, enhancing productivity. The underlying concept is to create a more efficient command execution environment.\n\nBy leveraging Justfile, users can automate repetitive tasks, thereby reducing the likelihood of errors and increasing consistency in execution. The implications for industry practices include improved efficiency in software development and operations, as well as the potential for better collaboration among team members through shared command definitions.\n\n### Important\n - Justfile enables the definition of reusable commands, reducing the complexity of command execution.\n - It promotes automation in workflows, which can lead to significant time savings and error reduction in repetitive tasks.\n\n### Example\n An example of using Justfile could be defining a command to automate the deployment process of an application, allowing users to execute a single command instead of multiple steps.\n\n### Follow up questions\n - How does the use of Justfile compare to traditional [[Makefile]] in terms of flexibility and ease of use?\n - What are the potential security implications of using command runners like Justfile in production environments?\n### Related Topics\n - Automation in software development  \n - [[Command Line]] interface (CLI) tools and their impact on productivity  \n### Topics of interest:\n- Continuous integration and deployment (CI/CD) practices  \n- Scripting languages for task automation  \n### Use Case: Web Application Deployment\n\nIn this scenario, a team is working on a web application that requires several steps to set up the development environment and deploy the application. The Justfile will simplify these processes by defining reusable commands.\n\n### Example Justfile Content\n\n```\n# Justfile for Web Application Deployment\n\n# Define variables for environment\nENV := development\n\n# Command to install dependencies\ninstall:\n    @echo \"Installing dependencies...\"\n    npm install\n\n# Command to run the application\nrun:p\n    @echo \"Starting the application in $(ENV) mode...\"\n    npm start\n\n# Command to run tests\ntest:\n    @echo \"Running tests...\"\n    npm test\n\n# Command to build the application for production\nbuild:\n    @echo \"Building the application for production...\"\n    npm run build\n\n# Command to deploy the application\ndeploy:\n    @echo \"Deploying the application...\"\n    # Assuming deployment script is defined\n    ./deploy.sh\n\n# Command to clean up the environment\nclean:\n    @echo \"Cleaning up...\"\n    rm -rf node_modules\n    rm -rf dist\n```\n\n### Explanation of Justfile Content\n\n- **Variables**: The `ENV` variable is defined to specify the environment (development in this case).\n- **Commands**:\n  - `install`: Installs the necessary npm dependencies.\n  - `run`: Starts the application in the specified environment.\n  - `test`: Runs the application's test suite.\n  - `build`: Builds the application for production deployment.\n  - `deploy`: Executes a deployment script to deploy the application.\n  - `clean`: Cleans up the environment by removing unnecessary files.\n\n### How to Use the Justfile\n\nTo use this Justfile, a developer would simply run commands in the terminal:\n\n- To install dependencies: \n  ```bash\n  just install\n  ```\n\n- To start the application:\n  ```bash\n  just run\n  ```\n\n- To run tests:\n  ```bash\n  just test\n  ```\n\n- To build the application:\n  ```bash\n  just build\n  ```\n\n- To deploy the application:\n  ```bash\n  just deploy\n  ```\n\n- To clean up the environment:\n  ```bash\n  just clean\n  ```\n\nThis Justfile provides a clear and organized way to manage common tasks in the development and deployment of a web application, making it easier for team members to collaborate and maintain consistency.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "justfile",
    "outlinks": [
      "makefile",
      "command_line"
    ],
    "inlinks": [
      "devops_portal"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Load Balancing",
    "sha": "fdf2abdcec818ceeb59dd5427c7612c4297ee8cd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Load%20Balancing.md",
    "text": "Load balancing is a technique used to distribute incoming network traffic across multiple servers. This helps ensure both reliability and performance by preventing any single server from becoming overwhelmed with too much traffic.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "load_balancing",
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Loading Google Sheets into Databricks",
    "sha": "fb8029a2e24598506ab15687d4601d010aa3035d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Loading%20Google%20Sheets%20into%20Databricks.md",
    "text": "**Purpose:**\nTo import and persist data from a [[Google Sheets]] into a [[Databricks]] managed [[Delta Tables in Databricks|Delta Table]] for querying and integration with Spark-based workflows.\n\n**Process Overview:**\n\n1. Export the Google Sheet as CSV via its public link:\n\n   ```python\n   sheet_url = \"https://docs.google.com/spreadsheets/d/.../export?format=csv&gid=0\"\n   ```\n2. Read the CSV into a pandas DataFrame:\n\n   ```python\n   pdf = pd.read_csv(sheet_url)\n   ```\n3. Convert to a Spark DataFrame:\n\n   ```python\n   df = spark.createDataFrame(pdf)\n   ```\n4. Save the data as a managed table in Databricks:\n\n   ```python\n   df.write.mode(\"overwrite\").saveAsTable(\"example.databricks.table_name\")\n   ```\n\n**Applications:**\n* Automating ingestion of lightweight reference data.\n* Collaborative data entry pipelines.\n* Quick [[ETL]] prototypes with spreadsheet-based inputs.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "data_collection",
      "data_integration",
      "data_pipeline",
      "data_storage"
    ],
    "normalized_filename": "loading_google_sheets_into_databricks",
    "outlinks": [
      "delta_tables_in_databricks",
      "google_sheets",
      "databricks",
      "etl"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Maintainability",
    "sha": "27ad9223ff09185fa142075d00539fa3e4cbec2b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Maintainability.md",
    "text": "",
    "aliases": [
      "maintainable"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management"
    ],
    "normalized_filename": "maintainability",
    "outlinks": [],
    "inlinks": [
      "batch_vs_powershell_scripts",
      "how_to_do_git_commit_messages_properly",
      "machine_learning_operations",
      "monolith_architecture",
      "performance_dimensions",
      "pyright",
      "pyright_vs_pydantic",
      "sqlalchemy",
      "sqlalchemy_vs._sqlite3",
      "view_use_case",
      "views"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Makefile",
    "sha": "5a57b39b9ca90fcd7c6de4a82f61163017ffa62f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Makefile.md",
    "text": "A Makefile is a special file used by the `make` build automation tool to manage the build process of a project. It defines a set of tasks to be executed, typically to compile and link a program. Here are some key functions of a Makefile:\n\n1. **Compilation Instructions**: It specifies how to compile and link the program. This includes defining the source files, the compiler to use, and any necessary flags or options.\n\n2. **Dependencies**: Makefiles list dependencies between files, ensuring that changes in source files trigger recompilation of only the necessary parts of the program.\n\n3. **Automation**: It automates repetitive tasks, such as cleaning up build artifacts, running tests, or deploying software.\n\n4. **Targets and Rules**: A Makefile consists of targets, dependencies, and rules. A target is usually a file to be generated, dependencies are files that the target depends on, and rules are the commands to create the target from the dependencies.\n\n5. **Variables**: Makefiles can use variables to simplify and manage complex build processes, making it easier to maintain and modify.\n\n## Example\n\n```makefile\n# Compiler\nCXX = g++\n\n# Compiler flags\nCXXFLAGS = -Wall -g\n\n# Target executable\nTARGET = myprogram\n\n# Source files\nSRCS = main.cpp utils.cpp\n\n# Object files\nOBJS = $(SRCS:.cpp=.o)\n\n# Default target\nall: $(TARGET)\n\n# Rule to build the target executable\n$(TARGET): $(OBJS)\n\t$(CXX) $(CXXFLAGS) -o $(TARGET) $(OBJS)\n\n# Rule to build object files\n%.o: %.cpp\n\t$(CXX) $(CXXFLAGS) -c $< -o $@\n\n# Clean up build artifacts\nclean:\n\trm -f $(OBJS) $(TARGET)\n\n# Phony targets\n.PHONY: all clean\n```\n\n### Explanation:\n\n- **CXX and CXXFLAGS**: These variables define the compiler and the flags used during compilation.\n- **TARGET**: The name of the final executable.\n- **SRCS and OBJS**: Lists of source and object files. The `OBJS` variable is automatically generated by replacing `.cpp` with `.o` in the `SRCS` list.\n- **all**: The default target that builds the executable.\n- **$(TARGET)**: This rule specifies how to link object files into the final executable.\n- **%.o: %.cpp**: A pattern rule to compile each `.cpp` file into a `.o` object file.\n- **clean**: A target to remove all object files and the executable, useful for cleaning up the build directory.\n- **.PHONY**: Declares `all` and `clean` as phony targets, meaning they are not actual files but just names for commands to run.\n\n### Running it\n\nIf you run this Makefile using the `make` command in a terminal, here's what would happen:\n\n1. **Compilation**: The `make` tool will look for a file named `Makefile` in the current directory. It will then execute the default target, which is `all` in this case.\n\n2. **Building the Executable**: \n   - `make` will check if the target executable `myprogram` needs to be built. It does this by comparing the timestamps of the source files (`main.cpp`, `utils.cpp`) and the corresponding object files (`main.o`, `utils.o`).\n   - If any of the source files are newer than their corresponding object files, or if the object files do not exist, `make` will compile the source files into object files using the rule `%.o: %.cpp`.\n   - Once the object files are up-to-date, `make` will link them together to create the `myprogram` executable using the rule for `$(TARGET)`.\n\n3. **Output**: During this process, you'll see the compilation and linking commands being executed in the terminal. If there are any errors in the source code, the compiler will output error messages.\n\n4. **Clean Up**: If you run `make clean`, it will execute the `clean` target, which removes the object files and the executable, cleaning up the build directory.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "makefile",
    "outlinks": [],
    "inlinks": [
      "devops_portal",
      "justfile"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Maintainable Code",
    "sha": "a90d04fa4cdc05fb908b9308d6afa933a6e3e8dd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Maintainable%20Code.md",
    "text": "[[Pydantic]] : runtine analysis\n\n[[Pyright]]: static analysis\n\n[[Testing]]\n\nWant robust and reliable Python applications.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "maintainable_code",
    "outlinks": [
      "pyright",
      "pydantic",
      "testing"
    ],
    "inlinks": [
      "pyright"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Master Observability Datadog",
    "sha": "2f569cb5615edd578aea72fb55690bed258d53fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Master%20Observability%20Datadog.md",
    "text": "what happens in prod, pre prod.\n\nmonitoring web [[frontend]].\n\nhow is infrastructure working in prod\n\nobservability\n\nDatadog\n\nagents\n\ntagging\n\nprofile how it was working versus other dates.\n\ndashboards\n\n[[Lambdas]]\n\nlogging",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "governance",
      "orchestration"
    ],
    "normalized_filename": "master_observability_datadog",
    "outlinks": [
      "lambdas",
      "frontend"
    ],
    "inlinks": [
      "model_observability"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Memory Caching",
    "sha": "ca78c693ef54cdeae0f6a7d56dea99e002638882",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Memory%20Caching.md",
    "text": "Memory Caching\n  - Use in-memory caches to store frequently accessed data closer to the user, reducing [[Latency]].",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "memory_caching",
    "outlinks": [
      "latency"
    ],
    "inlinks": [
      "cloud_providers"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Memory",
    "sha": "964eb0733a531eeffedb16002fb7563d7fc41e07",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Memory.md",
    "text": "[[Computer Science]] \n\nMemory within a computer.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "memory",
    "outlinks": [
      "computer_science"
    ],
    "inlinks": [
      "processes_vs_threads"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Microsoft",
    "sha": "d61aa992d59c62ff8d63ae32ac6e7660c3c73d70",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Microsoft.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cloud"
    ],
    "normalized_filename": "microsoft",
    "outlinks": [],
    "inlinks": [
      "fabric",
      "sharepoint"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "MongoDB",
    "sha": "cd9a9ebf66e24384aaf91784c710433df3ae6b07",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/MongoDB.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "mongodb",
    "outlinks": [],
    "inlinks": [
      "database",
      "database_management_system_(dbms)",
      "nosql"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "NET",
    "sha": "cd748292e1678c48dcd06c1cc500a03738684766",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/NET.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type",
      "software"
    ],
    "normalized_filename": "net",
    "outlinks": [],
    "inlinks": [
      "powershell",
      "powershell_versus_command_prompt",
      "powershell_vs_bash"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Overwriting and Refreshing Tables in Databricks",
    "sha": "522654f07e536858a5f0fc97558112616674d940",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Overwriting%20and%20Refreshing%20Tables%20in%20Databricks.md",
    "text": "When reloading data from external sources, you may need to refresh existing tables.\n\n**Methods:**\n\n1. **Drop and recreate:**\n\n   ```python\n   spark.sql(\"DROP TABLE IF EXISTS example.databricks.my_table\")\n   df.write.saveAsTable(\"example.databricks.my_table\")\n   ```\n2. **Overwrite with schema handling:**\n\n   ```python\n   df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"example.databricks.my_table\")\n   ```\n\n**Best Practices:**\n\n* Use `DROP TABLE` for inconsistent or manually edited sources (e.g., spreadsheets).\n* Use `mergeSchema` for additive schema changes.\n* Always preview with `df.show()` before writing.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "data_management",
      "data_pipeline",
      "data_storage"
    ],
    "normalized_filename": "overwriting_and_refreshing_tables_in_databricks",
    "outlinks": [],
    "inlinks": [
      "databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Normalisation of Text",
    "sha": "f7c51f27e10dd4ab4b983b2de4b7b74e64e6cb34",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Normalisation%20of%20Text.md",
    "text": "[[Preprocessing]] in NLP tasks is called Normalization involves reducing words to their base or root form, converting them to lowercase, and removing stop words.\n## Processes\n\nWhat are some steps involved in the pre-processing of a text?. These include making the text lower case, removal of punctuation, tokenize the text (split up the words in a sentence), remove stop words as they convey grammar rather than meaning, word stemming (reduce words to their stems).\n\n[[Tokenisation]]: Used to separate words or sentences.\n\n[[Stemming]]: returns part of a words that doesnt change ie breaks, breakthrough gives break. Use \n\n```python\nfrom nltk.stem.porter import PorterStemmer\ntemp=text #decomposed\nporter_stemmer = PorterStemmer()\nstemmed_tokens = [porter_stemmer.stem(token) for token in temp]\n\nprint(stemmed_tokens)\n```\n\n[[lemmatization]]:  reducing word to it's base form e.g. words \"is\", \"was\", \"were\" will turn into \"be\".\n\n```python\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntemp=text #decomposed\nwordnet_lemmatizer = WordNetLemmatizer()\nlemmatized_tokens = [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in temp]\nprint(lemmatized_tokens)\n```\n## Code\n\nmain normaliser\n```python\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\n\ndef normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):\n    \"\"\"Noramlizes data by performing following steps:\n        1. Changing each word in corpus to lowercase.\n        2. Removing special characters and interpunction.\n        3. Dividing text into tokens.\n        4. Removing english stopwords.\n        5. Stemming words.\n        6. Lemmatizing words.\n    \"\"\"\n    \n    temp = document.lower()\n    temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)\n    temp = word_tokenize(temp)\n    temp = [t for t in temp if t not in stopwords.words(\"english\")]\n    temp = [porter_stemmer.stem(token) for token in temp]\n    temp = [lemmatizer.lemmatize(token) for token in temp]\n    return temp\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "NLP"
    ],
    "normalized_filename": "normalisation_of_text",
    "outlinks": [
      "preprocessing",
      "stemming",
      "tokenisation",
      "lemmatization"
    ],
    "inlinks": [
      "nlp",
      "normalisation"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "PMML",
    "sha": "1cf185207f5345b4eb0e3e21813969969cd8aee3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/PMML.md",
    "text": "PMML is an XML-based standard developed by the Data Mining Group (DMG) that allows applications to describe and share predictive models across different platforms and tools.\n#### Purpose\n* Provides a common representation for predictive analytics models.\n* Enables model portability: train a model in one tool and deploy it in another without custom code.\n#### What It Contains\n* Model structure: Type (e.g., Decision Tree, Regression, Neural Network).\n* Data dictionary: Input and output fields, data types.\n* Transforms: Preprocessing steps (normalization, binning, etc.).\n* Model parameters: Coefficients, splits, weights.\n#### Why Important\n* Facilitates interoperability in heterogeneous environments.\n* Reduces the need for re-implementation.\n* Commonly used in banking, insurance, and enterprise systems for deploying models.\n#### Example Use\n* Train a logistic regression model in R or Python → export as PMML → load into a Java-based scoring engine.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      null
    ],
    "normalized_filename": "pmml",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Pandas Series vs DataFrame",
    "sha": "6f8d35d01825900245cdf4d59cf6ebccee24750f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Pandas%20Series%20vs%20DataFrame.md",
    "text": "Series: One-dimensional labeled array.\n\n  * Shape: `(n,)` -> single axis of length `n`.\n  * Can store numbers, strings, or other data types.\n  * Example:\n\n  ```python\n  import pandas as pd\n  s = pd.Series([22, 24, 19], index=['Day1', 'Day2', 'Day3'])\n  ```\n\n* DataFrame: Two-dimensional labeled data structure (rows × columns).\n\n  * Shape: `(n, m)` -> `n` rows, `m` columns.\n  * Each column is a Series.\n  * Example:\n\n  ```python\n  df = pd.DataFrame({'Temperature': [22, 24, 19]}, index=['Day1', 'Day2', 'Day3'])\n  ```\n\n* Single-column DataFrame vs Series:\n\n  * `df` with one column is still 2D -> shape `(n,1)`.\n  * `df['Temperature']` or `df.iloc[:,0]` converts it to Series (1D -> shape `(n,)`).\n  * Some functions require 1D Series; others require 2D DataFrame.\n\nRule of thumb:\n\n* Use Series for single-variable operations.\n* Use DataFrame for multiple columns or when 2D operations are needed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "pandas"
    ],
    "normalized_filename": "pandas_series_vs_dataframe",
    "outlinks": [],
    "inlinks": [
      "pandas"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Powerquery",
    "sha": "8925d068b59b3cbf2337c4d4080b2e774e19a583",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Powerquery.md",
    "text": "[[How to normalise a merged table]]\n\nUsed in [[Excel]]\n\nRelated:\n[[Data Ingestion]]\n\nUseful for [[Data Transformation]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "powerquery",
    "outlinks": [
      "data_ingestion",
      "excel",
      "how_to_normalise_a_merged_table",
      "data_transformation"
    ],
    "inlinks": [
      "excel"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Pandoc",
    "sha": "eab1041156bfbea4b294538b29fc8b3feb2fb49a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Pandoc.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "tool"
    ],
    "normalized_filename": "pandoc",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Powershell scripts",
    "sha": "0cb32a56da0fe1b874e452371ed97a29c870ef43",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Powershell%20scripts.md",
    "text": "You can save that [[PowerShell]] command as a `.ps1` ([[ps1]]) script file and run it on Windows using PowerShell.\n## Example\n### Save as a PowerShell Script\n\n1. Open a text editor (e.g., VS Code, Notepad++)\n2. Paste the code below into a new file:\n```powershell\n$themes = \"black\", \"white\", \"league\", \"beige\", \"sky\", \"night\", \"serif\", \"simple\", \"solarized\", \"blood\", \"moon\"\nforeach ($theme in $themes) {\n  jupyter nbconvert presentation.ipynb --to slides --SlidesExporter.reveal_theme=$theme --output \"presentation_$theme\"\n}\n```\n3. Save the file as:\n```\nconvert-themes.ps1\n```\n\n## How to Run the Script\n\n1. Open PowerShell\n2. Navigate to the script’s directory:\n```powershell\ncd \"C:\\path\\to\\your\\script\"\n```\n3. Run the script:\n```powershell\n.\\convert-themes.ps1\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "powershell_scripts",
    "outlinks": [
      "powershell",
      "ps1"
    ],
    "inlinks": [
      "powershell"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Powershell versus Command Prompt",
    "sha": "3193854bcdf9039af0ff8ab7dbe8166b7b0e7f5e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Powershell%20versus%20Command%20Prompt.md",
    "text": "[[PowerShell]] and [[Command Prompt]] (cmd) are both command-line interfaces available on Windows systems, but they differ significantly in their capabilities, syntax, and scripting abilities. \n\nPowerShell offers a more versatile and powerful environment for scripting, automation, and administrative tasks on [[Windows]] systems, while cmd remains useful for straightforward commands and basic system interactions.\n\n### Choosing Between PowerShell and cmd:\n\nUse PowerShell When:\n  - You need to work with complex data structures or objects.\n  - Automation and scripting tasks require advanced features like loops, conditions, and error handling.\n  - Integration with .[[NET]] Framework or other external libraries is necessary.\n\n- Use Command Prompt (cmd) When:\n  - Performing basic system tasks or operations.\n  - Working with simple text-based outputs.\n  - Using legacy batch scripts or when PowerShell is unavailable.",
    "aliases": [],
    "created": "2024-06-22 21:32",
    "date modified": "27-09-2025",
    "tags": [
      "#software",
      "system"
    ],
    "type": null,
    "normalized_filename": "powershell_versus_command_prompt",
    "outlinks": [
      "command_prompt",
      "windows",
      "net",
      "powershell"
    ],
    "inlinks": [
      "powershell"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Powershell vs Bash",
    "sha": "ce5b5e5652be153b277bacd8b617958a4c371c1b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Powershell%20vs%20Bash.md",
    "text": "The choice between [[PowerShell]] and [[Bash]] largely depends on the user's needs and the environment in which they are working. Here are some considerations for each:\n\n### PowerShell\n- **Windows Integration**: PowerShell is deeply integrated with Windows and is ideal for managing Windows systems and applications.\n- **Object-Oriented**: As mentioned earlier, PowerShell works with objects, making it easier to manipulate data and interact with .[[NET]] applications.\n- **Remote Management**: It has strong capabilities for remote management of Windows systems.\n- **Scripting**: PowerShell's scripting capabilities are robust, making it suitable for complex automation tasks.\n\n### Bash\n- **Unix/Linux Environment**: Bash is the default shell for many Unix and Linux systems, making it the go-to choice for system administrators and developers in those environments.\n- **Simplicity**: Bash scripts are often simpler and more straightforward for basic tasks, especially for file manipulation and text processing.\n- **Community and Resources**: There is a vast amount of community support, tutorials, and resources available for Bash, especially in the open-source community.\n- **Cross-Platform**: While traditionally associated with Unix/Linux, Bash can also be used on Windows through [[Windows Subsystem for Linux]] (WSL) or [[Git]] Bash.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "powershell_vs_bash",
    "outlinks": [
      "net",
      "git",
      "windows_subsystem_for_linux",
      "bash",
      "powershell"
    ],
    "inlinks": [
      "command_line"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Publish and Subscribe",
    "sha": "9a748f73f5e33479ad2bbf95a3eaa383b408f0b8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Publish%20and%20Subscribe.md",
    "text": "The **Publish-Subscribe (Pub-Sub) model** is a messaging pattern that enables real-time data distribution by decoupling message producers from consumers. This architecture is widely used in [[Data Streaming]], [[Event-Driven Architecture]], and [[Distributed Computing]].\n\nIt can help in designing more efficient and [[Scalability]] and data processing architectures.\n\n### Core Components\n\nThis model ensures that multiple consumers can receive the same data without requiring direct connections between producers and consumers, allowing for a more scalable and flexible system.\n\n- **Producers**: Entities or applications that generate data and publish messages to specific channels known as **Topics**. Each Topic represents a category or stream of data.\n\n- **Consumers**: Applications or services that subscribe to Topics to receive messages. They process incoming data in real-time, enabling immediate action or analysis.\n### Importance in Data Streaming\n\nEnsures continuous data flow, in contrast to [[Batch Processing]], which collects and processes data in groups at scheduled intervals. Streaming applications benefit from Pub-Sub by:\n\n- Enabling real-time analytics and monitoring\n- Supporting event-driven architectures\n- Improving scalability by decoupling message producers from consumers\n\n### Questions for Consideration\n\n- If you’re working with a streaming dataset, why might [[Batch Processing]] not be suitable, and what alternatives would you consider?\n- How does the decoupling of producers and consumers improve scalability in large-scale data systems?\n- What are the trade-offs between a Pub-Sub model and a point-to-point messaging system?\n\n### Example: [[Apache Kafka]]\n\n1. **Producers**: In a Kafka setup, a producer could be a web application that generates user activity events, such as clicks, page views, or purchases. This application publishes these events to a specific topic, for example, \"user-activity\".\n\n2. **Topics**: The \"user-activity\" topic acts as a channel where all user activity events are sent. Multiple producers can publish messages to this topic without needing to know about the consumers.\n\n3. **Consumers**: Various applications or services can subscribe to the \"user-activity\" topic to receive real-time updates. For instance:\n   - An analytics service that processes user activity data to generate insights.\n   - A notification service that sends alerts based on specific user actions (e.g., sending a welcome email after a user signs up).\n   - A monitoring service that tracks user engagement metrics.\n\n### Workflow:\n- When a user interacts with the web application, the producer generates an event and publishes it to the \"user-activity\" topic.\n- All subscribed consumers receive this event simultaneously, allowing them to process the data in real-time.\n- This decoupling means that the producer does not need to know how many consumers are listening or what they are doing with the data.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "orchestration"
    ],
    "normalized_filename": "publish_and_subscribe",
    "outlinks": [
      "event-driven_architecture",
      "apache_kafka",
      "scalability",
      "distributed_computing",
      "data_streaming",
      "batch_processing"
    ],
    "inlinks": [
      "apache_kafka",
      "data_streaming"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "PySpark",
    "sha": "7059b3775b13836d5b49298611a1d231fa26b116",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/PySpark.md",
    "text": "PySpark is the Python API for [[Apache Spark]], allowing users to write distributed data processing tasks using familiar Python syntax.\n\nPurpose: It enables [[big data]] computation by splitting large datasets across a cluster and processing them in parallel.\n\nKey Components:\n* SparkSession: Main entry point for Spark functionality.\n* RDDs: Low-level distributed datasets.\n* DataFrames: High-level structured data abstraction similar to [[pandas]].\n* Catalyst Optimizer: Optimizes DataFrame and [[SQL]] operations for performance.\n\nExample:\n\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Example\").getOrCreate()\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\ndf.show()\n```\n\nUse Context: Used within environments such as [[Databricks|Databricks]], which manage Spark clusters and simplify [[distributed computing]] workflows.\n\n### pyspark.sql Module\n\nDefinition:\n`pyspark.sql` is the core module for structured data in PySpark, supporting both DataFrame operations and SQL queries.\n\nKey Features:\n* Provides the DataFrame API for manipulating tabular data.\n* Enables SQL [[Querying|queries]] through temporary views.\n* Performs lazy evaluation and query optimization via Catalyst.\n* Integrates with other Spark components like MLlib and Structured Streaming.\n\nWhy Important: Allows analysts and engineers to work with massive structured datasets efficiently using a familiar SQL-like interface.\n\n### PySpark vs pandas\n\n| Feature              | pandas                     | PySpark                                 |\n| -------------------- | -------------------------- | --------------------------------------- |\n| Data scale       | In-memory (single machine) | Distributed (cluster)                   |\n| Execution        | Eager (immediate)          | Lazy (optimized plan)                   |\n| Integration      | Python only                | SQL, MLlib, Streaming, Databricks       |\n| Failure handling | Limited                    | Fault-tolerant (lineage-based recovery) |\n\nAnalogy:\n\n> PySpark is like *pandas + SQL*, but distributed and scalable.\n\nUsage Pattern:\n- For local, small datasets → use pandas.\n- For large-scale or production data → use PySpark.\n\n### SparkSession\n\nDefinition:\nA `SparkSession` is the entry point for all Spark functionality in PySpark.\n\nResponsibilities:\n\n* Creates and manages DataFrames.\n* Executes SQL queries.\n* Connects to the cluster for distributed execution.\n\nExample:\n\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n```\n\nImportance: Without a `SparkSession`, you cannot use Spark DataFrames, read data, or run SQL queries.\n### PySpark Use Cases\n\n1. ETL and Data Cleaning\n2. Data Aggregation and Reporting\n3. Joining Large Datasets\n4. SQL Query Execution\n5. Machine Learning Preparation\n### Related\n\n[[Why Use PySpark in Databricks]]",
    "aliases": [],
    "created": null,
    "date modified": "27-09-2025",
    "tags": [
      "#orchestration",
      "python"
    ],
    "type": null,
    "normalized_filename": "pyspark",
    "outlinks": [
      "sql",
      "querying",
      "pandas",
      "apache_spark",
      "databricks",
      "distributed_computing",
      "big_data",
      "why_use_pyspark_in_databricks"
    ],
    "inlinks": [
      "fabric",
      "spark_dataframes_in_databricks",
      "why_use_pyspark_in_databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Python",
    "sha": "d3217e67f6755ddd1d3313f8e6e7a9a5856bb8d9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Python.md",
    "text": "dynamic language\nlower learning, support\nobject orientated\n\n[[Immutable vs mutable]]",
    "aliases": [],
    "created": "2024-06-20 16:38",
    "date modified": "27-09-2025",
    "tags": [
      "python"
    ],
    "type": "language",
    "normalized_filename": "python",
    "outlinks": [
      "immutable_vs_mutable"
    ],
    "inlinks": [
      "bat",
      "csv_module",
      "duckdb_vs_sqlite",
      "generators_in_python",
      "immutable_vs_mutable",
      "jupytext",
      "langchain",
      "programming_languages",
      "streamlit",
      "testing",
      "tool.ruff",
      "vectorisation"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Quartz",
    "sha": "2c905a8e471a4534dbd21b92d0641e37ad922e82",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Quartz.md",
    "text": "[[Vim]]: telescope? Search preview feature?\n\nhttps://www.youtube.com/watch?v=v5LGaczJaf0\n\nHow does quartz work of a software level:\n- Transforming text. Think [[jinja template]]. \n- Manipulating markdown notes\n- There is a diagram showing how markdown goes to html.\n- [[JavaScript]] for static site generators already existed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "quartz",
    "outlinks": [
      "vim",
      "javascript",
      "jinja_template"
    ],
    "inlinks": [
      "jinja_template"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Pytest",
    "sha": "c53f29ac33b899b16d3b3ea6ab249b3ab109435c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Pytest.md",
    "text": "### **`@pytest.fixture` Explanation**\n\n`@pytest.fixture` is a decorator in `pytest` used to define reusable test setup functions. It allows tests to use shared resources without redundant code.\n\n#### **Example & Usage**\n\npython\n\nCopy code\n\n`import pytest  @pytest.fixture def sample_data():     return {\"name\": \"John Doe\", \"age\": 30}      def test_example(sample_data):     assert sample_data[\"name\"] == \"John Doe\"     assert sample_data[\"age\"] == 30`\n\n🔹 **How it works:**\n\n- The function `sample_data()` is decorated with `@pytest.fixture`, making it a fixture.\n- The test function `test_example()` receives `sample_data` as an argument.\n- `pytest` automatically provides the fixture data when running the test.\n\n#### **Why use fixtures?**\n\n- Avoids repetitive setup code.\n- Ensures clean test environments.\n- Can handle resource management (e.g., opening/closing database connections, creating temporary files).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "python",
      "ML_Tools"
    ],
    "normalized_filename": "pytest",
    "outlinks": [],
    "inlinks": [
      "testing",
      "testing_pytest.py",
      "testing_unittest.py",
      "toml"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "REST API",
    "sha": "33d0905b866d3b5ca2ec88cc20b04fe4bb50e0d6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/REST%20API.md",
    "text": "[[Postman]] is a free tool to test out REST API.\n#### REST [[API]]\n- REST stands for Representational State Transfer.\n- It is a ==standardized== software architecture style used for API communication between a client and a server.\n\n**Benefits of REST APIs:**\n1. **Simplicity and Standardization:**\n   - Data formatting and request structuring are standardized and widely adopted.\n2. **[[Scalability]] and Statelessness:**\n   - Easily modifiable as service complexity grows without tracking data states across client and server.\n3. **High Performance:**\n   - Supports ==caching==, maintaining high performance even as complexity increases.\n\n**Main Building Blocks:**\n1. **==Request==:**\n   - Actions (==[[CRUD]]==): Create (POST), Read (GET), Update (PUT), Delete (DELETE).\n   - Components: Operation (==HTTP method==), Endpoint, Parameters/Body, Headers.\n2. **Response:**\n   - Typically in [[Json]] format.\n\n**REST API Example:** \nice cream shop inteacting with cloud database.\n- Endpoint example: \"icecream.com/api/flavors\"\n  - \"api\" indicates the API portion.\n  - \"flavors\" refers to the ==resource== being accessed or modified.\n\n**Real-world Examples:**\n1. **Get Flavors:**\n   - Operation: ==GET==\n   - Endpoint: \"/api/flavors\"\n   - Response: Array of flavor resources.\n2. **Update Flavor:**\n   - Operation: ==PUT==\n   - Endpoint: \"/api/flavors/1\"\n   - Body: New flavor data.\n   - Response: Confirmation of update.\n3. **Create New Flavor:**\n   - Operation: ==POST==\n   - Endpoint: \"/api/flavors\"\n   - Body: New flavor data.\n   - Response: Confirmation of creation.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "rest_api",
    "outlinks": [
      "api",
      "scalability",
      "crud",
      "json",
      "postman"
    ],
    "inlinks": [
      "api",
      "graph_query_language"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Random Access Memory",
    "sha": "2250e244fe43b9a4da0c25142da997c3b2f5e97a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Random%20Access%20Memory.md",
    "text": "",
    "aliases": [
      "RAM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "random_access_memory",
    "outlinks": [],
    "inlinks": [
      "in-memory_format"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Registering a Scheduled Task",
    "sha": "a495520c9a736e0e8061225fe08ec3816d6ec787",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Registering%20a%20Scheduled%20Task.md",
    "text": "### Note: Registering a Scheduled Task with a [[bat]] File\n\nYou can automate the creation of a [[Windows Scheduled Tasks]] using a `.bat` file. This is useful for reusability, portability, and documentation — especially when deploying or maintaining recurring tasks like data scripts.\n#### Example: Create `register_quick_test.bat`\n\n```bat\n@echo off\nset PYTHON_PATH=C:\\Python\\python.exe\nset SCRIPT_PATH=C:\\Users\\RhysL\\Desktop\\DE_Tools\\Explorations\\Other\\Scheduled-Tasks\\example_task.py\nset TASK_NAME=QuickTestTask\n\nschtasks /create ^\n /tn \"%TASK_NAME%\" ^\n /tr \"\\\"%PYTHON_PATH%\\\" \\\"%SCRIPT_PATH%\\\"\" ^\n /sc minute /mo 1 ^\n /f\n\necho Task \"%TASK_NAME%\" registered to run every minute.\npause\n```\n\n#### What This Does:\n\n - Registers a task named `\"QuickTestTask\"`\n - Runs the Python script every 1 minute\n - Can be double-clicked to set up the task automatically\n#### Why Use a `.bat` File:\n\n - Avoid manual typing of `schtasks` commands\n - Easily re-run or deploy task setup\n - Useful in shared, scripted, or version-controlled environments",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "registering_a_scheduled_task",
    "outlinks": [
      "windows_scheduled_tasks",
      "bat"
    ],
    "inlinks": [
      "windows_scheduled_tasks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "React",
    "sha": "dc81a7ac6c7a636446ffd5ffd35e619711ba6e1e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/React.md",
    "text": "React is a [[JavaScript]] library developed by Meta for building user interfaces, particularly in web development.\n\nRelated to:\n- [[Dashboarding]]\n\n### Core Concepts\n\nReact's component-based architecture allows for reusable UI elements, enhancing maintenance and testing. It uses a Virtual DOM for efficient updates, minimizing direct DOM manipulation. Data flows unidirectionally from parent to child components.\n\n### Main Use Cases\n\nReact is ideal for Single Page Applications (SPAs) that load once and update dynamically, as well as for complex, interactive user interfaces and real-time applications like dashboards.\n### Common Tools\n\nPopular UI libraries include Tailwind CSS and shadcn/ui.\n\n### React vs JavaScript Security\n\n- You noted React is better than raw [[JavaScript]].\n- Reason: React automatically escapes user input (e.g., prevents Cross-Site Scripting attacks).\n- By using frameworks like React, you reduce injection risks by design.\n- See [[Security Vulnerabilities]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "frontend"
    ],
    "normalized_filename": "react",
    "outlinks": [
      "dashboarding",
      "javascript",
      "security_vulnerabilities"
    ],
    "inlinks": [
      "vercel"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "SQL vs NoSQL",
    "sha": "d86689b6febe327f63ad4550d24122798348498a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/SQL%20vs%20NoSQL.md",
    "text": "SQL and NoSQL are two broad classes of database technologies, distinguished by how they structure, query, and store data.\n\n* Use SQL when data integrity, relational logic, and transactional consistency are key.\n* Use NoSQL when scalability, performance, and schema flexibility are priorities.\n\n| Feature        | SQL                              | NoSQL                                |\n| -------------- | -------------------------------- | ------------------------------------ |\n| Data model     | Relational (tables, rows)        | Varies (documents, key-value, graph) |\n| Schema         | Fixed / predefined               | Flexible / dynamic                   |\n| Query Language | SQL                              | Varies (JSON queries, APIs, etc.)    |\n| Transactions   | ACID                             | Often BASE (eventual consistency)    |\n| Scalability    | Vertical (scale-up)              | Horizontal (scale-out)               |\n| Best for       | Structured data, complex queries | Unstructured data, high throughput   |\n\nRelated:\n- [[SQL]]\n- [[NoSQL]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "SQL",
      "storage"
    ],
    "normalized_filename": "sql_vs_nosql",
    "outlinks": [
      "nosql",
      "sql"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Scala",
    "sha": "2252a4e4f9f43972d499b76bb01457a88e456946",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Scala.md",
    "text": "> [!Summary]\n> **Scala** is a **functional programming language** primarily used for **[[Big Data]] processing**, particularly with frameworks like **[[Apache Spark]]**. It is known for its **concise syntax** and its ability to integrate seamlessly with the **Java ecosystem**, running on the **JVM** (Java Virtual Machine).\n\nWhile Scala has a **smaller user base** and is considered **hard to learn**, it is highly **expressive** and offers strong support for managing **distributed systems** and building large-scale data pipelines. Its robust features make it a top choice for **big data engineers**.\n\n### **Key Features of Scala**\n\n1. **Functional Programming**:\n   - Scala is built around functional programming principles, offering key features such as:\n     - **[[Lambdas]]** (anonymous functions)\n     - **Pattern matching**\n     - **Functions as first-class citizens**\n     - **Data classes** for concise data modeling.\n   \n2. **Immutability**:\n   - **Immutability** is a core principle in Scala. By default, data structures are **immutable**, which promotes **thread safety** and makes code easier to reason about. This feature aligns well with building reliable and scalable distributed systems.\n   \n3. **Advanced Language Features**:\n   - **Type inference** allows the compiler to deduce types, leading to shorter and cleaner code.\n   - **Higher-order types** and **meta-programming** support advanced abstractions and code expressiveness.\n   - **Meta programming** allows compile-time code generation, improving type correctness and reducing runtime errors.\n   \n4. **Expressive Data Manipulation**:\n   - Scala is renowned for its **concise and readable code** when it comes to data manipulation. Its type-safe methods provide **powerful tools** for working with data models efficiently and expressively.\n\n5. **Type System**:\n   - Scala has an **advanced type system** that enforces **strong typing** at compile time. This system helps prevent illegal states, reducing the need for runtime tests and making the code more robust and reliable.\n  \n6. **Library Over Framework**:\n   - Scala promotes the use of **libraries over frameworks**, which provides developers with more flexibility in how they design and structure their applications.\n\n### **Additional Notes**\n- **Integration with [[Java]]**: Scala can be seamlessly mixed with **Java**, allowing developers to use existing Java libraries and tools.\n  \n- **Used with Apache Spark**: Scala is the most common language used for **Apache Spark**, a leading big data processing framework.\n\n- **Niche and Learning Curve**: While Scala’s adoption is smaller compared to languages like Java or Python, its **expressiveness** and **power** make it a popular choice for niche applications, especially in big data environments.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "scala",
    "outlinks": [
      "lambdas",
      "big_data",
      "apache_spark",
      "java"
    ],
    "inlinks": [
      "big_data",
      "databricks",
      "fabric"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Security Vulnerabilities",
    "sha": "db9c95e8b07688b52e2f36818ff612ee8b8180a1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Security%20Vulnerabilities.md",
    "text": "[[Data Security]] vulnerabilities can be encountered and mitigated in [[Devops Portal]].\n\nUseful Tools\n- [[tool.bandit]]\n## Examples\n\n### Command Injection\n\nGeneral Description: Command injection is a security vulnerability that occurs when an attacker is able to execute arbitrary commands on the host operating system via a vulnerable application. This typically happens when user input is improperly handled and passed to a system shell.\n\nExample: \nThe `dangerous_subprocess` function uses `subprocess.call` with `shell=True`, which can lead to command injection if user input is not properly sanitized.\n  ```python\n  import subprocess\n  def dangerous_subprocess(user_input):\n      subprocess.call(user_input, shell=True)\n  ```\nMitigation:\n  - Avoid using `subprocess.call` with `shell=True`. Use `subprocess.run` or `subprocess.call` with a list of arguments.\n  - Validate and sanitize user inputs ([[Input is Not Properly Sanitized]]). \n### [[SQL Injection]]\n### AI Injection Attacks\n\nNew class of attacks where malicious input is designed to manipulate or confuse AI/ML models.\n\nExample:  \n  - Input crafted to cause large language models to leak secrets or behave incorrectly.\n  - Poisoning training data to introduce backdoors.\n\nDefending AI requires:\n- Input validation.\n- Monitoring model behavior.\n- Sanitizing training datasets.\n\n### Hardcoded Password\n\nGeneral Description: Hardcoded passwords refer to credentials that are embedded directly in the source code. This practice is insecure as it exposes sensitive information and makes it difficult to change passwords without modifying the code.\n\nExample:\nThe `hardcoded_password` function contains a hardcoded password, which is a common security issue.\n  ```python\n  def hardcoded_password():\n      password = \"123456\"\n      return password\n  ```\n  \nMitigation:\n  - Use environment variables or configuration files to store sensitive information.\n  - Consider using a secrets management tool.\n\n### Use of `eval`\n\nGeneral Description: The `eval` function in Python evaluates a string as a Python expression. If not properly controlled, it can execute arbitrary code, leading to security vulnerabilities.\n\nExample:\n  - The `unsafe_eval` function uses `eval`, which can execute arbitrary code if the input is not controlled.\n  ```python\n  def unsafe_eval(user_input):\n      return eval(user_input)\n  ```\nMitigation:\n  - Avoid using `eval`. Use safer alternatives like `ast.literal_eval`.\n  - Ensure input is strictly controlled and sanitized.\n\n### Insecure Deserialization\n\nGeneral Description: Insecure deserialization occurs when untrusted data is used to reconstruct objects. This can lead to arbitrary code execution, data tampering, or other malicious activities.\n\nExample:\n  - The `insecure_deserialization` function uses `pickle.loads`, which can be exploited if untrusted data is deserialized.\n  ```python\n  import pickle\n  def insecure_deserialization(data):\n      return pickle.loads(data)\n  ```\n\nMitigation:\n  - Avoid using `pickle` for untrusted data. Use safer formats like JSON ([[Why JSON is Better than Pickle for Untrusted Data]]).\n  - Ensure data is from a trusted source.\n\n### Cross-Site Scripting (XSS)\n\n**General Description**: Cross-Site Scripting (XSS) is a security vulnerability that allows an attacker to inject malicious scripts into content from otherwise trusted websites. It occurs when an application includes untrusted data in a web page without proper validation or escaping.\n\n**Example**:\n- The `display_user_input` function directly inserts user input into HTML, which can lead to XSS if the input is not properly sanitized.\n```html\n<div>\n\t<%= user_input %>\n</div>\n```\n**Mitigation**:\n- Escape user input before rendering it in HTML.\n- Use security libraries or frameworks that automatically handle escaping",
    "aliases": [],
    "date modified": "27-07-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "security_vulnerabilities",
    "outlinks": [
      "devops_portal",
      "tool.bandit",
      "why_json_is_better_than_pickle_for_untrusted_data",
      "sql_injection",
      "data_security",
      "input_is_not_properly_sanitized"
    ],
    "inlinks": [
      "data_security",
      "react",
      "testing",
      "tool.bandit"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Sharepoint",
    "sha": "06086f73fc84cb928d81ac32c0c8d08e8f1179b1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Sharepoint.md",
    "text": "SharePoint is a web-based collaboration platform developed by [[Microsoft]]. It is primarily used for creating intranet sites, document management, and team collaboration, providing a centralized platform for managing content and communication.\n\nSharePoint integrates with Microsoft Office and is highly customizable, making it a versatile tool for organizations of all sizes.\n\n### Key Features\n\n1. **Intranet Sites**:\n   - Create internal websites for team collaboration and communication.\n   - Share news, updates, and resources within an organization.\n\n2. **Document Repository**:\n   - Store, organize, and manage documents in a centralized location.\n   - Version control and access permissions ensure document integrity and security.\n\n3. **Lists and Libraries**:\n   - Create lists to manage data and tasks.\n   - Libraries for storing and organizing documents and other files.\n\n4. **Team and Communication Sites**:\n   - Team Sites: Facilitate collaboration within specific teams or projects.\n   - Communication Sites: Share information broadly across an organization.\n\n5. **Integration with Microsoft Teams**:\n   - SharePoint sites can be integrated with Microsoft Teams channels, providing a seamless collaboration experience.\n\n### Use Cases\n\n- **Document Management**: Centralize document storage and enable easy sharing and collaboration.\n- **Project Management**: Use lists and libraries to track project tasks and resources.\n- **Internal Communication**: Share company news, updates, and announcements through intranet sites.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cloud",
      "communication",
      "documentation"
    ],
    "normalized_filename": "sharepoint",
    "outlinks": [
      "microsoft"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Snowflake vs Hadoop",
    "sha": "d541c7fb064dbd10da33a5138dfa986c6a1127a1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Snowflake%20vs%20Hadoop.md",
    "text": "[[Snowflake]] and Hadoop are both [[Data Management]] systems, but they serve different purposes and have distinct architectures and functionalities. \n\nIn summary, Snowflake and Hadoop are both powerful tools for managing and analyzing data, but they are optimized for different types of workloads and use cases. Snowflake excels in ==cloud-based data warehousing== and real-time analytics, while Hadoop is suited for ==large-scale data processing== and storage in a distributed environment.\n\n[[Snowflake]]\n[[Hadoop]]\n### **Key Differences**\n\n1. **Deployment**:\n   - **Snowflake**: Cloud-based, requires no hardware or infrastructure management by users.\n   - **Hadoop**: Can be deployed on-premises or in the cloud, but typically requires more hands-on management.\n\n2. **Ease of Use**:\n   - **Snowflake**: User-friendly with a simple SQL interface, automated maintenance, and optimization.\n   - **Hadoop**: Requires more technical expertise to set up, manage, and optimize.\n\n3. **Performance and Scalability**:\n   - **Snowflake**: Excels in performance for analytical queries with the ability to scale compute resources independently.\n   - **Hadoop**: Scales horizontally by adding more nodes, suitable for large-scale data processing but may have higher query latency.\n\n4. **Cost**:\n   - **Snowflake**: Pay-as-you-go model based on compute and storage usage.\n   - **Hadoop**: Costs depend on the infrastructure (hardware or cloud resources) and maintenance overhead.\n\n### Example Use Case Scenarios\n\n- **Snowflake**: A retail company wanting to perform real-time analytics and reporting on sales data would benefit from Snowflake’s high performance and ease of use for SQL-based queries and dashboards.\n\n- **Hadoop**: A tech company needing to process and analyze massive amounts of log data for machine learning models might use Hadoop due to its ability to handle large-scale data processing and diverse data types.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "storage"
    ],
    "normalized_filename": "snowflake_vs_hadoop",
    "outlinks": [
      "snowflake",
      "data_management",
      "hadoop"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Snowflake",
    "sha": "d1f3b50916050fdf6bf1c3c49cfcf1adc1a5a296",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Snowflake.md",
    "text": "### Snowflake\n\n1. Architecture:\n   - Cloud-Native: Snowflake is a fully managed, cloud-native data warehousing service. It operates entirely on cloud platforms like AWS, Azure, and Google [[Cloud]].\n   - Separation of Storage and Compute: Snowflake separates storage from compute, allowing for independent scaling of each. This means you can scale up compute resources without affecting storage capacity and vice versa.\n   - Multi-Cluster Shared Data Architecture: Snowflake uses a multi-cluster architecture to handle concurrent workloads, ensuring high performance and minimal contention.\n\n1.  [[Data Storage]]\n   - Structured Data: Primarily designed for structured data and optimized for [[SQL]] queries and analytics.\n   - [[semi-structured data]]: Also supports semi-structured data like JSON, Avro, and Parquet through its native capabilities.\n\n1. Management:\n   - Fully Managed Service: Snowflake handles infrastructure management, optimization, and maintenance tasks automatically, requiring minimal administrative effort from users.\n\n1. Performance:\n   - High Performance: Optimized for fast [[Querying|query]] performance, particularly for complex analytical queries. It uses advanced optimizations like automatic clustering and caching.\n\n1. Use Cases:\n   - [[Data Warehouse]]: Ideal for enterprise data warehousing, [[business intelligence]], and analytics.\n   - [[Data Lake]]: Can function as a data lake with support for semi-structured data.\n   \nRelated:\n[[Databricks vs Snowflake]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "snowflake",
    "outlinks": [
      "data_warehouse",
      "sql",
      "querying",
      "databricks_vs_snowflake",
      "semi-structured_data",
      "data_lake",
      "data_storage",
      "cloud",
      "business_intelligence"
    ],
    "inlinks": [
      "apache_iceberg",
      "data_engineering_tools",
      "databricks_vs_snowflake",
      "dbt",
      "elt",
      "hadoop",
      "snowflake_vs_hadoop"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Software Development Life Cycle",
    "sha": "ae1a7efa750efc6012e441da4b7a70d315b08056",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Software%20Development%20Life%20Cycle.md",
    "text": "A structured approach like the Software Development Life Cycle (SDLC) ensures ==systematic progression through various stages of development==. SDLC remains relevant today by outlining the essential stages a product must undergo to achieve success.\n## SDLC Stages\n\nThe SDLC comprises several phases, each critical to the overall development process:\n\n1. **Planning and Analysis**\n    - **Purpose**: Collect business and user requirements, perform cost and time estimation, and conduct scoping activities.\n    - **Activities**: Define what the final product must do and how it should work. This phase may include a separate requirements analysis stage.\n\n2. **Designing**\n    - **Purpose**: Prepare the product's architecture and design.\n    - **Activities**: A software architect sets the high-level structure of the future system, selecting technology and drafting user experience and visual design.\n\n3. **Development**\n    - **Purpose**: Transform the design into actual code.\n    - **Activities**: Programmers write code according to the design specifications, revealing some aspects of the final product to stakeholders.\n\n4. **Testing**\n    - **Purpose**: Ensure the quality and functionality of the product.\n    - **Activities**: Testers and QA professionals review the code and usability, identifying bugs and errors for correction before deployment.\n\n5. **Deployment**\n    - **Purpose**: Release the product to users.\n    - **Activities**: Launch the product, making it available for use. Often combined with the maintenance phase.\n\n6. **Maintenance**\n    - **Purpose**: Provide ongoing support and updates.\n    - **Activities**: Gather user feedback, fix issues, and add new features as needed.\n\nAdditionally, some projects include a **Prototyping** stage between planning and designing to validate ideas with minimal effort and cost.\n\n## SDLC Methodologies\n\n### Waterfall\n\nThe Waterfall model, strictly follows the SDLC stages sequentially. ==Each phase must be completed before the next begins, making it straightforward and easy to manage==. However, its lack of flexibility and late testing phase often lead to delays and budget overruns when changes are needed. Out of favour. Opposite of flexible, needs roll backs.\n\n- detail documentation\n\n### Agile\n\n- Produce only essential documentation. Collaborative effort. \n- Focuses on features users will like.\n\nIn response to the rigid Waterfall model, Agile emerged in the early 2000s with a ==focus on flexibility and continuous delivery.== Agile methodologies like Scrum, Lean, and Extreme Programming (XP) integrate testing throughout the development process and welcome changes even in late stages.\n\nSee agile manifesto : Working software over documentation\n\n- **Scrum**: Breaks development into short cycles called ==sprints==, each lasting 1-4 weeks. At the end of each sprint==, a functional product increment is presented to stakeholders,== allowing for quick adaptation based on feedback.\n- **Lean**: Aims to ==eliminate waste== through a build-measure-learn feedback loop, continuously refining the product. Steps: build, measure, learn.\n- **Extreme Programming (XP)**: Emphasizes technical excellence with practices such as test-driven development, code refactoring, and pair programming.\n- Kanban: management method for efficiency. \n\n### [[DevOps]]",
    "aliases": [
      "sdlc"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#orchestration"
    ],
    "normalized_filename": "software_development_life_cycle",
    "outlinks": [
      "devops"
    ],
    "inlinks": [
      "ci-cd",
      "data_lifecycle_management",
      "debugging",
      "generative_ai_from_theory_to_practice"
    ]
  },
  {
    "category": null,
    "filename": "Spark DataFrames in Databricks",
    "sha": "a1d51db78e279b1eeb69d7e3408e4e1be6e14d91",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Spark%20DataFrames%20in%20Databricks.md",
    "text": "All data operations in Databricks are performed through **Spark DataFrames**, which provide distributed data structures for large-scale analytics.\n\nRelated:\n- [[PySpark]]\n\n**Typical Workflow:**\n\n1. **Load external data:**\n\n   ```python\n   pdf = pd.read_csv(sheet_url)\n   df = spark.createDataFrame(pdf)\n   ```\n2. **Transform and preview:**\n\n   ```python\n   df.show(5)\n   df.select(\"Type\", \"Status\").distinct().show()\n   ```\n3. **Write back to Delta:**\n\n   ```python\n   df.write.mode(\"overwrite\").saveAsTable(\"example.databricks.databricks_test\")\n   ```\n\n**Advantages:**\n\n* Scalable beyond pandas limits.\n* Integrates with SQL and machine learning tools.\n* Native interoperability with Delta Lake.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "data_engineering",
      "data_management",
      "data_pipeline",
      "pandas"
    ],
    "normalized_filename": "spark_dataframes_in_databricks",
    "outlinks": [
      "pyspark"
    ],
    "inlinks": [
      "databricks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Streamlit",
    "sha": "4e4997a464140e43abe6dd090764d9ab8e8e15a6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Streamlit.md",
    "text": "Streamlit is an open-source [[Python]] framework that lets you create interactive, data-driven web applications quickly and easily. It is designed for data scientists and machine learning engineers who want to turn Python scripts into shareable dashboards without needing front-end development skills. It’s widely used for building data exploration tools, ML model demos, and internal analytics apps.\n\nKey features:\n\n- Simple [[API]] using standard Python scripts\n- Supports charts (e.g., with Matplotlib, [[Plotly]], Altair)\n- Widgets (e.g., sliders, checkboxes) for interactivity\n- Auto-reloads on code changes\n- Easily integrates with Pandas, NumPy, scikit-learn, and other data/ML libraries\n\nExamples\n- https://database-querying.streamlit.app/",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "ml",
      "software",
      "visualization"
    ],
    "normalized_filename": "streamlit",
    "outlinks": [
      "plotly",
      "python",
      "api"
    ],
    "inlinks": [
      "altair",
      "altair_versus_seaborn",
      "recommender_systems"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "TOML",
    "sha": "30379497ee61842db5dcdce3d838f12dc59510e2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/TOML.md",
    "text": "A `.toml` file is a configuration file format that stands for \"Tom's Obvious, Minimal Language.\" \n\nIt is designed to be easy to read due to its simple syntax. TOML files are often used for configuration because they are straightforward to parse and write for both humans and machines. \n\nThe format supports basic data types like strings, integers, floats, booleans, arrays, and tables (which are similar to dictionaries or objects in other programming languages).\n\n```toml\ntitle = \"TOML Example\"\n\n[owner]\nname = \"Tom Preston-Werner\"\ndob = 1979-05-27T07:32:00Z\n\n[database]\nserver = \"192.168.1.1\"\nports = [ 8001, 8001, 8002 ]\nconnection_max = 5000\nenabled = true\n```\n\nIn this example, you can see how different data types and structures are represented in a TOML file.\n\n## What can you do with these files?\n\nTOML files are primarily used for configuration purposes. \n\n1. **Application Configuration**: Many software applications use TOML files to store configuration settings. This allows users to easily modify settings without altering the code.\n\n2. **Project Metadata**: In some programming environments, TOML files are used to define project metadata, such as dependencies, version numbers, and other project-specific information.\n\n3. **Data Serialization**: TOML can be used to serialize data in a format that is both human-readable and easy to parse programmatically.\n\n4. **Environment Settings**: TOML files can be used to manage environment-specific settings, such as database connections or API keys, which can vary between development, testing, and production environments.\n\n5. **Configuration for Build Tools**: Some build tools and package managers use TOML files to define build configurations and dependencies.\n\n## Contents of TOML file\n\n[[tool.ruff]]\n\n[[tool.bandit]]\n\n[[tool.uv]]\n\n[[Pytest]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "file_type"
    ],
    "normalized_filename": "toml",
    "outlinks": [
      "tool.uv",
      "tool.bandit",
      "pytest",
      "tool.ruff"
    ],
    "inlinks": [
      "dependency_manager",
      "devops_portal",
      "tool.ruff",
      "tool.uv"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Technical Design Doc Template",
    "sha": "b5a2578d7f6d371bde3a955dcfbe30e754fbb654",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Technical%20Design%20Doc%20Template.md",
    "text": "# [Project name] Design Doc\n\n# About this doc\n\n_Metadata about this document. Describe the scope and current status._\n\n_This doc describes the technical approach, milestones, and work planned for the [Project name linked to Product Requirements Doc]_\n\n|   |   |\n|---|---|\n|Sign off deadline|_Date_|\n|Status|_Draft_|\n|Author(s)|_Name 1, Name 2_|\n\nSign offs\n\n- *Name 1*\n    \n- *Name 2*\n    \n- Add your name here to sign off\n    \n\n# Context\n\n_A sentence or two on the “what.” What is being built._\n\n_Then include the “why” (metrics we intend for). Link off to Product Requirements Doc for details._\n\n# Non-goals\n\n_What is out of scope for this project that we don’t want to focus on?_\n\n- _Out of scope detail 1_\n    \n- _Out of scope detail 2_\n    \n- …\n    \n\n# Terminology\n\n_Define any new terms that are used in the document._\n\n# High level approach\n\n_Explain the technical approach in a few sentences so the reader understands the system flow_\n\n# Alternatives considered\n\n_Bullet points of alternative approaches considered and why you’re not going with them._\n\n- _Alternative 1_\n    \n- _Alternative 2_\n    \n- _…_\n    \n\n# Detailed design\n\n_APIs, DB tables modified, Data models changed, and any diagrams that would help the reader understand at a high level_\n\n# Risks\n\n_What can go wrong with the proposed approach? How are you mitigating that?_\n\n- _Risk 1_\n    \n- _Risk 2_\n    \n- _…_\n    \n\n# Test plan\n\n_How will your approach be tested? Browser testing? Manual testing? Will anything be tricky to test? Adding information here also makes it easier to make a case for a longer timeline._\n\n# Milestones\n\n_How will the work be divided into chunks of progress?_\n\n_Focus on the user milestones rather than technical ones. For example, having a minimal working feature behind a feature flag initially._\n\n_I usually add an additional 1 week per 2 weeks of expected feature work._\n\n- _Milestone 1: Date 1_\n    \n- _Milestone 2: Date 2_\n    \n- …\n    \n- _Rough ETA of project finish date: …_\n    \n- _Project retro: …_\n    \n\n# Rollout plan\n\n_[optional] How will you gradually ramp up usage of the feature for safety?_\n\n_A feature flag starting at 1% of users? Only testing in a specific region? What feature flags?_\n\n# Open Questions\n\n_Anything still being figured out that you could use some additional eyes or thoughts on._\n\n# Parties involved\n\n_Who is working on this? Are there any external teams that need to sign off on this as well?_\n\n- Eng 1: [Name]\n    \n- Eng 2: [Name]\n    \n- PM: [Name]\n    \n- Designer: [Name]\n    \n- [External team name]\n    \n\n# Appendix\n\n_Add any detailed figures you didn’t want to inline for space._",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "technical_design_doc_template",
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Terminal commands",
    "sha": "274ccc5af5f8f59abe4c002e5bc845ebe4e717e1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Terminal%20commands.md",
    "text": "jupyter nbconvert K-Means_VideoGames_Raw.ipynb --to python --no-prompt\n\nRelated:\n[[Command Line]]\n[[nbconvert]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "terminal_commands",
    "outlinks": [
      "command_line",
      "nbconvert"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Testing",
    "sha": "ada2e34ae31347114cd2516ffa49e09bfce61d25",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Testing.md",
    "text": "Testing in coding projects refers to the systematic process of evaluating software to ensure it meets specified requirements and functions correctly. It enhances software robustness, reduces maintenance costs, and improves user satisfaction.\n\nTesting is crucial for:\n- Identifying bugs\n- Ensuring code quality\n- Validating that the software behaves as expected under various conditions\n\nKey Insights\n- Testing reduces the probability of software failure, $P(\\text{failure})$, by identifying defects before [[Model Deployment|deployment]].\n- Effective testing strategies can lead to a decrease in the expected cost of errors, $E(\\text{cost})$, associated with software bugs.\n\n## Comprehensive Python Testing Strategy\nTesting a [[Python]] program effectively involves multiple levels to ensure correctness, performance, and security. Key testing types include:\n\n1. **Unit Testing**\n   - Tests individual functions and methods in isolation.\n   - **Tools:** pytest, [[unittest]], doctest.\n   - **Best Practices:** Use meaningful test names, mock dependencies, and write focused tests.\n\n2. **Integration Testing**\n   - Verifies interactions between modules and external dependencies (databases, APIs).\n   - **Best Practices:** Use in-memory databases, mock services, and reset the environment before tests.\n\n3. **Functional Testing**\n   - Ensures the application behaves as expected from a user’s perspective.\n   - **Best Practices:** Simulate real-world scenarios, automate UI/API tests, and validate expected outputs.\n\n4. **Performance Testing**\n   - Measures execution speed, scalability, and resource usage.\n   - **Best Practices:** Profile bottlenecks, stress-test under load, and monitor system performance.\n\n5. **Security Testing** [[Security Vulnerabilities]]\n   - Identifies vulnerabilities like SQL injection, XSS, and authentication flaws.\n   - **Best Practices:** Validate inputs, enforce authentication, and use static analysis tools (e.g., Bandit).\n## Related Topics\n- [[Continuous Integration]] and deployment (CI/CD)  \n- Test-driven development (TDD)  \n- Software quality assurance methodologies\n- Performance testing and optimization techniques\n- [[Pytest]]\n- Unit testing\n- Integration testing\n- System testing\n- [[Hypothesis testing]]\n- Test coverage\n- [[Types of Computational Bugs]]\n- [[Python]]: Use if `__name__ = __main__` (https://www.youtube.com/watch?v=KZpYtNtGxSU&ab_channel=VisuallyExplained)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "python",
      "software"
    ],
    "normalized_filename": "testing",
    "outlinks": [
      "pytest",
      "continuous_integration",
      "hypothesis_testing",
      "security_vulnerabilities",
      "model_deployment",
      "types_of_computational_bugs",
      "python",
      "unittest"
    ],
    "inlinks": [
      "ab_testing",
      "debugging",
      "devops_portal",
      "gitlab-ci.yml",
      "hypothesis_testing",
      "maintainable_code",
      "technical_debt"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Types of Computational Bugs",
    "sha": "f7aafc618e328380328402499e785fddcc7220b8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Types%20of%20Computational%20Bugs.md",
    "text": "Each of these types of bugs can have significant impacts on software functionality and performance, and understanding them is crucial for effective [[Debugging]] and software development.\n### Types of Computational Bugs\n1. **Cumulative Rounding Error**: This occurs when small rounding errors accumulate over time, potentially leading to significant inaccuracies. An example is the Vancouver Stock Exchange issue.\n2. **Cascades**: These are bugs that trigger a series of failures or errors in a system.\n3. **Integer Overflow**: This happens when an arithmetic operation attempts to create a numeric value that is outside the range that can be represented with a given number of bits.\n4. **Backend Issues**: These are problems that occur on the server-side of an application, affecting its functionality or performance.\n### Additional Types of Computational Bugs\n\n5. **Off-by-One Error**: This is a common programming error where an iterative loop iterates one time too many or one time too few. For example, iterating over an array with incorrect bounds can lead to accessing an out-of-bounds index.\n\n6. **Null Pointer Dereference**: Occurs when a program attempts to access or modify data through a null pointer, leading to crashes or undefined behavior. For instance, trying to access an object method without checking if the object is null.\n\n7. **Race Condition**: This happens when the behavior of software depends on the sequence or timing of uncontrollable events, such as threads accessing shared resources. An example is two threads modifying a shared variable simultaneously without proper synchronization.\n\n8. **Memory Leak**: Occurs when a program fails to release memory that is no longer needed, leading to reduced performance or system crashes. This is common in languages like C++ where manual memory management is required.\n\n9. **Buffer Overflow**: This happens when a program writes more data to a buffer than it can hold, potentially leading to data corruption or security vulnerabilities. An example is a classic stack buffer overflow attack.\n\n10. **Logic Error**: This is a bug where the program compiles and runs but produces incorrect results due to a flaw in the algorithm or logic. For example, using the wrong formula to calculate a result.\n\n11. **Deadlock**: Occurs when two or more processes are unable to proceed because each is waiting for the other to release resources. This is common in multithreaded applications.\n\n12. **Syntax Error**: These are errors in the code that violate the rules of the programming language, preventing the code from compiling or running. For example, missing a semicolon in languages like Java or C++.\n\n13. **Concurrency Issues**: These arise when multiple processes or threads execute simultaneously, leading to unpredictable results if not managed correctly. Examples include data races and inconsistent data states.\n\n14. **Configuration Error**: Occurs when software is incorrectly configured, leading to unexpected behavior or failures. An example is a misconfigured database connection string.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "software"
    ],
    "normalized_filename": "types_of_computational_bugs",
    "outlinks": [
      "debugging"
    ],
    "inlinks": [
      "debugging",
      "testing"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Ubuntu",
    "sha": "dfdc3a1cb229838b96eef410d228bce80946cacd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Ubuntu.md",
    "text": "Ubuntu is a popular open-source operating system based on the [[Linux]] kernel. It is designed to be user-friendly:\n\n1. **Desktop Environment**: Ubuntu provides a graphical user interface (GUI) that makes it accessible to users who may not be familiar with command-line interfaces. It is often used as a desktop operating system for personal computers.\n\n2. **Server Use**: Ubuntu Server is a version of Ubuntu designed for server environments. It is commonly used for hosting websites, applications, and databases due to its stability and security.\n\n3. **Development**: Many developers use Ubuntu for software development because it supports a wide range of programming languages and development tools. It is also compatible with various software libraries and frameworks.\n\n4. **Education**: Ubuntu is often used in educational institutions for teaching computer science and programming due to its open-source nature and the availability of free software.\n\n5. **Customization**: Being open-source, Ubuntu allows users to customize their operating system according to their needs. Users can modify the source code, install different desktop environments, and choose from a variety of applications.\n\n6. **Community Support**: Ubuntu has a large community of users and developers who contribute to its development and provide support through forums, documentation, and tutorials.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "ubuntu",
    "outlinks": [
      "linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "TypeScript",
    "sha": "bf9cadf8b3bab62bcc78993eaa1aebe79494aea1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/TypeScript.md",
    "text": "Superset of JavaScript adding static typing and object-oriented features for building large-scale applications.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "typescript",
    "outlinks": [],
    "inlinks": [
      "data_validation",
      "debugging"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Using requirements or env.yml",
    "sha": "7250ad1c865d50bf864d978919b589c76a33ff07",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Using%20requirements%20or%20env.yml.md",
    "text": "## Using requirements or env.yml\n\nThis comes up often when moving between **Python projects managed with `pip`** and those managed with **Conda**. Let’s clarify:\n### `requirements.txt`\n\n* Standard in Python projects.\n* Used by **pip**.\n* Lists Python packages (and optionally versions).\n* Example:\n\n  ```txt\n  numpy==1.26.0\n  pandas>=2.0.0\n  scikit-learn\n  ```\n\nInstall with:\n\n```bash\npip install -r requirements.txt\n```\n\n### `env.yml` (or `environment.yml`)\n\n* Used by **Conda**.\n\n* YAML format describing the full environment:\n\n  * Python version\n  * Conda channels\n  * Conda packages\n  * Pip dependencies\n\n* Example:\n\n  ```yaml\n  name: myproject\n  channels:\n    - conda-forge\n    - defaults\n  dependencies:\n    - python=3.11\n    - numpy=1.26\n    - pip\n    - pip:\n      - some_pip_only_package==0.3.1\n  ```\n\nCreate environment from it:\n\n```bash\nconda env create -f env.yml\n```\n\nUpdate an existing environment:\n\n```bash\nconda env update -f env.yml --prune\n```\n\nExport an existing environment:\n\n```bash\nconda env export > env.yml\n```\n\n### Key differences\n\n* **`requirements.txt`** → lightweight, only lists pip packages.\n* **`env.yml`** → full specification of environment, includes Python version, Conda channels, and both Conda + pip packages.\n### When to use\n\n* Use `requirements.txt` if:\n  * Project is pip/venv-based.\n  * You want compatibility with tools like `pip install` or deployment on services that expect pip.\n\n* Use `env.yml` if:\n  * You want to reproduce a full Conda-managed environment (exact Python version, Conda dependencies).\n  * You work in data science projects where binary compatibility matters (e.g., `numpy`, `pandas`, `pytorch`).",
    "aliases": [],
    "date modified": "4-10-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "using_requirements_or_env.yml",
    "outlinks": [],
    "inlinks": [
      "virtual_environments"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Web Feature Server (WFS)",
    "sha": "1f37f7fd11ec6c94fe7df08cffd4a30abcc540db",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Web%20Feature%20Server%20(WFS).md",
    "text": "Purpose: WFS is designed to serve raw geographic features (vector data) over the web.\n\nFunctionality:\n- Feature-Based: It delivers geographic features (such as points, lines, and polygons) and their associated attribute data in formats like GML (Geography Markup Language).\n- Interactivity: Allows clients to query and retrieve specific features, perform spatial and attribute queries, and even support transactions (e.g., inserting, updating, deleting features).\n- Data Access: Provides access to the actual data behind the map, enabling more detailed and customized analysis and processing compared to image-based services.\n- Standardization: Also standardized by the OGC, ensuring compatibility and interoperability across various GIS applications and systems.\n\nRelated:\n- [[GIS]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "web_feature_server_(wfs)",
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "gis"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Vercel",
    "sha": "4c0b36806929dcc3f76627ea16645585068a3033",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Vercel.md",
    "text": "Vercel is a cloud platform for front-end frameworks and static sites, optimized for serverless [[Model Deployment|deployment]] and instant global delivery.\n\nIn essence, Vercel simplifies and optimizes the build → deploy → scale workflow for modern web applications.\n### Core Concepts:\n\n* Jamstack-oriented: ideal for apps built with frameworks like Next.js (Vercel is the creator of Next.js), [[React]], Vue, Svelte, etc.\n* Serverless functions: easily define API endpoints as serverless functions (e.g., in `api/` directories).\n* Automatic CI/CD: Git-based deployments triggered on push.\n* Edge Network: content is cached and served globally via a CDN for low latency.\n* Previews: every pull request gets its own URL for preview and testing.\n\n### Use Cases:\n* Static sites\n* Front-end for full-stack apps\n* Headless CMS frontends\n* Real-time previews and collaboration (e.g., with design teams)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "devops"
    ],
    "normalized_filename": "vercel",
    "outlinks": [
      "react",
      "model_deployment"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "Virtual environments",
    "sha": "1d9ff1667b96f78b5e614a299eef0198dbd43654",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Virtual%20environments.md",
    "text": "[Setting up virtual env](https://www.youtube.com/watch?v=yG9kmBQAtW4)\n\nFor windows (need to not be in a venv before del)\n```cmd\nrmdir /s /q venv\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\npip freeze > requirements.txt\n\n.gitignore \nhttps://www.youtube.com/watch?v=_vejzukmn4s\n\nRemember to set python interpreter\n\nRelated terms:\n- [[Poetry]]\n\n- [ ]  How to set up venv using poetry:\n\nNow, instead of manually creating/activating `venv`:\n\n`poetry install   # creates venv + installs deps poetry shell     # activates venv`\n\nIf you want to **recreate the environment** (equivalent to `rmdir /s /q venv && python -m venv venv`):\n\n`poetry env remove python poetry install`\n\n\nOther:\n- [[Using requirements or env.yml]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "virtual_environments",
    "outlinks": [
      "poetry",
      "using_requirements_or_env.yml"
    ],
    "inlinks": [
      "dependency_manager",
      "tool.uv"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Windows Scheduled Tasks",
    "sha": "5b477f5401803b68fa3f55fede2abf8e15495fce",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Windows%20Scheduled%20Tasks.md",
    "text": "Related to:\n- Similar to [[Cron jobs ]]in [[Unix]]\n\nNote:\n- Running tasks every minute does not work.\n### Using `schtasks` (Command-Line)\n\nWindows provides `schtasks`, a command-line tool to schedule tasks.\n## Example\n\nIn [[DE_Tools]] see: \n- DE_Tools/Explorations/Other/Scheduled-Tasks\n- [[Registering a Scheduled Task]]\n### Command to Create the Task:\n\nRemember there may be different Python interpreters on the machine.\n\nRunning in cmd\n\n```cmd\nschtasks /create /tn \"QuickTestTask\" ^\n    /tr \"\\\"C:\\Python312\\python.exe\\\" \\\"C:\\Users\\RhysL\\Desktop\\DE_Tools\\Explorations\\Other\\Scheduled-Tasks\\example_task.py\\\"\" ^\n    /sc minute /mo 1 /f\n```\n\n `/tn` – Task name: `\"ExamplePythonTask\"`\n `/tr` – Task run: runs Python with your script as an argument\n `/sc daily` – Schedule type\n `/st 07:30` – Start time\n `/f` – Force create if it already exists\n\n> Note: Adjust `C:\\Python\\python.exe` to your actual Python path (`where python` can help identify it).\n\nOr run using a [[bat]] file <- Better option.\n\nDelete the Task When Done\n\n```cmd\nschtasks /delete /tn \"QuickTestTask\" /f\n```\n\n### To check a task\n\nAdd Logging to Your Python Script\n\nThis is the most reliable and script-controlled method.\n\nModify `example_task.py`:\n\n```python\nfrom datetime import datetime\nimport os\n\nlog_path = os.path.join(os.path.dirname(__file__), \"task_log.txt\")\n\nwith open(log_path, \"a\") as f:\n    f.write(f\"Task ran at: {datetime.now()}\\n\")\n```\n\nEvery time the task runs, it appends a timestamp to `task_log.txt` in the same directory.\n\n> Check the contents of `task_log.txt` to confirm it's being updated every minute.\n\n2. Use Task Scheduler GUI (Manual Inspection)\n\n3. Open Task Scheduler (`taskschd.msc`)\n4. In the Task Scheduler Library, find `QuickTestTask`\n5. Click on it and look at the \"History\" tab (if enabled)\n\n    Look for events like \"Task started\" or \"Action completed\"\n\n> If \"History\" is disabled, you can enable it:\n\n Right-click on the Task Scheduler Library\n Click \"Enable All Tasks History\"\n\n6. Use [[Command Line]] to Check Last Run Time\n\n```cmd\nschtasks /query /tn \"QuickTestTask\" /v /fo LIST\n```\n\nLook for:\n\n Last Run Time – shows the most recent execution\n Last Result – `0` means success\n\nExample output snippet:\n\n```\nTaskName:      \\QuickTestTask\nLast Run Time: 15/07/2025 12:15:00\nLast Result:   0\n```\n\n## Test if It Works Without Waiting\n\nTo verify the task works **right now**:\n```cmd\nschtasks /run /tn \"QuickTestTask\"\n```\nThis forces the task to run immediately",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "windows_scheduled_tasks",
    "outlinks": [
      "command_line",
      "unix",
      "bat",
      "de_tools",
      "registering_a_scheduled_task",
      "cron_jobs"
    ],
    "inlinks": [
      "registering_a_scheduled_task"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Why JSON is Better than Pickle for Untrusted Data",
    "sha": "76d7da2b1a7aa15f7acef6d3697596d0d6ebfc9c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Why%20JSON%20is%20Better%20than%20Pickle%20for%20Untrusted%20Data.md",
    "text": "JSON is preferred over Pickle when dealing with untrusted data, as it minimizes [[Data Security]] risks and offers better interoperability and readability.\n\n[[Json]] vs. [[Pickle]]:\n\nSecurity:\n   - JSON: JSON is a text-based data format that is inherently safer for handling untrusted data. It ==only== supports basic data types like strings, numbers, arrays, and objects, which reduces the risk of executing arbitrary code.\n   - Pickle: Pickle is a Python-specific binary serialization format that ==can serialize and deserialize complex Python objects.== However, it can ==execute arbitrary code== during deserialization, making it unsafe for handling untrusted data.\n\n[[Interoperability]]\n   - JSON: JSON is language-agnostic and widely used across different programming environments, making it ideal for data interchange between systems.\n   - Pickle: Pickle is specific to Python, which limits its use in cross-language applications.\n\nReadability:\n   - JSON: Being a text format, JSON is human-readable and easy to debug.\n   - Pickle: Pickle produces binary data, which is not human-readable and harder to debug.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "why_json_is_better_than_pickle_for_untrusted_data",
    "outlinks": [
      "json",
      "data_security",
      "pickle",
      "interoperability"
    ],
    "inlinks": [
      "security_vulnerabilities"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Web Map Tile Service (WMTS)",
    "sha": "cee84b2aaa4edfe469e24272e72d903d58b83fd6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Web%20Map%20Tile%20Service%20(WMTS).md",
    "text": "Purpose: WMTS is designed to serve pre-rendered, cached image tiles of maps. \n\nFunctionality:\n- Tile-Based: It serves map images as small, fixed-size tiles, usually in a format such as PNG or JPEG.\n- Performance: By using cached tiles, WMTS can quickly deliver map images, making it highly efficient for applications requiring fast map rendering, like web mapping applications.\n- Scalability: The tile-based approach allows for easy scaling and efficient handling of high load, as the same tiles can be reused for multiple requests.\n- Standardization: It is standardized by the Open Geospatial Consortium (OGC), ensuring interoperability between different systems and software.\n\nRelated:\n- [[GIS]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "web_map_tile_service_(wmts)",
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "gis"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "Windows",
    "sha": "59b040c5c5f9139c401dc70f1de7f07f0f8339ab",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/Windows.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "system"
    ],
    "normalized_filename": "windows",
    "outlinks": [],
    "inlinks": [
      "bat",
      "powershell_versus_command_prompt"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "bat",
    "sha": "af1b8741f31e80fe75c090f7f3b3f5556e6d6dee",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/bat.md",
    "text": "### Batch Files in [[Windows]]: Capabilities and Use Cases\n\nBatch (`.bat`) files are plain-text scripts interpreted by `cmd.exe`. ([[Command Prompt]]). They allow users to automate tasks, orchestrate program execution, and manage files or system state using built-in Windows commands. While simple, they are effective for both personal automation and lightweight operational workflows.\n\nBetter than repeatedly using commands. the script can be improved/edited.\n### General Capabilities of `.bat` Files\n\n1. Run programs and scripts\n   Batch files can launch executables, open documents, or run other scripts (including Python, [[PowerShell]], and other `.bat` files).\n\n2. Automate file and folder operations\n   They support creating, deleting, copying, renaming, and moving files or directories, which is useful for backups, cleanup routines, and organizing output from tools.\n\n3. Manage scheduled tasks\n   Batch files can use `schtasks` to create, modify, and delete scheduled tasks for recurring automation without needing a GUI.\n\n4. Control flow and logic\n   They support conditional execution (`if`, `else`), loops (`for`), and command grouping, allowing decision-making and iterative behavior.\n\n5. Set and use [[Environment Variables]]\n   Variables can be assigned and referenced within the script to manage inputs, paths, or intermediate results.\n\n6. Handle input and output\n   Batch files can prompt the user, read input, and redirect output to files. They can log events or script results to text files automatically.\n\n7. Run interactively or silently\n   You can run batch scripts in a visible console, a minimized window, or hidden using `start`, Task Scheduler, or shortcuts with specific properties.\n\n8. Integrate with other tools\n   Batch files can call PowerShell, VBScript, Python, or other command-line tools, making them flexible wrappers for more complex automation pipelines.\n\n### Use Case: Repeating Task Without Task Scheduler\n\nBatch files can run continuously using a loop and a wait/delay mechanism. This enables repeating tasks (e.g., a Python script that monitors or processes files) without relying on external schedulers. It's useful for ad hoc or always-on utilities triggered manually or via login/startup.\n\nThis approach is lightweight and works well for simple polling, reporting, or periodic logging jobs.\n### Stopping a Background or Startup `.bat` Loop\n\nFor looping scripts that run in the background or on system startup, you need a controlled way to stop execution:\n\n1. Interactive console:\n   Use `Ctrl + C` to manually interrupt.\n\n2. Forceful termination:\n   A separate `.bat` file can issue a command to kill all `cmd.exe` processes. This is effective but indiscriminate and should be used with caution.\n\n3. Targeted termination:\n   Assign a unique window title or capture the process ID when launching. This enables selective termination using `taskkill` or `powershell Stop-Process`.\n\n### Use Cases in Practice\n\n - Automate file backups or cleanup\n - Launch and manage ETL pipelines\n - Poll a folder for new files and process them\n - Control execution order of dependent tools\n - Provide lightweight GUI entry points via double-click\n - Integrate legacy tools into modern workflows\n - Simplify recurring admin tasks for non-technical users\n\n### When to Use `.bat` Files\n\nBatch files are ideal when:\n - You want fast automation without extra dependencies\n - You're working in a Windows-native environment\n - Tasks involve legacy systems or command-line tools\n - You need quick iteration or deployment\n\nThey’re less ideal for complex logic, advanced error handling, or heavy data manipulation — those cases are better served by PowerShell or [[Python]].",
    "aliases": [
      "Batch"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "file_type",
      "system"
    ],
    "normalized_filename": "bat",
    "outlinks": [
      "windows",
      "environment_variables",
      "command_prompt",
      "python",
      "powershell"
    ],
    "inlinks": [
      "batch_vs_powershell_scripts",
      "command_prompt",
      "powershell",
      "registering_a_scheduled_task",
      "windows_scheduled_tasks"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "dependency manager",
    "sha": "1871bd8af818b17c45662ba6cac19dca532b8edc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/dependency%20manager.md",
    "text": "[[Virtual environments]]\n\n[[requirements.txt]]\n\n[[TOML]]\n\n[[Poetry]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "devops",
      "system"
    ],
    "normalized_filename": "dependency_manager",
    "outlinks": [
      "requirements.txt",
      "virtual_environments",
      "toml",
      "poetry"
    ],
    "inlinks": [
      "poetry"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "dbt",
    "sha": "78c81284570805346b5b8173332ee74dc7b96d43",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/dbt.md",
    "text": "Data build tool is an open-source framework designed for [[Data Transformation]] within a modern data stack. \n\nIt enables analysts and engineers to transform, model, and manage data using [[SQL]] while ==adhering to software engineering best practices== like version control, testing, and [[Documentation & Meetings]]. \n\n### Key Concepts of dbt:\n1. **SQL-based Transformation**: dbt allows users to write SQL queries to define transformations and models, making it accessible for analysts who are already familiar with SQL. It doesn't handle extraction or loading of data, but focuses purely on transforming data that is already in a data warehouse.\n\n2. **Modular and Reusable Models**: dbt encourages the creation of modular, ==reusable SQL \"models.\"== Each model represents a transformation, and these models can be built on top of each other. A model is essentially a SQL query stored as a `.sql` file that dbt uses to transform raw data into a refined dataset. Models are run in sequence, with dbt handling dependencies between models.\n\n3. **Version Control and Collaboration**: dbt integrates with Git for version control, making it easy for teams to collaborate, track changes, and roll back to previous versions if needed. This promotes transparency and accountability within the data team.\n\n4. **Testing**: dbt allows users to write and run tests to ensure data integrity and consistency. You can define tests for specific models or fields, like checking for non-null values or ensuring data uniqueness.\n\n5. **Documentation**: dbt auto-generates documentation from your models, providing a clear overview of your data transformations, lineage, and dependencies. You can also add descriptions for models and fields to improve the clarity of your data pipelines.\n\n6. **[[data lineage]]**: dbt automatically tracks the lineage of your data by mapping dependencies between models. This makes it easy to understand how data flows through the pipeline and where any upstream or downstream issues might originate.\n\n7. **Extensibility**: dbt has a plugin architecture that allows users to extend functionality. For example, there are adapters for popular data warehouses like [[Snowflake]], [[BigQuery]], Redshift, and others, making dbt highly flexible in different data stack environments.\n\n8. **Cloud and Core Versions**: \n   - **dbt Core** is the open-source version that you run locally or in your cloud infrastructure.\n   - **dbt Cloud** is a fully managed service that adds features like scheduling, logging, and a web-based IDE for dbt workflows.\n\n### Workflow with dbt:\n1. **Data Loading**: First, data is loaded into a data warehouse from various sources using ELT tools (e.g., Fivetran, Stitch).\n2. **Transform with dbt**: Using dbt, you write SQL models to clean, transform, and aggregate the raw data into useful, analytical datasets.\n3. **Build Data Models**: You organize your models into layers, often referred to as staging, intermediate, and final models.\n4. **Testing and Documentation**: Run tests to validate data, generate lineage diagrams, and create documentation.\n5. **Deploy**: Schedule or trigger dbt jobs to run in production environments, ensuring consistent and accurate data transformations.\n\n### Example of a dbt Model:\n```sql\n-- models/staging_orders.sql\nWITH raw_orders AS (\n    SELECT * FROM {{ ref('raw_orders_data') }}\n)\nSELECT \n    order_id,\n    customer_id,\n    order_date,\n    amount\nFROM raw_orders\nWHERE order_status = 'completed';\n```\nIn this model:\n- `ref('raw_orders_data')` is referencing another model that contains raw order data.\n- The model selects and transforms only the completed orders.\n\n### Benefits of Using dbt:\n1. **Analyst Empowerment**: dbt empowers data analysts to own the transformation process using SQL, reducing dependency on data engineers for transformations.\n2. **Version Control and Testing**: Built-in version control and testing improve data reliability and reduce risks of errors in production.\n3. **Modularity and Scalability**: The modular nature of dbt models makes it easier to scale transformations and manage complex pipelines.\n4. **Transparency and Documentation**: dbt creates clear documentation and lineage automatically, improving visibility across teams.\n\n### Tools Integrating with dbt:\n- **Data Warehouses**: Redshift, Snowflake, BigQuery, Postgres.\n- **[[ELT]] Tools**: Stitch, Fivetran, Airbyte (for the extraction and loading phase).\n- **Version Control**: GitHub, GitLab, Bitbucket (for managing dbt code).\n  \n### Resources:\nhttps://www.getdbt.com/blog/what-exactly-is-dbt\n[dbt](https://docs.getdbt.com/docs/introduction) \n\n**dbt (Data Build Tool)** is primarily a **transformation and modeling tool** for your data warehouse or lakehouse.\n\n* It helps you **transform raw data into clean, analytics-ready tables** using SQL.\n* It enforces **version control**, **testing**, and **documentation** for your transformations.\n* You define transformations as **modular SQL models** (SELECT statements), and dbt manages **dependencies**, **execution order**, and **incremental builds**.\n\nIn short:\n\n> dbt = \"SQL-based data engineering framework for managing transformations as code.\"",
    "aliases": [
      "data build tool"
    ],
    "date modified": "22-10-2025",
    "tags": [
      "data_engineering",
      "data_modeling",
      "data_pipeline",
      "tool",
      "transformation",
      "governance"
    ],
    "normalized_filename": "dbt",
    "outlinks": [
      "elt",
      "sql",
      "data_transformation",
      "data_lineage",
      "bigquery",
      "snowflake",
      "documentation_&_meetings"
    ],
    "inlinks": [
      "data_contract",
      "data_engineering_tools",
      "data_observability",
      "databricks_&_dbt",
      "elt",
      "github_actions",
      "jinja_template"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "dagster",
    "sha": "a539a9140a72cf0221029ddb479f4b855b71ffb7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/dagster.md",
    "text": "[Dagster](https://dagster.io/) is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [[Data Lifecycle Management]] lifecycle, with integrated lineage and observability, a [[Declarative Data Pipeline]] programming model, and best-in-class testability.\n\nKey features are: \n- Manage your data assets with code\n- A single pane of glass for your data platform",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "dagster",
    "outlinks": [
      "declarative_data_pipeline",
      "data_lifecycle_management"
    ],
    "inlinks": [
      "data_management",
      "directed_acyclic_graph_(dag)",
      "etl"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "frontend",
    "sha": "958367bc38cb5603166d9360136c748924f269c2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/frontend.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "frontend"
    ],
    "normalized_filename": "frontend",
    "outlinks": [],
    "inlinks": [
      "flask",
      "master_observability_datadog"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "functional programming",
    "sha": "049198cb4f847e352bb2b441d10ac5c8422d29ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/functional%20programming.md",
    "text": "Functional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and [declarative](term/declarative.md) as opposed to imperative. It's getting more popular with the rise of Functional Data Engineering.\n\nSee also [Programming Languages](programming%20languages.md).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "functional_programming",
    "outlinks": [],
    "inlinks": [
      "pyright"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "gitlab-ci.yml",
    "sha": "6cef963c6d88fbe12fa65095a0299b2786aafad3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/gitlab-ci.yml.md",
    "text": "The purpose of a `gitlab-ci.yml` file is to define and configure the **GitLab CI/CD pipeline** for automating tasks such as building, [[Testing]], and deploying your code. It is the core configuration file that GitLab uses to orchestrate and execute CI/CD workflows in a repository. \n\n[[Continuous Integration]]\n[[yaml]]\n\n### Key Purposes:\n\n1. **Automation of Workflows:**\n    - Automates repetitive tasks like running tests, building applications, linting code, and deploying updates.\n      \n2. **Pipeline Definition:**\n    - Specifies the **stages** (e.g., `build`, `test`, `deploy`) and their sequence.\n    - Defines the **jobs** within each stage and their respective commands.\n      \n3. **Consistency and Reliability:**\n    - Ensures consistent execution of tasks across environments, reducing errors caused by manual intervention.\n      \n4. **Integration with [[Gitlab]]:**\n    - Automatically triggers pipelines in response to events such as code pushes, merge requests, or scheduled runs.\n      \n5. **Environment Management:**\n    - Manages deployments to various environments (e.g., development, staging, production) with variables, conditions, and manual approvals.\n      \n6. **Feedback and Reporting:**\n    - Provides immediate feedback on the status of tasks (e.g., whether tests passed) directly in the GitLab interface.\n    - Supports artifact generation and uploads (e.g., logs, reports, or compiled binaries).\n\n### Benefits:\n\n- Improves development velocity by automating workflows.\n- Increases code quality through consistent testing and linting.\n- Simplifies deployments to various environments.\n- Enables team collaboration with clear and visible pipeline progress.\n\n### Example \n\n```yaml\n# Define the stages of the pipeline in the order they will be executed\nstages:\n  - build    # The stage where the application is built\n  - test     # The stage where tests are executed\n  - deploy   # The stage where the application is deployed\n\n# Job to build the project\nbuild_job:\n  stage: build           # Assign this job to the 'build' stage\n  script:                # Commands to execute during this job\n    - echo \"Building the project\" # Example build command (replace with actual build steps)\n  artifacts:             # Files or directories to save for use in subsequent jobs\n    paths:\n      - build/           # Save the 'build' directory as an artifact for later stages\n\n# Job to test the project\ntest_job:\n  stage: test            # Assign this job to the 'test' stage\n  script:                # Commands to execute during this job\n    - echo \"Running tests\" # Example test command (replace with actual test steps)\n\n# Job to deploy the project\ndeploy_job:\n  stage: deploy          # Assign this job to the 'deploy' stage\n  script:                # Commands to execute during this job\n    - echo \"Deploying the application\" # Example [[Model Deployment|deployment]] command (replace with actual deployment steps)\n  only:                  # Specify when this job should run\n    - main               # Only run this job for commits to the 'main' branch\n\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "gitlab-ci.yml",
    "outlinks": [
      "gitlab",
      "continuous_integration",
      "model_deployment",
      "yaml",
      "testing"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "imperative",
    "sha": "66deb62a304ae1338e0bdfcc2ecb3d96b0d47288",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/imperative.md",
    "text": "An **imperative** pipeline tells ==_how_ to proceed== at each step in a procedural manner. \n\nIn contrast, a **[declarative](term/declarative.md)** data pipeline does not tell the order it needs to be executed but instead ==allows each step/task to find the best time and way to run.== \n\nThe *how* should be taken care of by the tool, framework, or platform running on. \n\nFor example, update an asset when upstream data has changed. \n\nBoth approaches result in the same output. \n\nHowever, the declarative approach benefits from **leveraging compile-time query planners** and **considering runtime statistics** to choose the best way to compute and find patterns to reduce the amount of transformed data.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration"
    ],
    "normalized_filename": "imperative",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "ipynb",
    "sha": "f9a939a1a4bd1cb6fc73ad5fb6c57ffa73ab886d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/ipynb.md",
    "text": "At a system level, working with `.ipynb` (Jupyter Notebook) files involves understanding them as structured [[Json]] documents, and then managing, manipulating, or integrating them into broader data workflows, systems, or tools. Here's a breakdown of what you can do at that level:\n### Programmatic Manipulation (as JSON)\n\n`.ipynb` files are JSON-structured documents. At a system level, you can:\n\n* Parse and modify them using Python (`json` module), `nbformat`, or external tools.\n* Extract code cells, markdown cells, outputs, and metadata.\n* Insert cells programmatically to generate templated or modular notebooks.\n\n### Execution Automation\n\nYou can execute `.ipynb` files non-interactively. Batch run notebooks with tools like:\n  * `papermill` (parameterised execution)\n  * `nbconvert --execute`\n  * `jupyter nbclient`\n\n### Conversion & Export\n\nConvert notebooks into various formats:\n\n* HTML, PDF (e.g., for reports, presentations)\n* Python scripts (`nbconvert`)\n* Slideshows ([[Reveal.js]])\n* Markdown for publishing\n### Embedding and Serving\n* Serve as interactive documentation or reports (e.g., [[Jupyter Book]])\n* Used in browser-based notebook environments (JupyterHub, [[Google Colab]]).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "ipynb",
    "outlinks": [
      "json",
      "jupyter_book",
      "google_colab",
      "reveal.js"
    ],
    "inlinks": [
      "binary_classification",
      "jupytext",
      "nbconvert_slideshows"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "jinja template",
    "sha": "ab8c7b210486d36f2ee71c70498bcfdaf4aa9de0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/jinja%20template.md",
    "text": "### Resources\n\n[LINK](https://www.youtube.com/watch?v=OraYXEr0Irg)\n\n### Practical\n\njinja2 works with python 3.\n\n![[Pasted image 20240922201606.png]]\n\nRenders templates with variable substitutions \n\nYou can use tags too.\n\n![[Pasted image 20240922202345.png]]\n\nGet gpt to generate example if necessary.\n\ncan get a csv to export the data.\n\ncontext dictionaries are used can do html and flask.\n\njinja used to manage web pages\n\n[[Flask]]\n\nmakes me think about how [[Quartz]] is constructed.\n\n### About\n\nJinja is a fast, expressive, extensible templating engine. Special placeholders in the template allow writing code similar to [Python](term/python.md) syntax. Then the template is passed data to render the final document.\n\nMost popularized by [[dbt]]. Read more on the [Jinja Documentation](https://jinja.palletsprojects.com/).\n\nintegrates with [[Flask]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "jinja_template",
    "outlinks": [
      "pasted_image_20240922201606.png",
      "quartz",
      "flask",
      "dbt",
      "pasted_image_20240922202345.png"
    ],
    "inlinks": [
      "quartz"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "kubernetes",
    "sha": "04ca79b6565a3a6393ad36669debbe948215c40a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/kubernetes.md",
    "text": "It’s a platform that allows you to run and orchestrate container workloads. [**Kubernetes**](https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/) **has become the de-facto standard** for your cloud-native apps to (auto-) [scale-out](https://stackoverflow.com/a/11715598/5246670) and deploy your open-source zoo fast, cloud-provider-independent. No lock-in here. Kubernetes is the **move from infrastructure as code** towards **infrastructure as data**, specifically as [YAML](term/yaml.md). With Kubernetes, developers can quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "orchestration",
      "software"
    ],
    "normalized_filename": "kubernetes",
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "google_cloud_platform"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "jupytext",
    "sha": "f99421402a6a6e96ee31263a3e4cd8d99e4edea5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/jupytext.md",
    "text": "Jupytext is a tool that lets you convert between Jupyter Notebooks (`.[[ipynb]]`) and plain text formats like:\n* [[Python]] scripts (`.py`)\n* Markdown (`.md`)\n* [[R]] (`.R`)\n* Julia (`.jl`)\n### What Can You Do with It?\n\n* Edit notebooks in your favourite text editor (e.g., VS Code, Vim, Emacs)\n* Version-control notebooks easily using Git (avoids messy JSON diffs)\n* Convert `.py` ↔ `.ipynb`:\n\n  * From script to notebook:\n    ```bash\n    jupytext --to notebook script.py\n    ```\n  * From notebook to script:\n    ```bash\n    jupytext --to py notebook.ipynb\n    ```\n* Pair `.ipynb` with `.py` so changes sync in both directions\n\n### Bidirectional Sync (\"Paired Notebooks\")\n\nYou can keep a `.ipynb` notebook and a `.py` (or `.md`, etc.) file synced:\n- Edit in Jupyter or in a text editor, and both formats stay up to date.\n- Useful for version control (text diffs) while still running interactively in notebooks.\n    \n\npython\n```\n# Add this to the top of a Python script for pairing:\n# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,py\n#     text_representation:\n#       extension: .py\n#       format_name: light\n# ---\n\"\"\"# This becomes a markdown cell in the notebook\"\"\"\n```\n\nThen run:\n\n`jupytext --sync notebook.ipynb`",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "software",
      "documentation"
    ],
    "normalized_filename": "jupytext",
    "outlinks": [
      "ipynb",
      "r",
      "python"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "nbconvert",
    "sha": "5fc9374d28282fbd5c7c5ab2fec58b4ba00da357",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/nbconvert.md",
    "text": "`nbconvert` is a powerful command-line tool for converting Jupyter notebooks into other formats such as HTML, PDF, Markdown, LaTeX, slides, and more. It supports exporting, cleaning, and customizing output, making it ideal for documentation, presentations, and reporting.\n\n## Typical Use Cases\n\n### 1. Printing or Exporting Without Code\n\nUseful for creating clean reports or meeting documents without code cells:\n\n```bash\njupyter nbconvert stock_analysis.ipynb --no-input --to pdf\n```\n\n```bash\njupyter nbconvert phi_analysis.ipynb --to html --no-input --no-prompt\n```\n\nAlternatively, using the `TemplateExporter` class:\n\n```bash\njupyter nbconvert Querying.ipynb --to html --TemplateExporter.exclude_input=True\n```\n\nSee:\n[Stack Overflow – Hide Code When Exporting](https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html)\n\n### 2. Converting to Reveal.js Slides: [[Reveal.js]]\n\nTurn a notebook into an interactive slideshow: See [[nbconvert slideshows]]\n\n```bash\njupyter nbconvert notebook.ipynb --to slides --post serve\n```\n\n* Each cell becomes a slide (if marked with slide metadata).\n* Opens a Reveal.js-based presentation in your browser.\n\nTo see possible themes see: https://revealjs.com/themes/\n\nBest themes:\n- serif\n- \n## Additional Capabilities\n\n| Format        | Command example            | Description                                      |\n| ------------- | -------------------------- | ------------------------------------------------ |\n| HTML          | `--to html`                | Default HTML export                              |\n| PDF           | `--to pdf`                 | Export to print-friendly format (requires LaTeX) |\n| Markdown      | `--to markdown`            | Convert for static blog/docs integration         |\n| Python script | `--to script`              | Convert notebook to plain `.py` script           |\n| LaTeX         | `--to latex`               | Generate LaTeX report for academic use           |\n| Slides        | `--to slides --post serve` | Interactive Reveal.js presentation               |\n\n## Common Options\n\n| Option                                  | Purpose                                   |\n| --------------------------------------- | ----------------------------------------- |\n| `--no-input`                            | Hide code cells (inputs)                  |\n| `--no-prompt`                           | Hide `In [ ]:` and `Out [ ]:` prompts     |\n| `--clear-output`                        | Remove all outputs from the notebook      |\n| `--output new_name`                     | Set a custom name for the output file     |\n| `--TemplateExporter.exclude_input=True` | Template-based approach for hiding inputs |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "software",
      "tool"
    ],
    "normalized_filename": "nbconvert",
    "outlinks": [
      "reveal.js",
      "nbconvert_slideshows"
    ],
    "inlinks": [
      "nbconvert_slideshows",
      "terminal_commands"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "tool.bandit",
    "sha": "c6b9110b0467d101da329e42190a4e0d5bb1686b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/tool.bandit.md",
    "text": "# Bandit: A Security Linter for Python\n\n## Resources\n- [Bandit Documentation](https://bandit.readthedocs.io/en/latest/)\n## How to Use Bandit\n\nInstallation\nTo install Bandit, use pip by running the following command in your terminal:\n```bash\npip install bandit\n```\nRunning Bandit\nAfter installation, you can run Bandit on your Python files or directories. For example, to scan a file named `example.py`, use:\n```bash\nbandit example.py\n```\nThis command will analyze the file and report any security issues it finds.\n### 3. Customizing Bandit\nYou can customize Bandit's behavior by specifying options. For example, to scan a directory and exclude certain subdirectories, use:\n\n```bash\nbandit -r example_directory -x example_directory/venv\n```\n\n- `-r` specifies the directory to scan.\n- `-x` specifies directories to exclude.\n\n### 4. Example Script\nHere's a simple Python script that Bandit can analyze:\n\n```python\nimport subprocess\n\nuser_input = input(\"Enter your name: \")\nsubprocess.call([\"echo\", user_input])\n```\n\nThis script takes user input and passes it to the `echo` command using `subprocess.call()`. This can be dangerous as it might allow command injection.\n\nTo analyze the script, run:\n```bash\nbandit example.py\n```\nBandit will generate a report highlighting potential security issues. For the script above, it might flag the use of `subprocess.call()` as a potential injection vector.\n\nFixing Issues\nBased on Bandit's report, you can modify your code to fix vulnerabilities. For example, to mitigate the risk of command injection, you can set `shell=False`:\n\n```python\nimport subprocess\n\nuser_input = input(\"Enter your name: \")\nsubprocess.call(f\"echo {user_input}\", shell=True)\n```\nThen rerun [[Bandit example output]]\n## Example Script for Bandit Analysis\n\nIn [[ML_Tools]] see: [[Bandit_Example_Nonfixed.py]]\n\nFeatures Demonstrated: [[Security Vulnerabilities]]\n1. **Command Injection**: The `dangerous_subprocess` function uses `subprocess.call` with `shell=True`, which can lead to command injection if user input is not properly sanitized.\n2. **Hardcoded Password**: The `hardcoded_password` function contains a hardcoded password, which is a common security issue.\n3. **Use of `eval`**: The `unsafe_eval` function uses `eval`, which can execute arbitrary code if the input is not controlled.\n4. **Insecure Deserialization**: The `insecure_deserialization` function uses `pickle.loads`, which can be exploited if untrusted data is deserialized.\n\n### Running Bandit on the Example Script\nTo analyze this script with Bandit, save it as `example.py` and run:\n\n```bash\nbandit example.py\n```\nBandit will generate a report highlighting the security issues in the script, providing insights into how each feature can be potentially exploited and suggesting ways to mitigate these risks.\n\nBy following these steps, you can use Bandit to identify and address security vulnerabilities in your Python code. Remember, while Bandit is a powerful tool, it's important to complement it with good coding practices and thorough security testing.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "python",
      "security"
    ],
    "normalized_filename": "tool.bandit",
    "outlinks": [
      "bandit_example_output",
      "ml_tools",
      "security_vulnerabilities",
      "bandit_example_nonfixed.py"
    ],
    "inlinks": [
      "security_vulnerabilities",
      "toml"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "tool.ruff",
    "sha": "13b35480647c140339b8f48715ad1ffa91b1576d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/tool.ruff.md",
    "text": "Ruff is a fast [[Python]] linter and code formatter.\n\nIt is designed to enforce coding style and catch potential errors in Python code. \n\nRuff aims to be efficient and comprehensive, supporting a wide range of linting rules and style checks. It can be used to automatically format code to adhere to a specified style guide, making it a useful tool for maintaining consistent code quality across a project.\n\n## in [[TOML]]\n\nhave:\n\n```toml\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "security"
    ],
    "normalized_filename": "tool.ruff",
    "outlinks": [
      "toml",
      "python"
    ],
    "inlinks": [
      "devops_portal",
      "technical_debt",
      "toml"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "shapefile",
    "sha": "be6e20f22eca1ada7f7bf3ed6cf0aa7eca79fcd2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/shapefile.md",
    "text": "A shapefile is a popular geospatial vector data format for geographic information system (GIS) software. It is widely used for storing the location, shape, and attributes of geographic features. Developed by Esri, shapefiles are commonly used in the GIS community for exchanging and managing geospatial data.\n\nA shapefile is a widely used [[GIS]] vector data format consisting of multiple files that store both spatial geometry and attribute data. Its ease of use and broad compatibility have made it a standard format for geospatial data exchange and analysis in the GIS community.\n### Components of a Shapefile\n\nA shapefile is not a single file, but rather a set of several files that work together. The primary components include:\n\n1. **.shp file**: This file contains the geometry data (points, lines, polygons) that represents the spatial features.\n\n2. **.shx file**: This is an index file that allows for quick access to the geometry data in the .shp file.\n\n3. **.dbf file**: This file stores attribute data in tabular format, linked to the spatial data in the .shp file. It uses the dBASE format to hold the attributes of each shape, such as names, categories, or other descriptive information.\n\nIn addition to these mandatory files, a shapefile can also include several optional files that provide additional information:\n\n4. **.prj file**: Contains the coordinate system and projection information for the spatial data. This file is crucial for ensuring that the data is displayed correctly in GIS software.\n\n5. **.cpg file**: Defines the character encoding to be used for the .dbf file, ensuring that text attributes are interpreted correctly.\n\n6. **.qix file**: An optional spatial index file that can improve the performance of spatial queries on the shapefile.\n\n### Characteristics of Shapefiles\n\n- **Geometry Types**: Shapefiles can store different types of geometric data, including points, lines, and polygons. However, each shapefile can contain only one type of geometry.\n- **Attribute Data**: The .dbf file allows shapefiles to store descriptive data about each spatial feature, which can be used for analysis and mapping.\n- **Limitations**: Shapefiles have some limitations, such as a maximum file size of 2 GB for each component file, lack of support for advanced geometric types (like curves), and potential data redundancy and inefficiencies.\n\n### Usage of Shapefiles\n\nShapefiles are extensively used in GIS for various purposes, including:\n\n- **Mapping**: Displaying geographic features on maps for visualization.\n- **Spatial Analysis**: Performing spatial queries, analysis, and geoprocessing tasks.\n- **Data Exchange**: Sharing geospatial data between different GIS software and systems.\n\n### Example Scenario\n\nConsider a city planning department that wants to map all the parks within the city. They might use a shapefile to store the polygon geometries representing park boundaries along with attributes such as park names, areas, and facilities available. This shapefile can then be loaded into GIS software to create maps, analyze park distributions, and manage urban planning tasks.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "file_type"
    ],
    "normalized_filename": "shapefile",
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "gis"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "tool.uv",
    "sha": "9fa46e1c9c8da404b464f869893c7d25eff09104",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/tool.uv.md",
    "text": "Appears in [[TOML]] file\n\nLink: https://github.com/astral-sh/uv\n\n---\n\n`uv` is a tool for managing Python development [[Virtual environments]], projects, and dependencies. It offers a range of features that streamline various aspects of Python development, from installing Python itself to managing projects and dependencies. \n\n1. **Python Version Management**: `uv` allows you to install, list, find, pin, and uninstall Python versions. This is useful for managing multiple Python versions across different projects, ensuring compatibility and ease of switching between environments.\n\n2. **Script Execution**: You can run standalone Python scripts and manage their dependencies directly with `uv`. This simplifies the process of executing scripts with specific dependencies without needing a full project setup.\n\n3. **Project Management**: `uv` provides commands to create new projects, manage dependencies, sync environments, and build and publish projects. This is particularly useful for maintaining consistent environments and dependencies across development, testing, and production stages.\n\n4. **Tool Management**: It supports running and installing tools from Python package indexes, making it easier to integrate tools like linters and formatters into your workflow.\n\n5. **Pip Interface**: `uv` offers a pip-like interface for managing packages and environments, which can be used in legacy workflows or when more granular control is needed.\n\n6. **Utility Commands**: It includes utility commands for managing cache, directories, and performing self-updates, which help maintain the tool's efficiency and keep it up-to-date.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "tool.uv",
    "outlinks": [
      "toml",
      "virtual_environments"
    ],
    "inlinks": [
      "devops_portal",
      "toml"
    ]
  },
  {
    "category": "DEVOPS",
    "filename": "unittest",
    "sha": "b3a7a76f14be1985b84c632bdde5a65422815a71",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/unittest.md",
    "text": "### **`@patch` (from `unittest.mock`) Explanation**\n\n`@patch` is used to replace objects/functions with mock versions during tests. It is part of Python’s `unittest.mock` module.\n\n#### **Example & Usage**\n\npython\n\nCopy code\n\n`from unittest.mock import patch  def fetch_data():     \"\"\"Simulated function that fetches data from an API\"\"\"     return \"Real Data\"  @patch(\"__main__.fetch_data\", return_value=\"Mocked Data\") def test_fetch_data(mock_fetch):     assert fetch_data() == \"Mocked Data\"`\n\n🔹 **How it works:**\n\n- `@patch(\"__main__.fetch_data\", return_value=\"Mocked Data\")` replaces `fetch_data()` with a mocked version returning `\"Mocked Data\"`.\n- Inside the test, `fetch_data()` will **always** return `\"Mocked Data\"` instead of calling the real function.\n\n#### **Why use `@patch`?**\n\n- Prevents tests from making actual API/database calls.\n- Speeds up testing by mocking expensive operations.\n- Allows control over return values and side effects.\n\n\n### **Your Case:**\n\n- **`@pytest.fixture`** is used to provide reusable test data (`mock_files`).\n- **`@patch`** is used to:\n    - Mock file operations (`builtins.open`, `os.walk`).\n    - Mock function calls (`process_file`, `log_action`, `write_updated_file`).\n    - Prevent real file modifications while testing.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "blank"
    ],
    "normalized_filename": "unittest",
    "outlinks": [],
    "inlinks": [
      "testing"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "AI Engineer",
    "sha": "f67e6fe5322be52cbc3eeaa24fb9025e3d46b1e9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/AI%20Engineer.md",
    "text": "They know what\n- [[LSTM]] means\n- [[Attention mechanism]]\n- [[Prompts]] optimisation\n- [[Neural network]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "ai_engineer",
    "outlinks": [
      "lstm",
      "prompts",
      "neural_network",
      "attention_mechanism"
    ],
    "inlinks": []
  },
  {
    "category": "DEVOPS",
    "filename": "yaml",
    "sha": "622e7e98a9e52d56979aab596d3ed87b9633dfd2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/devops/yaml.md",
    "text": "Stands for [YAML ain't markup language](https://github.com/yaml/yaml-spec) and is a superset of JSON\n\n- lists begin with a hyphen\n- dependent on whitespace / indentation\n- better suited for configuration than [[Json]]\n\nYAML is a data serialization language often used to write configuration files. Depending on whom you ask, YAML stands for yet another markup language, or YAML isn’t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.\n\n\n```YAML\n---\n# <- yaml supports comments, json does not\n# did you know you can embed json in yaml?\n# try uncommenting the next line\n# { foo: 'bar' }\n\njson:\n  - rigid\n  - better for data interchange\nyaml:\n  - slim and flexible\n  - better for configuration\nobject:\n  key: value\n  array:\n    - null_value: null\n    - boolean: true\n    - integer: 1\n    - alias: aliases are like variables\n    - alias: aliases are like variables\nparagraph: |\n  Blank lines denote\n  paragraph breaks\ncontent: |-\n  Or we\n  can auto\n  convert line breaks\n  to save space\nalias:\n  bar: baz\nalias_reuse:\n  bar: baz \n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "software"
    ],
    "normalized_filename": "yaml",
    "outlinks": [
      "json"
    ],
    "inlinks": [
      "gitlab-ci.yml"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "AI governance",
    "sha": "377604f17d4229862a39cbee44d23ebfc3fc445e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/AI%20governance.md",
    "text": "AI governance refers to the frameworks, policies, and standards that ensure the responsible development and use of artificial intelligence.\n\n#### Purpose\n* Ensures compliance in regulated sectors (e.g. finance, healthcare).\n* Aligns AI deployment with legal, ethical, and societal norms.\n\n#### Key Constraints\n* Legal compliance\n* Transparency (explainability, auditability)\n* Security (e.g. adversarial robustness)\n* Historical bias in data and models\n\n#### Notable Standards & Frameworks\n* EU AI Act – risk-based regulatory framework.\n* OWASP LLM Top 10 – security-focused guidelines for large language models.\n\n#### Tension\n* Governance vs Innovation: Oversight can slow down progress, but lack of it risks harm.\n* Ongoing challenge: Can bureaucracy keep pace with the speed of AI advancement?\n\n#### Related\n* [[Data Governance]]\n* [[Model Explainability]]\n* [[Data Ethics]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "governance"
    ],
    "normalized_filename": "ai_governance",
    "outlinks": [
      "data_ethics",
      "data_governance",
      "model_explainability"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Analytics Engineer",
    "sha": "ac4e7088d2adca31a7f6e82b2163e9b25d343e44",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Analytics%20Engineer.md",
    "text": "- Analytics engineers focus on the transformation of raw data into transformed data that is ready for analysis. This new role on the data team changes the responsibilities of data engineers and data analysts.\n- Data engineers can focus on larger data architecture and the EL in ELT.\n- Data analysts can focus on insight and dashboard work using the transformed data.\n- Note: At a small company, a data team of one may own all three of these roles and responsibilities. As your team grows, the lines between these roles will remain blurry.\n\nThey:\n- Use [[dbt 1]]\n- Do the jobs of an analyst and engineer",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "analytics_engineer",
    "outlinks": [
      "dbt_1"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Business Understanding",
    "sha": "4314c6aef888d69f4f9504e7fa7d5f7ff4c5e89a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Business%20Understanding.md",
    "text": "Purpose\n\n* Understand how the business measures success.\n* Translate business objectives into a [[Data Mining]] or modeling problem.\n* Develop a plan to align analytical work with business goals.\n\nKey Focus\n\n* Identify project objectives from a business perspective.\n* Ensure that the model addresses the underlying business problem, not just technical metrics.\n\nOutcome\n\n* Clear definition of the business problem.\n* Roadmap for [[Data Collection]], preparation, and modeling aligned with business goals.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "business_understanding",
    "outlinks": [
      "data_mining",
      "data_collection"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Business Values",
    "sha": "dbf9a63e9324f46d6c533d2cc0cf567fcf5a00d1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Business%20Values.md",
    "text": "Increasing Revenue\nDecreasing Costs\nImproving operational Efficiency",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "business_values",
    "outlinks": [],
    "inlinks": [
      "managing_people"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Business observability",
    "sha": "2e1221661bad9c5ff0f39739772042b33c1796fa",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Business%20observability.md",
    "text": "Business [[Model Observability|observability]] refers to the ability to gain insights into the internal state and performance of a business through the continuous monitoring and analysis of data. \n\nIt involves collecting, analyzing, and visualizing data from various sources to understand how different parts of the business are functioning and to identify areas for improvement. \n\nBusiness observability aims to provide a comprehensive view of operations, customer interactions, and other critical aspects to enable data-driven decision-making.\n\nIt helps businesses to detect issues early, optimize operations, enhance customer experiences, and drive growth and innovation.\n\nKey components of business observability include:\n\n1. **[[Data Collection]]**: Gathering data from various sources such as customer interactions, sales transactions, operational processes, and external market conditions.\n\n2. **Monitoring**: Continuously tracking key performance indicators (KPIs) and metrics to ensure that the business is operating efficiently and effectively.\n\n3. **Analysis**: Using analytical tools and techniques to interpret the data, identify patterns, and uncover insights that can inform strategic decisions.\n\n4. **[[Data Visualisation]]**: Presenting data in an accessible and understandable format, such as dashboards and reports, to facilitate quick comprehension and action by stakeholders.\n\n5. **Feedback Loops**: Implementing mechanisms to use insights gained from observability to make adjustments and improvements in business processes and strategies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "business_observability",
    "outlinks": [
      "model_observability",
      "data_visualisation",
      "data_collection"
    ],
    "inlinks": [
      "event_driven_events"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Data AI Education at Work",
    "sha": "2894b0be973a0723e425a1a1d6041293e4e23654",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Data%20AI%20Education%20at%20Work.md",
    "text": "### Introduction\n\nOrganizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are equipped with the necessary skills to leverage these technologies effectively.\n\nIntegrating data and AI education into organizational strategies is essential for maintaining competitiveness and fostering a culture of continuous learning. By addressing these areas, organizations can better prepare their workforce for the evolving technological landscape.\n\n### Practical Steps for Integration\n\n1. **Access to Training**:\n   - Provide clear guidance on how to access training courses.\n   - Offer details on accessing training funds and budgets.\n   - Partner with training providers to offer relevant courses.\n\n2. **Learning Resources**:\n   - Collect and distribute clear and concise training materials.\n   - Capture, document, and discuss use cases from staff experiences.\n   - Encourage peer-to-peer learning and collaboration.\n\n3. **Organizational Support**:\n   - Align training with professional competencies.\n   - Foster communication and collaboration with external partners.\n   - Encourage staff to experiment with AI tools to enhance efficiency.\n\n4. **Governance and Strategy**:\n   - Establish governance and skill strategies before deploying AI.\n   - Develop acceptable use policies for AI tools.\n   - Recognize that adopting AI is a gradual process requiring leadership support.\n\n### Fostering a Culture of Continuous Learning\n\n1. **Leadership and Culture**:\n   - Connect learning initiatives with employee incentives and pay.\n   - Allow time and space for learning by alleviating workloads.\n   - Protect training time and build networks to showcase use cases.\n\n2. **Mindset and Adaptability**:\n   - Promote digital literacy and adaptability among employees.\n   - Encourage openness to new ideas and recognize skills beyond the technical team.\n\n### Business Risks of Not Upskilling\n\n1. **Competitive Disadvantage**:\n   - Risk of losing competitive and productivity edges.\n   - Potential loss of market differentiation as competitors advance.\n\n2. **Staff Retention**:\n   - Risk of losing skilled staff uncomfortable with new technologies.\n   - Employees may move to companies at the cutting edge of AI.\n\n3. **Operational Challenges**:\n   - Inconsistencies in ways of working between partner organizations.\n   - Inappropriate use of AI tools by untrained staff.\n\n4. **Productivity and Trust**:\n   - AI's potential to enhance productivity is yet to be fully realized.\n   - Trust and verifiability of AI systems (black boxes) are crucial for business benefits.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "data_ai_education_at_work",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Data Governance",
    "sha": "a73f066c3c014937d4c57b127a02957ec48eb3cc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Data%20Governance.md",
    "text": "[**Data governance**](https://www.talend.com/resources/what-is-data-governance/) **is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.**\n\nIt establishes the processes and responsibilities that ensure the [Data Quality](Data%20Quality.md) and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.\n\n**Data Governance**: Focuses on ensuring that data is managed consistently and adheres to policies, often working in tandem with [[Data Observability]] to enforce quality standards.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#governance",
      "business"
    ],
    "normalized_filename": "data_governance",
    "outlinks": [
      "data_observability"
    ],
    "inlinks": [
      "ai_governance",
      "data_principles",
      "data_roles",
      "data_steward",
      "data_storage",
      "master_data_management"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Data Engineer",
    "sha": "60aaaa69d35c33a09c726d18b3c201b75af2df7c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Data%20Engineer.md",
    "text": "The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on\n- automating the data collection, \n- processing, \n- and analysis workflows,\n- solving how systems manage and handle the flow of data. \n\n==Develops data pipelines and ensures data flow between systems.==\n\nResources:\n- [Link](https://www.youtube.com/watch?v=qWru-b6m030)\n\n### Key Responsibilities:\n\n1. Infrastructure Design and Maintenance:  \n   Data engineers design, build, and maintain the necessary infrastructure to collect, process, and store large amounts of data. This infrastructure is crucial for ensuring data is accessible and usable for analysis and reporting.\n\n2. [[Data Pipeline]]: \n\n3. Support Role:  \n   Data engineers act as a bridge between ==data producers and consumers==, ensuring smooth and reliable data flow. They support business operations through scalable and efficient [[Data Management]] solutions, contributing indirectly to product delivery and decision-making.\n\n### Core Activities:\n\nWhat engineers do & interact with: see [[Data Engineering Portal]]\n\nStakeholders they interact with see [[Data Roles]]\n\nTools they use: [[Data Engineering Tools]]\n\nTasks They Are Usually Given\n  - Project Management: Tracking tasks, bugs, and progress through Azure Boards.\n  - Collaboration: Facilitating teamwork with shared repositories and [[Continuous Integration]] workflows.\n  - Continuous Learning: Keeping up-to-date with the latest technologies and updating pipelines due to obsolescence of tech\n  - [[Documentation & Meetings]] and Security: Creating documentation, implementing security measures, and exploring system upgrades for enhanced efficiency.",
    "aliases": [
      "Data Engineering"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "career",
      "field"
    ],
    "normalized_filename": "data_engineer",
    "outlinks": [
      "data_engineering_portal",
      "continuous_integration",
      "data_pipeline",
      "data_engineering_tools",
      "data_roles",
      "data_management",
      "documentation_&_meetings"
    ],
    "inlinks": [
      "apache_spark",
      "data_engineering",
      "data_roles",
      "databricks"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Data Roles",
    "sha": "7859ea0b152fa69f32bb30e02dd3d8ffaec9cfe0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Data%20Roles.md",
    "text": "A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. \n\nThe team collaborates across various functions to ensure data integrity, accessibility, and usability.\n\n## Key Roles and Responsibilities\n\n| Role                         | Focus Area                    | Key Responsibilities                                                                           |\n| ---------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------- |\n| **[[Data Steward]]**         | [[Data Quality]] & governance | Enforces data policies, resolves [[Data Quality]] issues, manages metadata.                    |\n| **[[Data Governance]] Team** | Policy & compliance           | Defines [[Data Management]] rules, ensures regulatory adherence.                               |\n| **[[Data Engineer]]**        | Data infrastructure           | Builds data pipelines, integrates data sources, and ensures data flow.                         |\n| **[[Data Scientist]]**       | [[Data Analysis]] & modeling  | Utilizes BI tools, analyzes data, develops and deploys ML models.                              |\n| **[[ML Engineer]]**          | Machine learning              | Configures and optimizes ML models, monitors performance in production.                        |\n| **[[Data Architect]]**       | Data architecture             | Designs and manages data infrastructure, ensures data accessibility.                           |\n| **[[Data Analyst]]**         | Reporting & visualization     | Gathers and processes data, generates reports, communicates insights using tools like Tableau. |\n| [[Security Researcher]]      |                               |                                                                                                |\n\n#### Other Stakeholders\n- **Business Analysts:** Ensure data is structured and accessible for analysis and reporting.\n- **Senior Stakeholders and Business Ambassadors:** Communicate requirements, progress, and solutions to align with business goals.\n- **Software Engineers and Data Teams:** Coordinate on data production and integration processes.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "data_roles",
    "outlinks": [
      "data_analyst",
      "security_researcher",
      "data_governance",
      "data_engineer",
      "data_scientist",
      "data_analysis",
      "data_steward",
      "ml_engineer",
      "data_architect",
      "data_quality",
      "data_management"
    ],
    "inlinks": [
      "data_engineer"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Data Steward",
    "sha": "78026fdecad12113a96c63920c17322a2c0f3646",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Data%20Steward.md",
    "text": "A Data Steward is responsible for ensuring the quality, integrity, and governance of an organization's data assets. They act as a bridge between business users, IT teams, and data governance policies, ensuring that data is well-defined, accurate, and used appropriately.\n\n### Key Responsibilities of a Data Steward\n\n1. [[Data Quality]] Management – Ensuring data accuracy, completeness, consistency, and reliability across systems.\n2. Metadata Management – Documenting data definitions, relationships, and lineage.\n3. Data Governance Compliance – Implementing policies, standards, and best practices for data handling.\n4. Master Data Management (MDM) – Managing critical business data entities like customers, products, and suppliers.\n5. Collaboration with Stakeholders – Acting as a liaison between business units, data engineers, and data governance teams.\n6. Issue Resolution – Identifying and resolving data-related issues such as duplicates, missing values, and inconsistencies.\n7. Data Security & Privacy – Ensuring compliance with regulations (e.g., GDPR, HIPAA) by monitoring access and usage.\n\n### Why is a Data Steward Important?\n\n- Enhances data trustworthiness, leading to better decision-making.\n- Reduces data inconsistencies and errors in analytics and reporting.\n- Supports regulatory compliance and risk management.\n- Enables efficient data integration across systems and departments.\n\n\n\n\n\n[[Data Steward]]\n  - Responsible for [[Data Governance]] and quality.\n  - Ensures that data policies and standards are adhered to across the organization.\n  - Acts as a liaison between data users and IT to facilitate [[Data Management]].",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "data_steward",
    "outlinks": [
      "data_steward",
      "data_quality",
      "data_management",
      "data_governance"
    ],
    "inlinks": [
      "data_roles",
      "data_steward"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Design Thinking Questions",
    "sha": "d52c7f3b5e569fcde547e4bc22304bfa3e497aa8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Design%20Thinking%20Questions.md",
    "text": "- \"What is the user’s need?\"\n- \"What constraints are at play?\"\n- \"How might we…?\" — a classic starter for idea generation.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "design_thinking_questions",
    "outlinks": [],
    "inlinks": [
      "asking_questions"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Documentation & Meetings",
    "sha": "07c0f43caa933f4283be5bbbc5ac543b94b8a082",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Documentation%20&%20Meetings.md",
    "text": "## Tools  \n- [[pdoc]] – Auto-generate Python API documentation  \n- [[Mermaid]] – Create diagrams and flowcharts from text in a Markdown-like syntax  \n\nMeetings:\n- Have rules for whether a meeting is worth having and who should be involved. \n## Templates  \n\n### Project & Technical Meetings  \n\n- [[Technical Design Doc Template]] – Document system architecture, components, data flow  \n- [[Pull Request Template]] – Checklist and summary for PRs (code, tests, reviewers, notes)  \n- [[Experiment Plan Template]] – Hypothesis, variables, metrics, and analysis plan  \n- [[Retrospective Template]] – Guide for reviewing past sprint/project cycles  \n### Data & Analytics Meetings  \n- [[Data Request Template]] – Intake form for new analysis or data extract needs  \n- [[One Pager Template]] – Summarize a project, idea, or proposal on a single page  \n- [[Meeting Notes Template]] – Standard format for taking concise and structured notes  \n### Cross-functional / Stakeholder Meetings  \n- [[One Pager Template]] – Summarize a project, idea, or proposal on a single page  \n- [[Meeting Notes Template]] – Standard format for taking concise and structured notes  \n- [[Data Request Template]] – Intake form for new analysis or data extract needs  \n### Reporting & Strategic Meetings  \n- [[Postmortem Template]] – Structure for documenting incidents and learnings  \n- [[Retrospective Template]] – Guide for reviewing past sprint/project cycles  \n- [[One Pager Template]] – Summarize a project, idea, or proposal on a single page  \n### Collaborative & Feedback Sessions  \n- [[1-on-1 Template]] – Agenda for 1-on-1 check-ins with manager or reports  \n- [[Feedback Template]] – Structure for giving/receiving peer or performance feedback  \n## Meeting Types\n\n### Project & Technical Meetings  \n- Sprint Planning / Stand-ups (Agile):  \n  Define priorities, plan tasks, and report blockers on a daily/weekly basis  \n\n- Design & Architecture Reviews:  \n  Evaluate technical designs—e.g., pipeline architecture, schemas, model design  \n- Code Reviews / Pair Programming:  \n  Collaborative code sessions for learning and validation  \n\n- Pipeline/Model Monitoring & Debugging:  \n  Diagnose failures, resolve data/model performance issues in production  \n\n### Data & Analytics Meetings  \n- Exploratory Data Analysis ([[EDA]]) Discussions:  \n  Share insights, anomalies, or feature engineering results  \n\n- Model Review & Evaluation:  \n  Present metrics, discuss trade-offs (e.g., ROC-AUC, fairness, overfitting)  \n\n- Data Quality & Validation:  \n  Ensure schema consistency, missing data checks, rule-based validations  \n\n- Experiment Review:  \n  Discuss A/B test results, statistical significance, and business impact  \n\n### Cross-functional / Stakeholder Meetings  \n\n- Product or Domain Team Syncs:  \n  Discuss project goals, KPIs, data availability  \n\n- Ad Hoc Analysis Requests:  \n  Clarify requirements for exploratory or one-off reporting  \n\n- User Feedback / Data Product Reviews:  \n  Gather user input on dashboards, ML outputs, or data access tools  \n\n- Requirements Grooming:  \n  Translate business requirements into data/technical specifications  \n\n### Reporting & Strategic Meetings  \n\n- Quarterly Business Reviews / OKR Alignment:  \n  Evaluate progress against metrics and strategic initiatives  \n\n- Dashboard Walkthroughs / Reporting Demos:  \n  Demonstrate key metrics, performance trends, and data tooling  \n\n- Metrics Definition Meetings:  \n  Align teams on KPI definitions, calculation logic, and data sources  \n\n### Collaborative Initiatives  \n\n- Knowledge Sharing / Lunch & Learns:  \n  Internal sessions to demo tools, share best practices, or teach concepts  \n\n- Workshops:\n  Hands-on learning sessions for technical upskilling  \n\n- Data Governance / Compliance:  \n  Ensure responsible data use, privacy adherence, and lineage tracking  \n\n- Hackathons / Innovation Days:  \n  Time-boxed events for experimentation and cross-team collaboration",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "communication",
      "documentation"
    ],
    "normalized_filename": "documentation_&_meetings",
    "outlinks": [
      "experiment_plan_template",
      "technical_design_doc_template",
      "pull_request_template",
      "1-on-1_template",
      "meeting_notes_template",
      "data_request_template",
      "postmortem_template",
      "feedback_template",
      "one_pager_template",
      "eda",
      "mermaid",
      "pdoc",
      "retrospective_template"
    ],
    "inlinks": [
      "code_diagrams",
      "data_analyst",
      "data_engineer",
      "data_principles",
      "data_warehouse",
      "dbt",
      "devops_portal",
      "fishbone_diagram",
      "pyright"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Energy ABM",
    "sha": "330e7aa44299c30a42fb134997aa5ff2b2bbd74a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Energy%20ABM.md",
    "text": "# Energy ABM\n\n- **Complex Systems Understanding**: Energy systems involve numerous stakeholders (producers, consumers, regulators) with diverse interests and behaviors. [[Agent-Based Modelling|ABM]] helps capture this complexity, providing a clearer picture of system dynamics.\n- **Adaptive Behavior**: Agents in ABM can adapt their behavior based on interactions, mirroring how consumers and producers might respond to incentives or changes in the market.\n- **Scenario Analysis**: ABM allows for \"what-if\" analyses, enabling stakeholders to explore different scenarios, such as the impact of implementing new technologies or policies on energy systems.\n- **Data-Driven Insights**: With the rise of smart meters and IoT devices, ABM can leverage real-time data to improve model accuracy and relevancy, enhancing decision-making processes.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "energy_abm",
    "outlinks": [
      "agent-based_modelling"
    ],
    "inlinks": [
      "energy"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Energy Demand Forecasting",
    "sha": "15e8c3da8c6e4feb8755ea7f5c3302c9567c4bd9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Energy%20Demand%20Forecasting.md",
    "text": "- **Overview**: Demand response programs encourage consumers to adjust their energy usage during peak periods in response to time-based rates or other incentives. RL can optimize how these programs are implemented.\n\n- **Applications**:\n    - **Incentive Management**: [[Reinforcement learning|RL]] models can dynamically adjust incentives for consumers to reduce usage during peak times based on real-time grid conditions and consumer behavior.\n    - **Behavioral Adaptation**: By learning from historical consumer response data, RL systems can predict how different consumers will react to incentives, allowing for more tailored and effective demand response strategies.\n\nHow can we model the effects of energy consumption patterns on demand forecasting\n\n- **Dynamic Programming**: Useful in solving multi-stage decision problems, such as optimal scheduling of power plants.\n- **Linear Programming**: Used for optimizing resource allocation in energy production and distribution, such as maximizing output while minimizing costs.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "energy"
    ],
    "normalized_filename": "energy_demand_forecasting",
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "energy",
      "growth_models_in_time_series",
      "out-of-sample_rolling_forecast_evaluation",
      "use_of_rnns_in_energy_sector"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Energy Storage",
    "sha": "dc89885e4836e3b50d2f4bd96eb4f42ec3fc1684",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Energy%20Storage.md",
    "text": "## Energy Storage\n\nBattery farms exist.\n\nStored energy can be traded.\n\nStored energy can be stored using distributed system such as EV cars.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "energy"
    ],
    "normalized_filename": "energy_storage",
    "outlinks": [],
    "inlinks": [
      "energy"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Energy",
    "sha": "7f7419e0a56e059d8f30fe632a62c0bee064bc20",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Energy.md",
    "text": "Areas of interest:\n- [[Smart Grids]]\n- [[Energy Storage]]\n- [[Energy Demand Forecasting]]\n- [[Network Design]]\n- [[Energy ABM]]\n\nQuestions:\n- [[How to model to improve demand forecasting]]\n- What patterns can be identified in consumer behavior data to inform energy pricing strategies?\n- How can predictive maintenance be implemented using data from smart sensors in energy infrastructure?\n\n**Techniques:**\n- **[[Differential Equations]]**: Used to model dynamic systems in energy generation and consumption. For example, they can describe the behavior of power systems over time or the thermal dynamics of energy storage systems.\n- **[[Stochastic Modeling]]**: Involves random variables to model uncertainties in energy production (e.g., variability in solar or wind energy) and consumption.\n- [[Agent-Based Modelling]]Simulates interactions of agents (consumers, producers, regulators) to understand complex systems and emergent phenomena in energy markets.\n- **Time Series Analysis**: Analyzing historical data to forecast future energy demand or production trends.\n- **[[Regression]] Analysis**: Used to model relationships between different variables, such as energy prices and consumption patterns.\n- [[Neural network|Neural Network]] Particularly deep learning, is applied for complex pattern recognition in large datasets, such as detecting anomalies in energy consumption or predicting equipment failures.\n\n\nDymanic pricing, incentised load management, local generation \n  \nUse green energy if on grid",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "energy"
    ],
    "normalized_filename": "energy",
    "outlinks": [
      "differential_equations",
      "regression",
      "energy_demand_forecasting",
      "energy_abm",
      "network_design",
      "stochastic_modeling",
      "how_to_model_to_improve_demand_forecasting",
      "energy_storage",
      "agent-based_modelling",
      "neural_network",
      "smart_grids"
    ],
    "inlinks": [
      "digital_twin",
      "industries_of_interest"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Facts",
    "sha": "a9bb771fdbd1105d3f63e7ae81e79e12835e983b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Facts.md",
    "text": "Facts are quantitative data points that are typically stored in the [[Fact Table]].\n\nThey represent measurable events or metrics, such as sales revenue or quantities sold.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "facts",
    "outlinks": [
      "fact_table"
    ],
    "inlinks": [
      "dimension_table",
      "dimensional_modelling",
      "dimensions",
      "fact_table",
      "granularity"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Gartner Hype Cycle",
    "sha": "cf8002d480e93e675b13fae527ffb5e93203c322",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Gartner%20Hype%20Cycle.md",
    "text": "The Gartner Hype Cycle is a [[Conceptual Model]] represent the typical progression of new technologies and innovations through five key phases of public attention and adoption.\n\nThe phases are:\n\nInnovation Trigger  \n   - A technology breakthrough or concept emerges.\n   - Early proof-of-concept stories and media interest trigger significant attention.\n   - Commercial viability is still unproven.\n\nPeak of Inflated Expectations  \n   - Early publicity produces success stories, often accompanied by scores of failures.\n   - Some companies take action; many do not.\n\nTrough of Disillusionment  \n   - Interest wanes as experiments and implementations fail to deliver.\n   - Producers of the technology either improve it or fail.\n\nSlope of Enlightenment  \n   - Some businesses persist and begin to understand the benefits and practical applications.\n   - More instances of how the technology can solve problems emerge.\n\nPlateau of Productivity  \n   - Mainstream adoption begins to take off.\n   - The technology’s broad market applicability and relevance are becoming clearer and more stable.\n\n![[Pasted image 20250427072645.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "learning"
    ],
    "normalized_filename": "gartner_hype_cycle",
    "outlinks": [
      "pasted_image_20250427072645.png",
      "conceptual_model"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Industries of interest",
    "sha": "e827926b5910531eb031cdd16e8910cb11adbcda",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Industries%20of%20interest.md",
    "text": "Industries to investigate related to my background & interests:\n- [[Energy]]\n- [[Telecommunications]]\n- [[Education and Training]]\n\nBoth Reinforcement Learning and Explainable AI offer exciting opportunities for mathematicians to contribute significantly. Your deep mathematical understanding allows you to tackle complex problems, develop new methodologies, and provide theoretical foundations for emerging techniques.\n\nExploratory Questions\n- [[What algorithms or models are used within the energy sector]]\n- [[What algorithms or models are used within the telecommunication sector]]\n\n### [[Reinforcement learning]]\n\n- **Stochastic Processes**: Your background will allow you to delve into the mathematical properties of [[Markov Decision Processes]] MDPs, optimizing transition dynamics, and improving algorithms based on theoretical insights.\n- **Theoretical Analysis**: You can contribute to the development of new algorithms by providing theoretical proofs of convergence and performance guarantees, applying concepts from real analysis and optimization.\n- **Complexity Analysis**: Understanding the computational complexity of various RL algorithms and contributing to the design of more efficient algorithms will leverage your mathematical skills.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "career"
    ],
    "normalized_filename": "industries_of_interest",
    "outlinks": [
      "markov_decision_processes",
      "reinforcement_learning",
      "what_algorithms_or_models_are_used_within_the_telecommunication_sector",
      "education_and_training",
      "energy",
      "what_algorithms_or_models_are_used_within_the_energy_sector",
      "telecommunications"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Knowledge Work",
    "sha": "50288f9f4feff1c4b797b70d49aa2374cb986c7e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Knowledge%20Work.md",
    "text": "Knowledge work refers to tasks that primarily involve handling or using information and require cognitive skills rather than manual labor. It is characterized by problem-solving, critical thinking, and the application of specialized knowledge.\n## Key Characteristics\n\n- Problem Solving: Knowledge work often involves identifying, analyzing, and solving complex problems. This requires creativity, analytical skills, and the ability to synthesize information from various sources.\n\n- Use of the [[Scientific Method]]: Many knowledge work tasks, especially in research and development, rely on the scientific method. This involves forming hypotheses, conducting experiments, analyzing data, and drawing conclusions.\n\n- Information Management: Knowledge workers must efficiently gather, process, and apply information to make informed decisions.\n\n## Examples of Knowledge Work\n\n- Research and development\n- Software development\n- Data analysis\n- Strategic planning\n- Writing and content creation",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "career"
    ],
    "normalized_filename": "knowledge_work",
    "outlinks": [
      "scientific_method"
    ],
    "inlinks": [
      "thinking_systems"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "ML Engineer",
    "sha": "2cf037343d660987b426aa21b1b0f26cb1e4d306",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/ML%20Engineer.md",
    "text": "ML Engineer\n  - Configures and optimizes production ML models.\n  - Monitors the performance and accuracy of ML models in production environments.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "career"
    ],
    "normalized_filename": "ml_engineer",
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Managing People",
    "sha": "993a98215338020079ac2e223f235f1f27db1426",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Managing%20People.md",
    "text": "Related:\n- [[Coaching & Mentoring]]\n\nHiring:\n- What is the business justification of this hire, and the impact they need to make.\n\nManaging Teams\n- Do the members feel ownership/recognitions\n- Break tasks down, and get easy wins\n- Remember the need for processes and rightsize the based on the organisation size.\n- Data science initiative need to tied to [[Business Values]]\n- Create visibility to outputs\n\nOnboarding system:\n- Business overview\n\t- what company does, and makes money, whats the mission\n\t- team level and how the contribute\n- People overview:\n\t- Inter personal dynamics\n\t- Peer groupsWho are the people above and reports\n- Technical overview:\n\t- data systems and aritechture\n\t- code repos\n\t- projudt docs and knowledge repos\n\t- data flows\n\t- propiritisation systems\n\t\nBurnout:\n- promote timeoff, breaks\n- set realistic goals and workloads\n- Cnsider engagement\n- culture of help and collab\n- automate mundane tasks to get to meaningful work\n- Vary the work, to reduce boredum.\n\nBlueSky thinking\n- creative thinking, and an enviroment for it.\n- Reqard risk taking, processes for submitting ideas: setting aside time.\n- ideas should move to prototyping, is there a process for this: [[Process for prototyping]]: problems statmenet and assessment\n- Ensure prototype are linked to buiesness problems\n- Remember to [[Showcase Value]] you are demonstrating\n\nRelated:\n- [[Managing Data Science Teams]]\n\n### Other ideas\n\nLeadership vs. Management\nLeadership is about vision, motivation, and direction; management is about planning, organizing, and controlling to achieve objectives.\n\n**Delegation**  \nAssigning responsibility and authority to others, while retaining accountability.\n\n**Feedback Models**\n- _SBI_ (Situation – Behavior – Impact).\n\n**OKRs (Objectives and Key Results)**  \nA goal-setting framework linking big objectives with measurable outcomes.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "management"
    ],
    "normalized_filename": "managing_people",
    "outlinks": [
      "process_for_prototyping",
      "coaching_&_mentoring",
      "business_values",
      "managing_data_science_teams",
      "showcase_value"
    ],
    "inlinks": [
      "project_management_portal"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Network Design",
    "sha": "57be50d9e31de7d61671ff0b4cf1d59b785461ba",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Network%20Design.md",
    "text": "Mixed-Integer Programming: Handles problems where some variables must be integers, commonly used in optimizing network design and capacity planning.\n\nHow to systems interact.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "energy"
    ],
    "normalized_filename": "network_design",
    "outlinks": [],
    "inlinks": [
      "energy"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Operational Resilience for Growth and Adaptability",
    "sha": "b047ef0ca7809af9203298eb4caf170676e12d7e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Operational%20Resilience%20for%20Growth%20and%20Adaptability.md",
    "text": "Core Principles:\n - Operational Resilience involves preparation, foundational planning, and adaptability to **future disruptions** (economic, regulatory, climate, supply chain, energy).\n - It’s both evidence-based and scenario-driven, using “severe but plausible” models for testing readiness.\n\nWhy It Matters:\n - Disruptions are inevitable; failing to prepare is a strategic threat.\n - Resilience creates competitive advantage—others may seize emerging opportunities while unprepared businesses falter.\n\nConceptual Shifts:\n - Move from point solutions to integrated, whole-business planning.\n - Resilience is about response during crises, not just post-event recovery.\n - It's **about how the business fits together under stress.**\n\nKey Practices:\n - Continuity & Recovery Planning embedded in daily operations.\n - Use resilience maturity assessments (Levels 1–5) to identify gaps and drive improvement.\n - Adopt Assess – Plan – Act as an operational loop.\n\nFoundations for Success:\n - Human & Cultural Foundations: **Encourage open communication, reward proactive behaviours,** build psychological safety.\n - Leadership must set expectations that resilience is everyone’s responsibility—like innovation.\n - **Trust-building is essential**; people must understand and value the role they play.\n\nEnabling Mechanisms:\n - Cybersecurity ([[Data Security]]) as a leading case of resilience in practice.\n - [[Data Analysis]] provide insight into business operations and emerging risks.\n - Resilience supports value retention and efficiency gains through better preparedness.\n\nStrategic View:\n - Operational resilience is not a one-off fix but a continuous, coherent capability.\n - It provides a real-time lens on how the business is functioning today and where it can adapt.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "operational_resilience_for_growth_and_adaptability",
    "outlinks": [
      "data_security",
      "data_analysis"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Reporting",
    "sha": "7f8c975b480e99456d7d65349a29026df0c7d301",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Reporting.md",
    "text": "Aim of reports:\n- Identify operational deviations\n- Balancing multiple parameters\n\nNeed to be:\n- User friendly",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "reporting",
    "outlinks": [],
    "inlinks": [
      "github_actions"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Smart Grids",
    "sha": "39643299a1d03eda50cc3af49be372dba7145fb0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Smart%20Grids.md",
    "text": "## Smart Grids\n\nWant adaptive grid that can handle the volatility of energy coming on or off. This occurs more often due to the variety of sources i.e wind.\n\nHelp with carbon commitment  \n\n- **Overview**: Smart grids utilize advanced technology and data analytics to improve the efficiency and reliability of electricity distribution. [[RL]] can optimize the operation and management of these grids.\n- **Applications**:\n    - **Demand Forecasting**: RL algorithms predict electricity demand based on historical data and real-time inputs. They adjust energy production and distribution to match forecasted demand.\n    - **Load Balancing**: RL can manage the distribution of electricity by dynamically balancing load across different sources, minimizing energy loss and enhancing stability.\n    - **Renewable Energy Integration**: RL helps in integrating renewable energy sources (e.g., solar, wind) into the grid by optimizing the usage of these variable resources and managing their unpredictability.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "energy"
    ],
    "normalized_filename": "smart_grids",
    "outlinks": [
      "rl"
    ],
    "inlinks": [
      "energy"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Scaling Data Science Capability",
    "sha": "f077c32a38478e203dad8026e2ae1c54907799c0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Scaling%20Data%20Science%20Capability.md",
    "text": "Phases of Scaling:\n1. Lab Setting – Experimental, low-risk environment for vision development.\n2. Joint Operational Model – Integration with existing delivery systems.\n3. Replication – Extend proven models to new areas.\n4. Enterprise Scaling – Strategic expansion aligned with regulatory and organisational value.\n\nKey Tensions:\n Innovation vs. Delivery – Balance experimentation with operational accountability.\n Value Delivery – Ensure tangible support for ongoing operations during scaling.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "scaling_data_science_capability",
    "outlinks": [],
    "inlinks": [
      "data_science"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Telecommunications",
    "sha": "60d2cef3844f0d3b35b21524d802cd8cd68a395e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Telecommunications.md",
    "text": "Network Optimization\n\n- **Overview**: In telecommunications, RL is used to enhance network performance, optimize resource allocation, and manage traffic efficiently.\n- **Applications**:\n    - **Traffic Management**: RL algorithms can analyze real-time network traffic to optimize routing and minimize congestion, ensuring that data packets are transmitted through the least congested paths.\n    - **Quality of Service (QoS)**: RL can be used to allocate bandwidth dynamically based on current demand and service-level agreements (SLAs), improving user experience by maintaining high QoS standards.\n    - **Fault Detection and Recovery**: RL systems can learn to identify and respond to network anomalies or failures, automatically rerouting traffic or reallocating resources to maintain service continuity.\n\n###### 8.2 Dynamic Resource Allocation\n\n- **Overview**: Dynamic resource allocation in telecommunications involves adjusting network resources (like bandwidth and processing power) in real-time based on user demand and network conditions.\n- **Applications**:\n    - **Load Balancing**: RL can help in distributing network loads across multiple servers or paths, ensuring optimal use of available resources while preventing any single point from becoming overloaded.\n    - **Adaptive Scheduling**: RL algorithms can manage the scheduling of data transmission and resource allocation in cellular networks, allowing for efficient handling of varying traffic patterns and user behaviors.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "career"
    ],
    "normalized_filename": "telecommunications",
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Thinking Systems",
    "sha": "0bd60f62678ba6247f47c4b0769d7d7d1898524f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Thinking%20Systems.md",
    "text": "A thinking system is a point of view that helps solve a problem. Part of [[Knowledge Work]]. We view problems through the view of our own specialism (mathematics). Thinking systems help with perspective.\n\nTypes of thinking systems:\n- Design\n- Engineering\n### Design thinking\n\nHistorically we trained people to use a product.  \n- users have choices, \n- be user focused, \n- empathy and contextual enquiry for the product/system  \n\nThis puts human at the center, and focus on thinking about ==pain points== of their experience.  \n\nRemember:\n- User segments are not the same and should be handled separately.  \n- It is important to understand how the user interacts with the environment/other beings  \n- ==Ludic properties== reduce design load, for example [[ChatGPT]] and chat feature, or use of the iphone.\n\n### Scientific thinking:  \n\nStrong ideas but loosely held.\n \nExamples:\n- AlphaFold and protein folding. They had lots of data and where able to derive insights from that using AI.  \n\nAs problems are too complex for human minds [[Scientific Method]] now:  \n- We have data,  \n- Pick an algo ,  \n- Compute then gives hypothesis.  \n### System Thinking\n\nWe need to design for the whole system not just the individual.  \n\nI am stuck in traffic, versus you are the traffic.  \n\nEmergent behaviour with lime moulds, tokyo metro network.  \n\nCollective intelligence. \n\nDesigning system so that individuals impact the whole.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "career"
    ],
    "normalized_filename": "thinking_systems",
    "outlinks": [
      "chatgpt",
      "knowledge_work",
      "scientific_method"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "Use of RNNs in energy sector",
    "sha": "8221c7f9d785842fe5ecd1be6c7dd30a1a599951",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Use%20of%20RNNs%20in%20energy%20sector.md",
    "text": "For energy data problems, many interpretable machine learning algorithms can be applied in place of or alongside RNNs. These models offer transparency, making it easier to understand the relationships between features and predictions, which is critical in areas like energy management, where interpretability can be as important as accuracy.\n\nFor each of the energy data questions that RNNs might solve, interpretable alternatives [[Machine Learning Algorithms]]: such as linear regression, decision trees, random forests, and ARIMA models can be employed. These models provide transparency by revealing which features (e.g., weather, demand) influence predictions the most, making them suitable for stakeholders who need clear explanations of the decisions made by the model.\n\n### [[Energy Demand Forecasting]]\n   - Algorithms:\n     - Linear Regression: Can model simple linear relationships between energy consumption and time (e.g., daily/seasonal trends).\n     - Decision Trees: Provides clear if-then rules for predicting future energy usage based on historical consumption, time of day, and other factors.\n     - Random Forests: An ensemble of decision trees that provides better accuracy than individual trees while still being interpretable using feature importance.\n     - [[Gradient Boosting]] (GBM): Can be used with feature importance or [[SHapley Additive exPlanations|SHAP]] values to understand which factors (e.g., time, weather) drive energy demand.\n   \n   - Why: These models allow for clear interpretation of how factors like temperature, time of day, and previous energy use contribute to predictions.\n\n### 2. Renewable Energy Generation Prediction\n   - Algorithms:\n     - Linear Regression: For simple relationships, like the effect of sunlight hours or wind speed on energy generation.\n     - Support Vector Machines (SVM): Can create interpretable linear boundaries when predicting renewable energy outputs, with clear separation of factors (e.g., wind speed thresholds).\n     - Random Forests: Offers feature importance metrics that explain which weather factors are most important for predicting energy generation.\n     - GBM: Using [[SHapley Additive exPlanations|SHAP]] values or feature importance to interpret the impact of weather variables on the energy output.\n\n   - Why: These algorithms can provide insights into the key weather conditions driving renewable energy generation and give transparent predictions for decision-making.\n### 3. Energy Price Forecasting\n   - Algorithms:\n     - ARIMA (AutoRegressive Integrated Moving Average): A traditional [[Time Series Forecasting]] method that models linear relationships in energy prices over time.\n     - Linear Regression: Can model the impact of factors like demand, supply, and historical prices in an interpretable way.\n     - Decision Trees: Easy to interpret and can show thresholds where prices change based on inputs like demand or fuel costs.\n     - XGBoost: Provides interpretability through SHAP values or feature importance, explaining which market factors (e.g., demand, fuel prices) drive price changes.\n\n   - Why: These algorithms offer interpretable insights into what drives price fluctuations, making them useful for energy market analysis and trading.\n\n### 4. Anomaly Detection in Energy Consumption\n   - Algorithms:\n     - Isolation Forests: Specifically designed for anomaly detection and provides interpretable results by isolating outliers.\n     - k-Nearest Neighbors (k-NN): Can flag anomalies by comparing new consumption data to known normal consumption patterns, with simple explanations of \"closeness\" to typical patterns.\n     - Logistic Regression: Can be used to classify energy consumption data into \"normal\" and \"anomalous\" categories based on clear feature contributions.\n     - One-Class SVM: A linear model that can classify whether energy usage deviates from typical patterns.\n\n   - Why: These interpretable algorithms can identify unusual patterns in energy data, providing clear reasons (e.g., thresholds exceeded) for flagging certain periods as anomalous.\n\n### 5. Load Balancing and Optimization\n   - Algorithms:\n     - Linear Programming (Optimization): Provides interpretable rules for how energy should be distributed across the grid to minimize costs and prevent overloads.\n     - Decision Trees: Can clearly show the impact of different factors (e.g., region, time of day) on grid load, and thresholds for balancing loads.\n     - Rule-Based Systems: Set explicit rules for load balancing based on historical data and real-time demand, offering full transparency.\n   \n   - Why: These interpretable models can assist grid operators in understanding which regions or time periods contribute most to load imbalances and suggest corrective actions.\n### 6. Customer Energy Usage Profiling\n   - Algorithms:\n     - k-Means Clustering: Can group customers into distinct profiles based on energy usage patterns, with each cluster representing a clear profile (e.g., high-energy consumers, off-peak users).\n     - Decision Trees: Can predict customer profiles based on historical usage data and explain which features (e.g., time of usage, appliance usage) define each profile.\n     - Logistic Regression: Can be used to classify customers into different segments based on usage characteristics, providing clear coefficient-based interpretations.\n\n   - Why: These models provide transparency into what factors drive a customer’s energy usage profile, which is essential for creating personalized recommendations.\n\n### 7. Demand Response Optimization\n   - Algorithms:\n     - Linear Programming (Optimization): Provides interpretable solutions for when and where to implement demand response programs to minimize peak energy use.\n     - Decision Trees: Can clearly define rules for when demand response should be triggered based on time of day, weather, and current load.\n     - k-Nearest Neighbors (k-NN): Can identify similar past scenarios where demand response was implemented successfully and explain why the current situation matches.\n\n   - Why: These methods give clear, interpretable guidelines for when and how to reduce energy demand during peak times, based on past patterns.\n\n### 8. Fault Detection in Power Systems\n   - Algorithms:\n     - Decision Trees: Can explain why certain operational conditions (e.g., voltage drops, temperature increases) are likely to lead to faults, with clear rules and thresholds.\n     - Random Forests: Provides feature importance scores that highlight which factors (e.g., temperature, load) are most indicative of impending faults.\n     - Logistic Regression: Offers simple, interpretable probabilities for whether a fault will occur, based on key factors like current and voltage.\n\n   - Why: Fault detection requires clear, interpretable models that help engineers understand the most important factors leading to equipment failures.\n\n### 9. Energy Usage Forecasting for Smart Buildings\n   - Algorithms:\n     - Multiple Linear Regression: Can model the relationship between building factors (e.g., temperature, occupancy) and energy usage, offering clear coefficients.\n     - Decision Trees: Provides an interpretable way to understand which building features (e.g., time of day, external temperature) influence energy consumption the most.\n     - k-Means Clustering: Can group similar time periods or usage patterns to explain different operational modes of the building.\n   \n   - Why: These algorithms provide interpretable insights into how building features and external factors impact energy consumption, allowing for more efficient energy management.\n\n### 10. Time Series Forecasting for Energy Production in Microgrids\n   - Algorithms:\n     - ARIMA: Traditional interpretable time series model that predicts future production based on past production data.\n     - Linear Regression: Can predict energy production based on simple factors like weather data, fuel availability, and historical output.\n     - Decision Trees: Helps identify which weather or resource factors are most critical for predicting energy production at a given time.\n\n   - Why: Time series models like ARIMA are highly interpretable and useful for understanding how different factors contribute to energy production in microgrids.\n\n### 11. Battery Storage Optimization\n   - Algorithms:\n     - Linear Programming (Optimization): Provides a clear, interpretable approach to optimizing charge/discharge schedules based on forecasted energy generation and consumption.\n     - Decision Trees: Can explain when and why batteries should be charged or discharged based on energy production, consumption, and cost factors.\n     - Rule-Based Systems: Establish clear rules for battery storage optimization, offering fully interpretable decision-making processes.\n\n   - Why: Optimizing battery storage requires clear, rule-based or linear models to understand how different variables (e.g., energy prices, consumption) impact storage decisions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "deep_learning",
      "energy",
      "time_series"
    ],
    "normalized_filename": "use_of_rnns_in_energy_sector",
    "outlinks": [
      "time_series_forecasting",
      "energy_demand_forecasting",
      "gradient_boosting",
      "shapley_additive_explanations",
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "recurrent_neural_networks"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "Working with SMEs",
    "sha": "060590b8064fcde5c92009d879ec462f7bcaf6c5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/Working%20with%20SMEs.md",
    "text": "**Purpose:**\n* Understand business processes and workflows.\n* Identify missing or poorly captured data fields.\n* Clarify and strengthen the business case.\n\n**Approach:**\n* Identify key variables and relationships in the data.\n* Leverage SME knowledge to validate assumptions and patterns.\n* Present observed patterns or relationships to confirm accuracy.\n* Investigate anything unusual or inconsistent.\n\n**Preparation:**\n* Perform initial **data understanding** before engaging SMEs.\n* Prepare predictive examples (e.g., top levels of a [[Decision Tree]]) to guide discussion.\n\n**Related Notes:**\n* [[Communication with Stakeholders]]\n* [[Data Understanding]]\n* [[Decision Tree]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "working_with_smes",
    "outlinks": [
      "decision_tree",
      "communication_with_stakeholders",
      "data_understanding"
    ],
    "inlinks": []
  },
  {
    "category": "INDUSTRY",
    "filename": "business intelligence",
    "sha": "995c11efa8ddbb6eb248fdb630adc3eb78f5e911",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/business%20intelligence.md",
    "text": "Business intelligence (BI) leverages software and services to [transform data](Data%20Transformation.md) into actionable insights that inform an organization’s business decisions. \n\nThe new term is [Data Engineer](Data%20Engineer.md). The language of a BI engineer is [SQL](SQL.md).\n\n## Goals of BI\nBI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail:\n\n\n  * **[[rollup]] capability** - (data) [Visualization](term/analytics.md) over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance.\n\n  * **Drill-down possibilities** - from the above high-level overview drill down the very details to figure out why something is not performing as planned. **Slice-and-dice or pivot your data from different angles.\n\n  * **[[Single Source of Truth]]** - instead of multiple spreadsheets or other tools with different numbers, the process is automated and done for all unified. Employees can talk about the business problem instead of the various numbers everyone has. Reporting, budgeting, and forecasting are automatically updated and consistent, accurate, and in timely manner.\n\n  * **Empower users**: With the so-called self-service BI, every user can analyze their data instead of only BI or IT persons.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "business_intelligence",
    "outlinks": [
      "single_source_of_truth",
      "rollup"
    ],
    "inlinks": [
      "data_ingestion",
      "data_scientist",
      "granularity",
      "olap",
      "snowflake"
    ]
  },
  {
    "category": "INDUSTRY",
    "filename": "data literacy",
    "sha": "e637125dd895d10135e91c420f250aa977d3caa3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/industry/data%20literacy.md",
    "text": "Part of [[Change Management]]\n\nData literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This skill set is crucial for employees across various levels of an organization, especially as data-driven decision-making becomes increasingly important.\n\nOrganizations should invest in data literacy training programs to empower their employees with the necessary skills to effectively engage with data. A data-literate employee can read charts, draw correct conclusions, recognize when data is being used inappropriately or misleadingly, and gain a deeper understanding of the business domain. This enables them to communicate more effectively using a common language of data, spot unexpected operational issues, identify root causes, and prevent poor decision-making due to data misinterpretation.\n\nExamples of data literacy in action include:\n* Implementing the Adoptive Framework to create a Data Literacy Program.\n* Employees working with spreadsheets to understand the rationale behind data-driven decisions and advocating for alternative courses of action.\n* Work teams identifying areas where data needs clarification for a project.\n\nBy nurturing a data-literate workforce, businesses can improve their ability to make informed decisions, drive innovation, and achieve better outcomes.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#business"
    ],
    "normalized_filename": "data_literacy",
    "outlinks": [
      "change_management"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "AIC in Model Evaluation",
    "sha": "43114d6983cd70156c7bef595785d404622ae102",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/AIC%20in%20Model%20Evaluation.md",
    "text": "AIC stands for Akaike Information Criterion. It is a metric used to compare the goodness-of-fit of statistical models, taking into account both accuracy and complexity. For SARIMA (or any [[ARIMA]]-type model), the formula is:\n\n$$\n\\text{AIC} = 2k - 2\\ln(L)\n$$\n\nWhere:\n\n* $k$ = number of estimated parameters in the model (including AR, MA, seasonal AR/MA, and variance terms).\n* $L$ = maximum likelihood of the model (how probable the observed data is given the model).\n\nKey points:\n\nLower AIC is better. It balances model fit and complexity.\n   * A model with too many parameters may fit the training data well but overfit; AIC penalizes extra parameters.\n\nRelative metric: AIC itself has no absolute meaning; it is only useful for comparing models on the same dataset.\n\nUse in [[SARIMA]]: When tuning $(p,d,q)(P,D,Q)_s$, we often compute AIC for each combination. The model with the lowest AIC is considered the most efficient balance of fit and ==parsimony==.\n\n#### Related\n\n[[Evaluation Metrics]]\n\n### BIC\n\n```\n# **Bayesian Information Criterion (BIC)**\n\n#\n\n# - Similar to AIC, the BIC is another criterion for model selection, but it introduces a stronger penalty for models with more parameters.\n\n# - $BIC = \\ln(n)k - 2\\ln(\\hat{L})$, where $n$ is the number of observations, $k$ is the number of parameters, and $\\hat{L}$ is the maximized likelihood.\n\n# - A lower BIC value indicates a better model, preferring simpler models to complex ones, especially as the sample size $n$ increases.\n```",
    "aliases": [
      "AIC"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "aic_in_model_evaluation",
    "outlinks": [
      "arima",
      "sarima",
      "evaluation_metrics"
    ],
    "inlinks": [
      "arima",
      "sarima"
    ]
  },
  {
    "category": "ML",
    "filename": "AUC",
    "sha": "5b35b2e8bddf728c4272611cf7685e4b8c073719",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/AUC.md",
    "text": "**AUC (Area Under the Curve)** is a metric for binary classification problems, representing the area under the [[ROC (Receiver Operating Characteristic)]]\n\n#### Key Concepts\n- Represents the area under the ROC curve.\n- AUC values range from 0 to 1, where 1 indicates perfect classification and 0.5 suggests no discriminative power (equivalent to random guessing).\n\n#### Roc and Auc Score\n\nThe `roc_auc_score` is a function from the `sklearn.metrics` module in Python that computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. It is a widely used metric for evaluating the performance of binary classification models.\n\nKey Points about `roc_auc_score`:\n\n- **Purpose**: It quantifies the overall ability of the model to discriminate between the positive and negative classes across all possible classification thresholds.\n- **Range**: The score ranges from 0 to 1, where:\n    - 1 indicates perfect discrimination (the model perfectly distinguishes between the positive and negative classes).\n    - 0.5 suggests no discriminative power (equivalent to random guessing).\n    - Values below 0.5 indicate a model that performs worse than random guessing.\n- **Input**: The function takes the true binary labels and the predicted probabilities (or decision function scores) as inputs.\n- **Output**: It returns a single scalar value representing the AUC.\n\n#### Example Code\n\n```python\nfrom sklearn.metrics import roc_auc_score\n\n# Actual and predicted values\ny_act = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]\n\n# Compute AUC\nauc = roc_auc_score(y_act, y_pred)\nprint(f'AUC: {auc}')\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "auc",
    "outlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "inlinks": [
      "feature_evaluation",
      "precision-recall_curve"
    ]
  },
  {
    "category": "ML",
    "filename": "Accuracy",
    "sha": "9418a5dae9b506239a359d48df1699766f556008",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Accuracy.md",
    "text": "## Definition\n\n- Accuracy Score is the proportion of correct predictions out of all predictions made. In other words, it is the percentage of correct predictions.\n- Accuracy can have issues with [[Imbalanced Datasets]]where there is more of one class than another.\n\n## Formula\n\n- The formula for accuracy is:\n  $$\\text{Accuracy} = \\frac{TN + TP}{\\text{Total}}$$\nIn the context of [[Classification]] problems, particularly binary classification, TN and TP are components of the confusion matrix:\n\n- TP (True Positive): The number of instances that are correctly predicted as the positive class. For example, if the model predicts a positive outcome and it is indeed positive, it counts as a true positive.\n- TN (True Negative): The number of instances that are correctly predicted as the negative class. For example, if the model predicts a negative outcome and it is indeed negative, it counts as a true negative.\n\nThe [[Confusion Matrix]] also includes:\n\n- FP (False Positive): The number of instances that are incorrectly predicted as the positive class. This is also known as a \"Type I error.\"\n- FN (False Negative): The number of instances that are incorrectly predicted as the negative class. This is also known as a \"Type II error.\"\n\nThese metrics are used to evaluate the performance of a classification model, providing insights into not just accuracy but also precision, recall, and other performance measures.\n## Exploring Accuracy in Python\n\nTo explore accuracy in Python, you can use libraries such as `scikit-learn`, which provides the `accuracy_score` function. This function compares the predicted labels with the true labels and calculates the accuracy.\n\n### Example Usage\n\n```python\nfrom sklearn.metrics import accuracy_score\n# Assuming pred and y_test are defined\naccuracy = accuracy_score(y_test, pred)\nprint(\"Prediction accuracy: {:.2f}%\".format(accuracy  100.0))\n```\n\n- Make sure to replace `pred` and `y_test` with your actual prediction and test data variables.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "accuracy",
    "outlinks": [
      "imbalanced_datasets",
      "confusion_matrix",
      "classification"
    ],
    "inlinks": [
      "class_separability",
      "confusion_matrix",
      "ds_&_ml_portal",
      "evaluation_metrics",
      "handling_different_distributions",
      "imbalanced_datasets_smote.py",
      "learning_curve",
      "model_observability",
      "precision",
      "test_loss_when_evaluating_models"
    ]
  },
  {
    "category": "ML",
    "filename": "Activation Function",
    "sha": "f9b98ce404d753d592e78b489fdc00ee652ca25a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Activation%20Function.md",
    "text": "Activation functions play a role in [[Neural network]] by introducing non-linearity, allowing models to learn from complex patterns and relationships in the data.\n\n[[How do we choose the right Activation Function]]\n### Key Uses of Activation Functions:\n\n1. Non-linearity: Without activation functions, neural networks would behave as linear models, unable to capture complex, non-linear patterns in the data\n2. [[Data Transformation]]: Activation functions modify input signals from one layer to another, helping the model focus on important information while ignoring irrelevant data,\n3. [[Backpropagation]]: They enable gradient-based optimization by making the network differentiable, essential for efficient learning.\n\n### Purpose of Typical Activation Functions\n\nLinear: Outputs a continuous value, suitable for regression.\n\n[[Relu]] (Rectified Linear Unit): \n  - Purpose: ReLU is used to introduce non-linearity by turning neurons \"on\" or \"off.\" It outputs the input directly if it is positive; otherwise, it outputs zero. This helps in efficiently training deep networks by mitigating the vanishing gradient problem.\n  - Function:$f(x) = \\max(0, x)$\n\nSigmoid:\n  - Purpose: Sigmoid is used primarily in [[Binary Classification]] tasks. It squashes input values to a range between 0 and 1, making it suitable for representing probabilities.\n  - Function:$f(x) = \\frac{1}{1 + e^{-x}}$\n\n Tanh:\n  - Purpose: Tanh is similar to the sigmoid function but outputs values in the range of -1 to 1. This zero-centered output can be beneficial for optimization in certain scenarios.\n  - Function:$f(x) = \\tanh(x)$\n\nSoftmax:\n  - Purpose: Softmax is used in multi-class classification tasks. It converts a vector of raw scores (logits) into a probability distribution, where each value is between 0 and 1, and the sum of all values is 1. This allows the outputs to be interpreted as probabilities, with larger inputs corresponding to larger output probabilities.\n  - Application: In both softmax regression and neural networks with softmax outputs, a vector$\\mathbf{z}$is generated by a linear function and then passed through the softmax function to produce a probability distribution. This enables the selection of one output as the predicted category.\n\nThe softmax function converts a vector of raw scores (logits) into a probability distribution. The formula for the softmax function for a vector $\\mathbf{z} = [z_1, z_2, \\ldots, z_N]$is given by:\n\n$$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}$$\n\nThis ensures that the output values are between 0 and 1 and that they sum to 1, making them [[Interpretability|interpretable]] as probabilities.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "activation_function",
    "outlinks": [
      "relu",
      "binary_classification",
      "data_transformation",
      "how_do_we_choose_the_right_activation_function",
      "backpropagation",
      "interpretability",
      "neural_network"
    ],
    "inlinks": [
      "forward_propagation",
      "neural_network",
      "relu",
      "typical_output_formats_in_neural_networks"
    ]
  },
  {
    "category": "ML",
    "filename": "Activation atlases",
    "sha": "7a64acea384c220ad0b3a4921dd6b20b364a8f15",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Activation%20atlases.md",
    "text": "Is a viewing method for high dimensional space that AI system use for predictions.\n\nExample AlexNet (cofounder of OpenAI)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "visualization"
    ],
    "normalized_filename": "activation_atlases",
    "outlinks": [],
    "inlinks": [
      "feature_extraction"
    ]
  },
  {
    "category": "ML",
    "filename": "Active Learning",
    "sha": "71d652e393a17af9e87288549f9b89dcafadde8a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Active%20Learning.md",
    "text": "Think captchas for training.\n  \nTo help the [[Supervised Learning]] models when they are less confident.  \n  \nReducing labelling time or need for it.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier"
    ],
    "normalized_filename": "active_learning",
    "outlinks": [
      "supervised_learning"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Adam Optimizer",
    "sha": "da7d0bcb2ed41ff1c42b2d236e16bc58e7adf03c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Adam%20Optimizer.md",
    "text": "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both [[Momentum]] and adaptive learning rates. It is widely used due to its efficiency and effectiveness in training [[Deep Learning]] models.\n\nAdam is particularly effective for large datasets and complex models, as it provides robust convergence and requires minimal tuning compared to other optimization algorithms. Its ability to ==dynamically adjust learning rates== makes it a popular choice in the deep learning community.\n#### Key Features of Adam:\n\n**Adaptive Learning Rates:** Adam adjusts the [[Learning Rate]] for each parameter individually, based on the first and second moments of the gradients. This allows for more precise updates and better convergence.\n\n**Momentum and RMSProp Combination:** Adam incorporates the concept of momentum by using moving averages of the gradients (first moment) and the squared gradients ==(second moment)==, similar to RMSProp.\n\n**Parameter Update Rule:** The update rule involves computing biased estimates of the first and second moments, which are then corrected to provide unbiased estimates. These are used to update the parameters.\n\n**[[Hyperparameter]]:**\n  - **Learning Rate (\\($\\alpha$\\)):** Typically set to 0.001, but can be tuned for specific tasks.\n  - **Beta1 and Beta2:** Control the decay rates for the moving averages of the first and second moments. Common values are 0.9 and 0.999, respectively.\n  - **Epsilon (\\($\\epsilon$\\)):** A small constant added for numerical stability, usually set to \\(1 \\times 10^{-8}\\).\n\n**Implementation Challenges:**\n  - **Parameter Tuning:** Careful tuning of learning rate, beta1, and beta2 is essential for optimal performance.\n  - **Numerical Stability:** Adjusting epsilon can help prevent division by zero and other numerical issues.  \n## Related concepts\n- [[Gradient Descent]]\n- [[Why does the Adam Optimizer converge]]\n-",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "optimisation"
    ],
    "normalized_filename": "adam_optimizer",
    "outlinks": [
      "hyperparameter",
      "gradient_descent",
      "why_does_the_adam_optimizer_converge",
      "momentum",
      "deep_learning",
      "learning_rate"
    ],
    "inlinks": [
      "adaptive_learning_rates",
      "backpropagation",
      "learning_rate",
      "lstm_in_time_series",
      "optimisation_techniques",
      "orthogonalization",
      "use_cases_for_a_simple_neural_network_like",
      "why_does_the_adam_optimizer_converge"
    ]
  },
  {
    "category": "ML",
    "filename": "Adaptive Learning Rates",
    "sha": "d0d1b7ef11309f4d11ccef4cee2a513e3b8c8f8a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Adaptive%20Learning%20Rates.md",
    "text": "Adaptive [[Learning Rate]] adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients. Adam (short for Adaptive Moment Estimation) combines ideas from [[Momentum]] and adaptive learning rates to help the optimization process.\n\nSee:\n-  [[Adam Optimizer]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "learning",
      "modeling",
      "optimisation"
    ],
    "normalized_filename": "adaptive_learning_rates",
    "outlinks": [
      "learning_rate",
      "adam_optimizer",
      "momentum"
    ],
    "inlinks": [
      "optimisation_techniques"
    ]
  },
  {
    "category": "ML",
    "filename": "Adjusted R squared",
    "sha": "efa171cc8f9dd430da7080e8a376ca5322217b8f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Adjusted%20R%20squared.md",
    "text": "Adjusted R-squared is a [[Regression Metrics]]or assessing the quality of a regression model, ==especially when multiple predictors== are involved. It helps ensure that the model remains [[parsimonious]] while still providing a good fit to the data.\n\nWhen evaluating a regression model, if you notice a ==large difference== between [[R squared]] and adjusted R², it indicates that the additional predictors may not be improving the model's performance.\n\nIn such cases, it may be beneficial to drop those extra variables to simplify the model without sacrificing predictive power.\n\nKey features:\n\n1. **Penalty for Number of Predictors**:\n   - Adjusted R² adjusts the R² value by penalizing the addition of ==unnecessary predictors==. This means that if you add a variable that does not improve the model significantly, the adjusted R² will decrease, reflecting that the model may be overfitting.\n\n2. **Comparison with R-squared**:\n   - Adjusted R² is always less than or equal to R². While R² can artificially inflate with the addition of more predictors (even if they are not useful), adjusted R² provides a ==more reliable== assessment of model fit by considering the number of predictors relative to the number of observations.\n\n3. **Interpretation**:\n   - Like R², adjusted R² values range from 0 to 1, where values closer to 1 indicate a better fit. However, a significant difference between R² and adjusted R² suggests that the model may be penalized for including extra variables that do not contribute meaningfully to the prediction.\n\n4. **Formula**:\n   $$R^2_{adj.} = 1 - (1 - R^2) \\cdot \\frac{n - 1}{n - p - 1}$$\n   - Where:\n     - $R^2$ = R-squared value\n     - $n$ = number of observations\n     - $p$ = number of predictors (independent variables)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "statistics"
    ],
    "normalized_filename": "adjusted_r_squared",
    "outlinks": [
      "parsimonious",
      "r_squared",
      "regression_metrics"
    ],
    "inlinks": [
      "linear_regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "r_squared",
      "regression_metrics"
    ]
  },
  {
    "category": "ML",
    "filename": "Agent-Based Modelling",
    "sha": "af28f7daca4c02b29d16ffe4966dc575fa814166",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Agent-Based%20Modelling.md",
    "text": "(ABM) is a computational approach that simulates the interactions of individual agents within a defined environment to observe complex phenomena and [[emergent behavior]] at a system level. \n\nAgent-based modeling provides a robust framework for understanding and analyzing complex systems, particularly in the energy sector. \n\nBy simulating individual agents and their interactions, researchers and practitioners can gain insights into system dynamics, evaluate [[Policy|policies]], and optimize strategies for energy production and consumption.\n### Principles of Agent-Based Modelling\n\n1. Agents: The primary components of ABM, agents can represent individuals, groups, or entities with defined behaviors and attributes. Each agent operates based on its rules and interactions with other agents and the environment.\n\n2. Environment: The space in which agents operate, which can be a physical or abstract setting. The environment influences agent behavior and can include various elements like resources, obstacles, or rules governing interactions.\n\n3. Interactions: Agents communicate and interact with each other and their environment. These interactions can be cooperative, competitive, or neutral, leading to complex system dynamics.\n\n4. Emergence: ABM focuses on emergent phenomena, where the collective behavior of agents leads to unexpected outcomes not evident from examining individual agents alone. This principle helps understand complex systems' dynamics and behaviors.\n\n### Techniques in Agent-Based Modeling\n\n1. Model Development: \n   - Define Agents: Specify agent types, behaviors, attributes, and decision-making processes.\n   - Environment Design: Create a representation of the environment, including spatial aspects and available resources.\n   - Interaction Rules: Establish rules governing how agents interact with each other and their environment.\n\n1. Simulation: \n   - Execute the model over time, allowing agents to make decisions, interact, and adapt based on predefined rules.\n   - Collect data on agents' behaviors and system-wide outcomes during the simulation.\n\n1. Analysis: \n   - Analyze the results to understand emergent patterns and behaviors. This can involve statistical analysis, visualization of agent interactions, and evaluating how different parameters influence outcomes.\n\n1. Validation: \n   - Compare model outputs with real-world data to validate the model's accuracy. Calibration may be necessary to ensure the model reflects observed behaviors accurately.\n\n### Applications of Agent-Based Modeling\n\n1. Energy Systems: \n   - Demand Response: ABM can simulate consumer behavior in response to dynamic pricing or demand response programs, providing insights into how to encourage energy conservation during peak demand.\n   - Renewable Energy Integration: It helps model the interactions between different energy producers (e.g., solar, wind) and consumers, examining how they adapt to changes in supply and demand.\n\n1. Market Dynamics: \n   - Simulate interactions between different energy providers and consumers to understand competitive behavior, pricing strategies, and market outcomes.\n   - Evaluate the impact of regulatory changes on market behavior and investment decisions.\n\n1. Resource Management: \n   - Model interactions among agents in managing shared resources, such as water or electricity, to study the effects of cooperation and competition on resource depletion or sustainability.\n\n### Example Frameworks and Tools\n\nSeveral tools and frameworks are available for building and simulating agent-based models, including:\n\n- NetLogo: A user-friendly platform for creating agent-based models, particularly in education and research.\n- AnyLogic: A powerful commercial tool that supports agent-based, system dynamics, and discrete event modeling.\n- Repast: An open-source framework for building ABMs, widely used in academic research.\n- MASON: A fast and flexible discrete-event simulation library for Java, suitable for developing complex ABMs.\n\n[[Agentic Solutions]]\n\n[[Agent-Based Modelling]]\n- Multi-Agent Systems in [[LLM|LLMs]]\n  - The need for shared context across different agents, ensuring consistency and coherence in interactions.\n\n## Agent Interactions\n\n### How do Agents Interact?\n- Horizontal Collaboration: Agents with local goals coordinate to achieve a shared objective.\n- Hierarchical Collaboration: Primary agents oversee specialized agents to manage complex tasks effectively.\n\n## Understanding Agents\n\n### Core Components:\n1. Tools: Resources such as APIs, databases, or GitHub.\n2. Strategy: Techniques like self-criticism, [[Chain of thought]] (CoT), and planning to improve reasoning.\n3. States: Memory, context tracking, and microservices for modularity.\n4. Goals: Specific objectives defined for each agent.\n\n## Agent Planning and Interaction\n1. Planning: Agents plan operations, such as managing workflows in a support center.\n2. Agent Collaboration: Agents align their individual goals with shared objectives to enhance system performance.\n3. Multi-step\n\n## Compounding Systems\n\n### Multi-Agent Systems\nThese systems reduce reliance on extensive [[Prompt Engineering]] by compartmentalizing tasks across specialized agents.  \nExample: A writer agent drafts content, while a reviewer agent ensures quality, both operating within defined scopes.",
    "aliases": [
      "ABM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "agent-based_modelling",
    "outlinks": [
      "prompt_engineering",
      "llm",
      "policy",
      "chain_of_thought",
      "agentic_solutions",
      "agent-based_modelling",
      "emergent_behavior"
    ],
    "inlinks": [
      "agent-based_modelling",
      "agentic_solutions",
      "energy",
      "energy_abm"
    ]
  },
  {
    "category": "ML",
    "filename": "Anomaly Detection in Time Series",
    "sha": "2ff90abfde2d51b19b9dfb710abf24b17d90931a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Anomaly%20Detection%20in%20Time%20Series.md",
    "text": "## [[Anomaly Detection]] in [[Time Series]] Data\n\nAnomaly detection in time series data involves identifying observations that **deviate significantly from expected temporal patterns**. Because time series data is ordered and may include trends and seasonality, specialized methods are often required.\n\n### 1. Statistical Methods\n\n* **Moving Average / Rolling Statistics**: Compute rolling mean or median and flag points that are significantly above or below expected values.\n* **Seasonal Decomposition**: Break the series into trend, seasonal, and residual components. Anomalies are typically detected in the **residuals**, which represent deviations from the expected pattern. Python’s `seasonal_decompose` (STL) can be used.\n* **[[Z-score]] / IQR**: Standardize residuals or raw values using Z-scores or the interquartile range to detect outliers.\n\n### 2. Time Series Models\n\n* **ARIMA / SARIMA**: Fit models to capture autocorrelations and trends; anomalies are detected as large residuals.\n* **State Space Models / ETS**: Forecast future values and detect anomalies based on residual deviations.\n\n### 3. Machine Learning Approaches\n\n* **[[LSTM]] Networks**: Train sequence models to predict next values; large prediction errors indicate potential anomalies.\n* **Isolation Forest / One-Class SVM**: Can be adapted for time series by encoding temporal features (e.g., lagged values).\n\n### 4. Change Point Detection\n\n* Detect points where the **statistical properties of the series shift** (mean, variance, correlation), which may indicate structural anomalies. Methods include Bayesian change point detection or `ruptures` library.\n\n### 5. Visual Methods\n\n* **Time Series Plots**: Interactive plots can help spot unusual spikes, drops, or patterns.\n* **Control Charts**: Define control limits around expected values; points outside limits are flagged.\n* **Residual Plots**: Plot residuals from models or decompositions to highlight anomalies.\n\n### 6. Practical Workflow\n\n1. **Explore the dataset**: Check time range, missing values, and initial statistics.\n2. **Decompose / model**: Apply [[STL decomposition]], ARIMA, or another time series model.\n3. **Detect anomalies**: Use residual-based methods (Z-score, IQR, model errors) or ML-based approaches.\n4. **Visualize results**: Highlight anomalies on interactive plots to interpret patterns.\n5. **Validate anomalies**: Compare against known events or domain knowledge to reduce false positives.",
    "aliases": [],
    "date modified": "9-11-2025",
    "tags": [
      "anomaly_detection",
      "model_explainability",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "anomaly_detection_in_time_series",
    "outlinks": [
      "lstm",
      "anomaly_detection",
      "time_series",
      "stl_decomposition",
      "z-score"
    ],
    "inlinks": [
      "anomaly_detection",
      "time_series"
    ]
  },
  {
    "category": "ML",
    "filename": "Anomaly Detection with Clustering",
    "sha": "ab8c5f90c1d05e07d98b9ea598912589641c8c36",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Anomaly%20Detection%20with%20Clustering.md",
    "text": "Outliers can often be detected using [[Clustering]] methods because they either form small, distinct groups or are isolated from major clusters without strict statistical assumptions.\n\n| Method                         | Key Assumption                                                         | Strengths                                                                                                              | Weaknesses                                                                                           | Typical Use Case                                            |\n| :----------------------------- | :--------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------- | :---------------------------------------------------------- |\n| [[DBSCAN]]                     | Clusters are areas of high density separated by low-density regions    | - No need to specify number of clusters- Can find arbitrarily shaped clusters- Explicitly identifies noise (anomalies) | - Struggles with varying densities- Sensitive to parameter choice (epsilon, minPoints)               | Spatial data clustering and density-based anomaly detection |\n| [[Isolated Forest]]            | Anomalies are easier to isolate via random splits                      | - Efficient on large, high-dimensional datasets- Requires fewer assumptions- Scales well with data size                | - Not suited for small datasets- Less [[Interpretability\\|interpretable]] than density-based methods | High-dimensional tabular data anomaly detection             |\n| [[Local Outlier Factor (LOF)]] | Anomalies have significantly lower local density compared to neighbors | - Good for local anomaly detection- Adapts to density variations                                                       | - Sensitive to choice of k (number of neighbors)- Poor performance on high-dimensional data          | Detecting subtle anomalies in medium-sized tabular datasets |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "clustering"
    ],
    "normalized_filename": "anomaly_detection_with_clustering",
    "outlinks": [
      "isolated_forest",
      "clustering",
      "interpretability\\",
      "local_outlier_factor_(lof)",
      "dbscan"
    ],
    "inlinks": [
      "anomaly_detection",
      "isolated_forest"
    ]
  },
  {
    "category": "ML",
    "filename": "Anomaly Detection with Statistical Methods",
    "sha": "eaa8a99d01d3f8947205358723d729387ff7b94d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Anomaly%20Detection%20with%20Statistical%20Methods.md",
    "text": "Basic:\n- [[Z-Normalisation|Z-Score]]\n- [[Interquartile Range (IQR) Detection]]\n- [[Percentile Detection]]\n\nAdvanced:\n- [[Gaussian Model]]\n- [[Isolated Forest]]\n- [[Kernel Density Estimation]] (KDE): Estimate the probability density, flag low-density regions as anomalies.\n\nRefernences:\n- Outlier detection Manning\n# Others\n### Histogram-Based Outlier Detection (HBOS)\n\nContext:  \n\nHBOS is a [[non-parametric]] method that detects anomalies by analyzing the distribution of individual features independently. It relies on histograms, which estimate feature density.\n\nPurpose:  \nTo identify outliers as data points falling in bins with low frequencies or densities.\n\nSteps:\n- Create histograms for each feature:\n    - Divide each feature's range into bins.\n    - Count the frequency of data points in each bin.\n- Calculate scores for each data point:\n    - Outliers are points in bins with significantly lower densities compared to others.\n\nAdvantages:\n- Does not assume a specific data distribution.\n- Scales well to large datasets.\n\nLimitations:\n- Assumes feature independence (not ideal for [[multivariate data]]).\n- Sensitive to bin size selection.\n\n### One-Class SVM\n\nOne-Class Support Vector Machine is a variation of the SVM algorithm used for anomaly detection. It learns a decision boundary around the normal data points.\n\nSteps:\n- Train the model on the normal data points.\n- The model attempts to find a hyperplane that separates the normal data from the origin.\n- Points that fall outside this boundary are classified as anomalies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "ml",
      "statistics"
    ],
    "normalized_filename": "anomaly_detection_with_statistical_methods",
    "outlinks": [
      "interquartile_range_(iqr)_detection",
      "isolated_forest",
      "z-normalisation",
      "kernel_density_estimation",
      "non-parametric",
      "percentile_detection",
      "gaussian_model",
      "multivariate_data"
    ],
    "inlinks": [
      "anomaly_detection"
    ]
  },
  {
    "category": "ML",
    "filename": "Anomaly Detection",
    "sha": "01eb0a81702f939169b22e5deb10194c7fbd30ba",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Anomaly%20Detection.md",
    "text": "Anomaly detection involves identifying [[uncategorised/Outliers|Outliers]]. Detecting these anomalies is necessary for maintaining [[Data Integrity]] and improving model performance.\n\nOutlier detection needs formalized criteria (e.g., statistical models, distance measures, density estimates) because \"weird\" is contextual and depends on distribution.\n## Methods for Detecting Anomalies\n\n- [ ] Explore [[PyOD]]\n## Process of Detection\n\n[[Data Preparation]]\n   - [[Data Cleansing]]: Handle missing values and remove any irrelevant data points.\n   - [[Normalisation]]/[[Standardisation]]: Scale the data if necessary, especially if using methods sensitive to the scale.\n\nNormality Assumption: Data Distribution\n- Many classic methods (like z-score, [[Gaussian Mixture Models]]) assume [[Gaussian Distribution|normally distributed data]]]\n- If the data is not normal, other methods (e.g., Isolation Forest, One-Class SVM) may be better.\n- Always inspect [[Distributions]] first (e.g., histogram, [[Q-Q Plot]].\n\nAnomaly Detection with a model: Use a chosen method to flag anomalies\n- [[Anomaly Detection with Clustering]]\n- [[PCA-Based Anomaly Detection]]\n- [[Anomaly Detection in Time Series]]\n- [[Anomaly Detection with Statistical Methods]]\n\nTraining and Testing on Same Set (Careful!)\n- In outlier detection, sometimes you train and test on the same dataset — especially in pure unsupervised settings.\n- However, overfitting is a risk. It's common to:\n  - Train initial model\n  - Remove detected outliers\n  - Re-train\n  - Repeat inspection -> removal-> retraining (iterative loop you wrote about).\n\n[[Interpretability]] & [[Data Validation]]: Why is it unusual\n   - Validate the detected anomalies by comparing them against known anomalies (if available) or using domain knowledge.\n   - Adjust thresholds or methods based on validation results.\n   - Finding an outlier is not enough. You must ask: \"Why is this point different?\" Necessary for Trust & Actionability.\n  - Example: Anomalous electricity usage — is it fraud, error, or legitimate increased demand?\n\n[[Data Visualisation]]\n- Visualize the results using plots (e.g., [[Scatter Plots]], box plots) to understand the distribution of data and the identified anomalies.\n- [[Boxplot]]: Displays the distribution and identifies outliers using the interquartile range (IQR).\n- Scatter Plot: Helps visually identify outliers.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection"
    ],
    "normalized_filename": "anomaly_detection",
    "outlinks": [
      "uncategorised/outliers",
      "gaussian_distribution",
      "scatter_plots",
      "data_integrity",
      "boxplot",
      "data_preparation",
      "anomaly_detection_in_time_series",
      "gaussian_mixture_models",
      "interpretability",
      "pyod",
      "normalisation",
      "data_validation",
      "anomaly_detection_with_statistical_methods",
      "standardisation",
      "anomaly_detection_with_clustering",
      "pca-based_anomaly_detection",
      "data_visualisation",
      "data_cleansing",
      "distributions",
      "q-q_plot"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "clustering",
      "ds_&_ml_portal",
      "gaussian_mixture_models",
      "imbalanced_datasets",
      "outliers",
      "pyod",
      "time_series_python_packages"
    ]
  },
  {
    "category": "ML",
    "filename": "Assessing Gen AI generated content",
    "sha": "be93da8786203ded6b5551e916d23ce12846f631",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Assessing%20Gen%20AI%20generated%20content.md",
    "text": "To assess whether the content generated by a [[Generative AI]] is truthful and faithful, several methods and frameworks can be employed. Truthfulness refers to whether the generated content ==is factually correct==, while faithfulness refers to whether it ==accurately== reflects the input data or prompt.\n\n[[Interpretability]]\n### 1. Frameworks for Truthfulness and Faithfulness\n\n   - Subject Matter Expert (SME) Reviews: One of the most reliable methods for verifying truthfulness and faithfulness is through SME validation. SMEs can manually check the content to ensure it aligns with domain-specific knowledge and is factually accurate.\n   \n   - [[Knowledge Graph]] and External Data: Generative AI models can be linked to external sources of truth, such as knowledge graphs, databases, or other verified resources. This allows the system to cross-check facts and improve the truthfulness of the content.\n   \n   - Retrieval-Augmented Generation ([[RAG]]): This framework involves retrieving relevant information from trusted sources before generating content. It helps ensure that the AI is providing up-to-date, reliable, and contextually accurate responses.\n   \n   - Evaluation Metrics: Some metrics can be used to evaluate faithfulness:\n     - Factual Consistency Metrics: Tools such as [[BERTScore]] or FactCC can compare generated text with reference text or factual databases to check for consistency.\n     - Human Evaluation: In certain contexts, human evaluators rate the content on aspects of truthfulness and faithfulness. This can be part of quality assurance processes.\n\n   - Cross-Referencing Data: AI-generated content should be cross-referenced with existing, credible sources to confirm its accuracy. For example, if the AI makes a historical claim or provides statistical data, those facts should be verifiable through known data repositories.\n\n   - Fact-Checking Tools: Using automated fact-checking tools or models trained to detect false information can provide another layer of defence against untruthful content.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "GenAI"
    ],
    "normalized_filename": "assessing_gen_ai_generated_content",
    "outlinks": [
      "bertscore",
      "generative_ai",
      "knowledge_graph",
      "interpretability",
      "rag"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "AutoML",
    "sha": "2d6b359064548fea033f0ec9b0b180472bf28fde",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/AutoML.md",
    "text": "AutoML\n: helps with prototying and exploration of implementing new ml models like [[PyCaret]]\n- Sagemaker\n- Google cloud automl",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "exploration",
      "ml"
    ],
    "normalized_filename": "automl",
    "outlinks": [
      "pycaret"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Automated Feature Creation",
    "sha": "abc36f8d46f7d9ead1711ff165d9031787e56a4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Automated%20Feature%20Creation.md",
    "text": "**Question:** Can we autodetect meaningful features.\n\n[[Feature Engineering]] is an ad-hoc manual process that depends on domain knowledge, intuition, data exploration, and creativity. However, this process is dataset-dependent, time-consuming, tedious, subjective, and not a scalable solution.\n\n[[Automated Feature Creation]] automatically generates features using a framework; these features can be filtered using [[Feature Selection]] to avoid feature explosion. \n\nBelow are some popular open-source libraries for automated feature engineering:\n\n- [[PyCaret]] – [PyCaret](https://pycaret.org/)\n- Featuretools for advanced usage – [Home](https://www.featuretools.com/) | [What is Featuretools? — Featuretools 1.1.0 documentation](https://featuretools.alteryx.com/en/stable/)\n- Optuna – [A hyperparameter optimization framework](https://optuna.org/)\n- Feature-engine – [A Python library for Feature Engineering for Machine Learning — 1.1.2](https://feature-engine.readthedocs.io/en/1.1.x/)\n- ExploreKit – [GitHub – giladkatz/ExploreKit](https://github.com/giladkatz/ExploreKit)\n- https://www.turintech.ai/blog/feature-generation-what-it-is-and-how-to-do-it",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "transformation"
    ],
    "normalized_filename": "automated_feature_creation",
    "outlinks": [
      "feature_engineering",
      "feature_selection",
      "automated_feature_creation",
      "pycaret"
    ],
    "inlinks": [
      "automated_feature_creation"
    ]
  },
  {
    "category": "ML",
    "filename": "Backpropagation",
    "sha": "2341cf66fc18d034dd825274e502af74440f17dd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Backpropagation.md",
    "text": "Backpropagation is an algorithm for training neural networks by iteratively correcting prediction errors. It calculates the gradient of the [[Loss function]] $L(\\theta)$ with respect to each model parameter $\\theta$, enabling updates via [[Gradient Descent]] to minimize the loss.\n\nMathematically, backpropagation employs the chain rule from calculus to propagate errors backward through the network. Each layer computes a partial derivative, which is used to adjust its weights. This iterative process continues until a convergence criterion is met, typically when the loss change falls below a threshold.\n\nBackpropagation is especially critical in [[Supervised Learning]], where models learn from labeled data to recognize patterns.\n## Important Notes\n- Gradient descent minimizes the loss by updating parameters in the direction of the negative gradient:  \n  $\\nabla L(\\theta) = \\frac{\\partial L}{\\partial \\theta}$.  \n- Backpropagation adjusts weights based on computed error gradients, enabling effective deep model optimization.\n- Computational Cost: Backpropagation can be expensive for deep networks, requiring computation of gradients across all layers.  \n- Vanishing/Exploding Gradients: In deep networks, gradients can become too small or too large, hindering effective training.\n## Example Application\n\nA [[Feed Forward Neural Network]] trained for image classification uses backpropagation to minimize cross-entropy loss. The gradient of the loss is calculated layer-by-layer and weights are updated using an optimizer like Adam.\n## Backpropagation Step-by-Step (Manual Calculation)\n\nWhen dealing with many parameters, like in neural networks, it can be helpful to think in terms of ==computation graphs.==\n\nBasic backpropagation procedure through a computation graph:\n1. Work right to left: starting from the output layer back towards the input.\n2. For each node:\n   - Calculate the local derivative(s) (i.e., the derivative of the node’s output with respect to its input).\n   - Combine this with the derivative of the loss with respect to the node using the chain rule.\n\nDefinition:  \nThe \"local derivative(s)\" are the derivatives of the current node’s output with respect to each of its inputs or parameters.\n## Computation Graph Example (using [[Sympy]])\nTo manually calculate gradients through a computation graph, symbolic differentiation can be used.\n\n```python\nfrom sympy import symbols, diff\n\n# Define symbols\nx, w, b = symbols('x w b')\ny = symbols('y')  # true label\n\n# Define a simple model: prediction = wx + b\nprediction = w*x + b\n\n# Define a loss function: squared error\nloss = (prediction - y)2\n\n# Compute gradients\ngrad_w = diff(loss, w)\ngrad_b = diff(loss, b)\n\nprint(grad_w)\nprint(grad_b)\n```\nThis symbolic approach helps verify gradient calculations during small-scale experiments.\n## Follow-Up Questions\n- How does backpropagation compare to optimization methods like Newton’s Method or evolutionary strategies?  \n- What role does [[Regularisation]] play in addressing [[Overfitting]] when training deep networks with backpropagation?\n## Related Topics\n- [[Gradient Descent Optimizers]] (e.g., [[Adam Optimizer]], RMSprop)  \n- [[vanishing and exploding gradients problem]]",
    "aliases": [
      "Backprop",
      "backward propagation",
      "BP"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "optimisation",
      "statistics"
    ],
    "normalized_filename": "backpropagation",
    "outlinks": [
      "gradient_descent",
      "regularisation",
      "supervised_learning",
      "sympy",
      "feed_forward_neural_network",
      "loss_function",
      "adam_optimizer",
      "vanishing_and_exploding_gradients_problem",
      "overfitting",
      "gradient_descent_optimizers"
    ],
    "inlinks": [
      "activation_function",
      "deep_learning",
      "fitting_weights_and_biases_of_a_neural_network",
      "forward_propagation",
      "named_entity_recognition",
      "recurrent_neural_networks",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "Bagging vs Boosting",
    "sha": "fda94a6fd2b8088be1e4c455573b2778a7e9c1cb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Bagging%20vs%20Boosting.md",
    "text": "[[Bagging]] and [[Boosting]] are both [[Model Ensemble|ensemble]] methods, but they have different purposes and perform better in different situations. There isn’t a single “better” method - it depends on your data and problem. Here’s a clear comparison:\n\n### Rule of Thumb\n\n* Use bagging if your base model is high-variance (like deep trees) and you want stability.\n* Use boosting if your base model is weak and you want to maximize accuracy, but ensure careful tuning.\n\n### Comparison Table\n\n| Feature        | Bagging                 | Boosting                          |\n| -------------- | ----------------------- | --------------------------------- |\n| Model Training | Parallel, independent   | Sequential, dependent             |\n| Focus          | Reduce variance         | Reduce bias                       |\n| Sensitivity    | Less sensitive to noise | Sensitive to noise                |\n| Typical Use    | Random Forest           | AdaBoost, XGBoost                 |\n| Best For       | High-variance models    | Weak learners needing improvement |",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "boosting",
      "clustering",
      "ml",
      "ml_process"
    ],
    "normalized_filename": "bagging_vs_boosting",
    "outlinks": [
      "model_ensemble",
      "bagging",
      "boosting"
    ],
    "inlinks": [
      "model_ensemble"
    ]
  },
  {
    "category": "ML",
    "filename": "Bagging",
    "sha": "1ee9693621f0aa461a99c62166e21bc401589bef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Bagging.md",
    "text": "Bagging, short for Bootstrap Aggregating, is an [[Model Ensemble]] technique designed to improve the stability and accuracy of machine learning algorithms. \n\nIt works by ==training multiple instances of the same learning algorithm on different subsets of the training data== and then ==combining their predictions.==\n\n### How Bagging Works:\n\n1. **[[Bootstrap Sampling]]**: Bagging involves creating multiple subsets of the training data by [[Resampling]] with replacement. This means that each subset, or \"bootstrap sample,\" is drawn ==randomly== from the original dataset, and some data points may appear multiple times in a subset while others may not appear at all.\n\n2. **Parallel Training**: Each bootstrap sample is used to train a separate instance of the same base learning algorithm. These models are trained independently and in parallel, which makes bagging computationally efficient.\n\n3. **Combining Predictions**: Once all models are trained, their predictions are combined to produce a final output. For regression tasks, this is typically done by ==averaging== the predictions. For classification tasks, ==majority voting== is used to determine the final class label.\n\n### Key Concepts of Bagging:\n\n- **Reduction of [[Overfitting]]**: By averaging the predictions of multiple models, bagging reduces the variance and helps prevent overfitting, especially in high-variance models like decision trees.\n\n- **Diversity**: The use of different subsets of data for each model introduces diversity among the models, which is crucial for the success of ensemble methods.\n\n- **Parallelization**: Since each model is trained independently, bagging can be easily parallelized, making it scalable and efficient for large datasets.\n### Example of Bagging:\n\n**Random Forest**: A well-known example of a bagging technique is the [[Random Forest]] algorithm. \n\nIt uses decision trees as base models and combines their predictions to improve accuracy and robustness. \n\nEach tree in a random forest is trained on a different bootstrap sample of the data, and the final prediction is made by averaging the outputs (for regression) or majority voting (for classification).\n# Further Understanding\n\n### Advantages of Bagging:\n\n- **Increased Accuracy**: By combining multiple models, bagging often achieves higher accuracy than individual models.\n- Reduce **variance** of high-variance models \n- **Robustness**: Bagging is less sensitive to overfitting, especially when using high-variance models like decision trees.\n- **Flexibility**: It can be applied to various types of base models and is not limited to a specific algorithm.\n\n### Challenges of Bagging:\n\n- **Complexity**: While bagging reduces overfitting, it can increase the complexity of the model, making it harder to interpret.\n- **Computational Cost**: Training multiple models can be computationally intensive, although this can be mitigated by parallel processing.\n- Less effective at reducing bias.\n\n\n### Sklearn\n\nThe change aligns the naming convention between ensemble models:\n\nBaggingClassifier(estimator=...)\n\nStackingClassifier(estimators=...)\n\nVotingClassifier(estimators=...)\n\nThis makes the API more consistent.\n\n\nExcellent question — these two ensemble methods are closely related but serve different purposes and have distinct internal mechanics.\n\n### Bagging Classifier vs Random Forest Classifier\n\n## **1. Conceptual overview**\n\n### **BaggingClassifier**\n\n* Implements **Bootstrap Aggregating (Bagging)**.\n* Trains multiple *independent* copies of a **base estimator** on random subsets of the training data (with replacement).\n* The final prediction is the **average (regression)** or **majority vote (classification)** of all estimators.\n\n### **RandomForestClassifier**\n\n* A **special case** of `BaggingClassifier` that uses **Decision Trees** as base estimators **plus an additional randomization** in feature selection.\n* Each tree is trained not only on a bootstrap sample of the data but also on a **random subset of features** at each split.\n\n---\n\n## **2. Main differences**\n\n| Feature                    | BaggingClassifier                           | RandomForestClassifier                                                           |\n| -------------------------- | ------------------------------------------- | -------------------------------------------------------------------------------- |\n| **Base model**             | Any estimator (e.g. DecisionTree, KNN, SVM) | Always `DecisionTreeClassifier`                                                  |\n| ==**Feature randomness**== | None by default                             | Adds randomness by selecting a subset of features at each split (`max_features`) |\n| **Correlation reduction**  | Only via bootstrapped samples               | Both via bootstrapping *and* random feature selection                            |\n| **Bias–variance tradeoff** | Reduces variance (not bias)                 | Reduces variance more effectively due to extra feature randomness                |\n| **Out-of-bag score**       | Supported (`oob_score=True`)                | Supported (`oob_score=True`)                                                     |\n| **Speed and tuning**       | Slower (depends on base model)              | Faster and easier to tune; optimized for trees                                   |\n| **Interpretability**       | Depends on base model                       | Easier to interpret (feature importances, etc.)                                  |\n| **API simplicity**         | More flexible                               | More specialized and efficient for trees                                         |\n\n\nTypically, the **Random Forest** performs as well or better than the plain **Bagging** ensemble, because of the ==decorrelated trees produced by random feature selection.==\n\n## **4. When to use which**\n\n* **Use `BaggingClassifier` when**:\n\n  * You want to ensemble *any* kind of model (not just trees).\n  * Example: bagging multiple **KNN classifiers** or **SVMs**.\n\n* **Use `RandomForestClassifier` when**:\n\n  * You specifically want an ensemble of **Decision Trees** with feature-level randomness.\n  * You want **better generalization** and **less overfitting** with minimal tuning.\n\n---\n\n## **5. Summary**\n\n| Aspect     | BaggingClassifier                                   | RandomForestClassifier                       |\n| ---------- | --------------------------------------------------- | -------------------------------------------- |\n| Type       | General-purpose ensemble wrapper                    | Specialized ensemble of trees                |\n| Randomness | Bootstrapped samples                                | Bootstrapped samples + random feature subset |\n| Strength   | Flexibility (can wrap any model)                    | Strong performance, low variance             |\n| Weakness   | May not generalize as well without extra randomness | Limited to trees only                        |\n\n### Random Forest is a special case of a Bagging Classifier",
    "aliases": [],
    "date modified": "5-11-2025",
    "tags": [
      "architecture",
      "classifer",
      "ensemble",
      "ml_process",
      "statistics"
    ],
    "normalized_filename": "bagging",
    "outlinks": [
      "bootstrap_sampling",
      "resampling",
      "random_forest",
      "overfitting",
      "model_ensemble"
    ],
    "inlinks": [
      "bagging_vs_boosting",
      "bias-variance_trade_off",
      "imbalanced_datasets",
      "model_ensemble",
      "random_forest"
    ]
  },
  {
    "category": "ML",
    "filename": "Batch Normalisation",
    "sha": "3a54f9502f3340ce3b5c276204ce3b628b94f09a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Batch%20Normalisation.md",
    "text": "Links:\n- [Batch normalization | What it is and how to implement it](https://www.youtube.com/watch?v=yXOMHOpbon8&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=2)\n\nCan be used to handle [[vanishing and exploding gradients problem]] and [[Overfitting]] problems within [[Neural network]].\n\nFirst note:\n[[Normalisation vs Standardisation]]\n\nHow does Batch normalisation work?\n\nBatch normalisation works by first standardising the inputs, then scales linearly - coefficients determined through training. This occurs between each layer.\n\nOutcomes of this process:\n- epochs take longer, but less epochs are required.\n\nBenefits:\n- Batch normalisation occurs at each layer, so do not need separate normalisation step for input data.\n- What about bias? We do not need bias in BN.\n\n![[Pasted image 20241219071904.png]]\n\n\n### Example: [[MNIST]]\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\nmnist = keras.datasets.mnist\n(X_train_full, y_train_full) , (X_test, y_test) = mnist.load_data()\n\nplt.imshow(X_train_full[12], cmap=plt.get_cmap('gray' ))\nX_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:]/255\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX test = x test/255\n\nmodel = keras.models.Sequential([\nkeras.layers.Flatten(input_shape=[28,28]),\nkeras.layers.Dense(300, activation = \"relu\"),\nkeras.layers.Dense(100, activation = \"relu\"),\nkeras.layers.Dense(10, activation = \"softmax\")])\n\n```\n\nIntroducing BN into this model.\n\nDo you put BN before or after a activation function? Author of Paper suggests before.\n```python\n# Dont need as have BN now\n# X valid, X train = X_train_full[ :5000] / 255, X_train_full[5000:]/255\n# y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n# X test = X test/255\n\nmodel = keras.models.Sequential ([\nkeras.layers.Flatten(input_shape=[28,28]),\nkeras.layers.BatchNormalization(), # normalisation layer.\nkeras.layers.Dense(300,use_bias=False),\nkeras.layers.BatchNormalization(),\nkeras.layers.Activation('relu'),\nkeras.layers.Dense(100,use_bias=False), I\nkeras.layers.BatchNormalization(),\nkeras.layers.Activation('relu'),\nkeras.layers.Dense(10, activation = \"softmax\")\n\n])\n```\n\n## Questions\n\nQ6: What is batch normalization, and how does it address the vanishing gradient problem?\n\nA6: Batch normalization is a technique that normalizes the inputs to each layer within a mini-batch. By normalizing the inputs, it reduces the internal covariate shift and helps maintain a stable gradient flow. Batch normalization alleviates the vanishing gradient problem by ensuring that the gradients do not vanish or explode during training.\n\n[[vanishing and exploding gradients problem]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "batch_normalisation",
    "outlinks": [
      "pasted_image_20241219071904.png",
      "normalisation_vs_standardisation",
      "mnist",
      "vanishing_and_exploding_gradients_problem",
      "overfitting",
      "neural_network"
    ],
    "inlinks": [
      "normalisation",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "Bias in ML",
    "sha": "80f76ca0fa43cec077023ed4af159b53ce95ca28",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Bias%20in%20ML.md",
    "text": "Bias occurs when a model produces ==consistently unfair or inaccurate results==. This can arise in two contexts:\n\n- **Algorithmic/statistical bias**: The error due to overly strong or ==simplistic assumptions== about the data. High bias leads to **underfitting**, where the model cannot capture the true relationship between inputs and outputs.\n- **Data/ethical bias**: The unfairness or systematic skew in predictions caused by biased data collection, feature choices, or training design.\n\nIn the bias–variance framework:\n- **Bias** = the error from the model being unable to learn the true mapping between inputs and outputs.\n  \nRelated:\n- [[Variance in ML]]\n- [[Bias-Variance Trade Off]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "explainability"
    ],
    "normalized_filename": "bias_in_ml",
    "outlinks": [
      "variance_in_ml",
      "bias-variance_trade_off"
    ],
    "inlinks": [
      "cross_validation",
      "machine_learning_algorithms",
      "overfitting",
      "variance_in_ml"
    ]
  },
  {
    "category": "ML",
    "filename": "Binary Classification",
    "sha": "6ca5cc1fbdeed1f53038d5634a9c4f1090ddd338",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Binary%20Classification.md",
    "text": "Binary classification is a type of [[Classification]] task that involves predicting one of two possible classes or outcomes. It is used in scenarios where the goal is to categorize data into two distinct groups, such as spam vs. not spam in email filtering or disease vs. no disease in medical diagnosis.\n\nUse pycaret for binary classification. This can use many models and compare them. Can do\n- ROC and AUC\n- [[Confusion Matrix]]\n- [[Feature Importance]]\n- [[Model Evaluation]] - must do in [[ipynb]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "ml",
      "ML_Tools"
    ],
    "normalized_filename": "binary_classification",
    "outlinks": [
      "confusion_matrix",
      "model_evaluation",
      "ipynb",
      "feature_importance",
      "classification"
    ],
    "inlinks": [
      "activation_function",
      "cosine_similarity",
      "determining_threshold_values",
      "ds_&_ml_portal",
      "fitting_weights_and_biases_of_a_neural_network",
      "logistic_regression",
      "precision-recall_curve",
      "typical_output_formats_in_neural_networks",
      "use_cases_for_a_simple_neural_network_like"
    ]
  },
  {
    "category": "ML",
    "filename": "Boosting",
    "sha": "dfdc84da6e9490c68199bc40b11efe29eb947bdb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Boosting.md",
    "text": "Boosting is a type of [[Model Ensemble]] in machine learning that focuses on improving the accuracy of predictions by building a ==sequence of models==. Each subsequent model focuses on correcting the errors made by the previous ones.\n\nIt combines [[Weak Learners]] (models that are slightly better than random guessing) to create a strong learner. \n\n### Key Concepts of Boosting:\n\n1. Sequential Learning: Boosting involves training models sequentially. Each new model is trained to correct the errors made by the previous models. This means that the models are not independent of each other; instead, ==each model is built on the mistakes of the previous ones.==\n\n2. Focus on Misclassified Data: As models are trained in sequence, more emphasis is placed on the data points that were misclassified by earlier models. This helps the ensemble model to gradually improve its performance by focusing on the difficult-to-classify instances.\n\n3. [[Weak Learners]]: Boosting combines multiple weak learners, which are models that perform slightly better than random guessing. By combining these weak learners, boosting creates a strong learner that has improved accuracy.\n\n4. Examples of Boosting Algorithms: Some well-known boosting algorithms include [[Ada boosting]], [[Gradient Boosting]], and [[XGBoost]]. Each of these algorithms has its own approach to boosting, but they all share the core principle of sequentially improving model performance.\n\n### Advantages of Boosting:\n\n- Increased Accuracy: By focusing on the errors of previous models, boosting can significantly improve the accuracy of predictions.\n- Flexibility: Boosting can be applied to various types of base models and is not limited to a specific algorithm.\n- Robustness: Boosting can handle complex datasets and is effective in reducing bias and variance.\n\n### Challenges of Boosting:\n\n- Complexity: Boosting models can be more complex and computationally intensive than single models.\n- [[Interpretability]]: The final model may be harder to interpret compared to simpler models like decision trees.\n\n## Related\n- [[Order matters in Boosting]]\n\n### [[Boosting]]\n\n* **How it works:**\n  * Train models **sequentially**, each correcting errors of the previous one.\n  * Combine predictions using **weighted sums**.\n\n* **Example:** AdaBoost, Gradient Boosting, XGBoost.\n\n* **Focus:** Reduce **bias** by turning weak learners into a strong model.\n\n* **Pros:**\n  * Can achieve very high accuracy.\n  * Strong at capturing patterns in complex datasets.\n\n* **Cons:**\n  * Sensitive to noisy data and outliers.\n  * Risk of overfitting if not tuned carefully.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "explainability"
    ],
    "normalized_filename": "boosting",
    "outlinks": [
      "weak_learners",
      "ada_boosting",
      "gradient_boosting",
      "boosting",
      "order_matters_in_boosting",
      "xgboost",
      "interpretability",
      "model_ensemble"
    ],
    "inlinks": [
      "ada_boosting",
      "bagging_vs_boosting",
      "bias-variance_trade_off",
      "boosting",
      "gradient_boosting",
      "gradient_boosting_regressor",
      "model_ensemble"
    ]
  },
  {
    "category": "ML",
    "filename": "Business value of anomaly detection",
    "sha": "a3b56a22cdc170e9912bc64b72a3b631e2ad3384",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Business%20value%20of%20anomaly%20detection.md",
    "text": "Applications: [[Data Quality]] Checking:\n- Outlier detection is essential for detecting errors (e.g., typos, sensor failures, strange business events).\n- But: Catching every outlier is not always worth it — it needs to be cost-effective.\n- Business Cost:  \n  - False positives → wasted time investigating non-issues.\n  - False negatives → missing important problems.\n\nAnomaly detection should optimize business value, not just technical accuracy.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "business"
    ],
    "normalized_filename": "business_value_of_anomaly_detection",
    "outlinks": [
      "data_quality"
    ],
    "inlinks": [
      "outliers"
    ]
  },
  {
    "category": "ML",
    "filename": "CART",
    "sha": "949bf48e4ad3f35a2ab33c5bcaf8fe6c8ca89571",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/CART.md",
    "text": "CART stands for Classification and Regression Trees. It is one of the most widely used algorithms for building decision trees.\n\nBreakdown:\n\n- Decision Trees: These are tree-like structures where internal nodes represent tests on features, branches represent outcomes of those tests, and leaves represent the predicted class (classification) or predicted value (regression).\n  \n- CART Algorithm:\n    - It can handle both classification (predicting categories) and regression (predicting continuous values).\n    - For classification, CART uses Gini Impurity (sometimes entropy) to measure the \"purity\" of splits.\n    - For regression, CART uses Mean Squared Error (MSE) or Mean Absolute Error (MAE) as the splitting criterion.\n    - CART builds the tree recursively:\n        1. Choose the feature and split point that best separates the data according to the criterion.\n        2. Partition the dataset into two child nodes.\n        3. Repeat the process until a stopping condition (e.g., max depth, min samples per leaf) is reached.\n    - CART always creates binary splits (each node splits into two children), unlike some older decision tree methods (like ID3 or C4.5) which can create multiway splits.\n\n- Advantages:\n    - Handles numerical and categorical data.\n    - Easy to interpret and visualize.\n    - Works for both classification and regression tasks.\n\n- Limitations:\n    - Can overfit easily (high variance).\n    - Sensitive to small changes in data.\n    - Requires pruning or ensemble methods (Random Forest, Gradient Boosted Trees) to generalize well.\n\nExample:\n- Classification: Predicting if a customer will churn (yes/no).\n- Regression: Predicting house prices given features like size, location, and number of rooms.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifer",
      "mlprocess",
      "regressor"
    ],
    "normalized_filename": "cart",
    "outlinks": [],
    "inlinks": [
      "decision_tree",
      "embedded_methods",
      "gini_impurity_vs_cross_entropy"
    ]
  },
  {
    "category": "ML",
    "filename": "CatBoost",
    "sha": "4999fa933f4ffb4323f591de353e36ab524c5b41",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/CatBoost.md",
    "text": "CatBoost is a [[Gradient Boosting]] library developed by Yandex, designed to handle [[categorical]] features efficiently and provide robust performance with minimal [[Hyperparameter|Hyperparameter tuning]]\n\nIt is particularly useful in scenarios where datasets contain a significant number of categorical variables.\n\n#### Key Advantages\n\n1. Handling Categorical Features: \n   - CatBoost natively processes categorical features without the need for extensive preprocessing like one-hot encoding, which simplifies the workflow and reduces the risk of introducing errors during data preparation.\n\n2. Robustness to Overfitting:\n   - It employs techniques such as ordered boosting and per-feature scaling to reduce overfitting, making it a reliable choice for complex datasets.\n\n3. Performance:\n   - CatBoost offers competitive performance with minimal hyperparameter tuning, making it suitable for quick experimentation and deployment.\n\n### Implementing CatBoost in Python\n\nTo implement CatBoost in Python, you need to install the CatBoost library and then follow these steps:\n\n#### Step 1: Install CatBoost\n\nYou can install CatBoost using pip:\n```bash\npip install catboost\n```\n#### Step 2: Import Necessary Libraries\n```python\nimport catboost as cb\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n```\n\n#### Step 3: Prepare Your Data\n\nAssume you have a dataset with features `X` and target `y`. Split the data into training and testing sets:\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n#### Step 4: Identify Categorical Features\n\nIdentify the indices of categorical features in your dataset:\n\n```python\ncategorical_features_indices = [0, 1, 2]  # Example indices of categorical features\n```\n#### Step 5: Create a CatBoost Pool\n\nCreate a Pool object for the training data, specifying the categorical features:\n\n```python\ntrain_pool = Pool(data=X_train, label=y_train,cat_features=categorical_features_indices)\ntest_pool = Pool(data=X_test, label=y_test, cat_features=categorical_features_indices)\n```\n\n#### Step 6: Initialize and Train the Model\n\nInitialize the CatBoostClassifier and fit it to the training data:\n\n```python\nmodel = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=100)\nmodel.fit(train_pool)\n```\n\n#### Step 7: Make Predictions and Evaluate\n\nMake predictions on the test set and evaluate the model's performance:\n\n```python\ny_pred = model.predict(test_pool)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```",
    "aliases": [
      "CAT"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "python"
    ],
    "normalized_filename": "catboost",
    "outlinks": [
      "hyperparameter",
      "categorical",
      "gradient_boosting"
    ],
    "inlinks": [
      "gradient_boosting",
      "lightgbm_vs_xgboost_vs_catboost",
      "optuna"
    ]
  },
  {
    "category": "ML",
    "filename": "Challenges to Model Deployment",
    "sha": "b2f35f5ae1f5ff0d7c5485ffd90f888b9596161f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Challenges%20to%20Model%20Deployment.md",
    "text": "The main challenge is that a prototype focuses on *accuracy in isolation*, while production requires reliability, [[Scalability]], monitoring, and integration into business systems.\n\nData-related issues\n   * [[Data Drift]]: Distribution of incoming data differs from training data.\n   * [[Performance Drift|Concept drift]]: Relationship between inputs and outputs changes over time.\n   * [[Data Quality]]: Missing, delayed, or corrupted data in live systems.\n   * Feature availability: Features used in prototyping may not exist or be delayed in production.\n\n[[Scalability]] & performance\n   * Prototype may work on a subset, but production must handle larger volumes with low [[Latency]].\n   * Need for real-time processing vs batch pipelines.\n   * Resource constraints (CPU/GPU/memory).\n\nModel [[Generalisation|Robustness]]\n   * Prototype may overfit; needs retraining and validation under production conditions.\n   * Handling edge cases and rare events.\n\nIntegration with systems\n   * APIs, databases, or event-driven architectures may need redesign.\n   * Version control for models and dependencies.\n   * Ensuring reproducibility between dev and prod environments.\n\nMonitoring & maintenance\n   * Must track model accuracy, latency, and drift.\n   * Establish alerting and retraining triggers.\n   * Logging predictions for auditing and debugging.\n\nGovernance & security\n   * Compliance with regulations (GDPR, HIPAA, etc.).\n   * Access control for models and data.\n   * Ethical considerations (bias, fairness, [[Interpretability|explainability]]).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_governance",
      "data_pipeline",
      "data_security",
      "devops",
      "explainability",
      "systems"
    ],
    "normalized_filename": "challenges_to_model_deployment",
    "outlinks": [
      "generalisation",
      "data_drift",
      "interpretability",
      "scalability",
      "performance_drift",
      "data_quality",
      "latency"
    ],
    "inlinks": [
      "model_deployment"
    ]
  },
  {
    "category": "ML",
    "filename": "Class Separability",
    "sha": "ae9c9a09bfacef6dc1a6b174f1f35514b1fc1c80",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Class%20Separability.md",
    "text": "If you have a perfectly balanced dataset (unlike [[Imbalanced Datasets]]) but still experience poor [[Classification]] [[Accuracy]], class separability might be an issue due to the following reasons:\n\n1. Overlapping Classes: The features of different classes may overlap significantly, making it difficult for the model to distinguish between them. If the decision boundaries are not well-defined, the model may struggle to classify instances correctly.\n\n2. Complex Decision Boundaries: The underlying relationship between the features and the classes may be complex, requiring a more sophisticated model to capture the nuances. If the model is too simple, it may not be able to learn the necessary patterns.\n\n3. Noise in the Data: If there is a significant amount of noise or irrelevant features in the dataset, it can obscure the true signal, leading to poor [[Classification]] performance despite balanced class representation.\n\n4. Insufficient Feature Representation: The features used for classification may not adequately represent the underlying characteristics that differentiate the classes. This can lead to a lack of separability, even in a balanced dataset.\n\n5. Model Overfitting or Underfitting: If the model is overfitting, it may perform well on training data but poorly on unseen data. Conversely, if it is underfitting, it may not capture the complexity of the data, leading to poor accuracy.\n\nAddressing these issues may require exploring different [[Feature Engineering]] techniques, selecting more appropriate models, or adjusting [[Hyperparameter]]s to improve class separability.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "evaluation"
    ],
    "normalized_filename": "class_separability",
    "outlinks": [
      "hyperparameter",
      "accuracy",
      "feature_engineering",
      "imbalanced_datasets",
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Classification",
    "sha": "aee6525e416cdb637a79082c408a9ffc84a37890",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Classification.md",
    "text": "C\nClassification is a type of [[Supervised Learning]] in machine learning, where the algorithm learns from labeled data to predict which category or class a new, unlabeled data point belongs to. The goal is to assign the correct label to input data based on patterns learned from the training set.\n## Choosing a Classifier Algorithm\n\n1. Data Characteristics: Some algorithms work better on structured data, while others perform better on unstructured data.\n2. Problem Complexity: Simple classifiers for straightforward problems, complex models for intricate tasks.\n3. Model Performance: Consider accuracy and speed requirements.\n4. Model [[Interpretability]]: Some models, like decision trees, are easier to interpret, while others, like neural networks, can be more challenging.\n5. Model [[Scalability]]: Large datasets need scalable models like SVM or Naive Bayes.\n6. Model Flexibility: Algorithms like KNN are flexible when the data distribution is unknown.\n\n## Classifiers\n\nClassifier: A model used for classification tasks, predicting discrete labels or categories. For example, determining whether an email is spam or not, or identifying the species of a flower based on its features. This contrasts with a Regressor ([[Regression]]), which predicts continuous values.\n\n- [[Naive Bayes Classifier]]\n- [[Decision Tree]]\n- [[Support Vector Machines]]\n- [[K-nearest neighbours]]\n- [[Neural network]]\n- [[Model Ensemble]]\n## Use Cases of Classification\n\n1. Object Recognition: Classifying objects in images (e.g., identifying a cat or a dog).\n2. Spam Filtering: Classifying emails as either spam or legitimate.\n3. Medical Diagnosis: Using patient symptoms and test results to classify diseases.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "ml"
    ],
    "type": null,
    "normalized_filename": "classification",
    "outlinks": [
      "regression",
      "supervised_learning",
      "naive_bayes_classifier",
      "scalability",
      "model_ensemble",
      "decision_tree",
      "neural_network",
      "interpretability",
      "k-nearest_neighbours",
      "support_vector_machines"
    ],
    "inlinks": [
      "accuracy",
      "binary_classification",
      "class_separability",
      "confusion_matrix",
      "cross_entropy",
      "decision_tree",
      "ds_&_ml_portal",
      "f-regression",
      "gini_impurity",
      "imbalanced_datasets",
      "labelling_data",
      "learning_styles",
      "loss_function",
      "machine_learning_algorithms",
      "neural_network_classification",
      "precision",
      "precision_or_recall",
      "recall",
      "sentence_transformer_workflow",
      "supervised_learning",
      "support_vector_classifier",
      "support_vector_machines",
      "text_classification",
      "typical_output_formats_in_neural_networks",
      "why_removing_outliers_may_improve_regression_but_harm_classification",
      "why_type_1_and_type_2_matter"
    ]
  },
  {
    "category": "ML",
    "filename": "Classification Report",
    "sha": "ea23451e3002a1beed900a60009a48833dd811fc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Classification%20Report.md",
    "text": "The `classification_report` function in `sklearn.metrics` is used to evaluate the performance of a classification model. It provides a summary of key metrics for each class, including precision, recall, F1-score, and support.\n\n## Function Signature\n\n```python\nsklearn.metrics.classification_report(\n    y_true, \n    y_pred, \n    , \n    labels=None, \n    target_names=None, \n    sample_weight=None, \n    digits=2, \n    output_dict=False, \n    zero_division='warn'\n)\n```\nParameters:\n- `y_true`: Array of true labels.\n- `y_pred`: Array of predicted labels.\n- `labels`: (Optional) List of label indices to include in the report.\n- `target_names`: (Optional) List of string names for the labels.\n- `sample_weight`: (Optional) Array of weights for each sample.\n- `digits`: Number of decimal places for formatting output.\n- `output_dict`: If `True`, return output as a dictionary.\n- `zero_division`: Sets the behavior when there is a zero division (e.g., 'warn', 0, 1).\n\n### Metrics Explained\n\n- [[Precision]]: The ratio of correctly predicted positive observations to the total predicted positives. It indicates the quality of the positive class predictions.\n  \n- [[Recall]] (Sensitivity): The ratio of correctly predicted positive observations to all actual positives. It measures the ability of a model to find all relevant cases.\n\n- [[F1 Score]]: The weighted average of precision and recall. It is a better measure than accuracy for imbalanced classes.\n\n- Support: The number of actual occurrences of the class in the specified dataset.\n\n## Resources\n\n[official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).\n\n\n![[Pasted image 20240404163858.png|500]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "classification_report",
    "outlinks": [
      "precision",
      "recall",
      "pasted_image_20240404163858.png",
      "f1_score"
    ],
    "inlinks": [
      "precision"
    ]
  },
  {
    "category": "ML",
    "filename": "Cluster Density",
    "sha": "86ff9fc261778a4a42f50f1017536a75db074494",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Cluster%20Density.md",
    "text": "**Definition:**  \nCluster density refers to how tightly packed the members of a cluster are. Often used to interpret the internal coherence of a cluster.\n\n**Metrics:**\n- Average intra-cluster distance\n- [[DBSCAN]]’s core point density\n- Silhouette score (as indirect proxy) [[Silhouette Analysis]]\n\n**Exploratory Questions:**\n- How does density interact with distance-based [[Clustering]]?\n- How do you define density in non-Euclidean spaces?\n- What assumptions does [[DBSCAN]] make about density?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "cluster_density",
    "outlinks": [
      "silhouette_analysis",
      "clustering",
      "dbscan"
    ],
    "inlinks": [
      "clustering",
      "inertia_k_means_cost_function"
    ]
  },
  {
    "category": "ML",
    "filename": "Cluster Seperation",
    "sha": "b53f73a9b8da0dc97be865c9ebddf23e8ff55a8f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Cluster%20Seperation.md",
    "text": "**Definition:**  \nCluster separation measures how far apart different clusters are from one another. It is a proxy for cluster distinctiveness.\n\n**Common Approaches:**\n- Inter-cluster distance\n- Davies–Bouldin index\n- Silhouette score, [[Silhouette Analysis]] (combines cohesion and separation)\n    \n**Exploratory Questions:**\n- What does high separation mean if internal density is low?\n- How does [[Curse of dimensionality]] affect separation metrics?\n- What clustering algorithms maximize separation explicitly?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "cluster_seperation",
    "outlinks": [
      "silhouette_analysis",
      "curse_of_dimensionality"
    ],
    "inlinks": [
      "clustering"
    ]
  },
  {
    "category": "ML",
    "filename": "Collaborative Filtering",
    "sha": "ee0d9785a02daed34a122d427dcad7a8476f0fca",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Collaborative%20Filtering.md",
    "text": "Collaborative Filtering (CF) is a recommendation technique that predicts a user’s preference for an item based on similarities between users or items. ==It assumes that users with similar tastes in the past will have similar tastes in the future.==\n### How It Works\n\n* Collect user–item interaction data (ratings, clicks, purchases).\n* Predict unknown preferences by leveraging patterns from known interactions.\n\n### Types of Collaborative Filtering\n\n1. User-Based CF\n\n   * Finds users similar to the target user and recommends items they liked.\n   * Example: *“Users like you also enjoyed…”*\n\n1. Item-Based CF\n\n   * Finds items similar to those the user liked and recommends them.\n   * Example: *“Because you watched X, you might like Y.”*",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "recommendation"
    ],
    "normalized_filename": "collaborative_filtering",
    "outlinks": [],
    "inlinks": [
      "recommender_systems"
    ]
  },
  {
    "category": "ML",
    "filename": "Clustering",
    "sha": "a58f3d150cebc6165b1cc566017cc773b92312e6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Clustering.md",
    "text": "Clustering involves grouping a set of data points into subsets or clusters based on inherent patterns or similarities. It is an [[Unsupervised Learning]]technique used for tasks like customer segmentation and [[uncategorised/Outliers|anomalies]] detection. \n\nThe primary goal of clustering is to organize data by grouping similar items.\n\nClustering is a tool for normality detection - not only grouping but finding *non-conforming points*.\n\nThe aim of **clustering** is to **group similar data points together** so that items in the same cluster are more alike to each other than to those in other clusters, without using predefined labels.\n## Methods\n\n- [[K-means]]\n- [[DBSCAN]]\n- [[Hierarchical Clustering]]\n- [[Gaussian Mixture Models]]\n- [[Spectral Clustering]]\n## [[Interpretability]]\n\nImportant to look into:\n- [[Cluster Density]]\n- [[Cluster Seperation]]\n\n [[Feature Scaling]]: Essential for bringing features to the same scale, as clusters may appear distorted without it.\n  ```python\n  from sklearn.preprocessing import scale\n  from sklearn.preprocessing import MinMaxScaler\n  ```\n\nUse clustering to find [[Correlation]] between features. Utilize a [[Dendrograms]] to visualize the relationship between features.\n\n## Applications\n\n- Customer Segmentation: Group customers with similar purchasing behavior or demographics for targeted marketing.\n- Image Segmentation: Group pixels in an image based on color or texture to identify objects or regions.\n- [[Anomaly Detection]]: Identify clusters of normal behavior to detect anomalies that deviate significantly from these clusters.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "clustering",
    "outlinks": [
      "spectral_clustering",
      "correlation",
      "cluster_seperation",
      "dendrograms",
      "unsupervised_learning",
      "interpretability",
      "feature_scaling",
      "cluster_density",
      "anomaly_detection",
      "k-means",
      "uncategorised/outliers",
      "gaussian_mixture_models",
      "hierarchical_clustering",
      "dbscan"
    ],
    "inlinks": [
      "anomaly_detection_with_clustering",
      "choosing_the_number_of_clusters",
      "cluster_density",
      "correlation",
      "covariance_structures",
      "data_modeling",
      "dbscan",
      "ds_&_ml_portal",
      "faiss",
      "feature_selection",
      "gaussian_mixture_models",
      "learning_styles",
      "machine_learning_algorithms",
      "model_parameters",
      "multicollinearity",
      "neural_network_classification",
      "problem_definition",
      "silhouette_analysis",
      "tf-idf",
      "umap",
      "unsupervised_learning",
      "wcss_and_elbow_method"
    ]
  },
  {
    "category": "ML",
    "filename": "Confusion Matrix",
    "sha": "d3510cd916703b2bec4eb51b01b5fee6ce2dbc2c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Confusion%20Matrix.md",
    "text": "A Confusion Matrix is a table used to evaluate the performance of a [[Classification]] model. It provides a detailed breakdown of the model's predictions across different classes, showing the number of true positives, true negatives, false positives, and false negatives.\n## Purpose\n\n- The confusion matrix helps identify where the classifier is making errors, indicating where it is \"confused\" in its predictions.\n## Structure\n\n\n![[Pasted image 20240120215414.png]]\n\n## Structure\n\n- True Positives (TP): Correctly predicted positive instances.\n- False Positives (FP): Incorrectly predicted positive instances (Type 1 error).\n- True Negatives (TN): Correctly predicted negative instances.\n- False Negatives (FN): Incorrectly predicted negative instances (Type 2 error).\n\n## Metrics\n\n- [[Accuracy]]: The overall percentage of correct predictions. In this case, the accuracy is 78.3%.\n- [[Precision]]: The ratio of true positives to all positive predictions (including both TPs and FPs). In this case, the precision for class 0 is 85.7% and the precision for class 1 is 66.4%.\n- [[Recall]]: The ratio of true positives to all actual positive cases (including both TPs and FNs). In this case, the recall for class 0 is 80.6% and the recall for class 1 is 74.1%.\n- [[F1 Score]]: A harmonic average of precision and recall. In this case, the F1-score for class 0 is 83.0% and the F1-score for class 1 is 70.0%.\n- [[Specificity]]\n- [[Recall]]\n\n## Further Examples\n![[Pasted image 20240116205937.png|500]]\n\n![[Pasted image 20240116210541.png|500]]\n\n## Example Code\n\n```python\nfrom sklearn.metrics import confusion_matrix\n\n# Assuming y_train and y_train_pred are your true and predicted labels\nconf_matrix = confusion_matrix(y_train, y_train_pred)\nprint(conf_matrix)\n```\n\nExample Output:\n\n```\narray([[377, 63],\n       [ 91, 180]], dtype=int64)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "confusion_matrix",
    "outlinks": [
      "accuracy",
      "f1_score",
      "recall",
      "pasted_image_20240116205937.png",
      "pasted_image_20240116210541.png",
      "pasted_image_20240120215414.png",
      "precision",
      "specificity",
      "classification"
    ],
    "inlinks": [
      "accuracy",
      "binary_classification",
      "evaluation_metrics",
      "logistic_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Cost Function",
    "sha": "97e80f459882193c890a1512bc836dc19914e25d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Cost%20Function.md",
    "text": "The concept of a cost function is central to [[Model Optimisation]], particularly during the training phase of a model.\n\nA cost function (also called a [[Loss function]] or error function) is a mathematical function used in optimization and machine learning to measure the difference between predicted values and actual values. It quantifies the error or \"cost\" of a model’s predictions. The main goal of most learning algorithms is to minimize this cost function, thereby improving model accuracy.\n\nCommon examples include:\n\n* [[Mean Squared Error]] (MSE) for regression tasks.\n* Cross-Entropy Loss for classification tasks.\n\n#### Relation to [[Loss function]]\n\n* The loss function measures the error for a single data point.\n* The cost function typically aggregates these errors over the entire dataset, often by taking an average.\n* See also: [[Loss versus Cost function]].\n\n#### Parameter Space and Visualization\n\n* Cost functions depend on [[Model Parameters]].\n* Plotting the cost function across the parameter space creates a surface that shows how different parameter values affect the cost.\n* This surface often contains peaks and valleys, making optimization challenging.\n\n#### Connection to [[Gradient Descent]]\n\n* Gradient-based optimization methods use the cost function’s derivatives to iteratively update parameters toward the minimum.\n\n#### Caveats\n\n* The shape of the cost function depends on the dataset.\n* There is often no explicit formula for complex models.\n* Finding the global minimum can be challenging due to local minima and saddle points.\n\n#### Summary\n\n* A cost function is a specific type of objective function used to measure model error.\n* It usually refers to empirical risk (average loss over all training samples).\n\nExample (Linear Regression):\n\n$$\nJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2\n$$\n\n#### Images\n\n\n![[Pasted image 20241216202825.png|500]]\n\n![[Pasted image 20241216202917.png|500]]\n\n#### Related\n\n- [[Reward Function]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "optimisation"
    ],
    "normalized_filename": "cost_function",
    "outlinks": [
      "gradient_descent",
      "reward_function",
      "model_optimisation",
      "model_parameters",
      "loss_versus_cost_function",
      "loss_function",
      "pasted_image_20241216202917.png",
      "pasted_image_20241216202825.png",
      "mean_squared_error"
    ],
    "inlinks": [
      "batch_gradient_descent",
      "ds_&_ml_portal",
      "evaluating_the_effectiveness_of_prompts",
      "gradient_descent",
      "lbfgs",
      "loss_function",
      "loss_versus_cost_function",
      "model_parameters_tuning",
      "momentum",
      "optimisation_techniques",
      "optimising_a_logistic_regression_model",
      "reward_function",
      "why_does_the_adam_optimizer_converge"
    ]
  },
  {
    "category": "ML",
    "filename": "Cost-Sensitive Analysis",
    "sha": "1c2d5aea665ea05a6669485e18bb5f356b1d88d3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Cost-Sensitive%20Analysis.md",
    "text": "Cost-sensitive analysis in machine learning refers to the practice of incorporating the costs associated with different types of errors into the model training and evaluation process. \n\nThis approach is particularly useful in scenarios where the consequences of false positives and false negatives are not equal, and it is important to ==minimize the overall cost== rather than just the error rate.\n\nIn cost-sensitive analysis, the objective is to find the best [[Model Parameters]] that minimize the overall cost, rather than just focusing on minimizing the error rate. This involves considering the costs associated with different types of classification errors, as defined in the cost matrix, and adjusting the model accordingly.\n\nTo achieve this, you can:\n\n1. Optimize Model Parameters: Use techniques like grid search or random search to find the optimal hyperparameters that minimize the total cost. This involves evaluating different combinations of parameters and selecting the ones that result in the lowest cost.\n\n2. Incorporate Cost Information: Modify the learning algorithm to directly incorporate cost information, such as using cost-sensitive decision trees or adjusting class weights in algorithms that support it.\n\n3. Adjust Decision Thresholds: Fine-tune the decision thresholds to balance the trade-off between false positives and false negatives, aiming to minimize the total cost.\n\n### Key Concepts:\n\nCost Matrix: A cost matrix is used to define the costs associated with different types of classification errors. For binary classification, it typically includes:\n  - True Positive Cost (C(TP)): The cost when a positive instance is correctly classified.\n  - False Positive Cost (C(FP)): The cost when a negative instance is incorrectly classified as positive.\n  - True Negative Cost (C(TN)): The cost when a negative instance is correctly classified.\n  - False Negative Cost (C(FN)): The cost when a positive instance is incorrectly classified as negative.\n\nObjective: The goal of cost-sensitive analysis is to minimize the total cost, which is calculated based on the cost matrix and the confusion matrix of the model's predictions.\n### Approaches to Cost-Sensitive Learning:\n\n1. Cost-Sensitive Algorithms: Modify existing algorithms to incorporate cost information directly into the learning process. For example, decision trees can be adapted to consider misclassification costs when splitting nodes.\n\n2. Reweighting Instances: Adjust the weights of instances in the training data based on their associated costs. This can be done by assigning higher weights to instances that are more costly to misclassify.\n\n3. Threshold Adjustment: Modify the decision threshold of a classifier to account for different costs. For example, in a binary classifier, the threshold for classifying an instance as positive can be adjusted to minimize the expected cost.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "cost-sensitive_analysis",
    "outlinks": [
      "model_parameters"
    ],
    "inlinks": [
      "choosing_a_threshold",
      "determining_threshold_values",
      "imbalanced_datasets"
    ]
  },
  {
    "category": "ML",
    "filename": "Cross Entropy",
    "sha": "9c9946de830616ab50fc163f420d20983fad23dc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Cross%20Entropy.md",
    "text": "Cross entropy is a [[Loss function]] used in [[Classification]] tasks, particularly for [[categorical data]]. The cross entropy loss function is particularly effective for multi-class classification problems, where the goal is to assign an input to one of several categories. \n\n==Cross entropy measures confidence.==\n\nCross entropy works by measuring the (difference/loss) ==dissimilarity between two probability distributions==: the true distribution (actual class labels) and the predicted distribution (model's output probabilities). \n\nFit of Predictions:\n- A low cross entropy loss means the predicted probabilities are close to the true labels (e.g., assigning high probability to the correct class).\n- A high loss indicates significant divergence, meaning the model's predictions are inaccurate or uncertain.\n\nBy minimizing cross entropy, the model learns to produce probability distributions that closely match the true class distributions, thereby improving its classification ==accuracy==.\n\n1. Probability Distributions: In a classification task, the model outputs a probability distribution over the possible classes for each input. For example, in a three-class problem, the model might output probabilities like [0.7, 0.2, 0.1] for classes A, B, and C, respectively.\n\n2. True Labels: The true class label is represented as a one-hot encoded vector. If the true class is A, the vector would be [1, 0, 0].\n\n3. Cross Entropy Calculation calculates the loss by comparing the predicted probabilities with the true labels. The formula for cross entropy loss $L$ for a single instance is:\n\n   $$ L = -\\sum_{i=1}^{N} y_i \\log(p_i)$$\n\n   where:\n   - $N$ is the number of classes.\n   - $y_i$ is the true label (1 if the class is the true class, 0 otherwise).\n   - $p_i$ is the predicted probability for class $i$.\n\n2. Interpretation: The cross entropy loss increases as the predicted probability diverges from the actual label. If the model assigns a high probability to the correct class, the loss is low. Conversely, if the model assigns a low probability to the correct class, the loss is high.\n\n3. Optimization: During training, the model's parameters are adjusted to minimize the cross entropy loss across all training examples. This process helps the model improve its predictions over time.\n\n## Where is it used\n\nCross entropy is widely used in classification for several reasons:\n\nProbabilistic Modeling:\n    - It directly aligns with the goals of probabilistic classifiers, as it measures how well the predicted probability distribution matches the true distribution.\n    \nFocus on Confidence:\n    - Encourages the model to assign higher probabilities to the correct classes, improving not just accuracy but also confidence in predictions.\n\nOptimization Efficiency:\n    - Cross entropy is smooth and convex for logistic regression-like models, enabling efficient gradient-based optimization.\n\nMulti-Class Support:\n    - Works seamlessly in multi-class scenarios where the true labels are one-hot encoded and predictions are probability distributions.\n\n### Implementation \n\nIn [[ML_Tools]] see: \n- [[Cross_Entropy_Single.py]]\n- [[Cross_Entropy.py]]\n- [[Cross_Entropy_Net.py]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "optimisation"
    ],
    "normalized_filename": "cross_entropy",
    "outlinks": [
      "cross_entropy_net.py",
      "cross_entropy.py",
      "loss_function",
      "cross_entropy_single.py",
      "ml_tools",
      "classification",
      "categorical_data"
    ],
    "inlinks": [
      "cross_entropy.py",
      "decision_tree",
      "ds_&_ml_portal",
      "gini_impurity_vs_cross_entropy",
      "language_model_output_optimisation",
      "loss_function",
      "neural_scaling_laws",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ]
  },
  {
    "category": "ML",
    "filename": "Customer Growth Modeling",
    "sha": "e82b8e4b7d40bf61e65f398e22263ef81bc192f0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Customer%20Growth%20Modeling.md",
    "text": "Both **Logistic** and **Gompertz models** are commonly used to model **growth that saturates** over time, such as: Customer adoption, Product uptake, Population growth\n\nThey are **S-shaped (sigmoidal) curves** that capture initial exponential growth followed by slowing as a **saturation point** is approached.\n\n- [[Logistic Model Curve]]\n- [[Gompertz Model]]\n\nSummary\n- Both models are **sigmoidal growth models** suitable for customer growth or population dynamics  \n- The **key difference** is symmetry vs asymmetry of growth  \n- **K** (carrying capacity) is the critical parameter to define saturation  \n- Early data can help estimate growth rate and K, guiding which model to choose\n## Saturation Effects\n\n- Both models incorporate a **carrying capacity (K)** that represents the maximum attainable size  \n- **Saturation occurs** as growth slows when approaching K  \n- Choosing K is critical: it sets the **upper bound** of your forecast  \n- If K is unknown, it can be **estimated from early growth trends** or inferred from historical analogues\n\n## Choosing Between Logistic and Gompertz\n\n| Aspect | Logistic | Gompertz |\n|--------|---------|----------|\n| Curve symmetry | Symmetric | Asymmetric |\n| Early growth | Faster | Slower, more gradual |\n| Inflection point | At 50% of K | Before 50% of K |\n| Use case | Uniform adoption | Cautious early adoption / biological growth |\n| Interpretability | Simple | Slightly more flexible |\n\n**Rule of Thumb:**  \n- Pick **Logistic** if growth is roughly symmetric and you expect early acceleration.  \n- Pick **Gompertz** if early growth is slow and adoption accelerates later.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "customer_growth",
      "forecasting",
      "growth_models",
      "ml",
      "time_series"
    ],
    "normalized_filename": "customer_growth_modeling",
    "outlinks": [
      "gompertz_model",
      "logistic_model_curve"
    ],
    "inlinks": [
      "gompertz_model",
      "logistic_model_curve"
    ]
  },
  {
    "category": "ML",
    "filename": "DBSCAN",
    "sha": "034af0efdd4c3e67fc9f9ceaf99bd3aaa65ead6b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/DBSCAN.md",
    "text": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a [[Clustering]] algorithm that groups together data points ==based on density==. It is particularly useful when K-means doesn't work well, such as in datasets with complex shapes or when there are outliers.\n\n- Used when [[K-means]] doesn't work: DBSCAN handles datasets with ==irregular cluster shapes== and is not sensitive to outliers like K-means.\n- When you have nesting of clusters: It can identify clusters of varying shapes and sizes without needing to predefine the number of clusters, unlike K-means.\n- Groups core points to make clusters: DBSCAN identifies core points, which have many nearby points, and groups them together.\n- Can identify [[uncategorised/Outliers]]: It detects noise points (outliers) that don't belong to any cluster. Points in low-density regions are identified as anomalies.\n\nSteps:\n  1. Cluster data points based on neighborhood density.\n  2. Identify points that do not belong to any dense cluster (noise) as anomalies.\n\n### Python Example:\n\n```python\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n# Create sample data\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nclusters = dbscan.fit_predict(X)\n\n# Plot results\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='plasma')\nplt.show()\n```\n\nThis will cluster the data and visualize it, highlighting core points and marking outliers as separate clusters.\n## Sources\n1. [hex.tech - When and Why To Choose Density-Based Methods](https://hex.tech/blog/comparing-density-based-methods/#:~:text=DBSCAN%20is%20a%20density%2Dbased)\n2. [newhorizons.com - DBSCAN vs. K-Means: A Guide in Python](https://www.newhorizons.com/resources/blog/dbscan-vs-kmeans-a-guide-in-python)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "ML_Tools"
    ],
    "normalized_filename": "dbscan",
    "outlinks": [
      "k-means",
      "uncategorised/outliers",
      "clustering"
    ],
    "inlinks": [
      "anomaly_detection_with_clustering",
      "cluster_density",
      "clustering",
      "isolated_forest",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Data Selection in ML",
    "sha": "ff8e575de0214f9ead40366f0aaca245eb5ed37e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Data%20Selection%20in%20ML.md",
    "text": "When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[[Model Optimisation]] and the insights you can derive from it. Here are key factors to consider:\n\n1. Relevance:\n   - Ensure that the features (input variables) you select are relevant to the problem you are trying to solve. Irrelevant features can introduce noise and reduce model accuracy.\n\n[[Data Quality]]\n   - Assess the quality of the data, including checking for missing values, outliers, and errors. Poor quality data can lead to inaccurate models.\n\n3. Quantity:\n   - Consider the size of your dataset. More data can lead to better models, but it also requires more computational resources. Ensure you have enough data to train your model effectively.\n\n4. Balance: [[Imbalanced Datasets]]\n   - Check for [[Imbalanced Datasets|class imbalance]] in classification problems. An imbalanced dataset can bias the model towards the majority class. Techniques like resampling, synthetic data generation, or using different evaluation metrics can help address this.\n\n5. Feature Distribution: [[Distributions]]\n   - Analyze the distribution of your features. Features with skewed [[Distributions]] may need transformation ([[Data Transformation]]) (e.g., [[Log transformation]]) to improve model performance.\n\n6. [[Correlation]]:\n   - Examine the correlation between features. Highly correlated features can lead to [[Multicollinearity]], which can affect model stability and interpretability. Consider removing or combining correlated features.\n\n7. Dimensionality: [[Dimensionality Reduction]]\n   - High-dimensional data can lead to overfitting. Techniques like [[Feature Selection]], dimensionality reduction (e.g., PCA), or regularization can help manage this.\n\n8. Temporal Considerations:\n- For time series data, ensure that the temporal order is maintained. Avoid data leakage by ensuring that future information is not used in training.\n\n9. Domain Knowledge:\n   - Leverage domain expertise to select features that are known to be important for the problem. This can guide feature engineering and selection.\n\n10. Data Leakage:\n  - Be cautious of [[Data Leakage]], where information from the test set is inadvertently used in training. This can lead to overly optimistic performance estimates.\n\n11. Scalability:\n- Consider the scalability of your data selection process. As datasets grow, ensure that your methods can handle larger volumes efficiently.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "selection"
    ],
    "normalized_filename": "data_selection_in_ml",
    "outlinks": [
      "data_leakage",
      "correlation",
      "data_transformation",
      "model_optimisation",
      "log_transformation",
      "multicollinearity",
      "distributions",
      "dimensionality_reduction",
      "imbalanced_datasets",
      "feature_selection",
      "data_quality"
    ],
    "inlinks": [
      "data_selection"
    ]
  },
  {
    "category": "ML",
    "filename": "Data Transformation in Machine Learning",
    "sha": "35dfd6e2314451074f254b7306cb47253be080fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Data%20Transformation%20in%20Machine%20Learning.md",
    "text": "Transforming raw data into a meaningful format is necessary for building effective models.  \n\n- [[Supervised Learning]]: Annotating datasets with correct labels (e.g., labeling images of apples vs. other fruits).  \n- Manual & Automated Labeling: Using human annotators or leveraging existing labeled datasets (e.g., Google reCAPTCHA).  \n- Feature Scaling & Encoding: Applying normalization and encoding to categorical variables.  \n- [[Encoding Categorical Variables]]: Converting categorical data into numerical format for machine learning models.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "transformation"
    ],
    "normalized_filename": "data_transformation_in_machine_learning",
    "outlinks": [
      "supervised_learning",
      "encoding_categorical_variables"
    ],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "ML",
    "filename": "Decision Theory",
    "sha": "95d4ae41a365b6fc329df8057ee12680b5957637",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Decision%20Theory.md",
    "text": "Related:\n- [[Model Optimisation]]\n- [[Secretary Problem]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "optimisation"
    ],
    "normalized_filename": "decision_theory",
    "outlinks": [
      "model_optimisation",
      "secretary_problem"
    ],
    "inlinks": [
      "secretary_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "Decision Tree",
    "sha": "84695bdd3b379361c383cc219ea0eaec720fe9a8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Decision%20Tree.md",
    "text": "A [[Decision Tree]] is a [[Supervised Learning]] algorithm that predicts a target variable from input features by recursively splitting data into subsets. The model takes a tree-like form where:\n\n* Internal nodes represent decisions based on features.\n* Branches represent outcomes of those decisions.\n* Leaf nodes provide final predictions.\n\nThe algorithm evaluates possible splits using measures such as [[Gini Impurity]] or [[Cross Entropy]] ([[Gini Impurity vs Cross Entropy]]), choosing the one that produces the ==lowest impurity==. For classification, a leaf assigns the majority class; for regression, it outputs the mean value.\n\nAlso see: [[CART]]\n\n### Key Concepts\n\n1. Objective: Predict a target variable from input features.\n2. Splitting: Select features that best divide the data into homogeneous subsets.\n3. Impurity: Evaluate splits with metrics like [[Gini Impurity]] or entropy.\n4. Purity: A node is pure if all samples belong to the same class.\n5. Leaf Node Output: Provides a prediction from the samples in the node.\n6. [[Overfitting]]: Deep trees may overfit; mitigate with [[pruning]] or depth limits.\n7. [[Cross Validation]]: Ensures the model generalizes well to new data.\n\n### Building Process\n\n1. Root Node: Begin by choosing the best feature and split based on impurity (for [[Classification]]) or variance reduction (for regression).\n2. Recursive Partitioning: Apply the same splitting process to each child node until stopping criteria are met (e.g., depth, minimum samples, no significant purity gain).\n3. Leaf Nodes: When splitting stops, assign the prediction at the leaf. See [[Distributions in Decision Tree Leaves]].\n\n### Refinement\n\nPruning reduces complexity and improves generalization:\n* Pre-pruning: Stop growth early using constraints such as maximum depth or minimum impurity decrease.\n* Post-pruning: Grow the full tree, then prune back based on performance metrics.\n\n![[Pasted image 20240404154526.png|500]]\n\n## Other Considerations\n\n### [[Hyperparameter]] of Decision Trees\n\nCan use [[GridSeachCv]] to pick the best paramaters.\n\n| Parameter           | Purpose                           | Effect                  | Example                                                              |\n| ------------------- | --------------------------------- | ----------------------- | -------------------------------------------------------------------- |\n| `criterion`         | Splitting criteria                | Impacts decision logic. | `criterion='gini'` or `criterion='entropy'`                          |\n| `max_depth`         | Maximum tree depth                | Prevents overfitting.   | `max_depth=5` limits the tree depth to 5.                            |\n| `min_samples_split` | Min samples to split a node       | Limits tree growth.     | `min_samples_split=10` requires at least 10 samples to split a node. |\n| `min_samples_leaf`  | Min samples at leaf node          | Reduces overfitting.    | `min_samples_leaf=5` ensures every leaf has at least 5 samples.      |\n| `max_features`      | Features considered for splitting | Adds randomness.        | `max_features='sqrt'` or `max_features=3`.                           |\n| `max_leaf_nodes`    | Max leaf nodes allowed            | Reduces overfitting.    | `max_leaf_nodes=20` caps the tree at 20 leaves.                      |\n| `class_weight`      | Adjusts for imbalanced data       | Improves fairness.      | `class_weight='balanced'` or `class_weight={0:1, 1:2}`.              |\n| `ccp_alpha`         | Pruning parameter                 | Simplifies tree.        | `ccp_alpha=0.01` prunes weak splits based on complexity.             |\n### Advantages and Disadvantages of Decision Trees\n\nAdvantages:\n- Simple and [[Interpretability|interpretable]] model.\n- Minimal data preparation required.\n- Transparent decision-making process.\n\nDisadvantages:\n- Prone to overfitting, especially with complex datasets.\n- Sensitive to small changes in data.\n- Can become complex with many features.\n- [[Decision Trees are Fragile]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "regressor"
    ],
    "normalized_filename": "decision_tree",
    "outlinks": [
      "gini_impurity",
      "hyperparameter",
      "pasted_image_20240404154526.png",
      "gridseachcv",
      "supervised_learning",
      "pruning",
      "decision_trees_are_fragile",
      "distributions_in_decision_tree_leaves",
      "overfitting",
      "cross_validation",
      "decision_tree",
      "cart",
      "interpretability",
      "gini_impurity_vs_cross_entropy",
      "classification",
      "cross_entropy"
    ],
    "inlinks": [
      "ada_boosting",
      "classification",
      "data_understanding",
      "decision_tree",
      "decision_trees_are_fragile",
      "ds_&_ml_portal",
      "embedded_methods",
      "feature_scaling",
      "gini_impurity",
      "gradient_boosted_trees",
      "gradient_boosting",
      "gradient_boosting_regressor",
      "machine_learning_algorithms",
      "model_parameters",
      "random_forest",
      "regularisation_of_tree_based_models",
      "supervised_learning",
      "weak_learners",
      "working_with_smes",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Decision Trees are Fragile",
    "sha": "2a81d09aa2901a4c7d46b0abf827839d9c41b072",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Decision%20Trees%20are%20Fragile.md",
    "text": "[[Decision Tree]]s are considered ==fragile== because small changes in the training data can lead to large changes in the tree structure and predictions.\n\n### Why this happens\n\n1. Greedy splitting\n\n   * Trees build by making locally optimal splits at each node.\n   * A slight change in data (e.g., one sample or a small noise) can change which split is chosen, altering the entire subtree downstream.\n\n1. High variance [[Variance in ML]]\n\n   * Decision trees have low bias but high variance.\n   * They overfit easily to noise or outliers unless pruned or regularized.\n\n1. Hierarchical structure\n\n   * Early splits strongly influence the rest of the tree.\n   * A different root split due to minor changes cascades into a completely different structure.\n\n1. Sensitivity to outliers\n\n   * Extreme values can dominate split selection, further increasing instability.\n\n### Consequence\n\n* Different training sets produce very different trees → poor generalization.\n* This is why [[Model Ensemble|ensemble]] methods like [[Random Forest|Random Forest ]]and [[Gradient Boosted Trees]] are widely used; they reduce variance by combining multiple trees.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifer",
      "explainability",
      "model"
    ],
    "normalized_filename": "decision_trees_are_fragile",
    "outlinks": [
      "variance_in_ml",
      "gradient_boosted_trees",
      "random_forest",
      "decision_tree",
      "model_ensemble"
    ],
    "inlinks": [
      "decision_tree"
    ]
  },
  {
    "category": "ML",
    "filename": "Deep Learning Frameworks",
    "sha": "0a4934bdd55f364f418e4a1871708ecc255ae352",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Deep%20Learning%20Frameworks.md",
    "text": "[Watch Overview Video](https://www.youtube.com/watch?v=MDP9FfsNx60)\n\n[[Tensorflow]]\n\n[[Scikit-Learn]]\n\n[[Keras]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "python"
    ],
    "type": null,
    "normalized_filename": "deep_learning_frameworks",
    "outlinks": [
      "tensorflow",
      "keras",
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Deep Q-Learning",
    "sha": "0d3e4d6060fcef4e56c2e42120e001836b6dbc18",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Deep%20Q-Learning.md",
    "text": "Deep [[Q-Learning]] is a type of [[Reinforcement learning]] algorithm that combines Q-Learning with [[Neural network]]. Necessary when Q-Table grows too large.\n\nUpdates the weights in the model.\n\n![[Pasted image 20250220133838.png]]\n\n## Key Concepts\n\n### Target Network\n\n- **Purpose**: The target network is used to stabilize the training process in Deep Q-Learning.\n- **When is it needed?**: It is needed when updating the Q-values to prevent oscillations and divergence during training.\n- **How it works**: The target network is a copy of the main Q-network and is used to generate target Q-values. It is updated less frequently than the main network, often using a technique called a \"soft update,\" where the target network is slowly adjusted towards the main network over time.\n\n### Experience Replay\n\n- **Purpose**: Experience replay is used to break the correlation between consecutive experiences, which can lead to inefficient learning and instability.\n- **Issue it resolves**: When an agent learns from sequential experiences, the strong correlations between them can cause problems such as oscillations and instability in learning.\n- **How it works**: \n  - Experiences (state, action, reward, next state) are stored in a memory buffer.\n  - During training, random mini-batches of experiences are sampled from this buffer to update the network.\n  - This random sampling helps to generate uncorrelated experiences, improving stability and efficiency.\n  - It also allows the agent to reuse experiences for multiple updates, increasing data efficiency.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "deep_q-learning",
    "outlinks": [
      "pasted_image_20250220133838.png",
      "neural_network",
      "q-learning",
      "reinforcement_learning"
    ],
    "inlinks": [
      "reinforcement_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Dendrograms",
    "sha": "6185860cb23ea18b0064fe0a43395cd2e2c3da4f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dendrograms.md",
    "text": "Dendrograms show **close** vectors is the data where taken as a vector.\n\nCan tell which ==features are the most similar== with [[Dendrograms]]\n\n![[Pasted image 20240405173403.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "visualization"
    ],
    "normalized_filename": "dendrograms",
    "outlinks": [
      "pasted_image_20240405173403.png",
      "dendrograms"
    ],
    "inlinks": [
      "clustering",
      "clustermap",
      "dendrograms",
      "dynamic_time_warping"
    ]
  },
  {
    "category": "ML",
    "filename": "Determining Threshold Values",
    "sha": "9ca224fe7db2d15845aaebad67c094a0332201ac",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Determining%20Threshold%20Values.md",
    "text": "In [[Binary Classification]] problems, a threshold value is used to convert predicted probabilities into discrete class labels. The choice of threshold significantly impacts the model's performance, affecting [[Evaluation Metrics]].\n\nImportant Considerations:\n* [[Imbalanced Datasets|Class Imbalance]]: If the classes are imbalanced, the choice of threshold can be significantly affected. Techniques like oversampling, undersampling, or using weighted loss functions can help mitigate the impact of class imbalance.\n* [[Data Quality]]: The quality of the training data can also influence the choice of threshold. If the data is noisy or contains outliers, the chosen values may not be optimal.\n* Choose [[Evaluation Metrics]] that are appropriate for the specific problem and the desired trade-off between different types of errors.\n\nHere are common methods for determining the optimal threshold value:\n- Receiver Operating Characteristic (ROC) Curve Analysis : [[ROC (Receiver Operating Characteristic)]]\n- [[Precision-Recall Curve]] Analysis\n- [[Cost-Sensitive Analysis]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "selection"
    ],
    "normalized_filename": "determining_threshold_values",
    "outlinks": [
      "cost-sensitive_analysis",
      "binary_classification",
      "roc_(receiver_operating_characteristic)",
      "precision-recall_curve",
      "imbalanced_datasets",
      "data_quality",
      "evaluation_metrics"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Dimension Table",
    "sha": "bbb5113bd5572ebf7dff382b9f0aedd78fb65799",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dimension%20Table.md",
    "text": "A dimension table is a key component of a [[Star Schema]] or snowflake schema in a data warehouse. It provides descriptive attributes (or dimensions) related to the [[Facts]] stored in a fact table.\n\nThey provide the context and descriptive information necessary for analyzing the quantitative data stored in fact tables (e.g., product names, customer demographics, time periods).\n\n1. **Descriptive Attributes**: Dimension tables contain qualitative data that describe the entities involved in the business process. For example, a product dimension table might include attributes such as product name, category, brand, and manufacturer.\n\n2. **Primary Key**: Each dimension table has a primary key that uniquely identifies each record in the table. This primary key is used as a foreign key in the [[Fact Table]] to establish relationships between the two.\n\n3. **Hierarchies**: Dimension tables often include hierarchies that allow for data to be analyzed at different levels of granularity. For example, a time dimension might include attributes for year, quarter, month, and day, allowing users to drill down or roll up in their analysis.\n\n4. **Smaller Size**: Compared to fact tables, dimension tables are typically smaller in size, as they contain descriptive data rather than large volumes of transactional data.\n\n5. **Static Data**: Dimension tables usually contain relatively static data that does not change frequently, such as product details or customer information. However, they can be updated as needed to reflect changes in the business.\n\n6. **Support for Filtering and Grouping**: Dimension tables enable users to filter and group data in reports and analyses. For example, users can analyze sales data by different dimensions such as time, geography, or product category.\n\nExamples\n  - **TimeDimension**: Contains information about the time period.\n    - Columns: `DateKey`, `Year`, `Quarter`, `Month`, `Day`\n  - **ProductDimension**: Contains product details.\n    - Columns: `ProductKey`, `ProductName`, `ProductCategory`\n  - **RegionDimension**: Contains regional information.\n    - Columns: `RegionKey`, `RegionName`, `Country`\n\n\n\n\n\n[[Dimension Table]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "dimension_table",
    "outlinks": [
      "dimension_table",
      "star_schema",
      "facts",
      "fact_table"
    ],
    "inlinks": [
      "components_of_the_database",
      "dimension_table",
      "dimensional_modelling",
      "fact_table",
      "slowly_changing_dimension"
    ]
  },
  {
    "category": "ML",
    "filename": "Dimensional Modelling",
    "sha": "530438c7fbcf3a55a18b222299ff8ac3e14b8543",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dimensional%20Modelling.md",
    "text": "Dimensional modeling is a design technique used in [[Data Warehouse]]used to structure data for efficient ==retrieval== and analysis. It is particularly well-suited for organizing data in a way that supports complex [[Querying]] and reporting, making it easier for business users to understand and interact with the data. \n\nDimensional modeling is a technique in building data warehouses and is associated with methodologies like the ==Kimball== approach, which emphasizes the use of [[Star Schema]] and the importance of understanding business processes and user requirements.\n\nKey Concepts in Dimensional Modeling\n- [[Fact Table]] & [[Facts]]\n- [[Dimension Table]]\n- [[Grain]]\n\nBenefits of Dimensional Modeling: [[Performance Dimensions]]\n\n\n\n\n\n[[Dimensional Modelling]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "dimensional_modelling",
    "outlinks": [
      "dimension_table",
      "facts",
      "dimensional_modelling",
      "querying",
      "star_schema",
      "grain",
      "fact_table",
      "performance_dimensions",
      "data_warehouse"
    ],
    "inlinks": [
      "dimensional_modelling",
      "granularity"
    ]
  },
  {
    "category": "ML",
    "filename": "Dimensionality Reduction",
    "sha": "919376586da61a07765859bb55b98aa278377082",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dimensionality%20Reduction.md",
    "text": "Dimensionality reduction is a step in the [[Preprocessing]] phase of machine learning that helps simplify models, enhance interpretability, and improve computational efficiency.\n\nIts a technique used to reduce the number of input variables (features) in a dataset while retaining as much information as possible. This process is essential for several reasons:\n\n1. **Improves Model Performance**: Reducing the number of features can help improve the performance of machine learning models by minimizing overfitting and reducing noise.\n\n2. **Enhances Visualization**: It allows for easier [[Data Visualisation]] of high-dimensional data by projecting it into lower dimensions (e.g., 2D or 3D).\n\n3. **Reduces Computational Cost**: Fewer features mean less computational power and time required for training models.\n\n### Common Techniques\n- **Principal Component Analysis ([[Principal Component Analysis]])**: A statistical method that transforms the data into a new coordinate system, where the greatest variance by any projection lies on the first coordinate ==(principal component/orthogonal components )==, the second greatest variance on the second coordinate, and so on.\n\n- **t-Distributed Stochastic Neighbor Embedding ([[t-SNE]])**: A technique particularly well-suited for visualizing high-dimensional data by reducing it to two or three dimensions while preserving the local structure of the data. t-SNE is a non-linear technique used for visualization and dimensionality reduction by preserving pairwise similarities between data points, making it suitable for exploring high-dimensional data.\n\n- [[Linear Discriminant Analysis]] method used for both classification and dimensionality reduction, which finds a linear combination of features that best separates two or more classes.\n\n### [[Curse of dimensionality]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "process",
      "visualization"
    ],
    "normalized_filename": "dimensionality_reduction",
    "outlinks": [
      "t-sne",
      "data_visualisation",
      "preprocessing",
      "principal_component_analysis",
      "linear_discriminant_analysis",
      "curse_of_dimensionality"
    ],
    "inlinks": [
      "addressing_multicollinearity",
      "curse_of_dimensionality",
      "data_reduction",
      "data_selection_in_ml",
      "ds_&_ml_portal",
      "dynamic_time_warping",
      "evaluate_embedding_methods",
      "factor_analysis",
      "feature_engineering",
      "feature_extraction",
      "feature_selection",
      "latent_dirichlet_allocation",
      "learning_styles",
      "machine_learning_algorithms",
      "manifold_learning",
      "preprocessing",
      "principal_component_analysis",
      "t-sne",
      "umap",
      "unsupervised_learning",
      "vector_embedding"
    ]
  },
  {
    "category": "ML",
    "filename": "Dimensions",
    "sha": "be6b1b471c9d1073b4df2b30d874c8e45395f80a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dimensions.md",
    "text": "Dimensions are the categorical buckets that can be used to segment, filter, or group—such as sales amount region, city, product, color, and distribution channel. \n\nTraditionally known from [[OLAP|OLAP]]cubes with Bus Matrixes, and [Dimensional Modeling](Dimensional%20Modelling.md). \n\nThey provide context to the [[Facts]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "dimensions",
    "outlinks": [
      "olap",
      "facts"
    ],
    "inlinks": [
      "granularity"
    ]
  },
  {
    "category": "ML",
    "filename": "Distributions in Decision Tree Leaves",
    "sha": "65780c0f52cc38f72c6efb1642b7f7031474fbaa",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Distributions%20in%20Decision%20Tree%20Leaves.md",
    "text": "In decision trees, each leaf node typically contains the outcome for all training examples that fall into that leaf. This outcome can be represented in two ways:\n\n#### 1. Single Predicted Value\n\n* Classification: The leaf predicts the majority class (e.g., if 60% of samples are class A, predict A).\n* Regression: The leaf predicts the mean of the target values in that leaf.\n\n#### 2. Distribution of Target Values\n\nInstead of storing just a single prediction, the leaf can store the distribution of the target variable:\n\n* Classification: Store class probabilities.\n  Example: If a leaf has 100 samples -> 60 class 1, 40 class 2 -> Distribution = {class 1: 0.6, class 2: 0.4}.\n* Regression: Store a histogram or density estimate of the continuous values rather than just their mean.\n\n#### Why use distributions?\n\n* Enables probabilistic predictions (e.g., predict class probabilities, not just hard labels).\n* Provides uncertainty estimates, useful for Bayesian methods and risk-sensitive decisions.\n* Improves interpretability by showing the variability in outcomes for samples reaching the same leaf.\n\nThis method is commonly applied in:\n\n* Random Forests and Gradient Boosted Trees (which use class probabilities for classification).\n* Probabilistic decision tree models for uncertainty-aware predictions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "explainability"
    ],
    "normalized_filename": "distributions_in_decision_tree_leaves",
    "outlinks": [],
    "inlinks": [
      "decision_tree"
    ]
  },
  {
    "category": "ML",
    "filename": "Dropout",
    "sha": "283134ef3a28f33439f3a05161304364f4f830d0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dropout.md",
    "text": "**Dropout** is a [[Regularisation]] technique used in [[Neural network]] training to prevent [[Overfitting]]. It works by randomly dropping units (neurons) during training, which helps the network to not rely too heavily on any single neuron.\n\nPurpose\n- The main goal of dropout is to improve the generalization of the model by reducing over-reliance on specific neurons. This encourages the network to learn more robust features that are useful in different contexts.\n\nHow It Works\n- During each training iteration, a subset of neurons is randomly selected and ignored (dropped out). This means their contribution to the activation of downstream neurons is temporarily removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n\nImplementation\n```python\nfrom tensorflow.keras.layers import Dropout\n# Add a dropout layer with a dropout rate of 0.5\n# The dropout rate (e.g., 0.5) specifies the fraction of neurons to drop during training. A rate of 0.5 means that half of the neurons will be dropped at each iteration.\nDropout(0.5)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "optimisation"
    ],
    "normalized_filename": "dropout",
    "outlinks": [
      "regularisation",
      "neural_network",
      "overfitting"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network"
    ]
  },
  {
    "category": "ML",
    "filename": "Dummy variable trap",
    "sha": "0e5ee92e29a3d4d096fc674a6d62f566172b4010",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Dummy%20variable%20trap.md",
    "text": "### Key Takeaways:\n\n- The dummy variable trap occurs due to [[Multicollinearity]], where ==one dummy variable can be perfectly predicted from others.==\n- Dropping one dummy variable avoids this issue and ensures that the model has a reference category against which the other categories are compared.\n- This approach leads to a well-conditioned model and allows for more interpretable regression coefficients.\n### Dummy Variable Trap\n\nThe dummy variable trap refers to a scenario in which there is multicollinearity in your dataset when you create dummy variables for categorical features. Specifically, it occurs when one of the dummy variables in a set of dummy variables can be perfectly predicted by a linear combination of the others.\n\nThis situation arises when you create dummy variables for a categorical feature with $n$ categories, leading to $n$ binary columns. However, if you include all $n$ dummy variables in your regression model, ==the model will face redundancy because knowing the values of $n-1$ dummy variables will already give you the value of the last one (since all the categories must add up to 1 for each observation)==. This results in perfect multicollinearity.\n\n### Why Do We Need to Drop One of the Dummy Variables?\n\nIn a regression model, multicollinearity can cause problems because it makes the estimation of coefficients unstable, leading to unreliable statistical inferences. Specifically, the model can't determine which of the correlated variables is truly responsible for explaining the variation in the target variable.\n### Example:\n\nSuppose you have a categorical feature `town` with three categories: `West Windsor`, `Robbinsville`, and `Princeton`. When you apply [[One-hot encoding]], you create three dummy variables:\n\n|town|West Windsor|Robbinsville|Princeton|\n|---|---|---|---|\n|West Windsor|1|0|0|\n|Robbinsville|0|1|0|\n|Princeton|0|0|1|\n\nNow, if you include all three dummy variables in a linear regression model, the columns `West Windsor`, `Robbinsville`, and `Princeton` will be perfectly correlated. For example, if the values of `West Windsor` and `Robbinsville` are both 0, then `Princeton` must be 1, and vice versa.\n\nThis creates multicollinearity because you can predict one dummy variable perfectly by knowing the others. Hence, you need to drop one of the dummy variables—usually, you drop one category, which becomes the reference group.\n\nIf you drop the `West Windsor` dummy column, your table would look like this:\n\n|town|Robbinsville|Princeton|\n|---|---|---|\n|West Windsor|0|0|\n|Robbinsville|1|0|\n|Princeton|0|1|\n\nNow, your model will use the `West Windsor` category as the baseline. The coefficients of `Robbinsville` and `Princeton` in the regression model will indicate how much higher or lower their prices are compared to `West Windsor`.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "modeling",
      "preprocessing"
    ],
    "normalized_filename": "dummy_variable_trap",
    "outlinks": [
      "one-hot_encoding",
      "multicollinearity"
    ],
    "inlinks": [
      "encoding_categorical_variables",
      "one-hot_encoding"
    ]
  },
  {
    "category": "ML",
    "filename": "Edge ML",
    "sha": "e0b39c05af98e0df37040dd2bbe03af78e49d25e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Edge%20ML.md",
    "text": "**Edge ML** refers to deploying machine learning models directly on edge devices, such as IoT sensors, smartphones, or embedded systems, instead of relying on cloud-based processing. This is crucial in scenarios requiring low-latency, real-time decision-making, or environments with limited connectivity.\n\n#### Key Characteristics of Edge ML Models:\n\n1. **Low Latency**\n   - Models running at the edge can make real-time decisions with minimal delay. This is critical in applications like autonomous vehicles, industrial automation, or real-time health monitoring, where delays can have serious consequences.\n\n2. **Reduced Bandwidth Usage**\n   - By processing data locally, edge ML models reduce the need to send large amounts of data to the cloud for analysis. This is particularly valuable in environments with limited or expensive connectivity (e.g., remote locations or bandwidth-constrained networks).\n   \n3. **Privacy Preservation**\n   - Processing sensitive data on-device, instead of sending it to the cloud, enhances privacy and reduces the risk of data breaches. This is important in healthcare, financial services, or any scenario involving personal data.\n\n4. **Energy Efficiency**\n   - Edge devices often have limited power resources. As a result, models deployed at the edge need to be optimized for low energy consumption, ensuring they can operate for extended periods without requiring frequent battery replacements or recharging.\n\n#### Common Applications of Edge ML:\n   \n1. **Autonomous Systems (e.g., Drones, Robots, Vehicles)**\n   - Autonomous systems rely on real-time decision-making for navigation, obstacle detection, and control. Edge ML allows these systems to react instantaneously to their surroundings without depending on external servers.\n\n3. **Smart Cities and Industrial IoT**\n   - Edge ML powers applications such as **traffic monitoring**, **environmental sensing**, and **predictive maintenance** in smart factories. For example, sensors in factories can use edge models to predict equipment failure before it occurs, ensuring smooth operations without cloud reliance.\n\n#### Challenges in Edge ML:\n\n1. **Model Compression**\n   - Since edge devices often have limited storage and computational power, ML models need to be compressed or optimized (e.g., using techniques like quantization, pruning, or knowledge distillation) to run efficiently while maintaining accuracy.\n\n2. **On-Device Model Updates**\n   - Keeping models updated without frequent cloud interactions is a challenge. Edge devices need mechanisms for **incremental learning** or efficient updates without disrupting normal operations.\n#### Popular Frameworks for Edge ML:\n\n- **TensorFlow Lite**: A lightweight version of TensorFlow, designed to run on mobile and embedded devices.\n- **[[PyTorch]] Mobile**: PyTorch’s framework for deploying ML models on mobile devices.\n- **[[ONNX]] Runtime**: Optimized for running machine learning models on various platforms, including edge devices.\n- **Edge Impulse**: A platform specifically for building ML models for edge devices, particularly for IoT applications.\n\nEdge ML is driving innovation in industries requiring decentralized, real-time intelligence, enabling devices to make smart decisions locally while minimizing reliance on cloud resources.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "edge_ml",
    "outlinks": [
      "onnx",
      "pytorch"
    ],
    "inlinks": [
      "small_language_models"
    ]
  },
  {
    "category": "ML",
    "filename": "Encoding Categorical Variables",
    "sha": "90a9fc64be8cc40dbd028a232a15c742de8056fb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Encoding%20Categorical%20Variables.md",
    "text": "Categorical variables need to be converted into numerical representations to be used in models, particularly in [[Regression]] analysis. This process is essential for transforming categorical results into a format that algorithms can interpret.\n### [[Label encoding]]\n\nThis method assigns a unique integer to each category in the variable.\n\n```python\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nvar1_cat = df['var1']  # Replace df with your DataFrame\nvar1_encoded = label_encoder.fit_transform(var1_cat)\n```\nFor example, if `df[col]` contains the categories `['apple', 'banana', 'orange']`, the `LabelEncoder` would transform them into `[0, 1, 2]`.\n\nHowever, keep in mind that this encoding can imply an order or hierarchy in the data, which might not be intended. In some cases, you might want to use `OneHotEncoder` instead, which creates a binary vector for each category.{}\n\nGiven a term in the df you can transform it without needing to look up its value.\n```python\ncompany=\"google\"\ncompany_n = LabelEncoder().transform([company])\n```\n### One-Hot Encoding\n\nThis technique creates a binary column for each category, allowing the model to treat each category as a separate feature.\n\n```python\nfrom sklearn.preprocessing import OneHotEncoder\n\nbinary_encoder = OneHotEncoder(categories='auto')\nvar1_1hot = binary_encoder.fit_transform(var1_encoded.reshape(-1, 1))\nvar1_1hot_mat = var1_1hot.toarray()\nvar1_DF = pd.DataFrame(var1_1hot_mat, columns=['cat1', 'cat2', 'cat3'])  # Adjust column names as needed\nvar1_DF.head()\n```\n\nUnderstanding OneHotEncoder:\n\nThe `OneHotEncoder` from `sklearn.preprocessing` is used to convert categorical integer values into a format that can be provided to machine learning algorithms to do a better job in prediction. It creates a binary column for each category and returns a sparse matrix or dense array.\n\nWhen using one-hot encoding, it's important to avoid the [[Dummy variable trap]], which occurs when one category can be perfectly predicted from the others. To prevent this, you can drop one of the dummy variables, as one column is sufficient to represent a binary choice (0 or 1).\n\n### Converting All Categorical Variables to Dummies\n\nTo convert all categorical variables in a DataFrame to dummy variables, you can use the following loop:\n\n```python\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        dummies = pd.get_dummies(df[col], drop_first=False)\n        dummies = dummies.add_prefix(f'{col}_')\n        df.drop(col, axis=1, inplace=True)\n        df = df.join(dummies)\n```\n\n### Alternative Encoding Method\nAnother way to encode categorical variables is by mapping them directly to integers:\n\n```python\ndataset['var1'] = dataset['var1'].map({'A': 0, 'B': 1, 'C': 2}).astype(int)\n```\n### Related Topics\n- **[[Regression]]**: Understanding how regression models utilize encoded variables.\n- **[[Feature Engineering]]**: Techniques to enhance model performance through better feature representation.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "preprocessing",
      "regressor"
    ],
    "normalized_filename": "encoding_categorical_variables",
    "outlinks": [
      "feature_engineering",
      "dummy_variable_trap",
      "label_encoding",
      "regression"
    ],
    "inlinks": [
      "data_transformation_in_machine_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Epoch",
    "sha": "e4f49af32dd9ef377d1e0c4f6678763f74bdad72",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Epoch.md",
    "text": "An epoch in machine learning is a single pass through the entire training dataset. The number of epochs, denoted as $N$, determines how many times the data is applied to the model.\n\nWhy Use Multiple Epochs?\n- **Repetition for Learning:** The data is applied to the model $N$ times to improve learning and accuracy. For example, if $N = 10$, the model will see the entire dataset 10 times.\n\nExample\n```\nEpoch 1/10\n6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n```\n\n- **Epoch 1/10:** Indicates the model is currently on the first epoch out of a total of 10.\n- **Batches:** For efficiency, the dataset is divided into smaller groups called 'batches'. In [[Tensorflow]], the default batch size is 32. With 200,000 examples, this results in 6,250 batches.\n- **Batch Execution:** The notation `6250/6250` shows the progress of batch execution within the current epoch.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml"
    ],
    "normalized_filename": "epoch",
    "outlinks": [
      "tensorflow"
    ],
    "inlinks": [
      "batch_gradient_descent",
      "cuda",
      "neural_network_in_practice",
      "pytorch"
    ]
  },
  {
    "category": "ML",
    "filename": "Evaluating Language Models",
    "sha": "5c8cb63f77a9effc6aa94e4507f6c8062f3fe5dd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Evaluating%20Language%20Models.md",
    "text": "The [[LMSYS]] Chatbot Arena is a platform where various large language models ([[LLM]]s), including versions of GPT and other prominent models like LLaMA or [[Claude]], are compared through side-by-side interactions. The performance of these models is evaluated using human feedback based on the quality of their generated responses.\n\nThe arena employs several techniques to rank and compare models:\n\n1. **[[Elo Rating System]]**: Adapted from chess, this system rates models based on their relative performance in head-to-head competitions. When one model's response is preferred over another's, the winning model gains points while the losing model loses points. The rating difference helps determine the strength of models in future predictions. The system adjusts ratings gradually to avoid bias towards more recent results, ensuring stability over time.\n\n2. [[Bradley-Terry Model]]: This model goes beyond simple win-loss records by taking into account the ==difficulty of the task== and the models' relative strengths. It helps fine-tune the ranking, especially when one model consistently performs better against tougher tasks.\n\nIn addition to these ranking systems, users can directly compare LLMs by giving them [[Prompts]] to handle, such as writing articles, answering questions, or performing translations. Human voters then decide which model's output is better, or they can declare a tie if neither response is satisfactory.\n\nThese methods ensure continuous improvement of the rankings, providing a transparent and evolving leaderboard of the best generative models, including GPT versions\n\nhttps://openlm.ai/chatbot-arena/\n\nhttps://www.analyticsvidhya.com/blog/2024/05/from-gpt-4-to-llama-3-lmsys-chatbot-arena-ranks-top-llms/",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "language_models"
    ],
    "normalized_filename": "evaluating_language_models",
    "outlinks": [
      "bradley-terry_model",
      "llm",
      "prompts",
      "claude",
      "elo_rating_system",
      "lmsys"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Evaluating Logistic Regression",
    "sha": "cc7d62375e4c14197d51a378997a2bb049e2c426",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Evaluating%20Logistic%20Regression.md",
    "text": "- [[Logistic Regression Statsmodel Summary table]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "evaluating_logistic_regression",
    "outlinks": [
      "logistic_regression_statsmodel_summary_table"
    ],
    "inlinks": [
      "logistic_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Evaluating the effectiveness of prompts",
    "sha": "554bf3efa09fb4f0da198f51a7ac82eb20e28c09",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Evaluating%20the%20effectiveness%20of%20prompts.md",
    "text": "- [ ] How to find the best prompt? ⏫ \n\nWhat metrics are involved in the [[Cost Function]]\nHow does it review the response. \nThere must be a paper on this.\n\nDistilML ? MLOPS talk",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "question"
    ],
    "normalized_filename": "evaluating_the_effectiveness_of_prompts",
    "outlinks": [
      "cost_function"
    ],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "prompt_engineering"
    ]
  },
  {
    "category": "ML",
    "filename": "Evaluation Metrics",
    "sha": "c4e5bc0e91477afa568d8ec5449b40d98b5bc640",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Evaluation%20Metrics.md",
    "text": "## Description\n\n[[Confusion Matrix]]\n[[Accuracy]]\n[[Precision]]\n[[Recall]]\n[[Precision or Recall]]\n[[F1 Score]]\n[[Recall]]\n[[Specificity]]\n\n\n![[Pasted image 20241222091831.png]]\n\n## Resources:\n[Link to good website describing these](https://txt.cohere.com/classification-eval-metrics/)\n## Types of predictions\n\nTypes of predictions in evaluating models. Also see [[Why Type 1 and Type 2 matter]]\n\n**True Positive (TP)**:\n- This occurs when the model correctly predicts the positive class. For example, if the model predicts that an email is spam and it actually is spam, that's a true positive.\n\n**False Positive (FP)**:\n- Also known as a ==\"Type I error,\"== this occurs when the model incorrectly predicts the positive class. For example, if the model predicts that an email is spam but it is not, that's a false positive.\n\n**True Negative (TN)**:\n- This occurs when the model correctly predicts the negative class. For example, if the model predicts that an email is not spam and it actually is not spam, that's a true negative.\n\n**False Negative (FN)**:\n- Also known as a ==\"Type II error,\"== this occurs when the model incorrectly predicts the negative class. For example, if the model predicts that an email is not spam but it actually is spam, that's a false negative.\n\n## Evaluation metrics in practice\n\nHaving many evaluation metrics is hard to understand and optimise. Sometimes it is best to combine into one.\n\nUse a single number i.e. accuracy or [[F1 Score]] . \n\nThis speeds up development of ml projects.\n\nIn order to use metrics to evaluate a model we can:\n- Can combine multiple metrics a formula, i.e. weighted average.\n- If there is a metrics we are happy that the model passes a given level then we can have it \"==Satisfying==\". So the for the given metric it just needs to pass a given level.\n- For metrics we are interested in we have it \"==Optimising==\", the one we want to be the best.\n- Setup: Pick N-1 satisfying and 1 optimising.\n\n![[Pasted image 20241217073706.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "normalized_filename": "evaluation_metrics",
    "outlinks": [
      "why_type_1_and_type_2_matter",
      "confusion_matrix",
      "pasted_image_20241217073706.png",
      "accuracy",
      "f1_score",
      "precision_or_recall",
      "recall",
      "pasted_image_20241222091831.png",
      "precision",
      "specificity"
    ],
    "inlinks": [
      "aic_in_model_evaluation",
      "determining_threshold_values",
      "ds_&_ml_portal",
      "forecasting_autoarima.py",
      "forecasting_using_lags",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "imbalanced_datasets",
      "metric",
      "model_evaluation",
      "model_selection",
      "neural_network_classification",
      "recall",
      "recommender_systems",
      "test_loss_when_evaluating_models",
      "time_series_forecasting",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "ML",
    "filename": "Exploration vs Exploitation",
    "sha": "58d92e70119851140d4b050facdb39232334edcf",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Exploration%20vs%20Exploitation.md",
    "text": "One of the major challenges in [[Reinforcement learning]] is balancing exploration (trying new actions) and exploitation (choosing the best-known actions). \n\nThe ==epsilon-greedy strategy== is commonly used, where a small probability (epsilon) allows for exploration while primarily exploiting the best-known actions.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "exploration_vs_exploitation",
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "q-learning",
      "reinforcement_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Exponential Smoothing",
    "sha": "45c3491962d108a56b8bae5c47a24bb49789b9a2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Exponential%20Smoothing.md",
    "text": "Exponential Smoothing provides a way to incorporate more structure into forecasts by assigning exponentially decreasing weights to past observations. This weighting scheme allows the model to be more responsive to recent data while still retaining information from the past.\n\nThese methods are especially effective for [[Time Series Forecasting]] that exhibit patterns such as: Level (baseline average), Trend (direction of change), Seasonality (regular fluctuations over time).\n\nExponential Smoothing is a family of forecasting methods where forecasts are generated by applying weights that decrease exponentially as observations get older.\n\n* Recent observations have the highest impact on the forecast.\n* Older observations are never fully discarded but contribute less over time.\n* This makes the method flexible for adapting to new trends or shifts in the data.\n### When to Use Each Method\n\nQu: What do the parameters for each model tell us?\n\n* SES -> Data with no trend/seasonality (e.g., stationary demand).\n\t* [[Simple Exponential Smoothing (SES)]]\n* Holt’s -> Data with trend but no seasonality (e.g., stock with upward drift).\n\t* [[Holt’s Linear Trend Model (Double Exponential Smoothing)]]\n* Holt-Winters -> Data with both trend and seasonality (e.g., sales data with monthly cycles).\n\t* [[Holt-Winters (Exponential Smoothing)]]\n\nAll three methods can be implemented using the `holtwinters` (or `statsmodels`) package in Python.\n\n```python\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "ml",
      "time_series"
    ],
    "normalized_filename": "exponential_smoothing",
    "outlinks": [
      "holt’s_linear_trend_model_(double_exponential_smoothing)",
      "time_series_forecasting",
      "simple_exponential_smoothing_(ses)",
      "holt-winters_(exponential_smoothing)"
    ],
    "inlinks": [
      "decomposition_in_time_series",
      "holt-winters_(exponential_smoothing)",
      "moving_average_forecast",
      "time_series_forecasting",
      "trends_in_time_series"
    ]
  },
  {
    "category": "ML",
    "filename": "F1 Score",
    "sha": "f5d36b0aee1994919ffa5822287d9e140342e7a9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/F1%20Score.md",
    "text": "### Definition\n- The F1 Score is the harmonic mean of precision and recall. It provides a balanced view of both metrics and is particularly useful when:\n  - There is an uneven class distribution.\n  - Both precision and recall are important.\n\nThe formula for the F1 Score is:\n\n$$F1 = 2 \\times \\frac{{\\text{Precision} \\times \\text{Recall}}}{{\\text{Precision} + \\text{Recall}}} $$\n\nWhere:\n- [[Precision]] is calculated as $\\frac{{\\text{True Positives}}}{{\\text{True Positives} + \\text{False Positives}}}$.\n- [[Recall]] is calculated as $\\frac{{\\text{True Positives}}}{{\\text{True Positives} + \\text{False Negatives}}}$.\n\nThe F1 Score is the harmonic mean of precision and recall, which ensures that the score is high only when both precision and recall are high.\n\n### Components\n- Precision: The ratio of true positive predictions to the total predicted positives. It measures the accuracy of positive predictions.\n- Recall: The ratio of true positive predictions to the total actual positives. It measures the ability to find all positive instances.\n\n### Importance\n- The F1 Score is suitable for imbalanced datasets where considering both false positives and false negatives is crucial.\n- It ranges from 0 to 1, where a higher score indicates better overall classifier performance.\n\n### Use\n- The F1 Score, also known as the F-measure, combines precision and recall using the harmonic mean. This makes it a useful metric for evaluating models where a balance between precision and recall is desired.\n\n### Understanding the F1 Score\n\nThe F1 Score is a metric used to evaluate the performance of a classification model, especially in scenarios where the classes are imbalanced. It helps in understanding how well the model is performing in terms of both precision (how many selected items are relevant) and recall (how many relevant items are selected).\n\n- Why [[Harmonic Mean]]?: The harmonic mean is used instead of the arithmetic mean because it punishes extreme values more. This means that if either precision or recall is very low, the F1 Score will also be low, reflecting the poor performance in one of the areas.\n\n- When to Use: Use the F1 Score when you need a single metric that considers both false positives and false negatives, especially in cases where the cost of false positives and false negatives is similar.\n\nBy focusing on both precision and recall, the F1 Score provides a more comprehensive measure of a model's accuracy, particularly in situations where one class is much more frequent than the other.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "f1_score",
    "outlinks": [
      "precision",
      "recall",
      "harmonic_mean"
    ],
    "inlinks": [
      "classification_report",
      "confusion_matrix",
      "evaluation_metrics",
      "model_observability"
    ]
  },
  {
    "category": "ML",
    "filename": "FAISS",
    "sha": "fd4daad434fc8a6ab278cd288d84074ce0169303",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/FAISS.md",
    "text": "FAISS (Facebook AI [[Similarity Search]]) is a library developed by Facebook AI Research that enables efficient [[Similarity Search]] and [[Clustering]] of dense vectors. It is especially well-suited for applications involving high-dimensional vector data, such as [[NLP]]\n### Core Concept\n\nAt its core, FAISS takes a large number of high-dimensional vectors (e.g., sentence or document embeddings), and enables fast [[Similarity Search]] to retrieve the most similar vectors to a given [[Querying|query]].\n\nFor example, in an NLP [[LLM Memory]]:\n- Documents or notes are embedded into vector space using a model like SBERT.\n- These embeddings are stored in a FAISS index.\n- Given a query, its embedding is computed, and FAISS returns the nearest neighbors (i.e., most semantically similar notes).\n### Index Types\n\nFAISS offers different types of indices depending on use case:\n- `IndexFlatL2`: exact search using L2 (Euclidean) distance.\n- `IndexIVFFlat`: approximate search using inverted files.\n- `IndexHNSW`: Hierarchical Navigable Small World graph-based index (good for high recall).\n- `IndexPQ`: product quantization for memory-efficient indexing.\n\nFAISS is optimized for:\n- Fast retrieval from large collections of vectors (millions or more).\n- Approximate nearest neighbor (ANN) search, which trades off accuracy for speed.\n- Exact search, depending on the chosen index type.\n- GPU acceleration for very large-scale search tasks.\n### Use Cases\n\n- Embedding search (e.g., text, image, or audio embeddings)\n- Recommendation systems\n- [[Semantic search]]\n- Clustering of high-dimensional data\n- [[Vector Embedding]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "python"
    ],
    "normalized_filename": "faiss",
    "outlinks": [
      "llm_memory",
      "vector_embedding",
      "clustering",
      "querying",
      "similarity_search",
      "semantic_search",
      "nlp"
    ],
    "inlinks": [
      "vector_database"
    ]
  },
  {
    "category": "ML",
    "filename": "Fact Table",
    "sha": "480153797abc0e04c257c7e8447759fc5eee184b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Fact%20Table.md",
    "text": "A fact table is a central component of a star [[Database Schema|schema]] or snowflake schema in a [[Data Warehouse]], it stores [[Facts]].\n\nEssential for [[Data Analysis]] in a data warehouse, providing the numerical data that can be analyzed in conjunction with the descriptive data stored in dimension tables.\n\n1. **Measures**: Fact tables contain measurable, quantitative data known as \"facts\" or \"measures.\" Examples include sales revenue, quantity sold, profit, or any other numeric data that can be aggregated.\n\n2. **Foreign Keys**: Fact tables include foreign keys that reference [[Dimension Table]]s. These keys link the fact table to related dimensions, allowing for contextual analysis. For example, a sales fact table might include foreign keys for dimensions such as time, product, and customer.\n\n3. **Granularity**: The [[granularity]] of a fact table refers to the level of detail stored in the table. For example, a sales fact table might store data at the transaction level (each sale) or at a higher level (daily sales totals).\n\n4. **Large Size**: Fact tables can become quite large, as they often store a significant amount of transactional data over time. This is in contrast to dimension tables, which are usually smaller and contain descriptive attributes.\n\n5. **Aggregation**: Fact tables are often used for aggregation and analysis, allowing users to perform operations such as summing, averaging, or counting the measures.\n\nExample:\n  - Columns: `DateKey`, `ProductKey`, `RegionKey`, `SalesAmount`, `UnitsSold`\n\n\n\n[[Fact Table]]\n   **Tags**:,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "fact_table",
    "outlinks": [
      "dimension_table",
      "facts",
      "data_analysis",
      "database_schema",
      "granularity",
      "fact_table",
      "data_warehouse"
    ],
    "inlinks": [
      "components_of_the_database",
      "dimension_table",
      "dimensional_modelling",
      "fact_table",
      "facts",
      "granularity"
    ]
  },
  {
    "category": "ML",
    "filename": "Feature Engineering for Time Series",
    "sha": "00b36dd4a8a06f97d702d37ef541ab76c08bcd7c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Engineering%20for%20Time%20Series.md",
    "text": "Core Concepts\n\n* Lag Features: Use past values as predictors for future points.\n\t* A **lag feature** is created by shifting the time series backward to capture past values as predictors for the current or future value.\n* Rolling Windows / Aggregations: Compute moving averages, sums, or other statistics over fixed intervals.\n* Time-Based Features: Include hour of day, day of week, or calendar effects; can be per SIM, per cell tower, etc.\n\n[[Data Quality]]\n* Handle missing timestamps and irregular events appropriately.\n* Impute missing values or resample data to regular intervals if needed.\n\nStatistical Properties\n\n* [[Stationary Time Series]]: Check if mean, variance, and autocorrelation are constant over time. Use tests like ADF.\n* Trend and Seasonality: Identify and potentially remove to stabilize the series.\n* ACF / PACF Analysis: Understand [[Autocorrelation]] structure; helps determine relevant lag features.\n\nPractical [[Feature Engineering]]\n\n* Combine lags, rolling statistics, and time-based features\n* Adjust features for irregular timestamps and [[Missing Data]].\n\nCommon Interview Questions\n* *“How do you check if a time series is stationary?”*\n\t* You typically examine whether its **statistical properties** (mean, variance, autocorrelation) remain constant over time. You can visualise it, get summary stats, do ADF tests.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "time_series"
    ],
    "normalized_filename": "feature_engineering_for_time_series",
    "outlinks": [
      "stationary_time_series",
      "missing_data",
      "feature_engineering",
      "autocorrelation",
      "data_quality"
    ],
    "inlinks": [
      "time_series",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Feature Evaluation",
    "sha": "ef0ffbab4db1b5ef953dc54afb7ad2b48d2a3909",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Evaluation.md",
    "text": "Effective models depend on high-quality, ==informative features==. Feature evaluation helps identify which features contribute meaningfully to model performance and which may be redundant or harmful.\n\n#### Objectives\n* Determine the relevance, predictive power, and redundancy of features.\n* Guide [[Feature Selection]], engineering, and model interpretation.\n\n#### Core Aspects\n\n[[Feature Importance]]\n\nFeature Relationships Assess:\n* [[Correlation]] between features to detect redundancy\n* Interactions that may impact the target variable\n\nPerformance Impact: Measure how feature inclusion/exclusion affects:\n* Model accuracy, precision, [[AUC]], etc.\n* Stability and generalisability of results\n\n#### When Evaluation is Complete\n* Feature set achieves optimal model performance\n* Further changes offer no significant improvement\n* Feature effects are [[Interpretability|interpretable]] and aligned with domain knowledge",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "exploration"
    ],
    "type": "process",
    "normalized_filename": "feature_evaluation",
    "outlinks": [
      "correlation",
      "auc",
      "feature_importance",
      "feature_selection",
      "interpretability"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Feature Extraction",
    "sha": "c0439c4fed11259503bf271f09ab3016f7a4b84f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Extraction.md",
    "text": "Feature extraction is the process of transforming raw data into a structured set of ==relevant informative features== to a machine learning task. \n\nThis process enhances both model performance and efficiency by simplifying input data and focusing on its most meaningful attributes.\n\nKey Concepts:\n- Purpose: Extract ==key attributes== from raw data to enable learning algorithms to detect patterns and make accurate predictions.\n- Informative Features: Feature extraction reduces complexity by generating a smaller, more meaningful set of features. This reduces noise, improves learning speed, and supports [[Interpretability]].\n- Dimensionality Reduction: A core strategy in feature extraction, [[Dimensionality Reduction]] compresses data while preserving its most important variance, aiding in both model performance and comprehensibility.\n\nExample Tools (scikit-learn):\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction import DictVectorizer\n```\n\nUse Cases:\n- Text: Bag-of-words, [[TF-IDF|TFIDF ]]representations\n- Images: CNN feature maps, [[Activation atlases]]\n- Tabular data: One-hot or embedding representations of categorical features",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "preprocessing",
      "transformation"
    ],
    "normalized_filename": "feature_extraction",
    "outlinks": [
      "activation_atlases",
      "interpretability",
      "tf-idf",
      "dimensionality_reduction"
    ],
    "inlinks": [
      "convolutional_neural_networks",
      "non-negative_matrix_factorization",
      "text_classification",
      "time_series_python_packages"
    ]
  },
  {
    "category": "ML",
    "filename": "Feature Importance",
    "sha": "8ad316fccca750d7527605d0d78a959504515aa6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Importance.md",
    "text": "Feature importance refers to ==techniques that assign scores to input features== (predictors) in a machine learning model to ==indicate their relative impact on the model's predictions.==. Part of [[Feature Selection]].\n\nFeature importance is typically assessed ==after== [[Model Building]]. It involves analyzing the trained model to determine the impact of each feature on the predictions.\n\nFeature importance helps in:\n- improving model [[Interpretability]], \n- identifying key predictors, \n- and possibly performing [[Feature Selection]] to reduce dimensionality, and refining performance\n\nThe ==outcome== is a ranking or scoring of features based on their importance.\n\nBy understanding which features contribute the most to the predictions, you can focus on the most relevant information in your data and potentially reduce model complexity without sacrificing performance.\n### Types of Feature Importance Methods\n\nModel-Specific Methods:\n- Tree-based models: Models like Random Forests, Gradient Boosted Trees, and Decision Trees have built-in mechanisms for calculating feature importance. They do so based on the decrease in impurity (e.g., [[Gini Impurity]] in classification tasks or variance in regression tasks) or based on the reduction in error when the feature is used for splitting. Tree-based algorithms like [[Random Forest]] or [[XGBoost]] automatically calculate feature importance. \n- Linear models: In models like linear regression or logistic regression, feature importance can be derived from the absolute values of the model coefficients, assuming features are standardized ([[Why standardise features]]).\n   \nModel-Agnostic Methods:\n- Permutation importance: This method measures the importance of a feature by randomly shuffling its values and observing the impact on the model's performance. The larger the decrease in performance, the more important the feature is.\n- [[SHapley Additive exPlanations]]\n- [[Local Interpretable Model-agnostic Explainations]]\n\nQuantify how much each feature influences model predictions using:\n* [[Statistical Tests]] (e.g. ANOVA, chi-squared)\n* Model-based scores (e.g. Gini importance in trees)\n* Permutation importance\n\n### Links\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\n### Feature Permutation\n\n- Better than impurity based importance.\n- Takes longer to compute.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "explainability",
      "process"
    ],
    "normalized_filename": "feature_importance",
    "outlinks": [
      "gini_impurity",
      "why_standardise_features",
      "model_building",
      "shapley_additive_explanations",
      "random_forest",
      "statistical_tests",
      "xgboost",
      "local_interpretable_model-agnostic_explainations",
      "feature_selection",
      "interpretability"
    ],
    "inlinks": [
      "arima_vs_random_forest_in_time_series",
      "binary_classification",
      "eda",
      "feature_evaluation",
      "feature_selection",
      "feature_selection_vs_feature_importance",
      "ice_plot",
      "local_interpretable_model-agnostic_explainations",
      "logistic_regression",
      "model-agnostic_feature_importance",
      "model_evaluation",
      "partial_dependence_plot",
      "shapley_additive_explanations"
    ]
  },
  {
    "category": "ML",
    "filename": "Feature Selection",
    "sha": "c1cab4ae72d173ae7e134217a66d01b7f48a041f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Selection.md",
    "text": "Feature selection is to identify and retain the most relevant features for model training while ==removing irrelevant or redundant ones==. This helps in simplifying the model, reducing overfitting, and improving computational efficiency.\n\nFeature selection is typically performed before model training. It involves evaluating features based on certain criteria or algorithms to decide which features to keep or discard.\n\nThrough an iterative process, feature selection experiments with different methods, adjusts parameters, and evaluates model performance until an optimal set of features is found.\n\nWhen selecting features we ask:\n- Which play the biggest role.\n- Can we control it/select it?\n- Can we control it easily & what do we gain from it\n- is it a sensible variable?\n- Focus on fundamental features aligned with business goals.\n* Avoid over-engineering; prioritize features that maximize value.\n\nSee:\n- [[Feature Selection vs Feature Importance]]\n- [[Feature Importance]]\n- [[Univariate Feature Selection]]\n- [[Forward Feature Selection]]\n### Methods\n\nThe choice of feature selection method depends on factors like the size of your dataset, the number of features, and the complexity of your model. It's often a balance between computational cost and performance improvement.\n\n- [[Filter Methods]]: Select features based on statistical properties, independent of any machine learning algorithm. (Separate stage to training)\n- [[Wrapper Methods]]: Involve training multiple models with different subsets of features and selecting the subset that yields the best performance. (Separate stage to training)\n- [[Embedded Methods]]: Perform feature selection as part of the model training process. (Part of training)\n\nAfter selecting features, it's essential to evaluate your model's performance ([[Model Evaluation]]) with the chosen subset. Sometimes, feature selection can inadvertently remove important information.\n\n### Detecting Noisy or Redundant Features\n\n- [[Correlation]] Analysis: Use a [[Heatmap]] or [[Clustering]]. Features with low correlation to the target or high correlation with other features may be candidates for removal.\n\n- [[Dimensionality Reduction]] Techniques: Techniques like [[Principal Component Analysis]] or Singular Value Decomposition ([[SVD]]) can transform the features into a lower-dimensional space while preserving as much variance as possible. Features with low contribution to the principal components can be considered for removal.\n\n- [[Data Visualisation]]: Plotting pairwise scatter plots or [[Heatmap]] of feature [[Correlation]] can provide visual insights into redundant features. Clusters of highly correlated features or [[Scatter Plots]] showing no discernible pattern with the target variable can indicate noisy or redundant features.\n### Investigating Features\n\n1. Variance Thresholding: Check the [[Variance]] & [[Distributions]] of each feature. Features with very low variance (close to zero) contribute little information and may be considered noisy. Removing such features can help simplify the model without sacrificing much predictive power.\n\n2. Univariate Feature Selection: Use statistical tests like chi-square for categorical variables or [[ANOVA]] for numerical variables to assess the relationship between each feature and the target variable. Features with low test scores or high p-values may be less relevant and can be pruned.\n### Related\n- [[Standardisation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "explainability",
      "modeling",
      "process",
      "selection"
    ],
    "normalized_filename": "feature_selection",
    "outlinks": [
      "embedded_methods",
      "principal_component_analysis",
      "scatter_plots",
      "dimensionality_reduction",
      "clustering",
      "anova",
      "forward_feature_selection",
      "univariate_feature_selection",
      "wrapper_methods",
      "feature_importance",
      "correlation",
      "standardisation",
      "svd",
      "data_visualisation",
      "feature_selection_vs_feature_importance",
      "filter_methods",
      "variance",
      "heatmap",
      "model_evaluation",
      "distributions"
    ],
    "inlinks": [
      "anova",
      "automated_feature_creation",
      "correlation",
      "curse_of_dimensionality",
      "data_selection_in_ml",
      "data_understanding",
      "ds_&_ml_portal",
      "embedded_methods",
      "f-regression",
      "feature_evaluation",
      "feature_importance",
      "feature_selection_vs_feature_importance",
      "filter_methods",
      "isolated_forest",
      "l1_regularisation",
      "logistic_regression_statsmodel_summary_table",
      "p_values",
      "pca_principal_components",
      "preprocessing",
      "regularisation",
      "regularisation_of_tree_based_models",
      "ridge",
      "wrapper_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "Feature Transformations",
    "sha": "61fdde3010c1597bf1b4b23596af1c91401d3d4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feature%20Transformations.md",
    "text": "Transforming continuous features: \n- Binning of values\n- Importance of Transformations: Transforming continuous features can reveal complex non-linear patterns and improve model performance.\n- Common Transformations: Techniques like log, square root,ratios, scaling, adding polynomial terms, and binning can help manage data ranges and uncover insights.\n- Practical Application: Use visualizations like histograms and scatterplots to identify beneficial transformations and evaluate their impact on model performance.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "feature_transformations",
    "outlinks": [],
    "inlinks": [
      "data_transformation"
    ]
  },
  {
    "category": "ML",
    "filename": "Feed Forward Neural Network",
    "sha": "b98b0f398a9a6ddb1c4432f8f431483e6aaf7de3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Feed%20Forward%20Neural%20Network.md",
    "text": "A **Feedforward Neural Network (FFNN)** is the simplest type of [[Neural network]]. In this model, connections between neurons do not form a cycle, allowing data to flow in one direction—from the input layer, through the hidden layers, to the output layer—without any loops or backward connections. This straightforward design is primarily used for [[Supervised Learning]] tasks.\n\n### Structure\n- Information flows in one direction: input → hidden layers → output.\n- During **[[Forward Propagation]]**, input data is passed through the network, processed by each layer, and an output is produced.\n- Unlike [[Recurrent Neural Networks]] (RNNs), FFNNs do not share information or weights between layers, meaning the model does not maintain memory of past inputs.\n\n### Learning\n- FFNNs learn by adjusting weights and biases during training to minimize the [[Loss function]].\n\n### Limitations\n- **Shallow vs. Deep Networks:** Simple feedforward networks with only a few hidden layers (shallow networks) may struggle to learn complex, hierarchical representations of data. Deeper networks (deep feedforward networks) with many layers can model more complex patterns but require more data and computational resources to train.\n- **[[Overfitting]]:** FFNNs can overfit on the training data, especially if they have many parameters and not enough regularization (e.g., dropout, [[Ridge|L2]] regularization).\n- **No Temporal Understanding:** Unlike [[Recurrent Neural Networks]] or transformers, FFNNs cannot model sequential dependencies in data. They are better suited for static, non-sequential tasks.",
    "aliases": [
      "FFNN"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "deep_learning"
    ],
    "normalized_filename": "feed_forward_neural_network",
    "outlinks": [
      "supervised_learning",
      "ridge",
      "forward_propagation",
      "recurrent_neural_networks",
      "loss_function",
      "overfitting",
      "neural_network"
    ],
    "inlinks": [
      "backpropagation",
      "sentence_transformer_workflow",
      "transformer",
      "types_of_neural_networks"
    ]
  },
  {
    "category": "ML",
    "filename": "Filter Methods",
    "sha": "7f5b8e45d2c6550e98549699c4e98d5ba2284274",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Filter%20Methods.md",
    "text": "For [[Feature Selection]]\n\n1. Pearson [[Correlation]] Coefficient:\n   - Measures the linear correlation between two continuous variables.\n   - Features with low correlation with the target variable are considered less relevant.\n   - Features with high correlation among themselves might be redundant.\n\n1. Mutual Information:\n   - Measures the amount of information obtained about one variable through another variable.\n   - High mutual information indicates strong dependency between features and the target variable.\n   - Can handle both continuous and categorical variables.\n   - used to rank or score features based on their relevance to the target variable.\n   - joint probability distribution\n   - information theory,\n\n[[ANOVA]]\n\n1. Chi-Squared Test:\n   - Tests the independence between two [[categorical]] variables.\n   - Calculates the chi-squared statistic and p-value to determine if the observed frequency distribution differs from the expected distribution.\n   - Helpful for [[Feature Selection]] in classification tasks with categorical variables.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "statistics"
    ],
    "normalized_filename": "filter_methods",
    "outlinks": [
      "feature_selection",
      "categorical",
      "correlation",
      "anova"
    ],
    "inlinks": [
      "embedded_methods",
      "f-regression",
      "feature_selection",
      "wrapper_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "Fitting weights and biases of a neural network",
    "sha": "1b4893c92ec5136a399d964e1ccd76fd4b1eec88",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Fitting%20weights%20and%20biases%20of%20a%20neural%20network.md",
    "text": "For a neural network model, fitting weights and biases involves optimizing these [[Model Parameters]] so the model learns to map input features ($X$) to target outputs ($y$) effectively. This is achieved through the training process, which minimizes the error between predictions and actual values.\n\nBest Practices\n- Use appropriate weight initializations like He or Xavier.\n- Choose a suitable [[Loss function]] for the task.\n- Optimize using advanced optimizers like Adam or RMSprop.\n- Experiment with batch sizes, epochs, and learning rates.\n- Apply regularization (L2, [[Dropout]]) to prevent overfitting.\n- Monitor validation metrics and use early stopping.\n\nIn [[ML_Tools]] see: Neural_Net_Weights_Biases.py\n\n## Initialization of Weights and Biases\n\nInitializing all weights randomly. The weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations.\n\nProper initialization is critical for training to converge efficiently. Poor initialization can lead to slow convergence or getting stuck in local minima. By starting with well-chosen initial values, the network can learn more effectively and avoid issues like vanishing or exploding gradients.\n\nWeights:\n- Use small random values (e.g., drawn from Gaussian or uniform [[Distributions]]) to break symmetry and ensure that neurons learn different features.\n- Initialization techniques like He initialization (for [[Relu]] activations) or Xavier initialization (for sigmoid/tanh activations) are commonly used because they help maintain the scale of gradients across layers, promoting stable and faster convergence.\n\n```python\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.initializers import HeNormal\n\n# Example of He initialization for ReLU activation\nDense(25, activation=\"relu\", kernel_initializer=HeNormal())\n```\nBiases:\n- Start with zeros (`0`) to ensure symmetry-breaking during optimization. This allows the network to learn offsets for the activations without introducing bias in the initial learning phase.\n\n## [[Forward Propagation]]\n\nDuring forward propagation, the network computes activations using the current weights and biases, and passes these activations to subsequent layers to generate predictions. This step is crucial as it determines how well the network can map inputs to outputs based on its current parameters.\n\n## Loss Function\n\nThe loss function quantifies the difference between predicted outputs and true labels. It serves as the objective function that the network aims to minimize during training. Choosing the right loss function is essential as it directly impacts the learning process and the network's ability to generalize.\n\n- Binary Cross-Entropy: For [[Binary Classification]].\n- Categorical Cross-Entropy: For multi-class classification.\n- [[Mean Squared Error]] (MSE): For regression tasks.\n\nExample:\n```python\nfrom tensorflow.keras.losses import BinaryCrossentropy\nloss_fn = BinaryCrossentropy()\n```\n## [[Backpropagation]]\n\nBackpropagation computes the gradients of the loss function with respect to weights and biases using the chain rule. This process is fundamental for learning, as it provides the necessary information to update the parameters in a way that reduces the loss.\n\n## [[Gradient Descent]] Optimization\n\nGradients from backpropagation are used to update weights and biases iteratively. Optimization algorithms like Adam, RMSprop, and [[Stochastic Gradient Descent|SGD]] with momentum are crucial as they determine the efficiency and speed of convergence, especially in large datasets and complex models.\n\nExample:\n```python\nfrom tensorflow.keras.optimizers import Adam\noptimizer = Adam(learning_rate=0.001)\n```\n\n## Batch Training\n\nWeights and biases are updated after processing a batch of data. Batch training helps in stabilizing the learning process and can lead to faster convergence compared to updating after each sample. The choice of batch size and number of epochs affects the trade-off between computational efficiency and the quality of the learned model.\n\n## [[Regularisation]] Techniques\n\nPrevent overfitting by penalizing large weights. Regularization is essential for improving the generalization of the model, ensuring it performs well on unseen data.\n\n[[Ridge]]\n[[Dropout]]\n## Learning Rate Tuning\n\nLearning rate impacts convergence. It is a [[Hyperparameter]] that determines the step size during optimization. A poorly chosen learning rate can lead to divergence or slow convergence.\n\nTechniques:\n- [[Learning Rate]] Scheduling: Reduce learning rate as training progresses to fine-tune the learning process.\n- Adaptive Learning Rates: Optimizers [[Optimisation techniques]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml"
    ],
    "normalized_filename": "fitting_weights_and_biases_of_a_neural_network",
    "outlinks": [
      "gradient_descent",
      "regularisation",
      "relu",
      "dropout",
      "binary_classification",
      "hyperparameter",
      "forward_propagation",
      "optimisation_techniques",
      "model_parameters",
      "loss_function",
      "learning_rate",
      "distributions",
      "backpropagation",
      "mean_squared_error",
      "ml_tools",
      "stochastic_gradient_descent",
      "ridge"
    ],
    "inlinks": [
      "neural_network"
    ]
  },
  {
    "category": "ML",
    "filename": "Framework for models",
    "sha": "aa53cbd38bbeb7aa2cbdff201c9d1d79f2e16559",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Framework%20for%20models.md",
    "text": "## Business Objects:\n\n- Basic business concepts (e.g., Customer, Contract, Employee)\n- Grouped into Data Domains\n- Have specific lifecycles\n- Can link across different domains\n- Serve as points of truth in information systems\n\n## Functional Objects:\n- Provide logical data perspective of Business Objects\n- Include mandatory and non-mandatory attributes\n- Can connect to other Functional Objects\n- Describe BOs in specific contexts\n- May have multiple technical implementations\n\n## Technical Objects:\n- How its implemented in code ect.\n\n## Related terms\n- [[Data Architecture]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "framework_for_models",
    "outlinks": [
      "data_architecture"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "GRU",
    "sha": "1116989726b4f4c04bf45a6ec7cd90d4af4b948e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/GRU.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml"
    ],
    "normalized_filename": "gru",
    "outlinks": [],
    "inlinks": [
      "lstm",
      "recurrent_neural_networks",
      "transformers_vs_rnns"
    ]
  },
  {
    "category": "ML",
    "filename": "Gaussian Model",
    "sha": "fe908e4601d6011fabf9c81acde53e25d855c8cb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gaussian%20Model.md",
    "text": "[[univariate data]]\n- Formula:  \n    $p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)$\n- Steps:\n    - Estimate $\\mu$ and $\\sigma^2$ from the data.\n    - Compute the probability density for each data point.\n    - Points with low probabilities (below a threshold $\\epsilon$) are considered anomalies.\n\n![[Pasted image 20241230202826.png|500]]\n\n### Multivariate Gaussian Distribution\n\nSteps:\n    - Extend the Gaussian model to include covariance across features.\n    - Fit the multivariate Gaussian model:  \n        $p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)$\n        - $\\mu$: Mean vector\n        - $\\Sigma$: Covariance matrix\n    - Threshold low-probability examples to identify anomalies.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "gaussian_model",
    "outlinks": [
      "univariate_data",
      "pasted_image_20241230202826.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "General Linear Regression",
    "sha": "45e46dd754c09b3ebf01f2cce93ab7304672e8c8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/General%20Linear%20Regression.md",
    "text": "[[Linear Regression]]\n\n[[T-test]] - to compare means between two populations.\n\n[[ANOVA]] - tests",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "regressor"
    ],
    "normalized_filename": "general_linear_regression",
    "outlinks": [
      "anova",
      "linear_regression",
      "t-test"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Generalisation",
    "sha": "c2ae0be5fdc17905c10f9abda76272cffd0b4133",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Generalisation.md",
    "text": "The end goal of a model is to have a well generalised model that behaves well to unseen data.",
    "aliases": [
      "generalisable",
      "generalise",
      "generalize",
      "robust",
      "Robustness"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "generalisation",
    "outlinks": [],
    "inlinks": [
      "challenges_to_model_deployment",
      "model_optimisation",
      "random_forest",
      "resampling"
    ]
  },
  {
    "category": "ML",
    "filename": "Generative Adversarial Networks",
    "sha": "369a6070ea495c3c2d8bfdb99d47a818498e4214",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Generative%20Adversarial%20Networks.md",
    "text": "Composed of two neural networks, a generator, and a discriminator, that compete against each other. GANs are used for tasks like generating realistic images or videos.",
    "aliases": [
      "GAN"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "generative_adversarial_networks",
    "outlinks": [],
    "inlinks": [
      "types_of_neural_networks"
    ]
  },
  {
    "category": "ML",
    "filename": "Gini Impurity vs Cross Entropy",
    "sha": "ce0b4e95bf5e2dea257910cce28a70556e392f2e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gini%20Impurity%20vs%20Cross%20Entropy.md",
    "text": "When working with decision trees, both [[Gini Impurity]] and [[Cross Entropy]] are metrics used to evaluate the quality of a split. They help determine how well a feature separates the classes in a dataset.\n\n### Gini Impurity\n\n- **Definition**: Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node.\n- **Computation**: Generally faster to compute than cross-entropy because it does not involve logarithms.\n- **Use Case**: Often used in the [[CART]] (Classification and Regression Trees) algorithm. It is a good default choice for classification tasks due to its simplicity and efficiency.\n\n### Cross Entropy (More refined than impurity)\n\n- **Definition**: Cross-entropy measures the amount of information needed to encode the class distribution of the node. It quantifies the expected amount of information required to classify a new instance.\n- **Computation**: Involves logarithmic calculations, which can be computationally more intensive than Gini impurity.\n- **Use Case**: Often used in algorithms like ID3 and C4.5. It can be more informative in cases where the class [[Distributions]] is skewed or when you need a more nuanced measure of impurity. See [[Imbalanced Datasets|Class Imbalance]].\n\n### Choosing Between Gini Impurity and Cross Entropy\n\n- **Performance**: In practice, both metrics often lead to similar results in terms of the structure and performance of the decision tree. The choice between them may not significantly affect the final model.\n- **Efficiency**: If computational efficiency is a concern, Gini impurity might be preferred due to its simpler calculation.\n- **Interpretability**: Cross-entropy provides a more information-theoretic perspective, which might be preferred if you are interested in the information gain aspect of the splits.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "gini_impurity_vs_cross_entropy",
    "outlinks": [
      "gini_impurity",
      "distributions",
      "imbalanced_datasets",
      "cart",
      "cross_entropy"
    ],
    "inlinks": [
      "decision_tree"
    ]
  },
  {
    "category": "ML",
    "filename": "Gini Impurity",
    "sha": "0205d9f650853d11b7c82e1fd72fdb615c929fee",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gini%20Impurity.md",
    "text": "Gini impurity is a metric used in decision trees to measure the degree or probability of misclassification in a dataset. It is associated with the leaves of a [[Decision Tree]] and helps determine the best split at each node.\n## Calculation\n\n- Mathematical Formula: Gini impurity is calculated as the probability of incorrectly classifying a randomly chosen element if it were randomly labelled according to the distribution of labels in the subset.\n- Formula: \n\n  $$ \\text{Gini Impurity} = 1 - \\sum_{i=1}^{n} p_i^2 $$\n  where $p_i$ is the probability of an element being classified into a particular class.\n\n## Usage\n\n- Decision Trees: Gini impurity is commonly used in decision trees to evaluate splits. A lower Gini impurity indicates a better split, as it means the data is more homogeneously classified.\n- [[Classification]] Tasks: It is particularly useful in classification tasks where the goal is to minimize misclassification.\n\n## Relationship to Other Metrics\n\n- Gini impurity is one of several [[Regression Metrics]] used to evaluate the performance of decision trees, alongside others like entropy.\n\n## Example\n\nSuppose you have a dataset with a binary classification problem, where the target variable can be either \"Yes\" or \"No\". You have a node in your decision tree with the following distribution of classes:\n\n- 10 samples labeled \"Yes\"\n- 5 samples labeled \"No\"\n\n### Gini Impurity Calculation\n\nThe formula for Gini impurity is:\n\n$$ \\text{Gini impurity} = 1 - \\sum (p_i^2) $$\n\nwhere$p_i$ is the proportion of class$i$ in the node.\n\n#### Step-by-Step Calculation\n\n1. **Calculate the proportion of each class**:\n   - Total samples = 10 (Yes) + 5 (No) = 15\n   - Proportion of \"Yes\" =$\\frac{10}{15} = 0.67$\n   - Proportion of \"No\" =$\\frac{5}{15} = 0.33$\n\n2. **Calculate the squared proportions**:\n   - $(0.67)^2 = 0.4489$\n   - $(0.33)^2 = 0.1089$\n\n3. **Sum the squared proportions**:\n   - Sum =$0.4489 + 0.1089 = 0.5578$\n\n4. **Calculate the Gini impurity**:\n   - Gini impurity =$1 - 0.5578 = 0.4422$\n\n\nA Gini impurity of 0.4422 indicates the level of impurity in this node. A Gini impurity of 0 would mean the node is pure (all samples belong to one class), while a higher value indicates more impurity or mixed classes.\n\nThis calculation helps in deciding whether to split the node further or not. The goal is to choose splits that ==minimize the Gini impurity==, leading to more homogeneous branches.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "ml",
      "statistics"
    ],
    "normalized_filename": "gini_impurity",
    "outlinks": [
      "decision_tree",
      "regression_metrics",
      "classification"
    ],
    "inlinks": [
      "decision_tree",
      "feature_importance",
      "gini_impurity_vs_cross_entropy"
    ]
  },
  {
    "category": "ML",
    "filename": "Gradient Boosted Trees",
    "sha": "8c98193d3ca79db560071a528fa448d42772da3c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gradient%20Boosted%20Trees.md",
    "text": "[[Decision Tree]] & [[Gradient Boosting]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "gradient_boosted_trees",
    "outlinks": [
      "decision_tree",
      "gradient_boosting"
    ],
    "inlinks": [
      "decision_trees_are_fragile"
    ]
  },
  {
    "category": "ML",
    "filename": "Gradient Boosting Regressor",
    "sha": "47424e2dc7b59efdde1a791f12744eb27520df34",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gradient%20Boosting%20Regressor.md",
    "text": "https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor\n\n[[Boosting]]\n\nThe `GradientBoostingRegressor` from the `sklearn.ensemble` module is a model used for regression tasks. It builds an [[Model Ensemble]] of [[Decision Tree]] in a sequential manner, where each tree tries to correct the errors made ==by the previous ones==. Here’s a breakdown of the key parameters:\n\n1. **loss**: Specifies the loss function to optimize. Default is `'squared_error'`, which is the least-squares loss function. Other options like `'absolute_error'` can be used for robustness against outliers.\n\n2. **learning_rate**: Controls the contribution of each tree to the final prediction. A smaller value (e.g., 0.01) makes the model learn more slowly, but it can lead to better generalization. Default is 0.1.\n\n3. **n_estimators**: The number of boosting stages (i.e., trees). More trees can improve performance but also increase the risk of overfitting. Default is 100.\n\n4. **subsample**: The fraction of samples to be used for fitting each tree. Setting this to a value less than 1.0 can help reduce overfitting, at the cost of a slight increase in bias. Default is 1.0 (use all samples).\n\n5. **criterion**: The function used to measure the quality of a split. `'friedman_mse'` is the default, which is an improved version of mean squared error for decision trees. Other options include `'mse'` and `'mae'`.\n\n6. **max_depth**: The maximum depth of the individual trees. This parameter controls the complexity of each tree. Default is 3, which typically works well for most tasks.\n\n7. **min_samples_split**: The minimum number of samples required to split an internal node. Default is 2, meaning any node can be split as long as there are at least 2 samples.\n\n8. **min_samples_leaf**: The minimum number of samples required to be at a leaf node. This helps control overfitting by requiring more data points at each leaf. Default is 1.\n\n9. **alpha**: The quantile used for the loss function in cases of robust regression. This is useful when dealing with data that includes outliers. Default is 0.9.\n\n10. **validation_fraction**: The fraction of training data to set aside for validation to monitor performance during training. Default is 0.1.\n\n11. **n_iter_no_change**: The number of iterations with no improvement on the validation score to wait before stopping the training early. Default is `None`, meaning no early stopping.\n\n12. **ccp_alpha**: Complexity parameter used for pruning the trees. A larger value leads to more pruning (simplifying the model), which can help prevent overfitting.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "regressor"
    ],
    "normalized_filename": "gradient_boosting_regressor",
    "outlinks": [
      "decision_tree",
      "model_ensemble",
      "boosting"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Gradient Boosting",
    "sha": "721b1a0a32af705c70ebb826e79212deed54ebcc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gradient%20Boosting.md",
    "text": "Gradient Boosting is a technique used for building predictive models [[Model Building]], particularly in tasks like regression and classification. It combines the concepts of [[Boosting]] and [[Gradient Descent]] to create strong models by sequentially combining multiple [[Weak Learners]] ([[Decision Tree]]. \n\nKey Idea: Instead of fitting a single strong model, Gradient Boosting builds multiple weak learners sequentially. Each new model focuses on ==correcting the mistakes made by the previous ones== by fitting to the residuals (differences between observed and predicted values).\n\nGradient Boosting builds an ensemble of [[Weak Learners]] (usually [[Decision Tree]]) sequentially. Each new model focuses on the errors of the previous ones, aiming to minimize the residual errors.\n\nFinal Prediction: The final prediction is made by aggregating the predictions of all the weak models, usually through a weighted sum.\n\nHigh Performance: Known for its high performance and efficiency in terms of speed and memory usage.\n\n[Watch Video Explanation](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n\n[[Model Ensemble]]\n### Key Components\n\n- [[Weak Learners]]: Typically decision trees used in the ensemble.\n- [[Loss function]]: Measures how well the model fits the data.\n- [[Learning Rate]]: Controls the contribution of each weak learner to the final model.\n\n### Examples\n\n- [[LightGBM]]\n- [[XGBoost]]\n- [[CatBoost]]\n### Benefits\n\n- Predictive Accuracy: Often outperforms other [[Machine Learning Algorithms]].\n- Feature Handling: Effectively manages [[heterogeneous features]] and automatically selects relevant ones.\n- [[Overfitting]]: Less prone to overfitting compared to other complex models.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "gradient_boosting",
    "outlinks": [
      "gradient_descent",
      "weak_learners",
      "lightgbm",
      "heterogeneous_features",
      "model_building",
      "loss_function",
      "overfitting",
      "machine_learning_algorithms",
      "model_ensemble",
      "xgboost",
      "catboost",
      "decision_tree",
      "learning_rate",
      "boosting"
    ],
    "inlinks": [
      "boosting",
      "catboost",
      "ds_&_ml_portal",
      "embedded_methods",
      "gradient_boosted_trees",
      "lightgbm_vs_xgboost_vs_catboost",
      "use_of_rnns_in_energy_sector",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Gradient Descent",
    "sha": "29211ef2a73e8c14f90d1f0fc9c7ce7fcb9ec401",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gradient%20Descent.md",
    "text": "Gradient Descent is an [[Optimisation function]] used to minimize errors in a model by iteratively adjusting its [[Model Parameters]]. It works by moving in the direction of the steepest decrease of the [[Loss function]], reducing the [[Cost Function]] over time.\n\n Core Idea\n\nAt any point on the cost function surface, gradient descent asks:\n*\"In what direction should I move to decrease the error most quickly?\"*\nThe answer is the negative gradient.\n \nHow It Works: Gradient Descent updates parameters using:\n\n$$\n\\theta = \\theta - \\alpha \\nabla_{\\theta} \\text{Cost}(\\theta)\n$$\n\nWhere:\n* $\\theta$ = [[Model Parameters]].\n* $\\alpha$ = [[Learning Rate]] (controls step size).\n* $\\nabla_{\\theta} \\text{Cost}(\\theta)$ = gradient of the [[Cost Function]] with respect to $\\theta$.\n\n Process\n1. Compute the gradient of the [[Loss function]] with respect to parameters.\n2. Update parameters in the opposite direction of the gradient.\n3. Repeat until:\n   * The cost converges (minimal change), or\n   * Maximum iterations are reached.\n\n Step Size Matters\n* Too small: Convergence is very slow.\n* Too large: May overshoot the minimum or diverge.\n\n Variants of Gradient Descent\n* [[Batch gradient descent]]: Uses the entire dataset for each update.\n* [[Stochastic Gradient Descent]]: Updates after each single example (fast but noisy).\n* [[Mini-batch gradient descent]]: Uses small subsets of data (balanced approach).\n\n Key Insights\n* Gradient Descent can be visualized using contour plots showing the path toward the minimum.\n\n Notes\n* Gradient descent uses the difference quotient for computing numerical derivatives.\n* The [[Cost Function]] value vs. iterations should decrease over time.\n* In online learning (e.g., [[Stochastic Gradient Descent]]), new data can be incorporated without recalculating the entire dataset.\n\n### Images\n\n![[Pasted image 20241224082847.png]]\n\n\n![[Obsidian_M4mzGSAx7d.png]]\n\n\n![[Obsidian_EPIqLAto5w.png]]\n\n![[Obsidian_FEGflF5RKQ.png]]",
    "aliases": [
      "GD"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "gradient_descent",
    "outlinks": [
      "optimisation_function",
      "batch_gradient_descent",
      "obsidian_epiqlato5w.png",
      "model_parameters",
      "loss_function",
      "obsidian_m4mzgsax7d.png",
      "pasted_image_20241224082847.png",
      "cost_function",
      "learning_rate",
      "stochastic_gradient_descent",
      "mini-batch_gradient_descent",
      "obsidian_fegflf5rkq.png"
    ],
    "inlinks": [
      "adam_optimizer",
      "backpropagation",
      "cost_function",
      "deep_learning",
      "ds_&_ml_portal",
      "feature_scaling",
      "fitting_weights_and_biases_of_a_neural_network",
      "gradient_boosting",
      "gradient_descent_in_linear_regression",
      "initialization_methods",
      "learning_rate",
      "lightgbm",
      "logistic_regression_in_sklearn_&_gradient_descent",
      "loss_function",
      "model_parameters_vs_hyperparameters",
      "momentum",
      "optimisation_function",
      "optimisation_techniques",
      "optimising_a_logistic_regression_model",
      "pytorch",
      "standardisation",
      "stochastic_gradient_descent",
      "xgboost",
      "z-normalisation"
    ]
  },
  {
    "category": "ML",
    "filename": "Gradient descent in linear regression",
    "sha": "1d715c7d92987b5026d018953397689a268df7c3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Gradient%20descent%20in%20linear%20regression.md",
    "text": "Gradient descent in [[Linear Regression]]\n\nRelated terms:\n- [[Gradient Descent]]\n\nIt ==iteratively== updates coefficients to minimize error.\n\nGradient descent is an optimization algorithm used to minimize the cost function in linear regression by iteratively adjusting the model parameters (coefficients). Here's how it works with linear regression:\n\n1. Initialize Parameters: Start with initial guesses for the coefficients (weights), typically set to zero or small random values.\n2. Compute Predictions: Use the current coefficients to make predictions for the dependent variable $\\hat{y}$ using the linear regression model: $\\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n$\n3. Calculate the Cost Function:Compute the loss function, SSE.\n4. Compute the Gradient: Calculate the gradient of SSE function with respect to each coefficient. The gradient is a vector of partial derivatives indicating the direction and rate of change of the cost function:\n     $$\\frac{\\partial J}{\\partial b_j} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i) x_{ij}$$\n   Here, $x_{ij}$ is the value of the $j$-th feature for the $i$-th observation.\n5. Update the Coefficients: Adjust the coefficients in the opposite direction of the gradient to reduce the cost function. This is done using a [[Learning Rate]]$\\alpha$, which controls the size of the steps taken: $b_j = b_j - \\alpha \\frac{\\partial J}{\\partial b_j}$\n\n6. Iterate & Converge Repeat steps 2 to 5 until the cost function converges to a minimum or a predefined number of iterations is reached. The algorithm converges when the changes in the cost function or the coefficients become very small, indicating that the minimum has been reached.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "optimisation"
    ],
    "normalized_filename": "gradient_descent_in_linear_regression",
    "outlinks": [
      "gradient_descent",
      "learning_rate",
      "linear_regression"
    ],
    "inlinks": [
      "linear_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Graph Neural Network",
    "sha": "6bd47bdb2b72a3ed271c6357053ffe06c63d5a19",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Graph%20Neural%20Network.md",
    "text": "A Graph Neural Network (GNN) is a type of neural network designed to operate on graph-structured data. See `PyTorch Geometric` or `DGL`.\n\nResources:\n- [How Graph Neural Networks Are Transforming Industries](https://www.youtube.com/watch?v=9QH6jnwqrAk&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=6)\n\nUse cases:\n- [[Recommender systems]] i.e. Uber, Pinterest (PinSage)\n- Traffic Prediction - Deepmind in google maps\n- Weather forecasting - GraphCast - Deepmind\n- Data Mining - Relational Deep Learning\n- Material Science - Deepmind - GNome - Density function theory.\n- Drug Discovery - MIT - antibiotic activities\n\nRelated:\n- [[Neural network]]",
    "aliases": [
      "GNN"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "graph"
    ],
    "normalized_filename": "graph_neural_network",
    "outlinks": [
      "recommender_systems",
      "neural_network"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Graph Theory Community",
    "sha": "4dd58dfb8c6c0e562e81307ab3caabb71466bfec",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Graph%20Theory%20Community.md",
    "text": "In graph theory, a community (also known as a cluster or module) is a group of nodes that are more densely connected to each other than to the rest of the network.\n \n\nhttps://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html\n\n### Intuition\nCommunities often represent:\n- Functional units in biological networks (e.g., protein complexes)\n- Groups of friends or followers in social networks\n- Topical clusters in knowledge graphs or citation networks\n\nThey capture meso-scale structure—between the local (node/edge) and global (graph-level) scale.\n\n### Formal Definition\nThere is no single universal definition, but communities typically exhibit:\n\n- High intra-community density: lots of edges within the group\n- Low inter-community density: few edges connecting to other groups\n\nMathematically, a common goal is to maximize modularity, a measure that quantifies the density of links inside communities compared to links between them.\n\n### Community Detection Algorithms\nSome widely used algorithms:\n\n| Algorithm | Description |\n|-|-|\n| Louvain | Fast and widely used; optimizes modularity |\n| Girvan–Newman | Based on removing high-betweenness edges |\n| Label Propagation | Propagates labels through the network |\n| Leiden | Improved version of Louvain for better quality and performance |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "graph"
    ],
    "normalized_filename": "graph_theory_community",
    "outlinks": [],
    "inlinks": [
      "graph_theory"
    ]
  },
  {
    "category": "ML",
    "filename": "GridSeachCv",
    "sha": "1b4ceff118a8acdeb820d0796e022b693264e360",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/GridSeachCv.md",
    "text": "Used [[GridSeachCv]] to search through the [[Hyperparameter]] space\n\n```python\nrf_regressor = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n\n# Model Training with best hyperparameters\nrf_regressor = RandomForestRegressor(**best_params, random_state=42)\nrf_regressor.fit(X_train, y_train)\n```\n\nGiven a parameter grid of [[Hyperparameter]], a model, then you model it on the hypers, then gives you the best hypers, that gives the highest cross validation performance.\n\n![[Pasted image 20240128194244.png|500]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "gridseachcv",
    "outlinks": [
      "hyperparameter",
      "gridseachcv",
      "pasted_image_20240128194244.png"
    ],
    "inlinks": [
      "decision_tree",
      "gridseachcv",
      "hyperparameter_tuning",
      "model_selection"
    ]
  },
  {
    "category": "ML",
    "filename": "Growth Models in Time Series",
    "sha": "c16734a623112bc9835c6cf3b97d37496f0669ac",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Growth%20Models%20in%20Time%20Series.md",
    "text": "### Logistic vs. Gompertz growth\n\nBoth are **S-shaped (sigmoid) growth curves**, often used to model adoption, population growth, or diffusion of innovations. They differ mainly in **where the inflection point (maximum growth rate) occurs**.\n\n#### **Logistic Growth**\n\n* Symmetric S-shape.\n* Growth accelerates until halfway to saturation, then slows down.\n* Formula:\n\n  $$\n  f(t) = \\frac{K}{1 + \\exp(-r (t - t_0))}\n  $$\n\n  * $K$ = carrying capacity (max customers)\n  * $r$ = growth rate\n  * $t_0$ = midpoint (inflection point, where growth is fastest)\n\n#### **Gompertz Growth**\n\n* Asymmetric S-shape.\n* Inflection point occurs **earlier** than in the logistic curve.\n* Growth slows sooner, which can better reflect **customer adoption curves** in real-world settings.\n* Formula:\n\n  $$\n  f(t) = K \\cdot \\exp\\big(-\\exp(-r (t - t_0))\\big)\n  $$\n\n  * $K$ = asymptotic maximum (saturation level)\n  * $r$ = growth rate\n  * $t_0$ = time at which growth rate is maximal\n\n### Key Differences\n\n* **Logistic:** Symmetric, assumes adoption accelerates until 50% of maximum.\n* **Gompertz:** Skewed, assumes fastest growth happens **earlier** (common in customer/product growth where early adopters drive rapid initial uptake).\n\n### Visual intuition\n\nIf you plotted them:\n\n* Logistic = balanced \"S\" around the midpoint.\n* Gompertz = “front-loaded” S, with rapid rise at the start and slower taper toward saturation.\n\n### Why consider Gompertz for your case?\n\n* Logistic might overestimate remaining growth (too symmetric), while Gompertz would more realistically taper earlier.\n\n### Saturation Check\n\nWhen you use **logistic** or **Gompertz** models, both assume that customer growth will eventually **plateau** at some maximum value $K$ (the **carrying capacity**).\n\nA **saturation check** is simply a way of asking:\n\n* **How close are we to that plateau already?**\n* **At what time will growth essentially stop (reach, say, 90–95% of $K$)?**\n\n### Why it matters\n\n* **Business planning**: If you’re already close to saturation, forecasts should expect only small incremental growth.\n* **Model choice**: If $K$ is unrealistically high compared to your market size, the logistic/Gompertz fit may not be appropriate.\n* [[Energy Demand Forecasting]]/**Forecast horizon**: If you hit 95% saturation in 18 months, forecasting 5 years ahead isn’t meaningful.\n\n### How to check saturation\n\n#### 1. Define a threshold (e.g., 95% of $K$)\n\nFor the fitted curve Logistic/Gompertz above. We solve for $t$ where $f(t) = 0.95 , K$.\n\n#### 2. Interpret the result\n\n* If $t$ is **in the past** -> you’re already basically saturated.\n* If $t$ is **within your 2-year forecast horizon** -> growth will flatten soon.\n* If $t$ is **well beyond forecast horizon** -> you’re still in growth mode.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "growth_models_in_time_series",
    "outlinks": [
      "energy_demand_forecasting"
    ],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "ML",
    "filename": "Hierarchical Clustering",
    "sha": "903bfc3639bf275f5e19ccdaa3e546ccaf906df8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Hierarchical%20Clustering.md",
    "text": "Hierarchical clustering builds a treelike structure of clusters, with similar clusters merged together at higher levels.\n\nHierarchical clustering builds a tree-like structure of clusters, with similar clusters merged together at higher levels.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "hierarchical_clustering",
    "outlinks": [],
    "inlinks": [
      "clustering"
    ]
  },
  {
    "category": "ML",
    "filename": "High cross validation accuracy is not directly proportional to performance on unseen test data",
    "sha": "a34b556409176352ee716e91dcae70eabb7a50d9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/High%20cross%20validation%20accuracy%20is%20not%20directly%20proportional%20to%20performance%20on%20unseen%20test%20data.md",
    "text": "Reasons a Model with High [[Cross Validation]] Accuracy May Perform Poorly on Unseen Test Data\n\n[[Data Leakage]]: \n  - Information from test folds leaks into training, inflating CV accuracy.\n  - Solution: Apply [[Preprocessing]] independently within each CV fold.\n\nOverfitting: \n  - Model captures noise in training data, leading to high CV but low test accuracy.\n  - Solution: Use simpler models, regularization, and evaluate test performance during [[Hyperparameter Tuning]].\n\nInsufficient Cross-Validation Folds: \n  - Too few folds lead to high variance in performance estimates.\n  - Solution: Use more folds (e.g., 5- or 10-fold CV) for reliable estimates.\n\nOver-Optimized Hyperparameters: \n  - Excessive tuning results in models that fail to generalize.\n  - Solution: Reserve a separate validation set for tuning and use nested cross-validation.\n\nSmall Dataset Size: \n  - Small datasets may lead to unreliable accuracy estimates.\n  - Solution: Use bootstrapping or collect more data if possible.\n\nInappropriate Performance Metric: \n  - CV accuracy may not align with the true objective (e.g., [[Imbalanced Datasets]]).\n  - Solution: Choose appropriate [[Evaluation Metrics]] based on the problem context.\n\n### Practical Recommendations\n- Evaluate the model on a completely independent test set after cross-validation.\n- Check for [[Distributions|distribution]] differences between training and test data.\n- Avoid data leakage by ensuring strict separation of preprocessing in CV folds.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "ml"
    ],
    "normalized_filename": "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
    "outlinks": [
      "preprocessing",
      "cross_validation",
      "hyperparameter_tuning",
      "distributions",
      "data_leakage",
      "imbalanced_datasets",
      "evaluation_metrics"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Histogram",
    "sha": "51562a22aa23f0819bc9b4aec0937e794e288d06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Histogram.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "histogram",
    "outlinks": [],
    "inlinks": [
      "data_visualisation"
    ]
  },
  {
    "category": "ML",
    "filename": "How do we evaluate of LLM Outputs",
    "sha": "83e16cab0e3828425eb964f0e502e36f992385be",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/How%20do%20we%20evaluate%20of%20LLM%20Outputs.md",
    "text": "Methods for assessing the quality and relevance of LLM-generated outputs, critical for improving model performance.\n\nThe evaluation of [[LLM]] outputs involves various methodologies to assess their quality and relevance. \n\n### Important\n - Evaluating LLM outputs requires both quantitative metrics ([[LLM Evaluation Metrics]]) and qualitative assessments (human judgment).\n - The iterative feedback loop from evaluations informs model improvements and prompt engineering strategies.\n\n### Follow up questions\n - How does the inclusion of diverse datasets impact the robustness of LLM evaluations\n - [[Evaluating the effectiveness of prompts]]\n### Related Topics\n - [[Prompt Engineering]] in natural language processing",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "how_do_we_evaluate_of_llm_outputs",
    "outlinks": [
      "llm",
      "evaluating_the_effectiveness_of_prompts",
      "llm_evaluation_metrics",
      "prompt_engineering"
    ],
    "inlinks": [
      "llm"
    ]
  },
  {
    "category": "ML",
    "filename": "How to use Sklearn Pipeline",
    "sha": "0ae8ea0e15b2250aa5d0e74fbb6f9b2f27eb51c6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/How%20to%20use%20Sklearn%20Pipeline.md",
    "text": "[[Scikit-Learn]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#question",
      "python"
    ],
    "normalized_filename": "how_to_use_sklearn_pipeline",
    "outlinks": [
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Hyperparameter Tuning",
    "sha": "f2465dc70ab11c9e720958f68609ab58d943ec31",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Hyperparameter%20Tuning.md",
    "text": "Objective:\n- Tune the model’s hyperparameters to improve performance. For example, in regularized linear regression, the main hyperparameter to tune is the regularization strength (e.g., `alpha` in Ridge or Lasso).\n- Use [[Cross Validation]] to evaluate the model’s performance with different hyperparameters.\n\nOptimization Techniques:\n- [[GridSeachCv]]: Exhaustively searches through a specified subset of hyperparameters.\n- Random Search: Randomly samples from the hyperparameter space, often more efficient than grid search.\n- [[uncategorised/Optuna]]\n- [[Regularisation]]: Often part of the hyperparameter tuning process, especially in models prone to overfitting.\n\nKey Considerations\n- Balance Between Exploration and Exploitation: Ensure a good balance between exploring the hyperparameter space and exploiting known good configurations.\n- [[Cross Validation]]: Use cross-validation to ensure that the hyperparameter tuning process is robust and not overfitting to a particular train-test split.\n\nOrder matters: See [[Interpretable Decision Trees]]. Example when tuning hyperparameters for [[Random Forest]] try the following:\n\n1. High Train Accuracy, Low Test Accuracy (Overfitting)\n- Objective: Reduce model complexity to prevent overfitting.\n- Parameters to Adjust:\n\t- `max_depth`: Limit the depth of each tree.\n\t- `min_samples_split`: Increase the minimum number of samples required to split a node.\n\t- `max_features`: Reduce the number of features considered for splitting.\n\t- `n_estimators`: Decrease the number of trees in the forest.\n\n2. Low Train Accuracy, Low Test Accuracy (Underfitting)\n- Objective: Increase model complexity to improve learning capacity.\n- Parameters to Adjust:\n\t- `n_estimators`: Increase the number of trees.\n\t- `max_depth`: Allow deeper trees.\n\t- `min_samples_split`: Decrease the minimum number of samples required to split a node.\n\n3. Moderate Train Accuracy, Moderate Test Accuracy (Balanced but Low Performance)\n- Objective: Fine-tune the model for better performance.\n- Parameters to Adjust:\n\t- `max_features`: Experiment with different numbers of features.\n\t- `max_depth`: Adjust the depth of trees.\n\t- `n_estimators`: Fine-tune the number of trees.\n\t- `min_samples_split`: Adjust the minimum samples for splitting.\n\n4. High Train Accuracy, High Test Accuracy (Optimal)\n- Objective: Make minor adjustments for incremental improvements.\n- Parameters to Adjust:\n\t- `n_estimators`: Slightly adjust the number of trees.\n\t- `max_features`: Fine-tune the number of features.\n\t- `min_samples_split`: Make small adjustments to the minimum samples for splitting.\n\n### Links\n\nSee [[ML_Tools]]: [[Hyperparameter_tuning_GridSearchCV.py]]\n\nHyperparameter_tuning_RF.py\nVideo link: https://youtu.be/jUxhUgkKAjE?list=PLtqF5YXg7GLltQSLKSTnwCcHqTZASedbO&t=765",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "process"
    ],
    "normalized_filename": "hyperparameter_tuning",
    "outlinks": [
      "regularisation",
      "gridseachcv",
      "uncategorised/optuna",
      "cross_validation",
      "random_forest",
      "interpretable_decision_trees",
      "hyperparameter_tuning_gridsearchcv.py",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "embedded_methods",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "hyperparameter",
      "optuna",
      "pycaret",
      "test_loss_when_evaluating_models",
      "train-dev-test_sets",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Hyperparameter",
    "sha": "3637d432ad372dea42b3f249d327f7781568416f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Hyperparameter.md",
    "text": "Hyperparameters are parameters set before training that control the learning process, such as:\n- the number of nodes in a [[Neural network]] \n- or k in [[K-nearest neighbours|KNN]].\n\nThe best ones are found with [[Hyperparameter Tuning]].\n\nAlso see:\n- [[Model Parameters]]\n- [[Model parameters vs hyperparameters]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "optimisation"
    ],
    "normalized_filename": "hyperparameter",
    "outlinks": [
      "model_parameters_vs_hyperparameters",
      "model_parameters",
      "hyperparameter_tuning",
      "k-nearest_neighbours",
      "neural_network"
    ],
    "inlinks": [
      "adam_optimizer",
      "catboost",
      "class_separability",
      "cross_validation",
      "decision_tree",
      "ds_&_ml_portal",
      "fitting_weights_and_biases_of_a_neural_network",
      "gridseachcv",
      "isolated_forest",
      "k-means",
      "learning_rate",
      "model_optimisation",
      "model_parameters_vs_hyperparameters",
      "momentum",
      "neural_network",
      "optuna",
      "pycaret",
      "random_forest",
      "regularisation_of_tree_based_models",
      "weak_learners"
    ]
  },
  {
    "category": "ML",
    "filename": "ICE Plot",
    "sha": "2d59e6af97d28f5d05237e4efa3b21c51096696a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/ICE%20Plot.md",
    "text": "Unlike a Partial Dependence plot, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample.\n\nhttps://scikit-learn.org/stable/modules/partial_dependence.html#individual-conditional-expectation-ice-plot\n\nRelated:\n- [[Partial Dependence Plot]]\n- [[Feature Importance]]",
    "aliases": [],
    "date modified": "2-11-2025",
    "tags": [],
    "normalized_filename": "ice_plot",
    "outlinks": [
      "partial_dependence_plot",
      "feature_importance"
    ],
    "inlinks": [
      "partial_dependence_plot"
    ]
  },
  {
    "category": "ML",
    "filename": "Impact of multicollinearity on model parameters",
    "sha": "cebca9d0c1c5cdab75896d8f650a4e8bda2bb888",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Impact%20of%20multicollinearity%20on%20model%20parameters.md",
    "text": "See https://youtu.be/StSAJIZuqws?t=655\n\n```R\n)\n\n# [[Monte Carlo Simulation]]: Multicollinearity & Harm\nresults = expand_grid(\nrho = seq(0, 0.95, 0.05),\nrep = 1:1000\n) %>%\nmutate(\nsim = map(rho, function(p) {\n\nset.seed(runif(1, 1, 10000) %>% ceiling)\nR = matrix(c(1, p, p, 1), nrow = 2, ncol = 2, byrow = TRUE)\nSigma = cor2cov(R, c(1, 1))\n\ndata = MASS :: mvrnorm(n= 30, mu = c(0, 0), Sigma = Sigma) %>%\nas_tibble %>%\nmutate( Y=1+0.5*V1+0.5*V2+ rnorm(30) )\n\nmodel = 1m(Y ~ V1 + V2, data = data)\n\nsummary(model) $coefficients %>% as_tibble\n})\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "modeling",
      "statistics"
    ],
    "normalized_filename": "impact_of_multicollinearity_on_model_parameters",
    "outlinks": [
      "monte_carlo_simulation"
    ],
    "inlinks": [
      "multicollinearity"
    ]
  },
  {
    "category": "ML",
    "filename": "Inertia K Means Cost Function",
    "sha": "850bc7a7b83f453491d432f22322a2b5ee3dd5d4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Inertia%20K%20Means%20Cost%20Function.md",
    "text": "Definition:  \nInertia is the sum of squared distances from each point to its assigned cluster center. Lower values indicate more compact clusters.\n\n==Is this WCSS?==\n\nUse in [[K-means]]:\n- Used to determine optimal $k$ via the elbow method: [[WCSS and elbow method]]\n- Sensitive to scale and number of features\n    \nExploratory Questions:\n- How does inertia correlate with cluster separation?\n- Is it meaningful when clusters are non-spherical?\n- What are its limitations in high-dimensional spaces?\n\nRelated:\n- [[Cluster Density]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "evaluation"
    ],
    "normalized_filename": "inertia_k_means_cost_function",
    "outlinks": [
      "k-means",
      "wcss_and_elbow_method",
      "cluster_density"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Interoperability",
    "sha": "4e9018351f8546414085e66417063619da0f2678",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Interoperability.md",
    "text": "The ability of computer systems or software to exchange and make use of information",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "interoperability",
    "outlinks": [],
    "inlinks": [
      "batch_vs_powershell_scripts",
      "benefits_of_data_transformation",
      "data_engineering",
      "data_principles",
      "digital_transformation",
      "performance_dimensions",
      "pydantic",
      "why_json_is_better_than_pickle_for_untrusted_data"
    ]
  },
  {
    "category": "ML",
    "filename": "Interpretability",
    "sha": "77bd2f5ef88e07337d920a09578779f50c2c2402",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Interpretability.md",
    "text": "# Links\n1. [Interpretability Importance](https://christophm.github.io/interpretable-ml-book/interpretability-importance.html)\n2. https://christophm.github.io/interpretable-ml-book/index.html\n\n# Interpretability\n\nInterpretability in machine learning (ML) is about understanding the reasoning behind a model's predictions. It involves making the model's decision-making process comprehensible to humans, which is crucial for trust, debugging, and ensuring fairness and reliability. \n\nImportance of Interpretability\n- **Trust**: Stakeholders are more likely to trust models they understand.\n- **Debugging**: Easier to identify and fix issues in interpretable models.\n- **Bias Detection**: Helps identify biases in data and model predictions.\n- **Social Acceptance**: Models that can explain their decisions are more socially acceptable.\n- **Fairness and Reliability**: Ensures models are fair and reliable, especially in high-impact areas.\n\nLevels of Interpretability\n- **Global, Holistic Model Interpretability**: Involves comprehending the entire model at once, including feature importance and interactions. This level of interpretability is challenging, especially for models with many parameters.\n- **Global Model Interpretability on a Modular Level**: Focuses on understanding parts of the model (e.g., weights in linear models or splits in decision trees). While individual parameters may be interpretable, their interdependence complicates interpretation.\n- **Local Interpretability for a Single Prediction**: Allows for detailed examination of why a model made a specific prediction for an individual instance. This can provide clearer insights as local predictions may exhibit simpler relationships than the global model.\n\nChallenges in Achieving Interpretability\n- Effective interpretation requires **==context==**; for instance, understanding the significance of weights in linear models is often conditional on other feature values.\n- **Trade-offs**: Users must weigh the need for predictions against the need for understanding the rationale, particularly in contexts where decisions have significant consequences.\n- **Human Learning**: Interpretability supports human curiosity, facilitating updates to mental models based on new information.\n- **Safety and Bias Detection**: Essential for high-risk applications (e.g., self-driving cars) and for identifying biases in decision-making.\n- **Social Acceptance**: Machines that explain their decisions tend to be more accepted.\n\n---\n# Properties of Explanations\n\nThese properties provide a framework for evaluating and comparing explanation methods in interpretable machine learning, ensuring they are effective and useful for understanding model predictions. \n\n### Properties of Explanation Methods\n\n1. **Expressive Power**: Refers to the types of explanations generated (e.g., rules, decision trees, natural language).\n\n2. **Translucency**: Measures the extent to which an explanation method examines the model's internal parameters. High translucency allows for more informative explanations, while low translucency enhances portability.\n\n3. **Portability**: Indicates the range of models compatible with the explanation method. Methods that treat models as black boxes (e.g., surrogate models) are more portable.\n\n4. **Algorithmic Complexity**: Reflects the computational demands of generating explanations, which is crucial when processing time is a concern.\n\n### Properties of Explanations\n\n1. **Accuracy**: Assesses how well the explanation predicts unseen data. High accuracy is vital if the explanation is used in place of the model.\n\n2. **Fidelity**: Evaluates how closely the explanation matches the black box model's predictions. High fidelity is essential; otherwise, the explanation is ineffective.\n\n3. **Consistency**: Measures how similar explanations are across models trained on the same task. High consistency is desirable when models rely on similar relationships.\n\n4. **Stability**: Examines how consistent explanations are for similar instances. High stability is preferred to avoid erratic changes due to minor variations in input features.\n\n5. **Comprehensibility**: Assesses how easily humans understand the explanations. This property is challenging to define but is critical for effective communication of model behavior.\n\n6. **Certainty**: Indicates whether the explanation reflects the model's confidence in its predictions, adding value by clarifying prediction reliability.\n\n7. **Degree of Importance**: Evaluates how well the explanation identifies the importance of features involved in a decision.\n\n8. **Novelty**: Addresses whether the instance to be explained lies outside the training data distribution, affecting prediction accuracy.\n\n9. **Representativeness**: Measures how many instances an explanation covers, ranging from individual predictions to broader model interpretations.\n\n# Understanding an Explanation\n\nHere are the key takeaways on human-friendly explanations in interpretable machine learning:\n\nNeed comprehensibility and accuracy in explanations to enhance user understanding and trust in machine learning models. \n### Importance of Human-Friendly Explanations\n1. **Preference for Short Explanations**: Humans favor concise explanations (1-2 causes) that contrast current situations with hypothetical scenarios where the event did not occur.\n\n2. **Nature of Explanations**: An explanation answers \"why\" questions, focusing primarily on everyday situations rather than general scientific queries.\n\n### Characteristics of Good Explanations\n1. **Contrastive Nature**: Good explanations highlight differences between predicted outcomes, aiding comprehension. For instance, explaining why a loan was rejected by comparing it to a hypothetical accepted application is more effective.\n\n2. **Selective Focus**: People tend to prefer explanations that identify one or two key causes rather than exhaustive lists. This selective approach aligns with the \"Rashomon Effect,\" where multiple valid explanations can exist for the same event.\n\n3. **Social Context**: Explanations are influenced by the social context and audience. Tailoring explanations to the audience’s knowledge level enhances understanding.\n\n4. **Emphasis on Abnormal Causes**: Humans focus on rare or abnormal causes to explain events. Including these in explanations can significantly enhance clarity.\n\n5. **Truthfulness vs. Selectivity**: While truthfulness is important, selectivity often takes precedence. A concise, selective explanation is more impactful than a comprehensive but complex one.\n\n6. **Consistency with Prior Beliefs**: Explanations that align with the explainee's existing beliefs are more readily accepted, highlighting the challenge of integrating complex model behaviors that contradict common intuitions.\n\n7. **Generality**: Good explanations should be generalizable, but abnormal causes can sometimes provide more compelling insights.\n\n### Implications for Interpretable Machine Learning\n- **Design Considerations**: Create explanations that are short, contrastive, and tailored to the audience’s background.\n- **Methodology**: Incorporate techniques that can produce contrastive explanations while maintaining accuracy and fidelity to the model's predictions.\n- **Audience Awareness**: Understanding the audience's social context and prior beliefs is crucial for effective communication of model outcomes.\n\n- Understand how the model makes predictions.\n- Use techniques like feature importance scores or LIME to explain individual predictions.\n\n- **How can we design machine learning models that are both accurate and interpretable?** While deep learning models often achieve high accuracy, their complexity can make them difficult to interpret. This raises questions about how to balance accuracy and interpretability. Exploring techniques for visualizing and understanding the internal representations learned by deep networks, or developing inherently interpretable models that still achieve high performance, could lead to greater trust and adoption of machine learning in critical applications like healthcare and finance.",
    "aliases": [
      "explainability",
      "interpretable"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "drafting",
      "explainability"
    ],
    "normalized_filename": "interpretability",
    "outlinks": [],
    "inlinks": [
      "activation_function",
      "addressing_multicollinearity",
      "anomaly_detection",
      "assessing_gen_ai_generated_content",
      "boosting",
      "chain_of_thought",
      "challenges_to_model_deployment",
      "classification",
      "clustering",
      "covariance_vs_correlation",
      "decision_tree",
      "ds_&_ml_portal",
      "embedded_methods",
      "feature_evaluation",
      "feature_extraction",
      "feature_importance",
      "feature_selection_vs_feature_importance",
      "graphrag",
      "holt-winters_(exponential_smoothing)",
      "holt-winters_vs_arima",
      "how_llms_store_facts",
      "k-nearest_neighbours",
      "language_models_large_(llms)_vs_small_(slms)",
      "local_interpretable_model-agnostic_explainations",
      "machine_learning_algorithms",
      "model_ensemble",
      "model_interpretability",
      "model_observability",
      "partial_dependence_plot",
      "pca_explained_variance_ratio",
      "principal_component_analysis",
      "regression_metrics",
      "regularisation",
      "root_mean_squared_error",
      "small_language_models",
      "standard_deviation",
      "statistical_assumptions",
      "text2cypher",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Interpreting logistic regression model parameters",
    "sha": "a76e309d88dabc27b29bd885d986bd96bad31354",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Interpreting%20logistic%20regression%20model%20parameters.md",
    "text": "How do this in terms of odds, probabilities ,odds ratio.\n\n[[Logistic Regression]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "ml"
    ],
    "normalized_filename": "interpreting_logistic_regression_model_parameters",
    "outlinks": [
      "logistic_regression"
    ],
    "inlinks": [
      "logistic_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Isolated Forest",
    "sha": "a4ff4b9b5914cbc795d6cfd4a0820d9bbd69872f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Isolated%20Forest.md",
    "text": "Isolation Forest (iForest) is an [[Model Ensemble]]-based method used for anomaly detection. It operates by isolating data points using a series of random binary splits.\n \nThe key idea is that [[uncategorised/Outliers|anomalies]], beingM rare and different, are easier to isolate and thus require fewer splits. \n\nMathematically, the isolation of a point is captured by the path length in a decision tree, where shorter paths indicate anomalies. The algorithm constructs multiple isolation trees, and the ==anomaly score of a point== is determined by the average path length across all trees. \n\nIsolation Forest is highly efficient for large datasets and is particularly useful when the assumption is that anomalies are rare and distinct from normal instances.\n\n**Steps:**\n- Randomly select a feature and a split value between the maximum and minimum values of that feature.\n- Repeat this process to create a tree structure.\n- Anomalies are isolated faster than normal points, leading to shorter path lengths in the tree.\n- The average path length across multiple trees is used to compute an anomaly score.\n\n Key Components:  \n - **Isolation Trees (iTrees)**: Binary trees where the goal is to isolate observations based on randomly chosen features and split values.  \n - **Anomaly Score**: Calculated based on the average path length across all isolation trees.  \n - **Path Length**: Anomalies tend to have shorter path lengths as they are easier to isolate.  \n - **Random Splitting**: Random [[Feature Selection]] and splitting result in the separation of instances, with fewer splits isolating anomalies.\n\nImportant\n - Anomalies are identified based on shorter average path lengths in the isolation forest, ==indicating that fewer splits are needed to isolate them.==  \n - The method scales well with large datasets because it relies on randomly generated trees, avoiding complex distance or density computations.\n - Isolation Forest assumes that anomalies are few and distinct; it may perform poorly when anomalies are not easily distinguishable.  \n - The method is sensitive to the [[Hyperparameter]] such as the number of trees and sample size.\n\nFollow up questions\n - How does the isolation forest compare to density-based methods like [[DBSCAN]] in terms of detecting complex anomalies? [[Anomaly Detection with Clustering]]\n - What impact does the choice of sample size have on the performance and accuracy of isolation forests in high-dimensional data?\n\nRelated Topics\n - [[Random Forest]] for classification and regression  \n - One-Class [[Support Vector Machines|SVM]] for anomaly detection",
    "aliases": [
      "anomaly isolation",
      "iForest"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "data_quality"
    ],
    "normalized_filename": "isolated_forest",
    "outlinks": [
      "hyperparameter",
      "anomaly_detection_with_clustering",
      "random_forest",
      "uncategorised/outliers",
      "support_vector_machines",
      "feature_selection",
      "model_ensemble",
      "dbscan"
    ],
    "inlinks": [
      "anomaly_detection_with_clustering",
      "anomaly_detection_with_statistical_methods",
      "model_ensemble",
      "model_observability",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Jaccard Coefficient",
    "sha": "07ad7c0dc2134b853a088b4b855e3f5c9c8cf6b4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Jaccard%20Coefficient.md",
    "text": "Definition: Measures similarity between two sets: Formula:\n\n  $$\n  J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\n  $$\nInterpretation: Proportion of shared elements relative to the union.\nBest for: Binary boolean data (presence or absence of attributes, keywords, clicks).\n\nJaccard = \"How much overlap do two sets have?\"",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering",
      "math"
    ],
    "normalized_filename": "jaccard_coefficient",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "K-means",
    "sha": "a71b576580ee03abe06423e5acbee903d17a05ef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/K-means.md",
    "text": "K-means clustering is an [[Unsupervised Learning]] algorithm that partitions data into (k) clusters. Each data point is assigned to the cluster with the nearest centroid.\n\nThe algorithm partitions a dataset into k clusters by assigning data points to the closest cluster mean. The means are updated iteratively until convergence is achieved.\n## Key Features\n\n- Unsupervised Learning: K-means organizes unlabeled data into meaningful groups without prior knowledge of the categories.\n- [[Hyperparameter]] k: The number of clusters must be specified beforehand. The optimal number of clusters can be determined using [[WCSS and elbow method]].\n\nAlgorithm Process:\n  1. Randomly choose k initial centroids.\n  2. Assign each data point to the nearest centroid.\n  3. ==Recalculate== the centroids based on the current cluster assignments.\n  4. Repeat steps 2 and 3 until convergence (i.e., centroids no longer change significantly).\n\nVisualization: Scatterplots can be used to visualize clusters and their centroids.\n\nAdaptability: K-means can be updated with new data and allows for comparison of changes in centroids over time.\n\nThe initial centroids can effect the end results. \n\nTo correct this the algo is run multiple times with varying starting positions.\n\nThe centroids are updated after each iteration.\n\n![[Pasted image 20241230200255.png]]\n\n\n## Limitations\n\n- Sensitivity to Initialization: The algorithm is sensitive to the initial placement of centroids, which can affect the final clustering outcome.\n- Predefined Number of Clusters: The number of clusters k must be specified in advance, which may not always be straightforward.\n## Resources\n- [Statquest Video on K-means](https://www.youtube.com/watch?v=4b5d3muPQmA)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "k-means",
    "outlinks": [
      "hyperparameter",
      "unsupervised_learning",
      "pasted_image_20241230200255.png",
      "wcss_and_elbow_method"
    ],
    "inlinks": [
      "algorithms",
      "clustering",
      "dbscan",
      "feature_scaling",
      "gaussian_mixture_models",
      "inertia_k_means_cost_function",
      "kmeans_vs_gmm",
      "machine_learning_algorithms",
      "model_parameters",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "K-nearest neighbours",
    "sha": "b460f9ca783d996370570891af74cdd6b763c3b8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/K-nearest%20neighbours.md",
    "text": "K-Nearest Neighbors is a [[non-parametric]], [[Supervised Learning]] algorithm used for both classification and regression tasks. It predicts the label of a new data point based on the labels of its $k$ nearest neighbors in the training data, where $k$ is a user-defined positive integer.\n\n### How It Works\n\n* Classification: Assigns the class most common among the $k$ nearest neighbors.\n* Regression: Predicts the average of the target values of the $k$ nearest neighbors.\n* Distance Metric: Common choices include Euclidean and Manhattan distance; the choice affects neighbor selection and model performance.\n* Choice of $k$: Small $k$: sensitive to noise, Large $k$ : smoother but may blur decision boundaries\n### Characteristics\n\n* [[non-parametric]]: Makes no assumptions about the underlying data distribution.\n* Instance-based: Stores training data and delays computation until prediction.\n* Simple and [[Interpretability|interpretable]]: Easy to understand and implement.\n* Computationally expensive: Requires distance computation to all training points at prediction time.\n\n### Use Cases\n\n* Works well when the decision boundary is irregular or non-linear.\n* Most effective on smaller datasets due to computational cost.\n\n### Applications\n\n* [[Recommender systems]]\n* Pattern recognition",
    "aliases": [
      "KNN"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "ml"
    ],
    "normalized_filename": "k-nearest_neighbours",
    "outlinks": [
      "recommender_systems",
      "interpretability",
      "non-parametric",
      "supervised_learning"
    ],
    "inlinks": [
      "classification",
      "ds_&_ml_portal",
      "dynamic_time_warping",
      "hyperparameter",
      "standardisation",
      "supervised_learning",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Keras",
    "sha": "d38b1db3e03c0a82d3cb56b6c3c2469a636f9c23",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Keras.md",
    "text": "Keras is a high-level deep learning API in Python, designed to simplify building and training neural networks.\n\nRelated:\n- [[Tensorflow]]\n\n\n\n### [[Keras]]\n\n**API Level**: \n  Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It allows for easy and fast prototyping through user-friendly, modular, and extensible code.\n  \n**Integration**: \n  Keras is tightly integrated with TensorFlow 2.0, providing a simplified interface for building and training deep learning models.\n  \n**Purpose**: \n  Designed to enable fast experimentation, Keras is ideal for beginners and researchers who need to quickly prototype and test new ideas.\n  \n**Performance**: \n  While Keras simplifies [[Model Building]], it may not be as performant as lower-level frameworks like TensorFlow when it comes to fine-tuning and optimizing models for production.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "python"
    ],
    "normalized_filename": "keras",
    "outlinks": [
      "tensorflow",
      "model_building",
      "keras"
    ],
    "inlinks": [
      "deep_learning_frameworks",
      "keras",
      "lstm",
      "pytorch_vs_tensorflow",
      "transfer_learning",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "Kernel Density Estimation",
    "sha": "0793e84cdb9dc24b56674e44d7616ce4f233cf31",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Kernel%20Density%20Estimation.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "statistics"
    ],
    "normalized_filename": "kernel_density_estimation",
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "Kernelling",
    "sha": "39eb00693a2c11427fef9de25fc277870d37746f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Kernelling.md",
    "text": "[[Kernelling]] is a technique where the [[Support Vector Machines|SVM]] uses a kernel function to map the dataset into a higher-dimensional space, making it easier to identify separable clusters that may not be apparent in the original low-dimensional space.\n\n Kernel Trick:\n   - When the data cannot be separated by a straight line or plane in its original (low-dimensional) space, SVM uses a technique called kernelling to project the data into a higher dimension where it becomes easier to separate.\n   - The Kernel Trick allows the transformation of data into a higher dimension without explicitly computing the transformation. There are different types of kernels, with common examples being:\n     - Polynomial kernel\n     - Radial Basis Function (RBF) or exponential kernel\n\n![[Pasted image 20251005094421.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "process"
    ],
    "normalized_filename": "kernelling",
    "outlinks": [
      "support_vector_machines",
      "pasted_image_20251005094421.png",
      "kernelling"
    ],
    "inlinks": [
      "kernel_machines",
      "kernelling",
      "support_vector_machines"
    ]
  },
  {
    "category": "ML",
    "filename": "Kmeans vs GMM",
    "sha": "79bf84225219914018f5ea750b483107d0a4c4e8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Kmeans%20vs%20GMM.md",
    "text": "### Key Differences Between [[K-means]] and [[Gaussian Mixture Models|GMM]]\n\n#### Cluster Shape\n\n- k-Means: Assumes clusters are spherical and equidistant from their centroids.\n- GMM: Models clusters as Gaussian distributions, allowing for different shapes (e.g., ellipses) by incorporating mean and covariance matrices. GMM can model clusters of varying shapes and sizes by adjusting the [[Covariance Structures]] (e.g., full, diagonal, spherical).\n\n#### Probability-Based Assignments\n\n- k-Means: Assigns each point deterministically to the nearest cluster centroid.\n- GMM: Provides a probability [[Distributions|distribution]] for cluster membership, making it a soft clustering method.\n- GMM handles overlapping clusters effectively by assigning probabilities to data points for each cluster, instead of enforcing hard boundaries like k-means.\n\n#### Flexibility\n\n- k-Means: Performs well when clusters are spherical and well-separated.\n- GMM: Handles overlapping clusters and clusters with different shapes, leveraging its covariance modeling capability.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "kmeans_vs_gmm",
    "outlinks": [
      "distributions",
      "k-means",
      "covariance_structures",
      "gaussian_mixture_models"
    ],
    "inlinks": [
      "gaussian_mixture_models"
    ]
  },
  {
    "category": "ML",
    "filename": "L1 Regularisation",
    "sha": "7235823c46cee180f54a45841fed032db74f9116",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/L1%20Regularisation.md",
    "text": "L1 regularization adds a penalty proportional to the ==absolute values== of the model coefficients to the [[Loss function]]. This penalty encourages ==sparsity==-some coefficients become exactly zero-making it useful for [[Feature Selection]].\n\nRemember how L1 and L2 metrics work with regards to the unit ball.\n\nLoss Function:\n\n$$\n\\text{Loss} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} |w_i|\n$$\n\nwhere:\n\n* $\\text{MSE}$ = Mean Squared Error\n* $\\lambda$ = Regularization strength\n* $w\\_i$ = Model weights\n\nKey Properties:\n\n* Adds penalty based on absolute value of coefficients.\n* Drives some coefficients to zero by removes less relevant features.\n* Produces a sparse model (subset of important features retained).\n\nExample (Lasso in scikit-learn):\n\n```python\nfrom sklearn.linear_model import Lasso\n\n# Initialize and fit Lasso model\nmodel = Lasso(alpha=0.1)  # alpha controls regularization strength\nmodel.fit(X_train, y_train)\n```\n\nUse Case:\n\n* Ideal for feature selection when dealing with many predictors.\n\n[Video Explanation](https://www.youtube.com/watch?v=NGf0voTMlcs)",
    "aliases": [
      "L1",
      "Lasso"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "optimisation",
      "regularization",
      "selection"
    ],
    "normalized_filename": "l1_regularisation",
    "outlinks": [
      "feature_selection",
      "loss_function"
    ],
    "inlinks": [
      "elastic_net",
      "embedded_methods",
      "regression",
      "regularisation",
      "when_and_why_not_to_us_regularisation"
    ]
  },
  {
    "category": "ML",
    "filename": "LBFGS",
    "sha": "ed71c99c06537c38839eee9a5e3b0ae9805a2d67",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/LBFGS.md",
    "text": "LBFGS stands for Limited-memory Broyden-Fletcher-Goldfarb-Shanno, which is an [[Optimisation function]]optimization algorithm used to find the minimum of a function. In the context of [[Logistic Regression]], LBFGS is a method for optimizing the cost function to find the optimal [[Model Parameters]] (such as the intercept and coefficients).\n\nHere's a breakdown of the key features of LBFGS:\n\n1. Quasi-Newton Method: LBFGS is a type of Quasi-Newton method, which approximates the inverse of the Hessian matrix (second-order derivatives of the cost function). Instead of computing the full Hessian matrix, it uses an approximation, which makes it more efficient for large datasets.\n    \n2. Limited Memory: The \"limited-memory\" part refers to the fact that LBFGS does not store the entire Hessian matrix, which is computationally expensive and memory-intensive. Instead, it keeps a limited amount of information from previous iterations, making it well-suited for large-scale problems where full memory-based methods might not be feasible.\n    \n3. Optimization for Smooth, Differentiable Functions: It is designed to optimize smooth, differentiable functions like the [[Cost Function]] in logistic regression.\n    \n\nIn the context of logistic regression with sklearn, LBFGS is used as a solver for optimization. When you set `solver='lbfgs'`, [[Scikit-Learn]]'s logistic regression uses this algorithm to iteratively adjust the model parameters (the intercept and coefficients) to minimize the logistic loss (the cost function) while possibly incorporating regularization.\n\nLBFGS is often preferred for its efficiency and ability to converge quickly without needing a lot of iterations, especially when the number of features is large.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "regressor"
    ],
    "normalized_filename": "lbfgs",
    "outlinks": [
      "optimisation_function",
      "logistic_regression",
      "model_parameters",
      "cost_function",
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "LLM Evaluation Metrics",
    "sha": "e7b5f66a04ce9af1fe77f770163a619a504e12a6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/LLM%20Evaluation%20Metrics.md",
    "text": "[[LLM Evaluation Metrics]]\n- BLEU, \n- ROUGE, \n- perplexity\nwhich quantify the similarity between generated text and reference outputs.\n\n[[LLM]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "NLP"
    ],
    "normalized_filename": "llm_evaluation_metrics",
    "outlinks": [
      "llm",
      "llm_evaluation_metrics"
    ],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "llm_evaluation_metrics"
    ]
  },
  {
    "category": "ML",
    "filename": "Label encoding vs One-hot encoding",
    "sha": "5f76324daa96902e95dc1cbb33bc9f8683ea641d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Label%20encoding%20vs%20One-hot%20encoding.md",
    "text": "[[Label encoding]] and [[One-hot encoding]] give different predictions because they represent categorical variables in fundamentally different ways. \n\n- [[Label encoding]] might cause issues by implying an ==ordinal== relationship between categories, leading to biased predictions.\n- One-Hot Encoding prevents this by treating categories independently, resulting in more accurate predictions when there's no natural order among the categories.\n\nKey Differences in Predictions:\n1. Ordinal vs. Non-Ordinal Data Representation:\n    - With Label Encoding, the model might treat \"Robbinsville\" (encoded as 1) as closer to \"West Windsor\" (encoded as 0) than \"Princeton\" (encoded as 2), even though these categories don't have any inherent numerical relationship. This can lead the model to incorrectly infer relationships based on these numeric values.\n    - With One-Hot Encoding, no such relationship is assumed. Each category is represented as a vector of 0s and 1s, and the model treats them as distinct entities, preventing any assumptions about their order.\n\n2. Model Interpretation:\n    - Label Encoding introduces an implicit ordinal relationship (e.g., 0 < 1 < 2) that can influence the model, especially for linear models like Linear Regression, which assumes that the input features are on a similar scale. This may lead to inappropriate relationships in the regression model.\n    - One-Hot Encoding avoids this issue by using binary columns for each category, effectively preventing the model from assuming an ordinal relationship between the categories.\n\n3. Feature Space:\n    - Label Encoding results in a single feature column for the categorical variable.\n    - One-Hot Encoding expands the feature space, creating as many columns as there are categories. In the case of a categorical feature with many unique values, this can significantly increase the dimensionality of the model.\n\nWhy Predictions Differ:\n- In Label Encoding, a linear regression model might learn that \"Robbinsville\" is numerically closer to \"West Windsor\" than \"Princeton,\" and this might distort the predictions.\n- In One-Hot Encoding, the model treats each category independently, leading to different relationships being learned (if any) between the categories and the target variable.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing"
    ],
    "normalized_filename": "label_encoding_vs_one-hot_encoding",
    "outlinks": [
      "label_encoding",
      "one-hot_encoding"
    ],
    "inlinks": [
      "one-hot_encoding"
    ]
  },
  {
    "category": "ML",
    "filename": "Labelling data",
    "sha": "b4faa0914405192cb2030dbcb6819c0b6d65e399",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Labelling%20data.md",
    "text": "Possible missing labelling or bias in the data, or under-represented data. Construction of the data set comes from the group collecting it.\n\nExamples:\n- ImageNet\n\nRelated:\n- [[Classification]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "process"
    ],
    "normalized_filename": "labelling_data",
    "outlinks": [
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Lagrange multipliers in optimisation",
    "sha": "232adc4eb3c034302e70fb00df3d01ede29c2729",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Lagrange%20multipliers%20in%20optimisation.md",
    "text": "Lagrange multipliers let us embed constraints into optimization problems, and many ML models (like SVMs and constrained likelihoods) rely on them.\n\nLagrange multipliers are a mathematical method for solving constrained optimization problems. In machine learning, they are used when we want to maximize or minimize a function subject to one or more constraints.\n### General Idea\n\nSuppose we want to minimize (or maximize): $f(x,y)$ subject to a constraint: $g(x,y) = c$. We introduce a new variable $\\lambda$ (the Lagrange multiplier) and define the Lagrangian:\n\n$\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda \\cdot (c - g(x,y))$\n\nThen solve by setting derivatives = 0:\n\n$\\nabla{x,y,\\lambda} \\mathcal{L} = 0$\n\n### Why It Matters in ML\n\nLagrange multipliers let us handle constraints directly in optimization problems common in machine learning. Examples:\n\n[[Support Vector Machines|SVM]]\n  * The optimization problem is: minimize margin loss subject to constraints on classification.\n  * Lagrange multipliers are used to transform it into a dual problem, making the solution tractable.\n\n[[Regularisation]]:\n  * Can be seen as introducing constraints on weights (e.g., $|w|^2 \\leq C$).\n  * Lagrange multipliers turn this into a penalty term (like in ridge regression).\n\n### Intuition\n* Without constraints: move downhill on $f(x,y)$ until reaching the minimum.\n* With constraints: must stay on the surface defined by $g(x,y) = c$.\n* The Lagrange multiplier $\\lambda$ balances the trade-off between optimizing $f$ and satisfying the constraint.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "optimisation"
    ],
    "normalized_filename": "lagrange_multipliers_in_optimisation",
    "outlinks": [
      "support_vector_machines",
      "regularisation"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Latent Dirichlet Allocation",
    "sha": "f98fc378ef3d55ea63e6780ab75211654373bd53",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Latent%20Dirichlet%20Allocation.md",
    "text": "Latent Dirichlet Allocation (LDA) is a generative probabilistic model used in Natural Language Processing (NLP) and machine learning for topic modeling. It assumes that documents are mixtures of topics, and topics are mixtures of words. The goal of LDA is to uncover the latent topics in a collection of text documents by identifying groups of words that frequently appear together in the same documents.\n\nLibraries like gensim in Python are highlighted as being particularly suitable for topic modeling\n\n### Key Concepts:\n- Topic: A distribution over a fixed vocabulary of words. A topic might represent a certain theme, like \"sports\" or \"politics.\"\n- Document: A mixture of topics. A document is modeled as a combination of topics with different proportions.\n- Words: Each word in the document is associated with a topic based on the topic proportions of that document.\n\n### How LDA Works:\n1. Assume each document has a mixture of topics: Each document is made up of a certain proportion of topics. For example, a document could be 70% about \"sports\" and 30% about \"politics.\"\n2. Assume each topic is a distribution of words: Each topic has a specific distribution over words. For example, the \"sports\" topic might have a high probability for words like \"football,\" \"game,\" and \"score.\"\n3. Infer the topics from the documents: LDA tries to discover these hidden topics from the words in the documents, without knowing the topics in advance.\n\n### Why Use LDA?\n- Topic Discovery: It helps discover hidden themes in a large corpus of text data.\n- [[Dimensionality Reduction]]: LDA reduces the complexity of the text data by modeling it with a smaller number of topics instead of many individual words.\n\nRelated terms:\n- [[topic modeling]]\n- [[Semantic Relationships]]\n- [[NLP]]\n- [[Unsupervised Learning]]\n\n## Example:\n\nImagine you have a set of three simple documents:\n\n1. Document 1: \"Football is a great sport\"\n2. Document 2: \"Basketball is also fun to play\"\n3. Document 3: \"Soccer and football are similar sports\"\n\nLDA might identify two topics:\n- Topic 1: Words related to \"sports\" (e.g., \"football,\" \"basketball,\" \"soccer\").\n- Topic 2: Words related to \"competition\" or \"play\" (e.g., \"game,\" \"score,\" \"fun\").\n\nThe algorithm would assign a mixture of these topics to each document based on the words in the documents.\n\n### Example Code:\n\nHere’s a simple implementation of LDA using `sklearn` in Python:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocs = [\n    \"Football is a great sport\",\n    \"Basketball is also fun to play\",\n    \"Soccer and football are similar sports\"\n]\n\n# Step 1: Convert the documents into a document-term matrix (DTM)\nvectorizer = CountVectorizer(stop_words='english')\ndtm = vectorizer.fit_transform(docs)\n\n# Step 2: Apply LDA to discover topics\nlda = LatentDirichletAllocation(n_components=2, random_state=42)\nlda.fit(dtm)\n\n# Step 3: Display topics\nterms = vectorizer.get_feature_names_out()\n\nfor index, topic in enumerate(lda.components_):\n    print(f\"Topic #{index + 1}:\")\n    print([terms[i] for i in topic.argsort()[-5:]])  # Print the top 5 words in the topic\n    print()\n\n# Example Output:\n# Topic #1:\n# ['football', 'sports', 'game', 'score', 'play']\n# Topic #2:\n# ['basketball', 'soccer', 'sport', 'fun', 'great']\n```\n\n### Output Explanation:\n- Topic might have words like football, sports, game, because these terms appear frequently in the documents related to sports.\n- Topic could have terms like basketball, soccer, fun, because these are associated with the activities discussed in the documents.\n\n### How to Interpret:\n- Document 1 (\"Football is a great sport\"): LDA might classify this document as being 60% about Topic (sports-related) and 40% about Topic (competitive play).\n- Document 2 (\"Basketball is also fun to play\"): LDA might classify this document as being 80% about Topic (competitive play) and 20% about Topic (sports-related).\n\nRelated:\n- [[Latent Semantic Indexing]]",
    "aliases": [
      "LDA"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "NLP"
    ],
    "normalized_filename": "latent_dirichlet_allocation",
    "outlinks": [
      "semantic_relationships",
      "unsupervised_learning",
      "latent_semantic_indexing",
      "dimensionality_reduction",
      "nlp",
      "topic_modeling"
    ],
    "inlinks": [
      "why_standardise_features"
    ]
  },
  {
    "category": "ML",
    "filename": "Latent Semantic Indexing",
    "sha": "a1cc27226f6c09efb69cf9e2ce16c9c13611dd49",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Latent%20Semantic%20Indexing.md",
    "text": "uses singular value decomposition (SVD) to identify patterns in the relationships \nbetween the terms and concepts contained in an unstructured collection of text.\n\nwords that are used in the same contexts tend to have similar meanings.\n\nextracts the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "latent_semantic_indexing",
    "outlinks": [],
    "inlinks": [
      "latent_dirichlet_allocation"
    ]
  },
  {
    "category": "ML",
    "filename": "Learning Curve",
    "sha": "8dbc51c36d88ab1eb0d2f4f6d25083fc29086504",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Learning%20Curve.md",
    "text": "A learning curve is a diagnostic plot that shows model performance (e.g., [[Accuracy]] or error) on training and validation sets as a function of training set size. It helps assess ([[Model Selection]]) whether a model suffers from underfitting, overfitting, or has converged in performance.\n\nCore Concepts:\n- A learning curve plots model scores (typically training and validation) against the number of training samples. It reveals how the model generalizes as it is trained on more data.\n\nTypical behaviours:\n\n- [[Overfitting]] (small data): High training score, low validation score. Model memorizes training data but generalizes poorly.\n- Underfitting (high bias): Both training and validation scores are low. Model is too simple for the data.\n- Convergence: As the dataset grows, training and validation scores approach each other. Once convergence is reached, adding more data does not improve performance significantly.\n\nKey Insight:\n- If both curves flatten and are close together, the model has likely reached its capacity. In this case, increasing training data further will not help—you may need a more expressive model.\n\nImplementation Example (scikit-learn):\n\n```python\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, val_scores = learning_curve(estimator, X, y, cv=5)\n```\n\nUse this to plot performance curves and analyze model behavior as training size varies.\n\nUse Cases:\n- Diagnose model complexity\n- Identify data limitations\n- Support decisions on collecting more data vs. changing the model",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "ml"
    ],
    "normalized_filename": "learning_curve",
    "outlinks": [
      "model_selection",
      "accuracy",
      "overfitting"
    ],
    "inlinks": [
      "cross_validation"
    ]
  },
  {
    "category": "ML",
    "filename": "Learning Rate",
    "sha": "8a07b13dea2e17ba4b4bc8b873558efbff1aed5b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Learning%20Rate.md",
    "text": "The learning rate is a [[Hyperparameter]] in machine learning that ==determines the step size at which a model's parameters are updated during training==. It plays a significant role in the optimization process, particularly in algorithms like [[Gradient Descent]] which are used to minimize the [[Loss function]].\n\n### Key Points about Learning Rate:\n\nParameter Updates:\n   - During training, the model's parameters (such as weights and biases in neural networks) are adjusted iteratively to minimize the loss function.\n   - The learning rate controls how much the parameters are changed in response to the estimated error each time the model weights are updated.\n\nImpact on Training/ Convergence\n   - A high learning rate can lead to faster convergence but ==risks overshooting== the optimal solution, potentially causing the model to diverge.\n   - A low learning rate ensures more stable and precise convergence but may result in slow training and can get stuck in local minima. A lower learning rate makes the model more robust but requires more iterations to converge.\n\nTuning:\n   - The learning rate is a hyperparameter that needs careful tuning. It can be adjusted manually or through automated hyperparameter optimization techniques like [[uncategorised/Optuna]]. \n   - The optimal learning rate depends on various factors, including the dataset, model complexity, and the specific optimization algorithm used.\n\nPractical Considerations:\n   - It's common to start with a moderate learning rate and adjust based on the model's performance during training.\n   - Techniques like learning rate schedules or adaptive learning rate methods (e.g., [[Adam Optimizer]]) can dynamically adjust the learning rate during training to improve convergence.\n\nLearning rate:\n- impacts the efficiency of [[Gradient Descent]]\n- Effects occur if too small (takes long), or too large (over shoots missing the minima).\n- What happens if you are at a local minima? Then no change.\n\n![[Pasted image 20241216204925.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "learning_rate",
    "outlinks": [
      "hyperparameter",
      "gradient_descent",
      "uncategorised/optuna",
      "loss_function",
      "adam_optimizer",
      "pasted_image_20241216204925.png"
    ],
    "inlinks": [
      "adam_optimizer",
      "adaptive_learning_rates",
      "ds_&_ml_portal",
      "fitting_weights_and_biases_of_a_neural_network",
      "gradient_boosting",
      "gradient_descent",
      "gradient_descent_in_linear_regression",
      "model_parameters_vs_hyperparameters",
      "momentum",
      "optimisation_techniques",
      "stochastic_gradient_descent",
      "train-dev-test_sets",
      "weak_learners",
      "why_does_the_adam_optimizer_converge",
      "xgboost",
      "z-normalisation"
    ]
  },
  {
    "category": "ML",
    "filename": "Learning Styles",
    "sha": "d8fab8800a5daf8c08ce2cdaa94980d625048a0b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Learning%20Styles.md",
    "text": "What does the data look like [[continuous]] or [[categorical]]? \n\n![[ Pasted image 20240112101344.png|500]]\n\n[[Unsupervised Learning]]\n\t [[Regression]] \n\t [[Classification]]\n\n[[Unsupervised Learning]]\n\t[[Dimensionality Reduction]]\n\t[[Clustering]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture"
    ],
    "normalized_filename": "learning_styles",
    "outlinks": [
      "categorical",
      "regression",
      "clustering",
      "unsupervised_learning",
      "continuous",
      "pasted_image_20240112101344.png",
      "dimensionality_reduction",
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "LightGBM vs XGBoost vs CatBoost",
    "sha": "cf120a38cddf0a8a762db2f4e6faba3f7dba047a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/LightGBM%20vs%20XGBoost%20vs%20CatBoost.md",
    "text": "This table summarizes the key differences and strengths of each [[Gradient Boosting]] framework.\n\n| Feature/Aspect                    | [[LightGBM]] (LGBM)                                                        | [[XGBoost]]                                                                     | [[CatBoost]]                                                           |\n| --------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n| Tree Growth Strategy          | Leaf-wise growth, leading to deeper trees and potentially better accuracy. | Level-wise growth, resulting in more balanced trees.                            | Ordered boosting, reducing overfitting and improving generalization.   |\n| Speed and Memory              | High speed and low memory usage, especially with large datasets.           | Balanced speed and accuracy with robust regularization options.                 | Competitive performance with minimal hyperparameter tuning.            |\n| Handling Categorical Features | Requires preprocessing (e.g., [[Label encoding]]).                             | Requires preprocessing of categorical features.                                 | Natively handles categorical features without preprocessing.           |\n| [[Regularisation]]            | Supports regularization but not as robust as XGBoost.                      | Strong regularization options (L1 and L2) to prevent overfitting.               | Utilizes techniques like ordered boosting to mitigate overfitting.     |\n| Use Cases                     | Ideal for large datasets and when computational efficiency is a priority.  | Suitable for structured data and tabular datasets; widely used in competitions. | Useful for datasets with many categorical features and missing values. |\n| Performance                   | Fast training and efficient on large datasets.                             | Accurate and flexible, often used in competitions.                              | Provides competitive performance, especially with categorical data.    |",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "lightgbm_vs_xgboost_vs_catboost",
    "outlinks": [
      "regularisation",
      "lightgbm",
      "gradient_boosting",
      "label_encoding",
      "catboost",
      "xgboost"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "LightGBM",
    "sha": "0e1ecda6cfea6d47ba3ecc24f1fb727ca13c4475",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/LightGBM.md",
    "text": "LightGBM is a gradient boosting framework that is designed for speed and efficiency. It is particularly well-suited for handling large datasets and high-dimensional data.\n\n- **Tree Growth**: Splits the tree leaf-wise, which can lead to faster convergence compared to level-wise growth.\n- **Learning Rate**: Similar to [[Gradient Descent]], LightGBM uses a learning rate to control the contribution of each tree.\n- **DART**: A variant of LightGBM known for its performance.\n- **Parameter Definition**: Requires parameters to be defined in a dictionary for model configuration.\n\n[Watch Video Explanation](https://www.youtube.com/watch?v=n_ZMQj09S6w)\n\n### Key Parameters\n\n- **Learning Rate**: Controls the step size at each iteration while moving toward a minimum of the loss function.\n- **Number of Leaves**: Determines the complexity of the tree model.\n\n### Advantages\n\n- **Speed**: Renowned for its speed, often outperforming other gradient boosting implementations.\n- **Memory Usage**: Optimizes memory usage, enabling efficient handling of large datasets.\n- **Leaf-Wise Growth**: Grows trees leaf-wise, leading to faster convergence.\n- **Parallel and GPU Learning**: Supports parallel and GPU learning for further speedup.\n\n### Use Cases\n\n- **Large Datasets**: Ideal for applications where speed is crucial.\n- **High-Dimensional Data**: Efficient when dealing with high-dimensional data and categorical features.",
    "aliases": [
      "LGBM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "lightgbm",
    "outlinks": [
      "gradient_descent"
    ],
    "inlinks": [
      "gradient_boosting",
      "lightgbm_vs_xgboost_vs_catboost",
      "optuna",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Linear Regression",
    "sha": "8ea1b58e545abc5c824e1cc7bc9a8fb7470c6465",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Linear%20Regression.md",
    "text": "Linear regression assumes [[linearity]] between the input features and the target variable. Assumes that the relationship between the independent variable(s) and the dependent variable is linear.\n\nDuring the training phase, the algorithm adjusts the slope (m) and the intercept (b) of the line to minimize the [[Loss function]].\n\nThe linear regression model is represented as:\n\n$$y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n$$\n\n- $y$ is the dependent variable (the variable we want to predict).\n- $x_1, x_2, \\ldots, x_n$ are the independent variables (features or predictors).\n- $b_0, b_1, b_2, \\ldots, b_n$ are the coefficients (weights) associated with each independent variable.\n- $b_0$ is the intercept term.\n\nThe goal of linear regression is to find the values of coefficients $$b_0, b_1, b_2, \\ldots, b_n$$ that ==minimize the sum of squared errors (SSE),== also known as the residual sum of squares (RSS) or (MSE - mean square error).\n\nYou ==evaluate== the performance of your model by comparing its predictions to the actual values in a separate test dataset. Common metrics for evaluating regression models are:\n-  Mean Squared Error (MSE)\n-  [[Root Mean Squared Error]] (RMSE)\n-  [[R squared]].\n\nRelated terms:\n- [[Ordinary Least Squares]]\n- [[Gradient descent in linear regression]]\n- [[Variability in linear models]]\n- [[Why standardise features]]\n\n### Impact of Extra Variables on Intercept of linear regression\n\nWhen additional variables are introduced, it can impact the intercept ($b_0$) in the linear regression model. The intercept is the value of $y$ when all independent variables ($x_1, x_2, \\ldots, x_n$) are zero. The presence of extra variables can affect the baseline value of the dependent variable.\n\n### [[Model Evaluation]]\n\n- [[R squared]]\n- [[Adjusted R squared]] takes into account the number of variables.\n- [[f-regression]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "model": null,
    "tags": [
      "regressor"
    ],
    "normalized_filename": "linear_regression",
    "outlinks": [
      "r_squared",
      "f-regression",
      "linearity",
      "gradient_descent_in_linear_regression",
      "adjusted_r_squared",
      "why_standardise_features",
      "loss_function",
      "variability_in_linear_models",
      "ordinary_least_squares",
      "model_evaluation",
      "root_mean_squared_error"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "f-regression",
      "general_linear_regression",
      "gradient_descent_in_linear_regression",
      "logistic_regression",
      "machine_learning_algorithms",
      "maximum_likelihood_estimation",
      "model_parameters",
      "outliers",
      "pytorch",
      "recommender_systems",
      "regression",
      "ridge",
      "supervised_learning",
      "variability_in_linear_models"
    ]
  },
  {
    "category": "ML",
    "filename": "Local Interpretable Model-agnostic Explainations",
    "sha": "6a8d2f0017084c904a873b1ccacfe06029678859",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Local%20Interpretable%20Model-agnostic%20Explainations.md",
    "text": "LIME explains individual predictions ==by approximating the model locally== with an [[Interpretability|interpretable]] model and calculating the [[Feature Importance]] based on the surrogate model.\n\n### Key Points\n\n- **Purpose**: LIME focuses on explaining individual predictions by approximating the model locally using a simpler, interpretable model (like linear regression).\n  \n- **How it Works**: \n  - For a given prediction, LIME generates perturbed samples (e.g., by modifying input features).\n  - It observes how the predictions change, thus inferring feature importance for that specific instance.\n\n- **Use Cases**: Useful for understanding why a specific decision was made in complex black-box models.\n\n- **Advantage**: \n  - LIME can work with any model type.\n  - It is relatively easy to apply to tabular, text, and image data.\n\n- **Scenario**: \n  - A healthcare provider uses a deep learning model to classify whether patients have a high or low risk of heart disease based on several health metrics, such as cholesterol levels, age, and blood pressure.\n\n  - **LIME Explanation**: LIME is used to explain why the model flagged a specific patient as high-risk. By perturbing the input data (e.g., altering cholesterol levels and re-running the prediction), LIME shows that the patient’s high cholesterol level and advanced age are the main reasons for the high-risk classification. This makes it easier for the healthcare provider to justify the decision to recommend lifestyle changes or further medical tests.\n\n### Example Code\n\nTo use LIME for feature importance, you can use the LIME package:\n\n```python\nimport lime\nimport lime.lime_tabular\n\n# Create a LIME explainer\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names)\n\n# Explain a single prediction\nexplanation = explainer.explain_instance(X_test[0], model.predict_proba)\nexplanation.show_in_notebook()\n```",
    "aliases": [
      "LIME"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "local_interpretable_model-agnostic_explainations",
    "outlinks": [
      "interpretability",
      "feature_importance"
    ],
    "inlinks": [
      "feature_importance",
      "model-agnostic_feature_importance",
      "model_interpretability"
    ]
  },
  {
    "category": "ML",
    "filename": "Local Outlier Factor (LOF)",
    "sha": "9a7bd6685150d800ccb51e981f4e2a701fc10fc9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Local%20Outlier%20Factor%20(LOF).md",
    "text": "## Local Outlier Factor (LOF)\n\nPurpose:  \n  - A density-based method that identifies anomalies by comparing a point's local density to its neighbors.\n\nKey Idea:  \n  - Points with substantially lower local density than their neighbors are considered outliers.\n\nSteps:\n  1. For each point, compute local density based on its k-nearest neighbors.\n  2. Calculate the LOF score: a value higher than 1 suggests potential anomaly.\n  3. Points with high LOF scores are flagged as anomalies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "local_outlier_factor_(lof)",
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_clustering"
    ]
  },
  {
    "category": "ML",
    "filename": "Logistic Regression Statsmodel Summary table",
    "sha": "03b38f1fda6ff357411889442d868181dd1760a9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Logistic%20Regression%20Statsmodel%20Summary%20table.md",
    "text": "Statsmodel has this summary table unlike [[Scikit-Learn]]\n\n[Explanation of summary](https://youtu.be/JwUj5M8QY4U?t=658)\n\nThe dependent variable is 'duration'. The model used is a Logit regression (logistic in common lingo), while the method \n- Maximum Likelihood Estimation ([[MLE]]). It has clearly converged after classifying 518 observations.\n- The Pseudo R-squared is 0.21 which is within the 'acceptable region'.\n- The duration variable is significant and its coefficient is 0.0051.\n- The constant is also significant and equals: -1.70 (p value close to 0)\n- High p value, suggests to remove from model, drop one by one, ie [[Feature Selection]].\n\nSpecifically a graph such as,\n![[Pasted image 20240124095916.png]]\n\n\n\n$$\\mathbb{N}$$",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "regressor"
    ],
    "normalized_filename": "logistic_regression_statsmodel_summary_table",
    "outlinks": [
      "feature_selection",
      "mle",
      "pasted_image_20240124095916.png",
      "scikit-learn"
    ],
    "inlinks": [
      "evaluating_logistic_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Logistic Regression does not predict probabilities",
    "sha": "b2a09ca4ffff219713a26fd220668c5631816483",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Logistic%20Regression%20does%20not%20predict%20probabilities.md",
    "text": "In logistic regression, the model predicts the ==odds of an event happening rather than directly predicting probabilities.== The odds are defined as:\n\n$$ \\text{Odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{p}{1-p} $$\n\n  where $p$ is the probability of success. The log-odds (or logit function) is the natural logarithm of the odds:\n  $$ \\text{Log-Odds} = \\ln\\left(\\frac{p}{1-p}\\right) = b_0 + b_1 x $$\n  This ==transformation makes the relationship between the independent variables and the dependent variable linear,== allowing logistic regression to estimate the parameters $b_0$ and $b_1$.\n\nResources:\n- [Explanation of log odds](https://www.youtube.com/watch?v=ARfXDSkQf1Y)\n- [Explanation of log odd function](https://www.youtube.com/watch?v=fJ53tIDbvTM)\n\n[[Odds vs Probability]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "regressor",
      "statistics"
    ],
    "normalized_filename": "logistic_regression_does_not_predict_probabilities",
    "outlinks": [
      "odds_vs_probability"
    ],
    "inlinks": [
      "logistic_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Logistic Regression",
    "sha": "26830f4022704e69574cbed05500cf89937d9421",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Logistic%20Regression.md",
    "text": "==Logistic regression models the log-odds of the probability as a linear function of the input features.==\n\nIt models the probability of an input belonging to a particular class using a logistic (sigmoid) function.\n\nThe model establishes a decision boundary (threshold) in the feature space.\n\nLogistic regression is best suited for cases where the decision boundary is approximately linear in the feature space.\n\nLogistic [[Regression]] can be used for [[Binary Classification]]tasks.\n\n### Related Notes:\n\n- [[Logistic Regression does not predict probabilities]]\n- [[Interpreting logistic regression model parameters]]\n- [[Model Evaluation]]\n- To get [[Model Parameters]] use [[Maximum Likelihood Estimation]]\n- [[Evaluating Logistic Regression]]\n## Key Concepts of Logistic Regression\n\n### Logistic Function (Sigmoid Function)\n\nLogistic regression models the probability that an input belongs to a particular class using the logistic (sigmoid) function. This function maps any real-valued input into the range (0,1), representing the probability of belonging to the positive class (usually class 1).\n\nThe sigmoid function is defined as:  \n$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$  \nwhere  \n$$ z = \\mathbf{w} \\cdot \\mathbf{x} + b $$  \nThus, the logistic regression model is given by:  \n$$ P(y=1 \\mid \\mathbf{x}) = \\sigma(z) $$  \n\n### Log odds: Transforming from continuous to 0-1\n\nLogistic regression is based on the ==log-odds== (logit) transformation, which expresses probability in terms of odds:\n\n$$ \\text{Odds} = \\frac{P(y=1 \\mid \\mathbf{x})}{1 - P(y=1 \\mid \\mathbf{x})} $$\n\nTaking the natural logarithm of both sides gives the logit function:\n\n$$ \\log \\left(\\frac{P(y=1 \\mid \\mathbf{x})}{1 - P(y=1 \\mid \\mathbf{x})} \\right) = \\mathbf{w} \\cdot \\mathbf{x} + b $$\n\nThis equation shows that ==logistic regression models the log-odds of the probability as a linear function of the input features.==\n\n### Decision Boundary\n\n- Similar to [[Support Vector Machines]], logistic regression defines a decision boundary that separates the two classes.\n- The logistic function determines the probability of a data point belonging to a specific class. If this probability exceeds a given ==threshold== (typically 0.5), the model assigns the point to the positive class; otherwise, it is classified as negative.\n\n### [[Binary Classification]]\n\n- Logistic regression is primarily used for binary classification tasks, where the target variable has only two possible values (e.g., \"0\" and \"1\").\n- It can handle multiple independent variables (features) and assigns probabilities to the target classes based on the feature values.\n- Examples include:\n\n### No Residuals\n\n- Unlike [[Linear Regression]], logistic regression does not compute standard residuals.\n- Instead, [[Model Evaluation]] is performed by comparing predicted probabilities with actual class labels using metrics such as accuracy, precision, recall, and the [[Confusion Matrix]].\n### Also see:\n\nRelated terms:\n- Cost function for logistic regression\n- Gradient computation in logistic regression\n- Regularized logistic regression\n- Cost function for regularized logistic regression\n\nLogistic regression can be extended to handle non-linear decision boundaries through:\n- Polynomial features to capture more complex relationships.\n- Regularization techniques to improve generalization.\n\n[Explaining logistic regression](https://www.youtube.com/watch?v=Iju8l2qgaJU)\n\nNotes:\n- coefficients help with [[Feature Importance]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "regressor",
      "ML_Tools"
    ],
    "normalized_filename": "logistic_regression",
    "outlinks": [
      "linear_regression",
      "regression",
      "binary_classification",
      "confusion_matrix",
      "model_parameters",
      "evaluating_logistic_regression",
      "logistic_regression_does_not_predict_probabilities",
      "model_evaluation",
      "support_vector_machines",
      "maximum_likelihood_estimation",
      "feature_importance",
      "interpreting_logistic_regression_model_parameters"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "evaluate_embedding_methods",
      "imbalanced_datasets_smote.py",
      "interpreting_logistic_regression_model_parameters",
      "knime",
      "lbfgs",
      "machine_learning_algorithms",
      "model_parameters",
      "optimising_a_logistic_regression_model",
      "parametric_vs_non-parametric_models",
      "regression",
      "ridge",
      "roc_(receiver_operating_characteristic)",
      "statistical_assumptions",
      "statistics"
    ]
  },
  {
    "category": "ML",
    "filename": "Logistic regression in sklearn & Gradient Descent",
    "sha": "1fab3300832e6b318280aa62e402e583f7febb1a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Logistic%20regression%20in%20sklearn%20&%20Gradient%20Descent.md",
    "text": "# Logistic regression in sklearn & Gradient Descent\n\nsklearn's Logistic Regression implementation does not use [[Gradient Descent]] by default. Instead, it uses more sophisticated optimization techniques depending on the solver specified. These solvers are more efficient and robust for finding the optimal parameters for logistic regression. Here's a summary:\n\n### [[Optimisation function]]: Solvers in sklearn's Logistic Regression**\n\n1. **`lbfgs` (default in many cases)**:\n    - Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno.\n    - It's a quasi-Newton method, which approximates the second derivatives (Hessian matrix) to find the minimum of the cost function efficiently.\n      \n2. **`liblinear`**:\n    - Uses the coordinate descent method for optimization.\n    - Ideal for small datasets or when `penalty='l1'` is used.\n      \n3. **`sag` (Stochastic Average Gradient)**:\n    - An iterative solver similar to stochastic gradient descent (SGD) but averages gradients over all samples.\n    - Efficient for large datasets.\n      \n4. **`saga`**:\n    - An improved version of `sag`, supporting both `l1` and `l2` penalties.\n    - Suitable for sparse and large datasets.\n      \n5. **`newton-cg`**:\n    - Uses the Newton method with conjugate gradients.\n    - Efficient for datasets with many features.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "logistic_regression_in_sklearn_&_gradient_descent",
    "outlinks": [
      "gradient_descent",
      "optimisation_function"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Loss function",
    "sha": "fb3973c54f715193bbf7c77711dcfd1ec2a029d3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Loss%20function.md",
    "text": "A loss function is a key component in training machine learning models. It is closely related to the [[Cost Function]], error function, and [[objective function]], and serves as an important metric for [[Model Evaluation]].\n\n#### Purpose\n* Measure predictive accuracy:\n  Quantifies the difference between predicted and actual values.\n  In other words, it measures how well a model’s predictions match the true targets by assigning an error value to each prediction.\n- Goal: To be minimized:\n  During training, the primary objective is to minimize the loss function so that predictions become more accurate, even on unseen data.\n\n#### Usage\n* Training: Guides updates to [[Model Parameters]] during optimization (e.g., using [[Gradient Descent]]).\n* Evaluation: Assesses how well the trained model performs on validation or test data.\n\n#### Examples\n* [[Mean Squared Error]] (MSE): Common in [[Regression]] tasks.\n* [[Cross Entropy]] (Log Loss): Common in [[Classification]] tasks.\n\n#### Related Notes\n* [[Loss versus Cost function]]\n* [[Model Optimisation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "loss_function",
    "outlinks": [
      "objective_function",
      "gradient_descent",
      "regression",
      "model_optimisation",
      "model_parameters",
      "loss_versus_cost_function",
      "model_evaluation",
      "cost_function",
      "mean_squared_error",
      "classification",
      "cross_entropy"
    ],
    "inlinks": [
      "backpropagation",
      "cost_function",
      "cross_entropy",
      "ds_&_ml_portal",
      "embedded_methods",
      "feed_forward_neural_network",
      "fitting_weights_and_biases_of_a_neural_network",
      "gradient_boosting",
      "gradient_descent",
      "l1_regularisation",
      "learning_rate",
      "linear_regression",
      "loss_versus_cost_function",
      "model_parameters_vs_hyperparameters",
      "neural_scaling_laws",
      "optimisation_function",
      "optimising_a_logistic_regression_model",
      "pytorch",
      "regularisation",
      "ridge",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "stochastic_gradient_descent",
      "test_loss_when_evaluating_models",
      "typical_output_formats_in_neural_networks",
      "variability_in_linear_models",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Loss versus Cost function",
    "sha": "b4efdf92185edb3948429f9f7818ad18f40996c7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Loss%20versus%20Cost%20function.md",
    "text": "In machine learning, the terms \"loss function\" and \"cost function\" are often used interchangeably, but they can have slightly different meanings depending on the context:\n\n1. [[Loss function]]: This typically refers to the function that measures the error for a single training example. It quantifies how well or poorly the model is performing on that specific example - data point. Common examples include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n\n2. [[Cost Function]]: This is generally used to refer to the average of the loss function over the entire training dataset. It provides a measure of how well the model is performing overall. \n   \n   The cost function is what is minimized during the training process to improve the model's performance, see [[Model Optimisation|Optimisation]]. Used with the parameters to determine the best ones.\n\n[[Model parameters vs hyperparameters]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "loss_versus_cost_function",
    "outlinks": [
      "cost_function",
      "model_optimisation",
      "model_parameters_vs_hyperparameters",
      "loss_function"
    ],
    "inlinks": [
      "cost_function",
      "loss_function"
    ]
  },
  {
    "category": "ML",
    "filename": "Machine Learning Operations",
    "sha": "9ba6c70200c8a34a65bcf6d5a329065d0eaf1e87",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Machine%20Learning%20Operations.md",
    "text": "MLOps is a set of practices and tools designed to streamline the entire lifecycle of machine learning models—from development to deployment and ongoing maintenance. It integrates [[DevOps]] principles to ensure models are reliable, scalable, and efficient in production.\n\nMLOps ensures that machine learning models are **reliable, [[Maintainability|maintainable]], and scalable** in production. Core principles include automation, monitoring, reproducibility, collaboration, and adaptability to changing data environments.\n\n### **Key Components**\n\n1. **Development:**\n   * Establish seamless workflows for [[Preprocessing|data preprocessing]], [[feature engineering]], [[Model Building]], and model training.\n   * Goal: Accelerate model development while ensuring quality and reproducibility.\n   * See [[DS & ML Portal]] for reference workflows.\n\n2. **Deployment:**\n\n   * Deploy models to production environments with the necessary infrastructure.\n   * Ensure models can handle real-world data, workloads, and user requirements.\n\n3. **Maintenance:**\n\n   * Monitor model performance continuously using [[Model Observability|observability]] tools.\n   * Detect [[data drift]] or [[Performance Drift|concept drift]] and retrain models as needed.\n\n4. **Generalization and Robustness:**\n\n   * Build models that generalize well to unseen data.\n   * Ensure robustness to noisy, incomplete, or unexpected inputs.\n\n5. **Collaboration and Automation:**\n\n   * Promote collaboration among data scientists, engineers, and operations teams.\n   * Automate repetitive tasks such as model training, evaluation, and [[Model Deployment|deployment]].\n\n6. **Versioning and CI/CD:**\n\n   * Maintain versioning for data and models.\n   * Use [[Continuous Integration|CI]] and [[Continuous Delivery - Deployment|CD]] pipelines for consistent and reproducible workflows.",
    "aliases": [
      "MLOPs"
    ],
    "date modified": "19-10-2025",
    "tags": [
      "automation",
      "ml",
      "ml_process",
      "MLOps",
      "model_explainability",
      "process"
    ],
    "normalized_filename": "machine_learning_operations",
    "outlinks": [
      "data_drift",
      "preprocessing",
      "continuous_integration",
      "model_building",
      "model_deployment",
      "feature_engineering",
      "model_observability",
      "maintainability",
      "ds_&_ml_portal",
      "performance_drift",
      "continuous_delivery_-_deployment",
      "devops"
    ],
    "inlinks": [
      "model_selection"
    ]
  },
  {
    "category": "ML",
    "filename": "Machine Learning",
    "sha": "cda5cbfe22e8c6483eefc45f31a0dfd3aa0c30ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Machine%20Learning.md",
    "text": "Machine learning (ML) is a type of artificial intelligence ([[AI]]) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. \n\nMachine learning [[Machine Learning Algorithms]] use historical data as input to predict new output values.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "machine_learning",
    "outlinks": [
      "ai",
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "ai",
      "data_ingestion",
      "remaining_useful_life_models",
      "tensorflow"
    ]
  },
  {
    "category": "ML",
    "filename": "Markov Decision Processes",
    "sha": "e2eb2275e94a6d3a735b6623e575168dfebe6718",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Markov%20Decision%20Processes.md",
    "text": "**Markov Decision Process ([[Markov Decision Processes|MDP]])** is a formal framework for decision-making where outcomes depend solely on the current state (Markov property).\n\\\n\narchitecture \n\n\n\n[[Markov Decision Processes]] \n([[Markov Decision Processes|MDP]]s): The mathematical framework for modelling decision-making, characterized by states, actions, transition probabilities, and rewards. Your understanding of probability theory and stochastic processes will be crucial here.",
    "aliases": [
      "MDP"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "markov_decision_processes",
    "outlinks": [
      "markov_decision_processes"
    ],
    "inlinks": [
      "industries_of_interest",
      "markov_decision_processes",
      "reinforcement_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Manifold Learning",
    "sha": "bfde49af697e9319e085bb2f7f7cc370d99412ff",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Manifold%20Learning.md",
    "text": "**Manifold learning** is a powerful approach for high-dimensional data exploration, focusing on uncovering the lower-dimensional manifold that the data resides on. These algorithms aim to identify and map the underlying low-dimensional structure, or **manifold**, that the data is assumed to lie on, within the high-dimensional space. This is particularly useful for reducing dimensionality while preserving the intrinsic properties of the data.\n\nMethods like **==Isomap==** aim to preserve the geodesic distances between points, which better represent the data's true structure than straight-line distances in high-dimensional space. This enables effective [[Dimensionality Reduction]] for non-linear data while preserving important relationships between data points.\n\n### Key Concepts of Manifold Learning:\n\n1. **High-Dimensional Data**:\n   - In many machine learning problems, data can have a high number of dimensions (features), making it challenging to analyze directly. However, often this high-dimensional data lies on a much simpler, lower-dimensional structure, or **manifold**, embedded in the high-dimensional space. Manifold learning seeks to find and represent this lower-dimensional structure, simplifying the analysis and visualization of complex datasets.\n\n2. **Manifold Assumption**:\n   - Manifold learning assumes that although the data may appear high-dimensional, the ==true degrees of freedom are much fewer.== This means the data can be represented in a lower-dimensional space without losing important information about its structure.\n\n3. **Geodesic Distances**:\n   - In manifold learning, the goal is often to ==preserve certain distances or relationships between data points==. **Isomap**, for example, is a popular manifold learning algorithm that aims to preserve **geodesic distances**—the shortest paths between points along the manifold. These distances represent the true relationships between points in the underlying lower-dimensional space, even though they may seem far apart in the high-dimensional space.\n\n4. [[Dimensionality Reduction]]\n   - Like other dimensionality reduction techniques (such as PCA), manifold learning helps reduce the number of features in the data. However, manifold learning is particularly effective when the data is non-linear, meaning traditional linear techniques like PCA might not capture the true underlying structure. Algorithms like **Isomap**, **Locally Linear Embedding (LLE)**, and **t-SNE** are examples of manifold learning methods that handle such ==non-linear structures==.\n### Example: Isomap\n\n- **Isomap** is a manifold learning algorithm that tries to preserve the **geodesic distances** between all pairs of data points. It computes these distances by constructing a graph in which the edges represent the shortest path along the manifold (not the straight-line distance in high-dimensional space). Then, it uses these distances to map the data to a lower-dimensional space, retaining the structure of the original data.\n\n![[Pasted image 20240127124620.png|500]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "exploration"
    ],
    "normalized_filename": "manifold_learning",
    "outlinks": [
      "dimensionality_reduction",
      "pasted_image_20240127124620.png"
    ],
    "inlinks": [
      "curse_of_dimensionality",
      "machine_learning_algorithms",
      "principal_component_analysis"
    ]
  },
  {
    "category": "ML",
    "filename": "Maximum Likelihood Estimation",
    "sha": "3d4187b25b5078d673028eceb55cc22370fde73a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Maximum%20Likelihood%20Estimation.md",
    "text": "Resource:\n- https://www.youtube.com/watch?v=YevSE6bRhTo\n\nUsed to infer [[Model Parameters]] from collected data for example in [[Linear Regression]] ($\\beta_0,\\beta_1$).\n\nDefinition: [[Likelihood]]\n\nWhy is it a good tool for guessing parameter values?\n\nThe likelihoods plot a distribution, the max gives the most likely.\n\nThis is called the MLE.\n\nProperties of a MLE:\n- As more data comes in the [[Estimator]] should approach a true value\n- MLE is a ==consistent== [[Estimator]], i.e it gets closer to the true parameter value as the sample size grows.\n- Asymptotical Normal\n- Asymptotic Efficiency\n\nAssumptions for MLE:\n- Regularity\n\n[[parametric vs non-parametric models]]\n\nLikelihood is a function of a parameter\n\nImages:\n\nTha value obtained from MLE is the average of the data collected\n\nRelated to [[EM Algorithm]]\n\nThe shape of the [[Likelihood]] helps with finding paramaters.\n\n![[Pasted image 20250902074843.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "statistics"
    ],
    "normalized_filename": "maximum_likelihood_estimation",
    "outlinks": [
      "linear_regression",
      "estimator",
      "model_parameters",
      "em_algorithm",
      "pasted_image_20250902074843.png",
      "parametric_vs_non-parametric_models",
      "likelihood"
    ],
    "inlinks": [
      "em_algorithm",
      "logistic_regression",
      "statistics"
    ]
  },
  {
    "category": "ML",
    "filename": "Median Absolute Error",
    "sha": "2a321b319e9e1a08f5d9f72882a19aa7d93e6507",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Median%20Absolute%20Error.md",
    "text": "#### Median Absolute Error:\n   - Definition: This metric measures the median of the absolute errors between predicted and actual values.\n   - Interpretation: It provides a robust measure of prediction accuracy, especially in the presence of [[uncategorised/Outliers]].\n   - Formula: \n   $$ \\text{MedAE} = \\text{median}(|y_i - \\hat{y}_i|) $$",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "median_absolute_error",
    "outlinks": [
      "uncategorised/outliers"
    ],
    "inlinks": [
      "regression_metrics"
    ]
  },
  {
    "category": "ML",
    "filename": "Mermaid",
    "sha": "4b93ec94146f21b844a8ab0b61965804815a2b53",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Mermaid.md",
    "text": "Step 1. Ask Chatgpt to make er diagram with mermaidcode.\n\nStep 2. Use https://mermaid.js.org/\n\nUse obsidians built in feature:\n\n\n```mermaid\nerDiagram\n    SAMPLING_POINT {\n        string notation PK \"Primary Key\"\n        string label\n        int easting\n        int northing\n    }\n    \n    SAMPLE {\n        string id PK \"Primary Key\"\n        string samplingPoint FK \"Foreign Key\"\n        datetime sampleDateTime\n        boolean isComplianceSample\n        string purposeLabel\n        string sampledMaterialTypeLabel\n    }\n    \n    DETERMINAND {\n        string notation PK \"Primary Key\"\n        string label\n        string definition\n        string unitLabel\n    }\n    \n    RESULT {\n        int result PK \"Primary Key\"\n        string sampleID FK \"Foreign Key\"\n        string determinandNotation FK \"Foreign Key\"\n        string resultQualifierNotation\n        string codedResultInterpretation\n    }\n    \n    SAMPLING_POINT ||--o{ SAMPLE : \"has\"\n    SAMPLE ||--o{ RESULT : \"has\"\n    DETERMINAND ||--o{ RESULT : \"has\"\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "mermaid",
    "outlinks": [],
    "inlinks": [
      "code_diagrams",
      "documentation_&_meetings",
      "er_diagrams"
    ]
  },
  {
    "category": "ML",
    "filename": "Metadata Handling",
    "sha": "c830c15538a2d7ec7f34b47f1faed30ddbd4848a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Metadata%20Handling.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "metadata_handling",
    "outlinks": [],
    "inlinks": [
      "parquet"
    ]
  },
  {
    "category": "ML",
    "filename": "Methods for Handling Outliers",
    "sha": "2a08bd4aca5ee514c87633cbc0809ff4a52ad312",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Methods%20for%20Handling%20Outliers.md",
    "text": "### Trimming\n\n- Description: Removing data points identified as outliers based on criteria such as being beyond a certain number of standard deviations from the mean or outside a specified percentile range.\n- Implementation Example:\n  ```python\n  lower_quantile = df[\"var1\"].quantile(0.01)\n  upper_quantile = df[\"var1\"].quantile(0.99)\n  df_no_outliers = df[(df[\"var1\"] >= lower_quantile) & (df[\"var1\"] <= upper_quantile)]\n  ```\n\n### Capping or Flooring\n\n- Description: Setting a maximum or minimum threshold beyond which data points are considered outliers and replacing them with the threshold value.\n\n### Winsorizing \n\n- Description: Similar to capping and flooring, winsorizing replaces extreme values with less extreme values within a specified range, typically using percentiles.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "preprocessing"
    ],
    "normalized_filename": "methods_for_handling_outliers",
    "outlinks": [],
    "inlinks": [
      "outliers"
    ]
  },
  {
    "category": "ML",
    "filename": "Metric",
    "sha": "56ace5965a1f7026dfd446a196199df04e490fe9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Metric.md",
    "text": "### Metrics in Machine Learning\n\n[[Evaluation Metrics]]\n[[Regression Metrics]]\n### Metrics in business\n\nA metric, also called [KPI](term/key%20performance%20indicator%20(kpi).md) or (calculated) measure, are terms that serve as the building blocks for how business performance is both measured and defined, as knowledge of how to define an organization's KPIs. It is fundamental to have a common understanding of them. Metrics usually surface as business reports and dashboards with direct access to the entire organization.\n\nFor example, think of ==operational metrics== that represent your company's performance and service level or financial metrics that describe its financial health. \n\nCalculated measures are part of metrics and apply to specific [Dimensions](Dimensions.md) traditionally mapped inside a [Bus Matrix](term/bus%20matrix.md). \n\n\n[[Metric]]\nMetrics: Pick two to focus on\n- [[Metric|KPI]]\n- Whats the goal of it,\n- [[Evaluation Metrics]]: Help to evaluate the model [[Model Evaluation]]\n- The context matters.\n- [[Imbalanced Datasets|Class Imbalance]] matters.",
    "aliases": [
      "KPI",
      "Measure"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "evaluation"
    ],
    "normalized_filename": "metric",
    "outlinks": [
      "regression_metrics",
      "model_evaluation",
      "imbalanced_datasets",
      "evaluation_metrics",
      "metric"
    ],
    "inlinks": [
      "cosine_similarity",
      "dashboards",
      "metric",
      "semantic_layer"
    ]
  },
  {
    "category": "ML",
    "filename": "Mini-batch gradient descent",
    "sha": "238c401789a3ab3ec12b51ffc1a4a8798d8646ca",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Mini-batch%20gradient%20descent.md",
    "text": "Mini-Batch Gradient Descent is an optimization algorithm that serves as a compromise between Batch Gradient Descent and Stochastic Gradient Descent (SGD). Instead of using the entire dataset or a single data point, it updates model parameters using small, random subsets (mini-batches) of the training data.\n\nPros:\n- Faster than batch gradient descent.\n- More stable than [[Stochastic Gradient Descent]]\n- Leverages hardware acceleration (GPUs).",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "ml",
      "optimisation"
    ],
    "normalized_filename": "mini-batch_gradient_descent",
    "outlinks": [
      "stochastic_gradient_descent"
    ],
    "inlinks": [
      "gradient_descent"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Building",
    "sha": "8de2ed85c8dca2b77dfaed93ff0a09e189fc6f57",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Building.md",
    "text": "The Model Building phase follows the [[Preprocessing]] phase, where data is organized and prepared for analysis. This phase focuses on selecting and setting up the appropriate machine learning models to solve the problem at hand.\n## Key Steps\n\nTypes of Models:\n- Choose a model to apply based on the problem requirements and data characteristics.\n- Explore different [[Machine Learning Algorithms]] to find the best fit for your data.\n- Consider the tradeoffs between [[parametric vs nonparametric models]].\n\nSetting Up a Model:\n- Divide the data into [[Train-Dev-Test Sets]] to ensure robust evaluation and tuning.\n- Optimize [[Model Parameters]] and configurations for best performance.\n\n[[Model Selection]]:\n- Evaluate the appropriateness of models in the [[Model Selection]] phase.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "selection"
    ],
    "normalized_filename": "model_building",
    "outlinks": [
      "preprocessing",
      "parametric_vs_nonparametric_models",
      "model_selection",
      "model_parameters",
      "machine_learning_algorithms",
      "train-dev-test_sets"
    ],
    "inlinks": [
      "feature_importance",
      "gradient_boosting",
      "heterogeneous_features",
      "keras",
      "machine_learning_operations",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Deployment using PyCaret",
    "sha": "f82a2be1a20618638c0b93edbb496bc08362a6db",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Deployment%20using%20PyCaret.md",
    "text": "#### Model Deployment using [[PyCaret]]\n\nAWS: When deploying model on AWS [[Amazon S3|S3]], environment variables must be configured using the command-line interface. To configure AWS environment variables, type `aws configure` in terminal. The following information is required which can be generated using the Identity and Access Management (IAM) portal of your amazon console account:\n\n- AWS Access Key ID\n- AWS Secret Key Access\n- Default Region Name (can be seen under Global settings on your AWS console)\n- Default output format (must be left blank)\n\n[[Google Cloud Platform|GCP]]To deploy a model on Google Cloud Platform ('gcp'), the project must be created using the command-line or GCP console. Once the project is created, you must create a service account and download the service account key as a JSON file to set environment variables in your local environment. Learn more about it: https://cloud.google.com/docs/authentication/production\n\nAzure: To deploy a model on Microsoft Azure ('azure'), environment variables for the connection string must be set in your local environment. Go to settings of storage account on Azure portal to access the connection string required.\nAZURE_STORAGE_CONNECTION_STRING (required as environment variable)\nLearn more about it: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?toc=%2Fpython%2Fazure%2FTOC.json",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "model_deployment_using_pycaret",
    "outlinks": [
      "pycaret",
      "amazon_s3",
      "google_cloud_platform"
    ],
    "inlinks": [
      "model_deployment"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Ensemble",
    "sha": "19c229952bc94e90c64bb6308af7b272ba393e4e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Ensemble.md",
    "text": "Ensemble models in machine learning are techniques that ==combine the predictions of multiple individual models== to improve overall performance. Ensemble methods can achieve better accuracy and robustness than any single model alone. \n\nKey Concepts of Ensemble Models:\n1. **Diversity**: The strength of ensemble models lies in the ==diversity== of the base models. Different models may capture different patterns or errors in the data, and combining them can lead to more accurate predictions.\n2. **Combination**: Ensemble methods aggregate the predictions of individual models using ==techniques like averaging, voting, or weighted sums== to produce a final prediction.\n\nMain Ensemble Techniques:\n- [[Bagging]]\n- [[Boosting]]\n- [[Bagging vs Boosting]]\n- [[Stacking]]\n- [[Isolated Forest]]\n# Further Understanding\n### Analogy:\n- Ensemble methods can be likened to consulting multiple doctors for a diagnosis. Each doctor (model) may have a different opinion, but by considering all opinions, the final diagnosis (prediction) is more accurate than relying on a single doctor's opinion.\n\n### Advantages of Ensemble Models:\n- **Increased Accuracy**: By combining multiple models, ensemble methods often achieve higher accuracy than individual models.\n- **Robustness**: They are less sensitive to overfitting, especially when using techniques like bagging.\n- **Flexibility**: Ensemble methods can be applied to various types of base models and are not limited to a specific algorithm.\n\n### Challenges:\n- **Complexity**: Ensemble models can be more complex and computationally intensive than single models.\n- **[[Interpretability]]**: The final model may be harder to interpret compared to simpler models like decision trees.",
    "aliases": [
      "ensemble"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "modeling"
    ],
    "normalized_filename": "model_ensemble",
    "outlinks": [
      "isolated_forest",
      "bagging_vs_boosting",
      "bagging",
      "stacking",
      "interpretability",
      "boosting"
    ],
    "inlinks": [
      "bagging",
      "bagging_vs_boosting",
      "boosting",
      "chain_of_thought",
      "classification",
      "comparing_ensembles.py",
      "decision_trees_are_fragile",
      "ds_&_ml_portal",
      "gradient_boosting",
      "gradient_boosting_regressor",
      "isolated_forest",
      "model_optimisation",
      "random_forest",
      "regularisation_of_tree_based_models",
      "stacking",
      "weak_learners",
      "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Evaluation vs Model Optimisation",
    "sha": "9741a8d7c0379b09b1570e7239f8ccd948881d00",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Evaluation%20vs%20Model%20Optimisation.md",
    "text": "[[Model Evaluation]]focuses on assessing a model's performance, while [[Model Optimisation]]aims to improve that performance through various techniques. \n\nIterative Process: Model evaluation and optimization are often iterative. After evaluating a model, insights gained can guide further optimization. Conversely, after optimizing a model, it needs to be re-evaluated to ensure improvements.\n\nFeedback Loop: Evaluation provides feedback on the effectiveness of optimization efforts, helping refine the model further.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "optimisation"
    ],
    "normalized_filename": "model_evaluation_vs_model_optimisation",
    "outlinks": [
      "model_optimisation",
      "model_evaluation"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Model Evaluation",
    "sha": "544c4ba6c944bec59aa3aba230302db0dd1646f5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Evaluation.md",
    "text": "Assess the model's performance using various metrics to ensure it meets the desired accuracy and reliability.\n\nAppropriate evaluation metrics are used based on the problem type (classification vs. regression), to assess how well the model predicts.\n\n==The aim is to improve accuracy but also to generalize and avoid biases== and [[Overfitting]].\n\n- **Performance Assessment**: Models are evaluated on a testing set using metrics relevant to the problem type.\n- **Generalization and Bias**: Evaluation includes assessing how well the model generalizes to new data and identifying any biases.\n\nFor categorical classifiers: [[Evaluation Metrics]]: Use metrics such as accuracy, precision, recall, F1-score, and confusion matrix to evaluate performance.\n\nFor regression tasks: [[Regression Metrics]]: Metrics like [[Mean Absolute Error]] (MAE), Mean Squared Error (MSE), [[Root Mean Squared Error]] (RMSE), and R-squared (R²) are used.\n\n[[Cross Validation]] is a technique used to assess the performance of a model by splitting the data into multiple subsets for training and testing to assesses performance and generalization. It helps detect [[Overfitting]], provides reliable performance estimates.\n\n[[Feature Importance]]: After training, analyze which features have the most significant impact on the model's predictions.\n\nRelated:\n- [[Test Loss When Evaluating Models]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "modeling"
    ],
    "type": "process",
    "normalized_filename": "model_evaluation",
    "outlinks": [
      "evaluation_metrics",
      "mean_absolute_error",
      "regression_metrics",
      "cross_validation",
      "test_loss_when_evaluating_models",
      "feature_importance",
      "overfitting",
      "root_mean_squared_error"
    ],
    "inlinks": [
      "binary_classification",
      "cross_validation",
      "ds_&_ml_portal",
      "feature_selection",
      "imbalanced_datasets",
      "linear_regression",
      "logistic_regression",
      "loss_function",
      "metric",
      "model_evaluation_vs_model_optimisation",
      "model_optimisation",
      "model_selection",
      "neural_network_in_practice",
      "out-of-sample_rolling_forecast_evaluation",
      "pycaret",
      "test_loss_when_evaluating_models",
      "train-dev-test_sets",
      "why_type_1_and_type_2_matter",
      "wrapper_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Interpretability",
    "sha": "bda5d1e0c524dcecbc94b12843fbbb2a8c5698b0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Interpretability.md",
    "text": "Model [[Interpretability]] tools are crucial in ensuring that machine learning models are transparent, explainable, and understandable to stakeholders, particularly in industries where decisions need to be justifiable (e.g., finance, healthcare). \n\nThese tools are becoming standard for ensuring trustworthiness and transparency in ML models, enabling organizations to defend model predictions in regulated industries and maintain user trust.\n\n[[p values]] and [[Confidence Interval]]: If statistical significance is needed, interpret these values to determine which features significantly contribute to the model.\n\n[[SHapley Additive exPlanations]]\n\n[[Local Interpretable Model-agnostic Explainations]]\n\nCounterfactual Explanations:\n\n   - Purpose: Counterfactual explanations aim to provide insight into ==how small changes in the input features could lead to different outcomes==, helping users understand model behavior.\n\n   - How it works: It identifies the minimal changes needed to alter a prediction. For example, in a credit scoring model, it might show how an individual could change their features (e.g., increasing income) to get approved for a loan.\n\n   - Use cases: Particularly useful in sensitive fields like credit scoring, hiring, and medical diagnosis, where actionable explanations are critical.\n\n   - Advantage: Provides intuitive and actionable feedback on predictions.\n\n\nGlobal Surrogate Models\n\n   - Purpose: A global surrogate is an interpretable model that is trained to approximate the predictions of a black-box model.\n\n   - How it works: It uses simpler models (like decision trees) to mimic the behavior of a complex model and provide a global, easy-to-understand representation of how the model makes decisions.\n\n   - Use cases: Provides ==insight into overall model behavior==, though not as accurate as local explanations for specific predictions.\n- \n   - Advantage: Simplicity and clarity for non-technical stakeholders.\n\n- Scenario: An e-commerce company uses a neural network to predict customer churn based on features like purchase history, browsing behavior, and customer support interactions.\n\n- Surrogate Model: To explain the overall decision-making process of the complex neural network, the data science team trains a decision tree as a global surrogate model. This decision tree offers a simplified view, showing that customers with a decline in recent purchases and frequent negative support interactions are most likely to churn.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "model_interpretability",
    "outlinks": [
      "shapley_additive_explanations",
      "p_values",
      "confidence_interval",
      "local_interpretable_model-agnostic_explainations",
      "interpretability"
    ],
    "inlinks": [
      "model-agnostic_feature_importance",
      "model_selection"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Observability",
    "sha": "dcb40bd72cd82c8cd73b1f45500328f6ab66af10",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Observability.md",
    "text": "Monitor the model's performance over time (in production). Similar to [[Model Validation]].\n\nIn the context of machine learning (ML), Observability refers to the ability to ==monitor, understand, and diagnose the performance and behaviour of ML models== in production. \n\nIt encompasses the processes, tools, and techniques that help practitioners ensure models are functioning as expected and identify when they deviate from desired outcomes. \n\n[[Master Observability Datadog]]\n## Key aspects of observability in machine learning include:\n\nObservability is a process in ML, and is usually achieved through logging, metrics collection, real-time monitoring, and advanced diagnostic tools integrated into the ML pipeline.\n\n1. Monitoring Model Performance ==(monitoring metrics)==:\n   - Tracking key metrics such as [[Accuracy]], [[Precision]],[[Recall]],[[F1 Score]],[[ROC (Receiver Operating Characteristic)]]d other relevant KPIs over time to identify performance degradation or improvements.\n   - Monitoring [[Performance Drift]] in model inputs (features) and outputs (predictions) to detect when the model no longer performs well due to changes in data distribution ([[Data Drift]]) or changes in relationships between variables ([[Performance Drift]]).\n\n1. Error and [[Isolated Forest]]:\n   - Identifying when predictions are out of the expected range or when the model behaves abnormally, such as high error rates on specific subsets of data or excessive latency in prediction generation.\n   \n2. [[Interpretability]]:\n   - Ensuring that the internal workings of the model (e.g., feature importance, decision pathways) are visible, interpretable, and explainable to humans. This allows for easier debugging and accountability, especially in critical applications such as finance, healthcare, or autonomous systems.\n\n2. [[data lineage]] and Provenance:\n   - Tracking the data sources, transformations, and processes that influence the model’s input data. This provides visibility into how data flows through the pipeline and helps in reproducing results or addressing data-related issues.\n\n2. Pipeline Monitoring:\n   - Observing the entire ML pipeline from data ingestion and preprocessing to model training, [[validation]], and deployment. This includes identifying bottlenecks, delays, and system failures that may affect the model's ability to make predictions in real-time.\n\n2. Alerts and Automation:\n   - Setting up ==automated alerts== when certain thresholds are breached, such as a sudden drop in accuracy or an increase in response time. This allows for prompt interventions, whether retraining the model, adjusting the pipeline, or tuning hyperparameters.\n\n## Why Observability Matters in Machine Learning:\n\n- Ensures Reliability: Observability provides insights into how models behave under different conditions, ensuring that they remain reliable and consistent in their performance.\n- Prevents Model Drift ([[Performance Drift]]): With observability, teams can detect model drift early, enabling them to retrain or recalibrate the model before performance deteriorates.\n- Improves Accountability: Particularly in high-stakes applications, having observability in place allows organizations to understand and justify the model’s decisions.\n- Supports Continuous Monitoring: Observability is critical in ML systems that operate continuously in production, ensuring they are making accurate and meaningful predictions over time.\n\nMonitor the model's performance over time. If the data distribution changes (concept drift), or the model's accuracy declines, retraining or updating the model may be necessary.\n\n## Related to:\n- [[Data Observability]]\n- [[Model Validation]]",
    "aliases": [
      "Observability"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#explainability",
      "modeling"
    ],
    "normalized_filename": "model_observability",
    "outlinks": [
      "isolated_forest",
      "accuracy",
      "f1_score",
      "data_drift",
      "data_lineage",
      "recall",
      "roc_(receiver_operating_characteristic)",
      "interpretability",
      "model_validation",
      "master_observability_datadog",
      "precision",
      "performance_drift",
      "data_observability",
      "validation"
    ],
    "inlinks": [
      "business_observability",
      "machine_learning_operations",
      "model_deployment",
      "model_validation"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Optimisation",
    "sha": "424388221ba995535e12a72e9abaa22b82c193c6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Optimisation.md",
    "text": "Model optimization is a step in the machine learning workflow aimed at enhancing a model's performance by fine-tuning its parameters and hyperparameters. The goal is to improve the model's accuracy, efficiency, and ability to generalize to new data. \n\n[[Convex Optimisation]]\n### Purpose:\n- Accuracy: Improve the model's predictive performance.\n- Efficiency: Ensure the model runs efficiently in terms of computation and resource usage.\n- [[Generalisation]]: Enhance the model's ability to perform well on unseen data, avoiding overfitting.\n- Minimising the [[objective function]]. \n### Process:\n0. [[Model Parameters]] tuning\n\n1. [[Hyperparameter|Hyperparameter tuning]]\n   - Adjust hyperparameters such as learning rate, number of layers in a neural network, and regularization strength to find the optimal configuration.\n   - Techniques like grid search, random search, or Bayesian optimization can be used for this purpose.\n\n2. [[Feature Engineering]]\n   - Involves selecting, transforming, or creating new features that can improve model performance.\n   - This step can significantly impact the model's ability to learn patterns from the data.\n\n3. [[Model Evaluation]]\n   - Evaluate the model using appropriate metrics based on the problem type (e.g., classification or regression).\n   - Metrics for classification include accuracy, precision, recall, F1-score, and confusion matrix.\n   - Metrics for regression include [[Mean Absolute Error]] (MAE), Mean Squared Error (MSE), [[Root Mean Squared Error]] (RMSE), and R-squared (R²).\n\n4. [[Cross Validation]]\n   - A technique to assess the model's performance by splitting the data into multiple subsets for training and testing.\n   - Helps in detecting overfitting and provides reliable performance estimates.\n\n5. [[Model Ensemble]]: Combining models to get better performance",
    "aliases": [
      "Optimisation"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "model_optimisation",
    "outlinks": [
      "objective_function",
      "hyperparameter",
      "generalisation",
      "mean_absolute_error",
      "model_parameters",
      "cross_validation",
      "feature_engineering",
      "convex_optimisation",
      "model_evaluation",
      "root_mean_squared_error",
      "model_ensemble"
    ],
    "inlinks": [
      "cost_function",
      "cross_validation",
      "data_selection_in_ml",
      "decision_theory",
      "ds_&_ml_portal",
      "loss_function",
      "loss_versus_cost_function",
      "model_evaluation_vs_model_optimisation",
      "momentum",
      "stochastic_gradient_descent"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Parameters Tuning",
    "sha": "01638875dcdd5f6d6e108bc13aec36bf88db8a58",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Parameters%20Tuning.md",
    "text": "To find optimal [[Model Parameters]].\n## Finding Optimal Model Parameters\n\n1. Parameter Space Exploration:\n   - It's useful to visualize slices of the parameter space by selecting two parameters at a time. This helps in understanding how different parameter values affect the model's performance.\n\n2. [[Cost Function]]\n   - The cost function is used to find the minimum error in predictions. It measures the difference between predicted and actual values, and the goal is to minimize this function to improve model accuracy.\n\n3. [[Optimisation function]]\n   - Ideal parameters are found using optimization functions, which adjust the model parameters to minimize the loss function. Common optimization algorithms include Gradient Descent, Adam Optimizer, and Stochastic Gradient Descent.\n\n4. Data Splitting:\n   - Split the data into training and cross-validation sets to evaluate model performance. Plot the parameter of interest on the x-axis and accuracy on the y-axis to visualize performance.\n\n[[Optimisation techniques]]\n### Example\n\n\n![[Pasted image 20241231142918.png]]\n\n\nTo find optimal model parameters, graph the parameter against error of the model.\n\nOn the left plot, the solid lines represent the predictions from these models. A polynomial model with degree 1 produces a straight line that intersects very few data points, while the maximum degree hews very closely to every data point. \n\nOn the right:\n    - the error on the trained data (blue) decreases as the model complexity increases as expected\n    - the error of the cross-validation data decreases initially as the model starts to conform to the data, but then increases as the model starts to over-fit on the training data (fails to *generalize*).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "selection"
    ],
    "normalized_filename": "model_parameters_tuning",
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "pasted_image_20241231142918.png",
      "optimisation_techniques",
      "cost_function"
    ],
    "inlinks": [
      "model_parameters",
      "regularisation"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Parameters",
    "sha": "765ec036be55bda82e9855420d55b8986c97ee5e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Parameters.md",
    "text": "Model parameters are also called weights and biases.\n\nThese parameters are adjusted during the training process to optimize the model's performance on the given task.\n\nSee also: \n- [[Model Parameters Tuning]]\n- [[Optimisation techniques]]\n### Examples\n\n1. **[[Linear Regression]]**: \n   - Coefficients (weights) for each feature in the input data.\n   - Intercept term (bias).\n\n2. **[[Logistic Regression]]**:\n   - Similar to [[Linear Regression]], it has coefficients for each feature and an intercept term, but it models the probability of a binary outcome.\n\n3. **[[Deep Learning|Neural Networks]]**:\n   - Weights: The connections between neurons in different layers.\n   - Biases: Additional parameters added to the weighted sum of inputs to a neuron.\n\n4. **[[Support Vector Machines]] (SVM)**:\n   - Support vectors: Data points that define the decision boundary.\n   - Coefficients for the hyperplane equation.\n\n5. **Decision Trees**: [[Decision Tree]]\n   - Splitting thresholds for each node.\n   - Structure of the tree (which features are used at each split).\n\n6. **[[K-means]] [[Clustering]]**:\n   - Centroids: The center points of each cluster.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "optimisation"
    ],
    "normalized_filename": "model_parameters",
    "outlinks": [
      "linear_regression",
      "decision_tree",
      "clustering",
      "logistic_regression",
      "deep_learning",
      "k-means",
      "model_parameters_tuning",
      "optimisation_techniques",
      "support_vector_machines"
    ],
    "inlinks": [
      "batch_gradient_descent",
      "cost-sensitive_analysis",
      "cost_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "forecasting_autoarima.py",
      "gradient_descent",
      "hyperparameter",
      "lbfgs",
      "logistic_regression",
      "loss_function",
      "maximum_likelihood_estimation",
      "model_building",
      "model_optimisation",
      "model_parameters_tuning",
      "model_parameters_vs_hyperparameters",
      "model_validation",
      "neural_network",
      "optimisation_function",
      "optimising_a_logistic_regression_model",
      "pmdarima",
      "stochastic_gradient_descent",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Random States",
    "sha": "b998827dce2e756f10093995e5dd6c17e8793d6e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Random%20States.md",
    "text": "### **Robustness and Testing**\n\n**Why it matters:**\nModel performance can vary due to randomness in initialization, sampling, or feature selection. Testing robustness involves checking whether results remain stable across different random seeds or random states.\n\n**Scikit-learn resources:**\n\n* [Glossary: random_state](https://scikit-learn.org/dev/glossary.html#term-random_state)\n* [Common Pitfalls: Randomness](https://scikit-learn.org/dev/common_pitfalls.html#randomness)\n\n**Definition:**\n\n* `random_state` controls the source of randomness.\n* An *integer* seed (e.g. 0, 42) ensures full reproducibility — the same pseudo-random sequence is used every time.\n* A `RandomState` instance generates new random numbers each time, providing variability between model fits or folds.\n* Integer values must be within $[0, 2^{32} - 1]$.\n\n**Example:**\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nX, y = make_classification(random_state=0)\n\nrf_int = RandomForestClassifier(random_state=123)\nrf_rng = RandomForestClassifier(random_state=np.random.RandomState(0))\n\ncross_val_score(rf_int, X, y)\n# same RNG across folds → identical randomness\n\ncross_val_score(rf_rng, X, y)\n# new RNG per fold → variable randomness\n```\n\n**Explanation:**\n\n* With an **integer seed**, each fold in [[Cross Validation]] reuses the same RNG. This means the random subset of features or samples in each fold is identical, possibly overestimating model stability.\n* With a **RandomState instance**, each fold starts from a fresh RNG, leading to more realistic variability in performance.\n\n**Recommendation:**\nFor robustness testing, evaluate model sensitivity across several distinct seeds or by passing a `RandomState` instance. This helps ensure that observed performance is not dependent on one random configuration.\n\n**Diagnostic approach:**\nPlot accuracy (or another metric) against the number of estimators and different seeds to identify:\n\n* The minimum ensemble size required for stable accuracy.\n* A performance plateau where adding estimators no longer improves results.",
    "aliases": [],
    "date modified": "2-11-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "model_random_states",
    "outlinks": [
      "cross_validation"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Model Selection",
    "sha": "95e3a765fd8271bccda663e50c0a784984db4a64",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Selection.md",
    "text": "Model selection is an integral part of building a [[Machine Learning Operations]] to ensure that the best performing model is chosen for a given task, avoiding issues like overfitting or underfitting.\n\nThis is a crucial step because the model's ability to ==generalize== to unseen data depends on selecting the right one.\n\nModel selection typically involves the following steps:\n\n1. Define candidate models: These can be models of different types (e.g., decision trees, support vector machines, neural networks) or the same model type but with varying hyperparameters.\n   \n2. Train each model: Train all the candidate models on the training set using different algorithms or parameter settings.\n   \n3. Evaluate performance ([[Model Evaluation]]): Use a validation set or cross-validation to evaluate the performance of each model. Common evaluation metrics include accuracy, precision, recall, F1 score, and mean squared error, depending on the type of problem (classification or regression).\n\n4. Select the best model: Based on the evaluation metrics, choose the model that performs best on the validation set. The aim is to balance bias and variance to achieve good generalization to unseen data.\n\n5. Test on unseen data: Finally, test the selected model on a test set to ensure that it generalizes well and has not been overfitted to the validation data.\n\nCommon approaches for model selection include:\n- [[GridSeachCv]] and [[Random Search]] for hyperparameter tuning.\n- [[Cross Validation]] to ensure robustness by evaluating model performance on different subsets of the data.\n- Bayesian Optimization, which can be used to efficiently search the hyperparameter space.\n- Choose the best-performing model based on [[Evaluation Metrics]] and optimization results.\n- [[Cross Validation]]: Evaluate the model more robustly by splitting the training data into smaller chunks and training the model multiple times.\n- [[Model Interpretability]]: Utilize tools to understand and interpret the model's predictions, ensuring transparency and trustworthiness.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "optimisation",
      "process"
    ],
    "normalized_filename": "model_selection",
    "outlinks": [
      "gridseachcv",
      "machine_learning_operations",
      "cross_validation",
      "random_search",
      "model_evaluation",
      "model_interpretability",
      "evaluation_metrics"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "handling_different_distributions",
      "learning_curve",
      "model_building",
      "parsimonious",
      "pycaret",
      "regularisation",
      "test_loss_when_evaluating_models"
    ]
  },
  {
    "category": "ML",
    "filename": "Model Validation",
    "sha": "5a914a68a25ca85aa49cbc304ebf8e5058aac2b0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20Validation.md",
    "text": "Model Validation refers to the process of evaluating a machine learning model's performance on a separate dataset (often called the validation set) to ensure it generalizes well to new, unseen data. This step is crucial for tuning [[Model Parameters]], selecting the best model, and preventing overfitting. Validation helps in assessing how well the model will perform in real-world scenarios.\n\n[[Model Observability]], on the other hand, involves monitoring and understanding the model's performance and behavior in production over time. It includes tracking metrics, detecting [[Performance Drift]], and ensuring the model continues to function as expected in dynamic environments.\n\nWhile model validation is a step in the model development process, model observability is an ongoing practice once the model is deployed. Both are related in that they aim to ensure the model's reliability and effectiveness, but they occur at different stages of the model lifecycle. Validation is about initial performance assessment, whereas observability is about continuous monitoring and maintenance.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "modeling"
    ],
    "normalized_filename": "model_validation",
    "outlinks": [
      "performance_drift",
      "model_observability",
      "model_parameters"
    ],
    "inlinks": [
      "cross_validation",
      "model_observability",
      "sklearn_pipeline"
    ]
  },
  {
    "category": "ML",
    "filename": "Model parameters vs hyperparameters",
    "sha": "4de7aa48143bd9f7e9e60188f87e882162a595cc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Model%20parameters%20vs%20hyperparameters.md",
    "text": "Model parameters and hyperparameters serve different roles:\n\n[[Model Parameters]]\n   - These are the internal variables of the model that are learned from the training data. They define the model's structure and are adjusted during the training process to minimize the [[Loss function]].\n   - Examples include:\n\t   - the weights and biases in a neural network,\n\t   - the coefficients in a linear regression model,\n\t   - or the support vectors in a support vector machine.\n   - Model parameters are directly influenced by the data and are optimized through algorithms like [[Gradient Descent]].\n\n[[Hyperparameter]]\n   - These are external configurations set before the training process begins. They are not learned from the data but are used for controlling the learning process and the model's architecture.\n   - Examples include the:\n\t   - [[Learning Rate]], \n\t   - the number of hidden layers in a [[Neural network]],\n\t   - the number of trees in a random forest,\n\t   - or the regularization parameter in a regression model.\n   - Hyperparameters are typically tuned through methods like grid search or random search to find the best configuration that results in optimal model performance.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "model_parameters_vs_hyperparameters",
    "outlinks": [
      "gradient_descent",
      "hyperparameter",
      "model_parameters",
      "loss_function",
      "learning_rate",
      "neural_network"
    ],
    "inlinks": [
      "hyperparameter",
      "loss_versus_cost_function"
    ]
  },
  {
    "category": "ML",
    "filename": "Momentum",
    "sha": "35afb03ced994c6a0ccaf7c10b043f6e2371ccf8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Momentum.md",
    "text": "Momentum is an [[Model Optimisation|Optimisation]] technique used to accelerate the [[Gradient Descent]] algorithm by incorporating the concept of inertia. It helps in reducing oscillations and speeding up convergence, especially in scenarios where the [[Cost Function]] has a complex landscape (surface). Momentum helps in dampening oscillations and achieving faster convergence. Momentum is a technique that helps accelerate gradient descent by adding a fraction of the previous update to the current update. Formula:\n  $$\n  v_{t+1} = \\beta v_t + (1 - \\beta) \\nabla_{\\theta} J(\\theta)\n  $$\n  $$\n  \\theta_{t+1} = \\theta_t - \\alpha v_{t+1}\n  $$\n  Where:\n  - $v_t$ is the velocity (the accumulated gradient).\n  - $\\beta$ is the momentum factor.\n  - $\\nabla_{\\theta} J(\\theta)$ is the gradient of the cost function with respect to the parameters $\\theta$.\n  - $\\alpha$ is the learning rate.\n\nIn [[ML_Tools]] see: [[Momentum.py]]\n## Key Features of Momentum\n\n**Inertia Effect:** Momentum uses the past gradients to smooth out the updates, which helps in navigating the parameter space more effectively.\n\n**Parameter Update Rule:** The update rule for momentum involves maintaining a velocity vector that accumulates the gradients. The parameters are then updated using this velocity, which is a combination of the current gradient and the previous velocity.\n\n[[Hyperparameter]]\n  - **[[Learning Rate]] ($\\alpha$):** Controls the size of the steps taken towards the minimum.\n  - **Momentum Coefficient ($\\beta$):** Determines the contribution of the previous gradients to the current update. A typical value is 0.9.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "momentum",
    "outlinks": [
      "gradient_descent",
      "hyperparameter",
      "model_optimisation",
      "momentum.py",
      "cost_function",
      "learning_rate",
      "ml_tools"
    ],
    "inlinks": [
      "adam_optimizer",
      "adaptive_learning_rates",
      "stochastic_gradient_descent"
    ]
  },
  {
    "category": "ML",
    "filename": "Moving Average Forecast",
    "sha": "1be77dec049b1d56a33eb070b3168ba7e968e0bb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Moving%20Average%20Forecast.md",
    "text": "### Moving Average Forecasts\n\nA **Moving Average (MA) forecast** works by taking the average of the last $n$ observed points and projecting this forward. It is one of the simplest forecasting approaches and is often used as a **baseline model**.\n\n#### Limitations of Simple Moving Averages\n\n* **Equal weighting**: Every value in the chosen window contributes equally, regardless of how recent it is. This means recent shifts in the data are not emphasized.\n* **No trend or seasonality**: SMAs produce flat forecasts that cannot capture upward/downward movements or repeating seasonal patterns.\n* **Window size sensitivity**: The choice of window length heavily influences the forecast. Too small a window may be noisy; too large a window may overly smooth the series.\n* **Forecast horizon problem**: An SMA essentially fails to produce meaningful predictions beyond the averaging window, since it always outputs a constant extension.\n\nBecause of these drawbacks, SMA is rarely used for serious forecasting beyond benchmarking. More advanced methods, such as [[Exponential Smoothing]], Holt’s, or Holt-Winters’, address these issues by giving more weight to recent data and modeling trend and seasonality explicitly.\n\n### Variants\n\n**Simple Moving Average (SMA)**\n* Uses the last $k$ values to compute a single average.\n* Produces a **flat forecast line** into the future.\n* Useful as a naive benchmark but not for dynamic patterns.\n\n**Rolling Moving Average**\n* Each new prediction is updated iteratively using the most recent window, which can include earlier forecasts.\n* Produces a line smoother than Holt’s, but still limited in capturing complex dynamics.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "forecasting",
      "ml",
      "time_series"
    ],
    "normalized_filename": "moving_average_forecast",
    "outlinks": [
      "exponential_smoothing"
    ],
    "inlinks": [
      "baseline_forecast",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Multinomial Naive bayes",
    "sha": "3d7c6d4ab648f239545780ba0e719831d08b79f5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Multinomial%20Naive%20bayes.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "ml",
      "statistics"
    ],
    "normalized_filename": "multinomial_naive_bayes",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Multiple Linear Regression",
    "sha": "51562a22aa23f0819bc9b4aec0937e794e288d06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Multiple%20Linear%20Regression.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "multiple_linear_regression",
    "outlinks": [],
    "inlinks": [
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Naive Bayes Classifier",
    "sha": "0a615430196ef76de5cb3bd49c63900af277801f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Naive%20Bayes%20Classifier.md",
    "text": "A probabilistic classifier based on Bayes’ theorem, assuming that features are conditionally ==independent== given the class. This assumption is why it’s called “naive.”\n\nBayes’ theorem:\n\n$$\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n$$\n\nwhere:\n\n* $P(A|B)$ = Posterior probability\n* $P(B|A)$ = Likelihood\n* $P(A)$ = Prior probability\n* $P(B)$ = Evidence\n\nWhen to use:\n* Text classification (spam detection, sentiment analysis).\n* ==Problems with many independent features.==\n### Why Naive Bayes?\n\nAssumes features are independent, which is rarely true, but works surprisingly well in practice.\n\n* Simple, fast, and works well for high-dimensional data (e.g., [[Text Classification]]).\n* Treats data as a ==bag of features== (order doesn’t matter).\n* Handles categorical and numeric data (with assumptions):\n\t- Categorical: Works directly or via encoding.\n\t- Numeric: Assumes normal distribution (Gaussian NB).\n\nAdvantages:\n* Scales well to large datasets.\n* Requires small amount of training data.\n* Effective for text and document classification.\n\nSmoothing Issue:\n- To avoid zero probabilities (when a feature value was not seen in training), we add a small value $\\alpha$ (Laplace smoothing).\n\n### Types of Naive Bayes\n\n* BernoulliNB: For binary features (e.g., word presence/absence).\n* MultinomialNB: For count-based features (e.g., word counts in text).\n* GaussianNB: For continuous features (assumes Gaussian distribution).\n\n### Example\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=1.0)  # Laplace smoothing\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "projects": null,
    "tags": [
      "analysis",
      "classifer",
      "ml",
      "NLP",
      "probability"
    ],
    "normalized_filename": "naive_bayes_classifier",
    "outlinks": [
      "text_classification"
    ],
    "inlinks": [
      "classification",
      "feature_scaling",
      "machine_learning_algorithms",
      "supervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Naive Forecast",
    "sha": "2d2aab4ca60ccf59fb4b3a4c9380e33d21071159",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Naive%20Forecast.md",
    "text": "Naive Forecast\nAssumes the next value will be the same as the last observed one.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "naive_forecast",
    "outlinks": [],
    "inlinks": [
      "baseline_forecast"
    ]
  },
  {
    "category": "ML",
    "filename": "Neural Network Classification",
    "sha": "33c089d14239015fc0cedb947d0b054b23167ec3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Neural%20Network%20Classification.md",
    "text": "Choosing Thresholds/Clusters in [[Neural network]] [[Classification]]\n\nWhen working with [[Deep Learning|neural networks]], the output is often a probability distribution across different classes. To make a final classification decision, we need to convert these probabilities into discrete class labels. This is typically done by comparing the probabilities against a threshold or by clustering them.\n\n## Threshold-Based Classification\n\nIn threshold-based classification, we set a specific probability value as the threshold. If the probability of a class exceeds this threshold, the input is classified as belonging to that class. Otherwise, it's classified as belonging to another class or as \"unknown.\"\n\n[[Choosing a Threshold]]\n\n## Clustering-Based Classification\n\nIn [[Clustering]]-based classification, we group the probability distributions into clusters. Each cluster represents a class. This approach is useful when the class boundaries are not well-defined or when there are multiple overlapping classes.\n\n[[Choosing the Number of Clusters]]\n\n## Additional Considerations:\n\n- [[Imbalanced Datasets]]: If the classes are imbalanced, the choice of threshold or number of clusters can be significantly affected. Techniques like oversampling, undersampling, or using weighted loss functions can help mitigate the impact of class imbalance.\n- [[Data Quality]]: The quality of the training data can also influence the choice of threshold or number of clusters. If the data is noisy or contains outliers, the chosen values may not be optimal.\n- [[Evaluation Metrics]]: Choose evaluation metrics that are appropriate for the specific problem and the desired trade-off between different types of errors.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "neural_network_classification",
    "outlinks": [
      "clustering",
      "deep_learning",
      "choosing_a_threshold",
      "choosing_the_number_of_clusters",
      "imbalanced_datasets",
      "neural_network",
      "data_quality",
      "evaluation_metrics",
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Neural Scaling Laws",
    "sha": "f1086f6c48d5d18ba7541a4ba76017ed77e8b699",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Neural%20Scaling%20Laws.md",
    "text": "Even scaled model cannot cross tthe [[compute efficent frontier]]\n\n![[Pasted image 20241017072233.png|500]]\n\n[[validation loss]]\n\nNeural scaling laws. That is error rates scale with compute,model size and dataset size, independant of model aritechture. Can we drive to 0?\n \nSame laws apear in video and image models.\n\nLLMs are auto progressive models.\n\nTheorical results guiding experiemental - saving compute time.\n\n#### How [[LLM|LLMs]] work\n\n![[Pasted image 20241017072732.png|500]]\n\nDuring training we know next value, hence we have a [[Loss function]] to help learning.\n\nL1 - loss functions\n\n[[Cross Entropy]]-loss function (uses negative log of probability). Why is cross enropy used over L1?\n\nunabigious next words. [[Entropy of natural language]] due to this will LLMs cannot drive [[Cross entropy loss]] to zero.\n\n### Manifolds?\n\nExample [[MNIST]] data set images of number, has high dimesnional dataset space.\n\n![[Pasted image 20241017073743.png|500]]\n\n[16:03](https://www.youtube.com/watch?t=963&v=5eqRuVp65eY)\nSimlar concepts group together.\n\ndensity of manifold \naverage distance between point. or size of neightbourhoods s\n\n![[Pasted image 20241017074030.png|500]]\n\n$S=L D^{-1/d}$\n\n### Manifold hypothesis (data points in high dim space) and scaling laws \n\nKnowing the manifold will help scaling. This is called\n\n[[Resolution limited scaling]]\n\n$$LOSS < D^{-4/d}$$\n\n[[Cross entropy loss]] should scale wrt manifold.\n\n[19:24](https://www.youtube.com/watch?t=1164&v=5eqRuVp65eY)\n\n[[intrinsic dimension of natural language]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "media_link": "https://www.youtube.com/watch?v=5eqRuVp65eY",
    "tags": [
      "drafting"
    ],
    "normalized_filename": "neural_scaling_laws",
    "outlinks": [
      "compute_efficent_frontier",
      "pasted_image_20241017072732.png",
      "resolution_limited_scaling",
      "llm",
      "pasted_image_20241017074030.png",
      "intrinsic_dimension_of_natural_language",
      "mnist",
      "validation_loss",
      "loss_function",
      "pasted_image_20241017073743.png",
      "entropy_of_natural_language",
      "cross_entropy_loss",
      "pasted_image_20241017072233.png",
      "cross_entropy"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Neural network in Practice",
    "sha": "99ad32f866348153cea08c217e8fc265c618c23f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Neural%20network%20in%20Practice.md",
    "text": "This guide provides practical insights into building and using [[Neural network]].\n\nRefer to [[ML_Tools]] for more details: `Neural_Net_Build.py`\n\n## Softmax Placement at the End\n\nNumerical stability is crucial. One way to enhance stability is by grouping the softmax function with the loss function rather than placing it at the output layer.\n\n### Building the Model\n\n**Final Dense Layer**: \n  - Use a 'linear' activation function, which means no activation is applied. This setup allows the model to output raw logits.\n  \n**Model Compilation**: \n  - When compiling the model, specify `from_logits=True` in the loss function to indicate that the outputs are raw logits.\n  ```python\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n  ```\n**Target Form**: \n  - The target format remains unchanged. For instance, with SparseCategoricalCrossentropy, the target is the expected class label (e.g., digits 0-9 in the [[MNIST]] dataset).\n  - See [[SparseCategorialCrossentropy or CategoricalCrossEntropy]]\n\n**Output Probabilities**: \n  - Since the model outputs logits, apply a softmax function to convert these logits into probabilities if needed for interpretation or further processing.\n\n## TensorFlow History Loss (Cost)\n\nMonitoring the cost, or loss, during training is essential for understanding how well the model is learning.\n\n**Monitoring Progress**: \n  - Track the progress of gradient descent by observing the cost, referred to as `loss` in TensorFlow. Ideally, the loss should decrease as training progresses.\n**Loss Display**: \n  - The loss is displayed at each [[Epoch]] during the execution of `model.fit`, providing real-time feedback on training performance.\n**History Variable**: \n  - The `.fit` method returns a `history` object containing various metrics, including the loss. This object is stored in the `history` variable for further analysis.\n\nThe `history` object can capture additional metrics such as accuracy, validation loss, and other performance indicators, depending on what was specified during model compilation and fitting. This information is valuable for evaluating the model's performance [[Model Evaluation]].",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "neural_network_in_practice",
    "outlinks": [
      "mnist",
      "ml_tools",
      "epoch",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "neural_network",
      "model_evaluation"
    ],
    "inlinks": [
      "neural_network"
    ]
  },
  {
    "category": "ML",
    "filename": "Neural network",
    "sha": "6ffefd44fa36d6e484bad916f4416bfd5ea1e34a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Neural%20network.md",
    "text": "A [[Neural network|Neural Network]] is a computational model inspired by biological neural networks in the human brain. It consists of layers of interconnected nodes (neurons) that process and transmit information. Neural networks are fundamental to [[Deep Learning]].\n\nResources:  \n- [Keras Guide](https://keras.io/guides/sequential_model/)\n\nAlso see:  \n- [[Types of Neural Networks]]  \n- [[Neural network in Practice]]\n### Key Components\n\nThe number of starting nodes depends on the input parameter, similar for output. The width and depth of the net are called [[Hyperparameter]].\n\nNeurons (Nodes):  \n- The basic units of a neural network. Each neuron receives input, processes it, and passes it to the next layer. A neuron’s output is typically computed using a mathematical function known as the activation function.\n\nLayers:  \n- Neural networks are structured in layers of neurons:\n  - Input Layer: Receives the raw input data (features) that are fed into the network.\n  - Hidden Layers: Process the inputs received from the previous layer. There can be multiple hidden layers, making a neural network \"deep.\" These layers transform the data to learn complex relationships and patterns.\n  - Output Layer: Produces the final prediction or result, such as a class label in classification tasks or a continuous value in regression.\n\nWeights and Biases: [[Model Parameters]]\n- There are weights and biases at each layer. The shape of the weights is determined by the number of units in the previous layer and the number of units in the current layer.\n- Each connection between neurons has a weight that determines how much influence one neuron has on another. Weights are adjusted during the learning process to minimize prediction errors.\n- Biases allow the network to shift the output of the [[Activation Function]] and help it better fit the data.\n\nTraining: See [[Fitting weights and biases of a neural network]]\n\nOptimization: See [[Optimisation techniques]]\n- The optimization process (often gradient descent) updates the network's weights to minimize the loss function, ensuring the model improves with training and generalizes well to new, unseen data.\n\nInputs:\n- We need [[Normalisation]] of values (inputs) here for feeding the network to have balanced weights at the nodes.\n\n### Context\n\nExample of Neural Network:  \nA neural network can be used for a task like image classification. For instance:\n- The input layer receives the pixel values of the image.\n- Hidden layers transform these pixel values through a series of mathematical operations, learning important features such as edges, shapes, and textures.\n- The output layer classifies the image into one of the predefined categories (e.g., \"cat\" or \"dog\").\n\nPros:  \n- Flexibility: Can model complex, non-linear relationships.\n- Adaptability: Can be applied to a wide range of problems like image recognition, speech processing, and game playing.\n- Automatic Feature Extraction: Neural networks, especially CNNs, can automatically learn important features from raw data without manual intervention.\n\nCons:  \n- Data-hungry: Neural networks typically require large datasets to perform well.\n- Computationally Intensive: Training deep networks can require substantial computational resources.\n- Black Box Nature: The internal decision-making process is often difficult to interpret, although research into interpretability is addressing this.",
    "aliases": [
      "Neural Network"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#deep_learning",
      "drafting"
    ],
    "normalized_filename": "neural_network",
    "outlinks": [
      "hyperparameter",
      "normalisation",
      "neural_network_in_practice",
      "model_parameters",
      "deep_learning",
      "fitting_weights_and_biases_of_a_neural_network",
      "activation_function",
      "optimisation_techniques",
      "types_of_neural_networks",
      "neural_network"
    ],
    "inlinks": [
      "activation_function",
      "ai_engineer",
      "batch_normalisation",
      "classification",
      "deep_learning",
      "deep_q-learning",
      "dropout",
      "ds_&_ml_portal",
      "energy",
      "feed_forward_neural_network",
      "graph_neural_network",
      "hyperparameter",
      "initialization_methods",
      "model_parameters_vs_hyperparameters",
      "neural_network",
      "neural_network_classification",
      "neural_network_in_practice",
      "optimising_neural_networks",
      "over_parameterised_models",
      "pytorch",
      "recurrent_neural_networks",
      "regularisation",
      "ridge",
      "types_of_neural_networks",
      "typical_output_formats_in_neural_networks",
      "use_cases_for_a_simple_neural_network_like",
      "word2vec"
    ]
  },
  {
    "category": "ML",
    "filename": "Non-negative matrix factorization in ML",
    "sha": "71dae9d62bb2cb602e118f78916ec5543e40fcd0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Non-negative%20matrix%20factorization%20in%20ML.md",
    "text": "Why use this?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      null
    ],
    "normalized_filename": "non-negative_matrix_factorization_in_ml",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Non-parametric tests",
    "sha": "dfdf4826ba044d93d29b66ee09b454f2ae374d85",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Non-parametric%20tests.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "statistics"
    ],
    "normalized_filename": "non-parametric_tests",
    "outlinks": [],
    "inlinks": [
      "parametric_vs_non-parametric_tests",
      "statistical_assumptions"
    ]
  },
  {
    "category": "ML",
    "filename": "Normalisation of data",
    "sha": "e47c45dc5f31a8249cbee81a544c90d8cb01a34f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Normalisation%20of%20data.md",
    "text": "Normalization is the process of structuring data from the source into a format appropriate for consumption in the destination. \n\nFor example, when writing data from a nested, dynamically typed source like a [[Json]] [[API]] to a relational destination like [[PostgreSQL]], normalization is the process that un-nests JSON from the source into a relational table format that uses the appropriate column types in the destination.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "modeling",
      "statistics"
    ],
    "normalized_filename": "normalisation_of_data",
    "outlinks": [
      "postgresql",
      "json",
      "api"
    ],
    "inlinks": [
      "data_transformation",
      "normalisation",
      "normalised_schema"
    ]
  },
  {
    "category": "ML",
    "filename": "Normalisation vs Standardisation",
    "sha": "367d0c40ff2254e990e71c1dd9db8e7b262991f8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Normalisation%20vs%20Standardisation.md",
    "text": "Key Differences:\n\n[[Normalisation]] changes the ==range== of the data, while standardisation ==changes the data distribution==.\n\nNormalisation is preferred when the data does not follow a Gaussian distribution, whereas [[Standardisation]] is used when the data is normally distributed.\n\n![[Pasted image 20241219071120.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "statistics"
    ],
    "normalized_filename": "normalisation_vs_standardisation",
    "outlinks": [
      "pasted_image_20241219071120.png",
      "standardisation",
      "normalisation"
    ],
    "inlinks": [
      "batch_normalisation",
      "normalisation"
    ]
  },
  {
    "category": "ML",
    "filename": "One-hot encoding",
    "sha": "111b7f0259c86eceb2882c757a59680fe3476e70",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/One-hot%20encoding.md",
    "text": "Related terms:\n- Why do we need to drop one of the dummy columns? [[Dummy variable trap]]: \n\n==Dummy variables & One-hot encoding are fundamentally different from [[Label encoding]]==\n\n[[Label encoding vs One-hot encoding]]\n\nOne-hot encoding is a technique used to convert categorical data into a numerical format that can be used by [[Machine Learning Algorithms]]. It is particularly useful when dealing with categorical variables that have no ordinal relationship. \n\nIn one-hot encoding, each category is transformed into a binary vector. If there are \\( n \\) unique categories, each category is represented by a vector of length \\( n \\) where one element is \"hot\" (set to 1) and the rest are \"cold\" (set to 0). For example, if you have a categorical variable with three categories: \"red,\" \"green,\" and \"blue,\" one-hot encoding would represent them as:\n\n- \"red\" -> [1, 0, 0]\n- \"green\" -> [0, 1, 0]\n- \"blue\" -> [0, 0, 1]\n\nOne-hot encoding is used because many machine learning [[Algorithms]] cannot work directly with categorical data. By converting categories into a numerical format, one-hot encoding allows these algorithms to process the data effectively. It is commonly used in [[Preprocessing]] steps for machine learning models, especially in neural networks and other algorithms that require numerical input.\n\nInterpretation in the Model: One-Hot Encoding treats each category as a separate binary feature and does not impose any ordinal relationship between them. This means the model doesn’t assume that one category is greater or lesser than another. Each category is treated independently.\n### Implementation\n\n```python\ncat_variables = ['Sex',\n'ChestPainType',\n'RestingECG',\n'ExerciseAngina',\n'ST_Slope'\n]\n# This will replace the columns with the one-hot encoded ones and keep the columns outside 'columns' argument as it is.\ndf = pd.get_dummies(data = df, prefix = cat_variables, columns = cat_variables)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing",
      "transformation",
      "ML_Tools"
    ],
    "normalized_filename": "one-hot_encoding",
    "outlinks": [
      "dummy_variable_trap",
      "preprocessing",
      "label_encoding",
      "machine_learning_algorithms",
      "label_encoding_vs_one-hot_encoding",
      "algorithms"
    ],
    "inlinks": [
      "dummy_variable_trap",
      "label_encoding_vs_one-hot_encoding",
      "nlp",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ]
  },
  {
    "category": "ML",
    "filename": "Optimisation function",
    "sha": "a81d129027621ae717df1d2637c7b9828fa87fa6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Optimisation%20function.md",
    "text": "Optimization functions adjust the [[Model Parameters]] to minimize the [[Loss function]], which measures how well the model performs. This is a fundamental step in training machine learning models.  \n\nGeneral Optimization Process:\n\nThe [[Optimisation function]] (e.g., LBFGS, Newton-CG) iteratively updates the [[Model Parameters]] by:  \n1. Calculating the gradient of the loss function with respect to the parameters.  \n2. Updating the parameters in the direction of the negative gradient (as described in [[Gradient Descent]]).  \n\nThis process is repeated until:  \n- The cost function converges (i.e., the change in the loss function becomes negligible), or  \n- The maximum number of iterations is reached.  \n\nSee [[Optimisation techniques]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "selection"
    ],
    "normalized_filename": "optimisation_function",
    "outlinks": [
      "optimisation_function",
      "gradient_descent",
      "model_parameters",
      "loss_function",
      "optimisation_techniques"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "gradient_descent",
      "lbfgs",
      "logistic_regression_in_sklearn_&_gradient_descent",
      "model_parameters_tuning",
      "optimisation_function",
      "optimising_a_logistic_regression_model"
    ]
  },
  {
    "category": "ML",
    "filename": "Optimisation techniques",
    "sha": "b0fc4b76412f194f2088423ef11d1487a49b4dac",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Optimisation%20techniques.md",
    "text": "Optimisation techniques\n- [[Adam Optimizer]]\n- RMSprop\n- [[Stochastic Gradient Descent]]\n- [[uncategorised/Optuna]]\n\n[[Gradient Descent]]\n- Iteratively updates parameters using the gradient of the [[Cost Function]] with respect to the parameters.  \n- Requires careful tuning of the [[Learning Rate]] ($\\alpha$), which controls the size of each update.  \n\nOptimization Solvers in [[Scikit-Learn]] : Scikit-learn solvers improve on Gradient Descent by leveraging advanced techniques:  \n- Use second-order information, such as approximations to the Hessian matrix.  \n- Achieve faster and more reliable convergence compared to Gradient Descent.  \n- Automatically adapt step sizes [[Adaptive Learning Rates]], eliminating the need for manual tuning.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation",
      "process"
    ],
    "normalized_filename": "optimisation_techniques",
    "outlinks": [
      "gradient_descent",
      "uncategorised/optuna",
      "adaptive_learning_rates",
      "adam_optimizer",
      "cost_function",
      "learning_rate",
      "stochastic_gradient_descent",
      "scikit-learn"
    ],
    "inlinks": [
      "deep_learning",
      "ds_&_ml_portal",
      "fitting_weights_and_biases_of_a_neural_network",
      "model_parameters",
      "model_parameters_tuning",
      "neural_network",
      "optimisation_function"
    ]
  },
  {
    "category": "ML",
    "filename": "Optimising Neural Networks",
    "sha": "21e60d5b477fcccf3ad0ef41ef4834f922fa2e72",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Optimising%20Neural%20Networks.md",
    "text": "[[Deep Learning]]\n\nWays to improve in using a [[Neural network]] \nmore data, \nbigger network, \ndiverse training set, \ntry dropout, \nchange network architechure.\n\n==Need strategies that will point towards whats the best methods to try.==",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "optimisation"
    ],
    "normalized_filename": "optimising_neural_networks",
    "outlinks": [
      "deep_learning",
      "neural_network"
    ],
    "inlinks": [
      "orthogonalization"
    ]
  },
  {
    "category": "ML",
    "filename": "Optimising a Logistic Regression Model",
    "sha": "4584d0e6a417f77800a99a58b14fcb2522a6db6d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Optimising%20a%20Logistic%20Regression%20Model.md",
    "text": "# Optimising a [[Logistic Regression]] Model\n\nIn `sklearn`, the logistic regression model uses an optimization algorithm to find the best parameters (intercept and coefficients) that minimize a loss function, typically the logistic loss (cross-entropy loss). Here's an overview of how it works:\n\nOptimization process: `sklearn` optimizes the logistic regression parameters using iterative solvers (like LBFGS, newton-cg, or liblinear) that minimize the logistic loss function.`sklearn` is using one of these optimization algorithms (likely `lbfgs` or `liblinear` by default) to minimize the logistic loss function. \n\n### Objective Function (Loss Function)\n\nThe objective of logistic regression is to minimize the logistic [[Loss function]] (also called cross-entropy loss) given by:\n\n$\\text{Cost}(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right)$\n\nWhere:\n- $h_{\\theta}(x) = \\frac{1}{1 + \\exp(-\\theta^T x)}$ is the sigmoid function that predicts probabilities.\n- $y^{(i)}$ is the actual label for the $i$-th sample (either 0 or 1).\n- $m$ is the number of samples.\n- $\\theta$ is the vector of parameters (intercept and coefficients).\n  \nThe goal is to minimize this [[Cost Function]] by finding the optimal $\\theta$ values (intercept and coefficients).\n\n### Optimization Algorithm (Solvers)\n\n[[Scikit-Learn]] provides different [[Optimisation function]] to find the optimal [[Model Parameters]] for logistic regression. These solvers use optimization techniques like Gradient Descent or more advanced methods like Newton’s Method or Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS). Here are the main solvers used in `sklearn`:\n\n- 'liblinear': This solver uses coordinate descent or regularized Newton's method and is a good choice for smaller datasets. It supports both L1 (lasso) and L2 ([[Ridge]]) regularization.\n  \n- 'lbfgs': This is a quasi-Newton method (Limited-memory BFGS), which approximates the Newton's method and is more efficient for larger datasets. It’s an iterative solver that converges faster and requires fewer iterations than simple gradient descent.\n  \n- 'newton-cg': This solver is based on Newton’s method, which computes the second-order derivative (Hessian matrix) and updates parameters using the inverse of this matrix. It works well for logistic regression with large datasets and is efficient when the number of features is large.\n  \n- 'saga': An efficient solver that can handle L1 regularization (for sparse solutions) and large datasets. It’s a variant of Stochastic Gradient Descent (SGD) and supports L1, L2, and ElasticNet regularization.\n\n- 'sgd': This is a solver for stochastic [[Gradient Descent]], which updates parameters iteratively using only a single data point (or a small batch) at a time. It can be slower but is useful for very large datasets.\n\n### 5. Convergence Criteria\n\n`sklearn`’s logistic regression solvers have specific criteria for when to stop the optimization process:\n- Convergence tolerance (`tol`): This is the threshold for when the optimization stops. When the improvement in the cost function between iterations becomes smaller than `tol`, the process stops.\n- Maximum iterations (`max_iter`): The maximum number of iterations allowed before the solver stops. If convergence is not achieved within the allowed iterations, the algorithm will stop, and a warning will be issued.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "optimisation"
    ],
    "normalized_filename": "optimising_a_logistic_regression_model",
    "outlinks": [
      "optimisation_function",
      "gradient_descent",
      "ridge",
      "logistic_regression",
      "model_parameters",
      "loss_function",
      "cost_function",
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Optuna",
    "sha": "5eb578480ad6d6dc2e40fb4a6d27e95fdf23d20b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Optuna.md",
    "text": "Optuna is a [[Hyperparameter]] optimization framework used to automatically tune hyperparameters for machine learning models.\n\nOptuna automates the process of tuning hyperparameters by defining an objective function, testing different hyperparameter combinations, training the model, and evaluating its performance. The best set of hyperparameters is chosen based on the performance metric (e.g., test accuracy) returned by the objective function.\n\n[[Hyperparameter Tuning]]\n## Benefits of Using Optuna\n\n1. Efficient Search: Utilizes algorithms like TPE (Tree-structured Parzen Estimator) to search the hyperparameter space more efficiently than grid search.\n2. Dynamic Search Space: Can explore continuous, categorical, and discrete spaces.\n3. Automatic Pruning: Supports pruning of unpromising trials during training, improving computational efficiency.\n4. Visualization: Offers built-in tools for visualizing the optimization process, aiding in understanding the impact of hyperparameters.\n\n## Steps to Use Optuna\n\n1. Define Objective Functions:\n   - For each model (e.g., [[LightGBM]], [[XGBoost]], [[CatBoost]]), define an objective function.\n   - The objective function takes trial parameters as input and returns a score to optimize.\n   - Specify hyperparameters to tune within each function, such as:\n     - LightGBM: learning rate, number of leaves\n     - XGBoost: eta, max depth\n     - CatBoost: learning rate, depth\n\n2. Running Hyperparameter Optimization:\n   - Create a study object for each model using `optuna.create_study()`.\n   - Run the optimization process using the `.optimize()` method, specifying the objective function and the number of trials.\n   - Retrieve the best hyperparameters from each study object using `.best_params`.\n\n3. Comparison and Evaluation:\n   - Compare the best hyperparameters obtained for each model.\n   - Evaluate the performance of the tuned models on a validation dataset.\n\n## Differences between Models with Optuna\n\nHyperparameters:\n  - The specific hyperparameters to tune may vary between models.\n  - Example: LightGBM involves tuning parameters like learning rate and number of leaves, while XGBoost involves parameters like eta and max depth.\n\nObjective Function:\n  - Tailor the objective function for each model to its respective API and requirements.\n  - Ensure the objective function properly trains and evaluates the model using the specified hyperparameters.\n\nOptimization Strategy:\n  - Optuna provides different optimization algorithms (e.g., TPE, CMA-ES) that may behave differently depending on the model and hyperparameter space.\n### Implementation\n\n```python\nimport optuna\n\n# 1. Creating an Optimization Study\n# Initialize a study to track and manage the optimization process.\n# The 'direction' parameter specifies whether to maximize or minimize the objective function.\nstudy = optuna.create_study(direction='maximize')\n\n# 2. Defining the Objective Function\ndef objective(trial):\n    # 3. Suggesting Hyperparameters\n    # Use trial.suggest_* methods to specify the hyperparameters to optimize.\n    # Each method defines the type and range of values to explore.\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    dropout = trial.suggest_uniform('dropout', 0.2, 0.5)\n    units_1 = trial.suggest_int('units_1', 64, 128)\n    \n    # 4. Training the Model\n    # Train the model using the suggested hyperparameters.\n    # Evaluate the model's performance on the validation dataset.\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=config['epochs'], batch_size=batch_size, callbacks=[...])\n    \n    # Returning the Result\n    # Return the model's test accuracy as the result of the trial.\n    return test_accuracy\n\n# Optimization\n# Run the optimization process, executing the objective function multiple times.\n# The 'n_trials' parameter specifies the number of trials to run.\nstudy.optimize(objective, n_trials=15)\n\n# Best Trial\n# After optimization, retrieve the best trial with the highest test accuracy.\n# The best hyperparameters are stored in 'best_trial.params'.\nbest_trial = study.best_trial\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "optuna",
    "outlinks": [
      "hyperparameter",
      "lightgbm",
      "hyperparameter_tuning",
      "catboost",
      "xgboost"
    ],
    "inlinks": []
  },
  {
    "category": null,
    "filename": "Order matters in Boosting",
    "sha": "887538b5e51f5c25c8733655a5092fb1485674a8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Order%20matters%20in%20Boosting.md",
    "text": "Order matters in Boosting\n\n\n**Summary:**\nBoosting is inherently sequential, and each step depends on the errors of the previous step. Therefore, **the order of training matters** and affects the final model performance.\n\n### **Why order matters**\n\n1. **Sequential learning:**\n\n   * Boosting algorithms (like **AdaBoost**, **Gradient Boosting**) build models **sequentially**.\n   * Each model is trained to **correct the mistakes** of the previous models.\n\n2. **Error weighting:**\n\n   * Observations that were mispredicted by earlier models get **higher weight** in subsequent models.\n   * The order determines which mistakes are focused on first and influences the final combined model.\n\n3. **Cumulative effect:**\n\n   * Since predictions are combined sequentially, the **early models have more influence** on later corrections.\n   * The order of learning affects convergence, bias, and variance.\n### **Contrast with Bagging**\n\n* In **bagging** (like Random Forests), models are trained **independently**, so the order **does not matter**.\n* Boosting relies on **dependency between models**, so changing the sequence can change the outcome.",
    "aliases": [],
    "date modified": "19-10-2025",
    "tags": [
      "ml",
      "ml_process"
    ],
    "normalized_filename": "order_matters_in_boosting",
    "outlinks": [],
    "inlinks": [
      "boosting"
    ]
  },
  {
    "category": "ML",
    "filename": "Ordinary Least Squares",
    "sha": "7855ae6570544db82d129afa6a11f784b4484271",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Ordinary%20Least%20Squares.md",
    "text": "**Derivation of Coefficients**:\n    - OLS derives the coefficients by setting the partial derivatives of the SSE with respect to each coefficient to zero. This results in a set of normal equations that can be solved to find the optimal coefficients.\n    - In matrix form, the solution can be expressed as:  \n$$b=(X^{T}X)^{-1} X^{T}y$$\n    - Here, $X$ is the matrix of input features (including a column of ones for the intercept),  is the vector of observed values, and  is the vector of coefficients.\n\n### [[Ordinary Least Squares]]\n\nThe Ordinary Least Squares method is used to minimize SSE. It achieves this by finding the values of  that minimize the sum of squared differences between the observed and predicted values. The formulas for  are derived by setting partial derivatives of SSE with respect to each coefficient to zero.\n\nOLS is an analytical method\n\n[[Ordinary Least Squares]]\n\n![[Pasted image 20240117145455 1.png|500]] ![[Pasted image 20240124135607.png|500]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "ordinary_least_squares",
    "outlinks": [
      "pasted_image_20240117145455_1.png",
      "ordinary_least_squares",
      "pasted_image_20240124135607.png"
    ],
    "inlinks": [
      "linear_regression",
      "ordinary_least_squares",
      "pytorch"
    ]
  },
  {
    "category": "ML",
    "filename": "Orthogonalization",
    "sha": "3e49562a44c926233d3f044bcf16c97a477586c4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Orthogonalization.md",
    "text": "[link](https://www.youtube.com/watch?v=UEtvV1D6B3s&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=2)\n\nWhen training ML model need Orthogonalization in order to Determine what to tune, and observe the effect it has.\n\nEach button does one thing.\n\nExample: Car with Accelerator and angle of steering wheel. \n\nAssumptions (controls for tuning):\n- model works well with cost functions\n\t- Try [[Adam Optimizer]], bigger network\n- work on training set\n\t- [[Regularisation]]\n\t- Try bigger training set\n- works on test set of data\n\t- Try bigger training set.\n- works well in real life.\n\t- Change training set\n\t- Change cost function.\n\nAvoid early stopping as effects network size, and training set size.\n\nRelated links:\n- [[Optimising Neural Networks]]\n- [[DS & ML Portal]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "ml"
    ],
    "normalized_filename": "orthogonalization",
    "outlinks": [
      "regularisation",
      "adam_optimizer",
      "ds_&_ml_portal",
      "optimising_neural_networks"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Outliers",
    "sha": "98c7cbdbdb0f262875c8293f0156c54292e9f530",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Outliers.md",
    "text": "What are they?\n- Outliers are data points that differ significantly from other observations in the dataset. \n- Outliers are not just points that are *far* from the mean or *visually strange*.\n\nTheir effects:\n- They can skew and mislead the training of machine learning models, especially those sensitive to the scale of data, such as [[Linear Regression]]. \n- They can sway the generality of the model, skewing predictions and increasing the standard deviation.\n\nRelated Concepts:\n- [[Anomaly Detection]]\n- [[Methods for Handling Outliers]] similar to [[Handling Missing Data]]\n- [[Why Removing Outliers May Improve Regression but Harm Classification]]\n- [[Business value of anomaly detection]]",
    "aliases": [
      "anomalies",
      "Handling Outliers"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "cleaning",
      "statistics"
    ],
    "normalized_filename": "outliers",
    "outlinks": [
      "why_removing_outliers_may_improve_regression_but_harm_classification",
      "linear_regression",
      "business_value_of_anomaly_detection",
      "methods_for_handling_outliers",
      "anomaly_detection",
      "handling_missing_data"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Over parameterised models",
    "sha": "37f32bc64732b0bab97cdc700e6e3ae9a520081e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Over%20parameterised%20models.md",
    "text": "[[Neural network]]\n\nUniversal approximation theory",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "modeling"
    ],
    "normalized_filename": "over_parameterised_models",
    "outlinks": [
      "neural_network"
    ],
    "inlinks": [
      "statistics"
    ]
  },
  {
    "category": "ML",
    "filename": "PCA Explained Variance Ratio",
    "sha": "0ef48406f8cfec51c121d20908901f43f2c54fd8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PCA%20Explained%20Variance%20Ratio.md",
    "text": "[[PCA Explained Variance Ratio]]\n- The variance explained by each principal component is printed using `pca.explained_variance_ratio_`.\n- The sum of the explained variances is calculated and printed, which should ideally equal 1 (indicating that all variance in the data is accounted for by the principal components).\n\nThe **explained variance** refers to how much of the total variance in the original dataset is captured or explained by each principal component (PC) in Principal Component Analysis (PCA).\n\n### In the context of PCA:\n    \n2. **Explained Variance**: After performing PCA, each principal component corresponds to a certain amount of variance in the original data. The **explained variance** of a principal component is the proportion of the total variance in the original dataset that is captured by that component.\n    \n3. **Explained Variance Ratio**: This is the proportion of the total variance explained by each principal component. It is calculated as:\n    \n    Explained Variance Ratio of PCi=Variance explained by PCiTotal variance of the dataset\\text{Explained Variance Ratio of PC}_i = \\frac{\\text{Variance explained by PC}_i}{\\text{Total variance of the dataset}}\n    \n    It tells us how much of the total variance in the data is accounted for by each component. For example, if the first principal component explains 70% of the total variance, then the explained variance ratio for that component would be 0.70.\n    \n4. **Cumulative Explained Variance**: If we sum up the explained variance ratios of the first few principal components, we can assess how many components are needed to explain a significant portion of the total variance. For example, if the first two components explain 90% of the variance, it means that we can reduce the dataset's dimensionality by keeping just those two components without losing much information.\n    \n\n### Example:\n\nIn PCA applied to the Iris dataset:\n\n```python\npca.explained_variance_ratio_\n```\n\nThis might return something like:\n\n```\n[0.924, 0.053, 0.018, 0.005]\n```\n\nThis means:\n\n- The first principal component explains 92.4% of the total variance.\n- The second principal component explains 5.3% of the variance.\n- The third and fourth components explain very little (1.8% and 0.5%, respectively).\n\nSumming these ratios gives the total variance explained by the first few components. In this case, the first two components explain over 97% of the variance in the dataset, meaning the remaining components contribute very little additional information.\n\n### Why it matters:\n\n- **Dimensionality Reduction**: PCA is used to reduce the number of dimensions (features) in the dataset. The goal is to retain as much variance as possible while reducing the number of dimensions. By selecting the components with the highest explained variance, we can reduce the dataset's complexity without sacrificing much information.\n    \n- **Data [[Interpretability]]**: The explained variance helps us understand how important each component is in representing the dataset. If the first few components explain most of the variance, we can focus on them for analysis and modeling.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "pca_explained_variance_ratio",
    "outlinks": [
      "pca_explained_variance_ratio",
      "interpretability"
    ],
    "inlinks": [
      "pca_explained_variance_ratio",
      "principal_component_analysis"
    ]
  },
  {
    "category": "ML",
    "filename": "PCA Principal Components",
    "sha": "3b528fd37d53ad0ac0447287e6544bbd1a074344",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PCA%20Principal%20Components.md",
    "text": "The principal components (or the new axes that explain the most variance) are stored in `pca.components_` and displayed as a DataFrame for easier reading\n\n## Interpretating\n\nSee [[PCA_Analysis.ipynb]]\n\n![[Pasted image 20250317093551.png]]\n\n### How to Interpret the PCA Heatmap\n\nThis heatmap represents the principal component loadings, which show how strongly each original feature contributes to each principal component (PC).\n\n### Understanding the Heatmap Content\n\n- Rows = Principal Components (PCs)\n    \n    - Each row corresponds to a principal component (e.g., PC1, PC2, PC3, etc.).\n    - PC1 captures the most variance, PC2 captures the second-most, and so on.\n- Columns = Original Features\n    \n    - Each column represents an original feature from the dataset (e.g., `sepal length`, `sepal width`, etc.).\n- Cell Values = Loadings\n    \n    - Each cell contains a loading coefficient, which tells us how much that feature contributes to the corresponding principal component.\n    - The values range from -1 to 1:Close to 1 → The feature strongly contributes to that PC in the positive direction, ect.\n### Key Insights from the [[Heatmap]]\n\n1. Which features are most important for each PC?\n    - Look for the largest absolute values in each row.\n    - Example: If `sepal length` has a high positive value in PC1, it means PC1 is largely influenced by sepal length.\n\n2. Feature Groupings & Correlations\n    - Features with similar values in a PC vary together in the data.\n    - Example: If `sepal length` and `sepal width` have similar values in PC1, they might be [[Correlation]] correlated in the dataset.\n\n3. Interpreting the First Few Principal Components\n    - PC1 often represents the main pattern in the data (e.g., overall size of the iris flowers).\n    - PC2 might represent a different pattern (e.g., a contrast between petal and sepal size).\n    - Together, PC1 and PC2 often explain the majority of variance in the dataset.\n\n### Example Interpretation (Hypothetical Output)\n\n| Sepal Length | Sepal Width | Petal Length | Petal Width |       |\n| ------------ | ----------- | ------------ | ----------- | ----- |\n| PC1          | 0.70        | -0.40        | 0.85        | 0.75  |\n| PC2          | -0.60       | 0.80         | -0.35       | -0.45 |\nPC1 Interpretation: `Petal length` and `sepal length` have high positive loadings, meaning PC1 mainly captures flower size.`Sepal width` has a negative loading, meaning flowers with large sepals tend to have smaller widths.\n\nPC2 Interpretation: `Sepal width` has the highest positive loading, while `sepal length` has a negative loading, suggesting that PC2 contrasts width vs. length.\n\n### How to Use This Information?\n\n[[Feature Selection]]: If one PC captures most of the variance, we can reduce dimensionality and keep only the most important PCs.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "ml"
    ],
    "normalized_filename": "pca_principal_components",
    "outlinks": [
      "correlation",
      "pca_analysis.ipynb",
      "heatmap",
      "feature_selection",
      "pasted_image_20250317093551.png"
    ],
    "inlinks": [
      "principal_component_analysis"
    ]
  },
  {
    "category": "ML",
    "filename": "PCA-Based Anomaly Detection",
    "sha": "a92d2d9cbcd841f9814a2b6429fe2c0239cd3439",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PCA-Based%20Anomaly%20Detection.md",
    "text": "For implementation, see: [[ML_Tools]]:\n- [[PCA_Based_Anomaly_Detection.py]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection"
    ],
    "normalized_filename": "pca-based_anomaly_detection",
    "outlinks": [
      "ml_tools",
      "pca_based_anomaly_detection.py"
    ],
    "inlinks": [
      "anomaly_detection",
      "principal_component_analysis"
    ]
  },
  {
    "category": "ML",
    "filename": "Partial Dependence Plot",
    "sha": "ecf43789f0856a8d41c744a51a01baa1d2bfacdf",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Partial%20Dependence%20Plot.md",
    "text": "Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\n\nNeed to do [[Feature Importance]] first.\n\nTells what happens to the target when you change the feature.\n\nhttps://scikit-learn.org/stable/modules/partial_dependence.html\n\nRelated:\n- [[Interpretability|interpretable]]\n- [[ICE Plot]]\n### Examples\n\nRegression example:\n\n![[Pasted image 20241204203338.png]]\n\nCategorical Example\n\n![[Pasted image 20241204203413.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "explainability"
    ],
    "normalized_filename": "partial_dependence_plot",
    "outlinks": [
      "ice_plot",
      "pasted_image_20241204203338.png",
      "pasted_image_20241204203413.png",
      "feature_importance",
      "interpretability"
    ],
    "inlinks": [
      "ice_plot"
    ]
  },
  {
    "category": "ML",
    "filename": "Percentile Detection",
    "sha": "d84b8323767b4b3af686a6cbd2521b3e89290d52",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Percentile%20Detection.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_percentile.py",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection"
    ],
    "normalized_filename": "percentile_detection",
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ]
  },
  {
    "category": "ML",
    "filename": "Polynomial Regression",
    "sha": "71e0759c87cb38ce5ed602c6a700bd80c908d09e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Polynomial%20Regression.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "modeling"
    ],
    "normalized_filename": "polynomial_regression",
    "outlinks": [],
    "inlinks": [
      "regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Performance Drift",
    "sha": "896264b7b2e6859ff3109cbcdd5f3324041818cd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Performance%20Drift.md",
    "text": "Not [[Data Drift]]\n\n **TL;DR**. Data drift is a change in the input data. Concept drift is a change in input-output relationships. Both often happen simultaneously.\n\n Performance drift refers to the ==gradual decline in a machine learning model's accuracy== or effectiveness over time as the underlying data distribution changes. \n \n This phenomenon occurs when the real-world data that the model is applied to differs from the data it was trained on. Mathematically, this is often represented by a shift in the joint distribution $P(X, Y)$ of the features $X$ and target variable $Y$. \n \n Performance drift can occur due to ==*concept drift* (==when the relationship between inputs and outputs changes) or ==*covariate shift*== (when the distribution of the inputs changes). The model’s prediction error increases, leading to suboptimal decisions or predictions.\n\n Key Components:  \n - **Concept drift**: Changes in the relationship between inputs and outputs, $P(Y|X)$.  \n - **Covariate shift**: ==Change in the input data distribution, $P(X)$.==  \n - **Model monitoring** [[Data Observability|monitoring]]: Continuous assessment of a model’s accuracy over time to detect drift.  \n - **Retraining**: Updating the model with new data to restore performance.\n\nImportant\n - Performance drift results from data distribution shifts, leading to increased prediction errors.  \n - Monitoring and retraining are key strategies to address performance drift in real-world applications.\n\n - A lack of continuous monitoring can result in undetected ==model performance degradation.==  \n - Overfitting a model to the original data without considering future data can accelerate drift.\n\nExample\n In a credit scoring model, performance drift may occur if consumer spending habits change due to an economic recession. The model trained on pre-recession data will perform poorly on post-recession data as the input patterns ($P(X)$) and the relationship between inputs and outputs ($P(Y|X)$) shift.\n\nQuestions\n - How can adaptive learning techniques help mitigate the effects of performance drift?  \n - What statistical methods can be used to detect early signs of concept drift in production models?\n\nRelated Topics\n - Model retraining strategies  \n \nImages\n\n![[Pasted image 20250113072251.png]]",
    "aliases": [
      "accuracy drift",
      "concept drift",
      "model degradation"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "evaluation",
      "explainability"
    ],
    "normalized_filename": "performance_drift",
    "outlinks": [
      "data_observability",
      "data_drift",
      "pasted_image_20250113072251.png"
    ],
    "inlinks": [
      "challenges_to_model_deployment",
      "data_drift",
      "machine_learning_operations",
      "model_observability",
      "model_validation",
      "transfer_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Positional Encoding",
    "sha": "3ce02b2247a66e8f954edcf86dd64b431d1c4bb3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Positional%20Encoding.md",
    "text": "Positional encoding enables [[Transformer|transformers]] to capture sequence order, which is necessary for understanding grammar, sentiment, and meaning in natural language. Without it, the model would treat all token permutations as equivalent.\n\nIn models like [[BERT]], token embeddings represent *what* words are present in the input, but they do not contain information about the *order* of words. For example, the tokens:\n\n> `[\"data\", \"science\", \"is\", \"great\", \"not\", \"so\"]`\n\nwill have the same embeddings regardless of their position in a sentence.\n\nTo address this, positional embeddings are added to token embeddings. These provide the model with information about the position of each token in the sequence. Without this, the model cannot distinguish between *\"not great\"* and *\"great not\"*, which have different meanings.\n\nIn BERT:\n* Each position in the input has its own learned positional embedding vector.\n* The final input embedding is obtained by adding the token [[Vector Embedding|embedding]] and the corresponding position embedding element-wise.\n\n### Embedding Example\n\n| Token     | Token Embedding  | Positional Embedding | Final Embedding  |\n| --------- | ---------------- | -------------------- | ---------------- |\n| `[CLS]`   | [0.1, 0.2, 0.3] | [0.0, 0.0, 0.0]     | [0.1, 0.2, 0.3] |\n| `data`    | [0.4, 0.5, 0.6] | [0.1, 0.1, 0.1]     | [0.5, 0.6, 0.7] |\n| `science` | [0.7, 0.8, 0.9] | [0.2, 0.2, 0.2]     | [0.9, 1.0, 1.1] |\n| `is`      | [0.3, 0.3, 0.4] | [0.3, 0.3, 0.3]     | [0.6, 0.6, 0.7] |\n| `great`   | [0.5, 0.6, 0.7] | [0.4, 0.4, 0.4]     | [0.9, 1.0, 1.1] |\n| `[SEP]`   | [0.0, 0.1, 0.2] | [0.5, 0.5, 0.5]     | [0.5, 0.6, 0.7] |\n\n> Final embedding = Token embedding + Position embedding (element-wise)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "normalized_filename": "positional_encoding",
    "outlinks": [
      "vector_embedding",
      "transformer",
      "bert"
    ],
    "inlinks": [
      "bert"
    ]
  },
  {
    "category": "ML",
    "filename": "Precision or Recall",
    "sha": "d01674930edd1c91687b39e24ffc4ad60019bf39",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Precision%20or%20Recall.md",
    "text": "[[Precision]] and [[Recall]] are two fundamental metrics used to evaluate the performance of a [[Classification]] model, particularly in binary classification tasks. They are related through a trade-off: improving one often comes at the expense of the other.\n\nKey Differences:\n- [[Precision]] focuses on the quality of positive predictions.\n- [[Recall]] focuses on the ability to identify all relevant positive instances\n\nTrade-off:\n- It is challenging to optimize both precision and recall simultaneously. Improving precision by reducing false positives may lead to an increase in false negatives, thereby reducing recall, and vice versa.\n- However, it's important to balance recall with precision. A model with high recall might also have a higher rate of false positives (non-spam emails incorrectly marked as spam), which can lead to important emails being missed.\n\nTask Dependency Example:\n- The choice between prioritizing precision or recall is task-dependent. In a spam classification task, it might be more important to avoid moving important emails to the spam folder (high precision) than to occasionally allow spam emails into the inbox (lower recall). Thus, precision is prioritized over recall in this context.\n\n\n![[Pasted image 20240116211130.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "precision_or_recall",
    "outlinks": [
      "precision",
      "recall",
      "pasted_image_20240116211130.png",
      "classification"
    ],
    "inlinks": [
      "evaluation_metrics",
      "precision",
      "precision-recall_curve"
    ]
  },
  {
    "category": "ML",
    "filename": "Precision-Recall Curve",
    "sha": "1301fd16ba29e13212a0bbec0cb110309950482e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Precision-Recall%20Curve.md",
    "text": "A [[Precision]]-[[Recall]] curve is a graphical representation used to evaluate the performance of a [[Binary Classification]] model, particularly in scenarios where the classes are imbalanced. It plots [[Precision]] (the positive predictive value) against [[Recall]] (the true positive rate) for different threshold values.\n\nOverall, precision-recall curves are a valuable tool for assessing the tradeoffs between precision and recall, helping to choose the optimal threshold for classification based on the specific requirements of the task.\n### Resources\n\n[Sklearn Link](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_precision_recall.html)\n### Precision Recall Curve:\n\nPlot: The curve is generated by varying the threshold for classifying a positive instance and plotting the corresponding precision and recall values. Each point on the curve represents a precision recall pair at a specific threshold.\n\nInterpretation: \n- Be cautious of the class [[Distributions]] impact on the curve's shape. A curve that appears favorable might still represent poor performance if the positive class is very rare.\n- A steep precision-recall curve indicates that the model maintains high precision across a range of recall values, which is desirable. Conversely, a more gradual curve might suggest a trade-off between precision and recall.\n- A model with high precision and high recall is considered to perform well. However, there is often a tradeoff between [[Precision or Recall]].\n- The area under the precision-recall curve (not the same as [[AUC]]) is a single scalar value that summarizes the performance of the model. A higher AUCPR indicates better model performance.\n\nUse Cases: precision-recall curves are particularly useful in situations where the positive class is rare or when the cost of false positives and false negatives is different. They provide more insight than [[ROC (Receiver Operating Characteristic)]] curves in these scenarios because they focus on the performance of the positive class.\n \n![[Pasted image 20241231172749.png]]\n\n## Other features\n\nMulti-Class Scenarios\n- **Adapting for Multi-Class Problems**: Precision-recall curves can be extended to multi-class classification problems by using strategies like one-vs-rest (OvR). In this approach, a separate precision-recall curve is computed for each class, treating it as the positive class while considering all other classes as negative.\n\nComparison with [[ROC (Receiver Operating Characteristic)]]\n- **When to Use Precision-Recall Curves**: Precision-recall curves are particularly useful over ROC curves in scenarios with highly [[Imbalanced Datasets]]. They provide a more informative picture of a model's performance by focusing on the positive class, which is often the minority class in imbalanced datasets.",
    "aliases": [
      "PR Curve"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "ML_Tools"
    ],
    "normalized_filename": "precision-recall_curve",
    "outlinks": [
      "binary_classification",
      "auc",
      "precision_or_recall",
      "pasted_image_20241231172749.png",
      "recall",
      "roc_(receiver_operating_characteristic)",
      "distributions",
      "imbalanced_datasets",
      "precision"
    ],
    "inlinks": [
      "choosing_a_threshold",
      "determining_threshold_values"
    ]
  },
  {
    "category": "ML",
    "filename": "Precision",
    "sha": "30f4a60490618e9c7b18c32f19ec959ebd3c7d18",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Precision.md",
    "text": "**Precision Score** is a metric used to evaluate the [[Accuracy]] of a [[Classification]] model, specifically focusing on the positive class.\n\n==How many retrieved items are relevant?==\n\nThis metric indicates the accuracy of positive predictions. The formula for precision is:\n\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n\nwhere:\n- **TP (True Positives):** The number of correctly predicted positive instances.\n- **FP (False Positives):** The number of instances incorrectly predicted as positive.\n\nImportance:\n- Precision is helpful in scenarios where the **cost of false positives is high**, such as in spam detection. It helps in understanding how many of the predicted positive instances are actually positive.\n\n![[Pasted image 20241222091831.png]]\n\n## Related Concepts\n\n- [[Classification Report]] & [[Precision or Recall]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "precision",
    "outlinks": [
      "accuracy",
      "precision_or_recall",
      "pasted_image_20241222091831.png",
      "classification_report",
      "classification"
    ],
    "inlinks": [
      "classification_report",
      "confusion_matrix",
      "ds_&_ml_portal",
      "evaluation_metrics",
      "f1_score",
      "model_observability",
      "precision-recall_curve",
      "precision_or_recall"
    ]
  },
  {
    "category": "ML",
    "filename": "Prediction Intervals vs Confidence Interval",
    "sha": "80340569cbc25d05083ed8aa077685251640bad4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Prediction%20Intervals%20vs%20Confidence%20Interval.md",
    "text": "A [[Prediction Intervals]] and a [[Confidence Interval]] are related but serve different purposes:\n\n1. Prediction Interval (`PI`)\n   * Estimates the range where a ==future single observation== is expected to fall.\n   * Accounts for both the uncertainty in the model parameters and the natural variability/noise of the data.\n   * Example: \"We predict that tomorrow’s temperature will be between 15°C and 20°C with 95% probability.\"\n\n1. Confidence Interval (`CI`)\n   * Estimates the uncertainty around an ==estimated parameter== (like the mean of a population).\n   * Only reflects the uncertainty in the parameter estimate, not the variability of individual observations.\n   * Example: \"We are 95% confident that the average temperature this month is between 16°C and 18°C.\"\n\nKey difference:\n* PI is about where a ==new observation== will fall.\n* CI is about where a ==statistical estimate== (like the mean) lies.\n\nIn [[Time Series Forecasting]] a 95% prediction interval tells you the likely range of the next actual value, while a 95% confidence interval would tell you the uncertainty around the expected forecasted mean.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "prediction_intervals_vs_confidence_interval",
    "outlinks": [
      "prediction_intervals",
      "time_series_forecasting",
      "confidence_interval"
    ],
    "inlinks": [
      "prediction_intervals"
    ]
  },
  {
    "category": "ML",
    "filename": "Principal Component Analysis",
    "sha": "a5b820b404d71f404d639820b8aa8c89311cb6e1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Principal%20Component%20Analysis.md",
    "text": "PCA is a tool for [[Dimensionality Reduction]] in [[Unsupervised Learning]] to reduce the dimensionality of data. \n\nIt transforms the original data into a new coordinate system defined by the principal components, which are ==orthogonal vectors== that capture the most [[Variance]] in the data.\n\nIt helps simplify models, enhances [[Interpretability]], and improves computational efficiency by transforming data into a lower-dimensional space while ==retaining the most significant variance==, and reducing noise.\n\n### How PCA Works\n\n- ==Linear Technique==: PCA is a linear technique, meaning it assumes that the relationships between features are linear. This distinguishes it from methods like [[Manifold Learning]] which can capture non-linear relationships.\n\n- Principal Components: PCA identifies principal components, which are linear combinations of the original features. These components are ordered by the amount of variance they capture from the data, with the first principal component capturing the most variance, the second capturing the second most, and so on.\n### Comparison with Other Techniques\n\n- [[t-SNE]]: Unlike PCA, t-SNE is a non-linear technique used for visualization, preserving local structures in high-dimensional data.\n- [[Manifold Learning]]: Techniques like Isomap and Locally Linear Embedding (LLE) are designed to capture non-linear structures, which PCA might miss due to its linear nature\n\n### Limitations\n\nScaling\n### Code Implementation\n\nIn [[ML_Tools]] see:\n- [[PCA-Based Anomaly Detection]]\n- [[PCA_Analysis.ipynb]]\n\n### Related terms\n\n- [[PCA Explained Variance Ratio]]\n- [[PCA Principal Components]]\n- [[Multiple Correspondence Analysis]]\n- [[Sammon’s Mapping]]",
    "aliases": [
      "PCA"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "cleaning",
      "visualization"
    ],
    "normalized_filename": "principal_component_analysis",
    "outlinks": [
      "t-sne",
      "pca-based_anomaly_detection",
      "manifold_learning",
      "pca_analysis.ipynb",
      "unsupervised_learning",
      "pca_explained_variance_ratio",
      "pca_principal_components",
      "sammon’s_mapping",
      "variance",
      "dimensionality_reduction",
      "multiple_correspondence_analysis",
      "interpretability",
      "ml_tools"
    ],
    "inlinks": [
      "addressing_multicollinearity",
      "covariance_structures",
      "dimensionality_reduction",
      "ds_&_ml_portal",
      "evaluate_embedding_methods",
      "feature_scaling",
      "feature_selection",
      "machine_learning_algorithms",
      "multiple_correspondence_analysis",
      "pca_analysis.ipynb",
      "standardisation",
      "t-sne",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "PyCaret",
    "sha": "431f362bbd409a3e8a3a174dd727b270d2bac84b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PyCaret.md",
    "text": "PyCaret is an open-source, low-code Python library designed to simplify machine learning workflows. \n\nIt allows users to build, evaluate, and deploy machine learning models with minimal coding and effort. \n\nPyCaret provides an end-to-end solution for automating repetitive tasks in machine learning, such as \n- [[Preprocessing]],\n- model training,\n- [[Hyperparameter]] tuning\n- [[Model Deployment|Deployment]]\n\n### Implementation\n\nSee: https://pycaret.gitbook.io/docs/get-started/quickstart\n\nResources: https://github.com/pycaret/pycaret/tree/master\n### Key Features of PyCaret\n\n1. Ease of Use: PyCaret is designed to be beginner-friendly, enabling users to build models without deep expertise in coding.\n2. Modular Design: PyCaret supports various machine learning tasks through its modular APIs:\n    - Classification: `pycaret.classification`\n    - Regression: `pycaret.regression`\n    - Clustering: `pycaret.clustering`\n    - Anomaly Detection: `pycaret.anomaly`\n    - NLP: `pycaret.nlp`\n    - [[Time Series Forecasting]]: `pycaret.time_series`\n3. Automated Machine Learning (AutoML): PyCaret automates data preprocessing, feature engineering, model selection, and [[Hyperparameter Tuning]].\n4. Integration: PyCaret integrates well with other Python libraries, such as Pandas, NumPy, and [[Plotly]].\n5. [[Model Evaluation]] and Comparison: [[Model Selection]]: It provides an easy way to compare multiple models and their performance metrics in a single function call.\n6. Deployment [[Model Deployment]]: Facilitates the deployment of trained models using tools like Flask, FastAPI, or Microsoft Power BI.\n\n### Notes\n\nObject or functional APIs\n\n\n\n### Advantages of PyCaret\n\n- Time-Saving: Reduces the coding and time required to build machine learning pipelines.\n- Quick prototyping of machine learning models.\n- Educational purposes for teaching machine learning concepts.\n- Rapid development of machine learning solutions for business problems.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "python",
      "software"
    ],
    "normalized_filename": "pycaret",
    "outlinks": [
      "hyperparameter",
      "plotly",
      "time_series_forecasting",
      "preprocessing",
      "model_selection",
      "model_deployment",
      "hyperparameter_tuning",
      "model_evaluation"
    ],
    "inlinks": [
      "automated_feature_creation",
      "automl",
      "model_deployment_using_pycaret"
    ]
  },
  {
    "category": "ML",
    "filename": "PyOD",
    "sha": "7f87b8c64a724968427d9d3b53380b2a9794abe8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PyOD.md",
    "text": "Python Outlier Detection library, covers many algorithms (tabular, numeric).\n\nPyOD - Ensembles Combines multiple detectors — if a point scores high across methods, it is more confidently an outlier. \n\nRelated to:\n- [[Anomaly Detection]]\n\n\n```python\n# Step 1: Import libraries\nfrom pyod.models.iforest import IForest  # Isolation Forest from PyOD\nfrom pyod.utils.data import generate_data\nfrom sklearn.model_selection import train_test_split\n\n# Step 2: Generate synthetic data (normal + outliers)\nX, y = generate_data(n_train=200, n_test=100, n_features=2, contamination=0.1)\n\n# Step 3: Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 4: Initialize detector\nclf = IForest(contamination=0.1)  # 'contamination' = expected proportion of outliers\n\n# Step 5: Train model\nclf.fit(X_train)\n\n# Step 6: Predict anomalies\ny_train_pred = clf.labels_    # 0 = normal, 1 = outlier\ny_train_scores = clf.decision_scores_  # raw outlier scores\n\ny_test_pred = clf.predict(X_test)      # predict on unseen test set\ny_test_scores = clf.decision_function(X_test)  # outlier scores for test\n\n# Step 7: Evaluate (basic accuracy example)\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_test_pred))\n\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "python"
    ],
    "normalized_filename": "pyod",
    "outlinks": [
      "anomaly_detection"
    ],
    "inlinks": [
      "anomaly_detection",
      "time_series_python_packages"
    ]
  },
  {
    "category": "ML",
    "filename": "PyTorch",
    "sha": "b3da2752d0e13a6a0c8a17b2f80e0c8e2c8b4231",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/PyTorch.md",
    "text": "[Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) want for [[PyTorch]]\n\nOpen-source [[Deep Learning]] framework with dynamic computational graphs, emphasizing flexibility and research. Similar to [[Tensorflow]].\n\nFramework pieces:\n- torch: a general purpose array library similar to [[Numpy]] that can do computations on GPU when the tensor type is cast to (torch.cuda.TensorFloat)\n- torch.autograd: a package for building a computational graph and automatically obtaining gradients\n- torch.nn: a [[Neural network|Neural Network]] library with common layers and [[Loss function]]\n- torch.optim: an optimization package with common optimization algorithms like [[Stochastic Gradient Descent]]\n\n# Basics of [[PyTorch]]\n\n### Tensors arrays \n\nPyTorch uses tensors, which are similar to NumPy arrays, but with GPU acceleration.\n\n```python\nimport torch\n# Creating a tensor\nx = torch.tensor([2.0, 3.0, 4.0])\n# Tensor operations\ny = x + 2\nz = x * 3\nprint(y)  # Output: tensor([4., 5., 6.])\nprint(z)  # Output: tensor([ 6.,  9., 12.])\n```\n\n### Automatic Differentiation  \nPyTorch can compute gradients automatically with `autograd`.\n\n```python\n# Create tensor with gradient tracking (input value)\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) #`requires_grad=True`, which tells PyTorch to track all operations performed on this tensor. T\n\n# Perform some operations\ny = x ** 2 #formula\nz = y.sum() #14\n\n# Compute gradients\nz.backward() #backpropagation (the chain rule of calculus). Since `z` is a scalar, PyTorch can compute the gradients of `z` with respect to each element in `x`.\n\n# Print gradients (dy/dx)\nprint(x.grad)  # Output: tensor([2., 4., 6.])\n```\n\nThe gradient is calculated by the chain rule:\n- For $y = x^2$, the derivative of $y$ with respect to $x$ is $\\frac{dy}{dx} = 2x$.\n- ==Since `z` is the sum of the elements of `y`, the gradient of `z` with respect to each element in `x` is $\\frac{dz}{dx_i} = 2x_i$ for each element in `x`.==\n\n==z is a formula but calculates the derivative wrt x, so the derivative of y with x.==\n\nSo, the gradients for each element in `x` are:\n$\\frac{dz}{dx} = [2 \\times 1.0, 2 \\times 2.0, 2 \\times 3.0] = [2.0, 4.0, 6.0]$\n\nThe gradients are stored in `x.grad`. After calling `z.backward()`, `x.grad` contains the derivative of `z` with respect to each element of `x`, which is `[2.0, 4.0, 6.0]`\n\nGradient Computation (Summary):\n- The gradient $\\frac{dz}{dx}$ represents how much the output `z` changes for a small change in `x`. In our case, if we slightly increase `x_1`, `x_2`, or `x_3`, the change in `z` can be predicted using these gradients.\n- This gradient information is used to update weights in neural networks during training. For example, in optimization algorithms like ([[Stochastic Gradient Descent]]), these gradients are used to adjust model parameters in the direction that minimizes the loss function.\n###### Confusion: total derivative is not the partial derivates\n\nThe confusion here comes from the distinction between **partial derivatives** (for each element in a tensor) and **total derivatives**. Let me clarify this.\n\nIn the context of:\n\n```python\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x ** 2\nz = y.sum()\nz.backward()\n```\n\n- `x` is a tensor: $ x = [1.0, 2.0, 3.0] $\n- `y = x^2` computes element-wise squares: $ y = [1.0^2, 2.0^2, 3.0^2] = [1.0, 4.0, 9.0] $\n- `z = y.sum()` adds the elements in `y`: $ z = 1.0 + 4.0 + 9.0 = 14.0 $\n\nNow, you're asking about the derivative of `z` with respect to `x`, specifically whether **`dz/dx` should be the sum of the derivatives** and equal to 4.\n\n**Derivative with Respect to Each Element (Partial Derivative)**\n\nWhen you calculate the gradient of `z` with respect to `x`, you're computing the **partial derivatives** of `z` with respect to each element in `x`. The function `z` depends on each element of `x` individually.\n\nFor each element $x_i $ in `x`, we are calculating:\n\n$\\frac{\\partial z}{\\partial x_i} = \\frac{\\partial (x_1^2 + x_2^2 + x_3^2)}{\\partial x_i} = 2x_i$\n\nThis gives:\n\n$\\frac{\\partial z}{\\partial x} = [2x_1, 2x_2, 2x_3] = [2 \\times 1.0, 2 \\times 2.0, 2 \\times 3.0] = [2.0, 4.0, 6.0]$\n\nThese are the gradients stored in `x.grad` after calling `z.backward()`.\n\n**Total Derivative vs Partial Derivatives**\n\nIf we are talking about **partial derivatives**, we get a gradient for each individual component of `x`:\n\n- $ \\frac{\\partial z}{\\partial x_1} = 2.0$\n- $ \\frac{\\partial z}{\\partial x_2} = 4.0$\n- $ \\frac{\\partial z}{\\partial x_3} = 6.0$\n\nThese partial derivatives form the gradient vector: `[2.0, 4.0, 6.0]`.\n\n**Why Is It Not Just 4?**\n\nIf you're thinking of the total derivative, that would be different from what we are calculating here. **The sum of the derivatives of `z` with respect to all components of `x`** is:\n\n$\n\\sum_{i=1}^{3} \\frac{\\partial z}{\\partial x_i} = 2 + 4 + 6 = 12\n$\n\nHowever, this total derivative is not what we are computing here. **We are computing the partial derivatives for each element of `x` separately**, which results in the gradient vector `[2.0, 4.0, 6.0]`.\n\nIn Summary:\n- **Gradient (`x.grad`)**: A vector of partial derivatives of `z` with respect to each element in `x`, giving us `[2.0, 4.0, 6.0]`.\n- **Sum of Gradients**: The sum of the elements of the gradient vector is `12`, but that’s not the gradient of `z` with respect to `x` as a whole—it's just a summation of the partial derivatives.\n### Basic [[Neural network|Neural Network]] Implementation \n\nA simple feedforward network using PyTorch.\n\n```python\nimport torch\nimport torch.nn as nn\n\n# Define a simple neural network\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc = nn.Linear(2, 1)\n\n    def forward(self, x):\n    #- **`forward()` method**: This defines the forward pass, i.e., how the input data is transformed as it moves through the network. In this case, the input `x` is passed through the linear layer.\n        return self.fc(x)\n\n# Create the network\nnet = SimpleNet()\n\n# Create input tensor\nx = torch.tensor([[1.0, 2.0]])\n\n# Forward pass\noutput = net(x)\n\nprint(output)  # Output: a tensor from the linear layer\n```\n\n**Input Tensor and Forward Pass**\n```python\nx = torch.tensor([[1.0, 2.0]])\noutput = net(x)\n```\n- **`torch.tensor([[1.0, 2.0]])`**: This defines a 2D input tensor with two features, which corresponds to the two input nodes in the network.\n- **`net(x)`**: This performs a **forward pass**, feeding the input tensor `x` into the network. The linear layer applies the learned weights and bias to compute the output.\n\n**Output**\nThe output of the network is a tensor from the linear layer, which corresponds to the result of the operation $y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b$, where:\n- $w_1$ and $w_2$ are the learned weights for each input feature.\n- $b$ is the learned bias.\n\n[[Use Cases for a Simple Neural Network Like]]\n\n### Training a Simple Model  \nAn example of training a linear model with\n\n- [[Gradient Descent]]:\n- [[Loss function]]: [[Ordinary Least Squares]]\n- [[Stochastic Gradient Descent|SGD]]:\n\nTraining of [[Linear Regression]] model. This model find best w,b in $y=wx+b$.\n\n```python\nimport torch\nimport torch.optim as optim\n\n# Data\nx = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\ny = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n\n# Model\nmodel = nn.Linear(1, 1) #**`nn.Linear(1, 1)`** defines a simple linear model with **one input feature** and **one output**. This model has two parameters:\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01) # stochastic gradient descent\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass #- **Forward pass**: For each [[epoch]], the model makes predictions (`y_pred`) by passing the input `x` through the linear model.\n    y_pred = model(x)\n    loss = criterion(y_pred, y)# - **Loss calculation**: The loss is computed by comparing `y_pred` with the true values `y` using the MSE loss function.\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward() #- **Backward pass**: The gradients of the loss with respect to the model’s parameters are computed using `loss.backward()`. This step calculates how much each parameter needs to be adjusted to reduce the loss.\n    optimizer.step() #- **Optimization step**: The optimizer (`SGD`) updates the model’s parameters (`w` and `b`) based on the computed gradients.\n\n    if epoch % 20 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item()}')\n\n# Output the trained model's parameters\nprint(list(model.parameters()))\n```\n\n### Moving Tensors to GPU  \n\nSummary:\n- speed up training e.g. of [[Neural network|Neural Network]]\n- use [[Parallelism]] for simultaneous calculations\n- GPU can do larger batches of computations, better on the memory, better for [[Gradient Descent]] estimations\n- \n\nPyTorch makes it easy to move computations to a GPU.\n\n```python\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create a tensor and move it to the GPU\nx = torch.tensor([1.0, 2.0, 3.0]).to(device)\n\nprint(x)  # Output: tensor([1., 2., 3.], device='cuda:0') (if GPU is available)\n```\n\nBoth the **model** and the **data** are moved to the GPU (`device='cuda'`). All computations, including the forward pass, loss calculation, backward pass, and optimizer step, happen on the GPU.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define a simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleNN().to(device)  # Move model to GPU\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Dummy input data and labels (move them to GPU)\ninputs = torch.randn(64, 1, 28, 28).to(device)  # 64 images of 28x28 pixels\nlabels = torch.randint(0, 10, (64,)).to(device)  # Random labels for 64 images\n\n# Forward pass (computation happens on the GPU)\noutputs = model(inputs)\nloss = criterion(outputs, labels)\n\n# Backward pass and optimization\noptimizer.zero_grad()\nloss.backward()  # Compute gradients on the GPU\noptimizer.step()  # Update model weights\n\nprint(\"Training step completed on:\", device)\n```",
    "aliases": [
      "PyTorch"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "python"
    ],
    "normalized_filename": "pytorch",
    "outlinks": [
      "gradient_descent",
      "linear_regression",
      "parallelism",
      "epoch",
      "numpy",
      "loss_function",
      "deep_learning",
      "tensorflow",
      "ordinary_least_squares",
      "pytorch",
      "use_cases_for_a_simple_neural_network_like",
      "1.0,_2.0",
      "stochastic_gradient_descent",
      "neural_network"
    ],
    "inlinks": [
      "deep_learning",
      "edge_ml",
      "lstm",
      "pytorch",
      "pytorch_vs_tensorflow",
      "recurrent_neural_networks",
      "scikit-learn",
      "spacy",
      "transfer_learning",
      "vector_embedding"
    ]
  },
  {
    "category": "ML",
    "filename": "Pytorch vs Tensorflow",
    "sha": "7c061dabed4b48f36b0dc9fd40f10b8bd47465ef",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Pytorch%20vs%20Tensorflow.md",
    "text": "- [[Tensorflow]] is widely adopted but pytorch picking up\n- Dynamic vs static graph\n- Tensorboard is better than [[PyTorch]] visualization\n- Plain tensorflow looks pretty much like a library\n- Abstraction is better in pytorch, even data parallelism\n- Tf.contrib, [[Keras]] to rescue",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml"
    ],
    "normalized_filename": "pytorch_vs_tensorflow",
    "outlinks": [
      "tensorflow",
      "keras",
      "pytorch"
    ],
    "inlinks": [
      "tensorflow"
    ]
  },
  {
    "category": "ML",
    "filename": "Q-Learning",
    "sha": "e5cdfa33c2e3982f274987e3205caa4c316f30c2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Q-Learning.md",
    "text": "Q-learning is a value-based, model-free [[Reinforcement learning]] algorithm where the agent learns the optimal [[Policy]] by updating Q-values based on the rewards received. It is particularly useful in discrete environments like grids.\n\nUses a Q-Table which is populated by Q-values which are the maximum expected future reward for the given state and action. We improve the Q-Table in an iterative approach\n\nResources:\n- [Q-Learning Explained - Reinforcement Learning Tutorial](https://www.youtube.com/watch?v=kEGAMppyWkQ&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl&index=9)\n\n**Q-learning update rule:**\n\nThe left hand side gets updated ([[Bellman Equations]])\n$$\nQ_{new}(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n$$\n\n**Explanation:**\n\n- **$Q(s_t, a_t)$**: The Q-value of the current state $s_t$ and action $a_t$.\n- **$\\alpha$**: The learning rate, determining how much new information overrides old information.\n- **$r_{t+1}$**: The reward received after taking action $a_t$ from state $s_t$.\n- **$\\gamma$**: The discount factor, balancing immediate and future rewards.\n- **$\\max_{a'} Q(s_{t+1}, a')$**: The maximum Q-value for the next state $s_{t+1}$ across all possible actions $a'$.\n\n![[Pasted image 20250220133556.png]]\n\n**Notes**:\n\n- Q-learning is well-suited for environments where the state and action spaces are discrete and manageable in size.\n- The algorithm is designed to converge to the optimal policy, even in non-deterministic environments, as long as each state-action pair is sufficiently explored.\n- [[Exploration vs Exploitation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "regressor"
    ],
    "normalized_filename": "q-learning",
    "outlinks": [
      "exploration_vs_exploitation",
      "reinforcement_learning",
      "policy",
      "pasted_image_20250220133556.png",
      "bellman_equations"
    ],
    "inlinks": [
      "deep_q-learning",
      "policy",
      "reinforcement_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "ROC (Receiver Operating Characteristic)",
    "sha": "a507f31e250a164ea167301f1c2d7343c9f4348b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/ROC%20(Receiver%20Operating%20Characteristic).md",
    "text": "**ROC (Receiver Operating Characteristic)** is a graphical ==representation of a classifier's performance across different thresholds,== showing the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate).\n\n A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n\nIt plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings.\n#### Why Use Predicted Probabilities?\n\nIn ROC analysis, predicted probabilities (`y_probs`) are used instead of predicted classes (`y_pred`) because the ROC curve evaluates the model's performance across different threshold levels. Probabilities allow you to adjust the threshold to see how it affects sensitivity and specificity.\n\n#### Threshold Level\n\nThe threshold level is the probability value above which an instance is classified as the positive class. Adjusting the threshold affects [[Recall]] and [[Specificity]]\n\n- Lower Threshold: Increases sensitivity but may decrease specificity.\n- Higher Threshold: Increases specificity but may decrease sensitivity.\n\n#### Example Code\n\n```python\nfrom sklearn.metrics import roc_curve, RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Actual and predicted values\ny_act = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_act, y_pred)\n\n# Display ROC curve\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\nplt.show()\n```\n\n#### [[Logistic Regression]] Example\n\n```python\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities for the positive class\ny_probs = logreg.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\nroc_auc = roc_auc_score(y_test, y_probs)\n\n# Plot ROC curve\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--', lw=2, label='Random Guessing')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.legend()\nplt.show()\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "ML_Tools"
    ],
    "normalized_filename": "roc_(receiver_operating_characteristic)",
    "outlinks": [
      "logistic_regression",
      "specificity",
      "recall"
    ],
    "inlinks": [
      "auc",
      "choosing_a_threshold",
      "determining_threshold_values",
      "model_observability",
      "precision-recall_curve"
    ]
  },
  {
    "category": "ML",
    "filename": "Random Forest for Time Series",
    "sha": "5d4a83a61fea9eafbd46a4550bd7a724c9128fc6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Random%20Forest%20for%20Time%20Series.md",
    "text": "A [[Random Forest]] is not inherently a time-series model (like [[ARIMA]] or [[SARIMA]]), but it can be adapted for forecasting by converting the series into a [[Supervised Learning]] problem.\n\nHow to apply Random Forest to time series:\n\n[[Feature Engineering]]\n   * Create lag features (e.g., $y_{t-1}, y_{t-2}, ..., y_{t-k}$).\n   * Add rolling window statistics (mean, std, min, max).\n   * Include calendar/time features (day of week, month, seasonality indicators).\n   * Incorporate exogenous variables if available (weather, prices, events).\n\nTraining\n   * Each tree in the Random Forest learns patterns from these lagged and engineered features.\n   * Predictions are aggregated across trees (mean for regression).\n\nForecasting strategies\n   * Iteration: One-step ahead forecasting: Predict $y_{t+1}$ using features up to $t$.\n   * Recursive forecasting: Use the model’s own predictions as inputs for subsequent steps (risk of error accumulation).\n   * Direct forecasting: Train separate models for each horizon (e.g., $y_{t+1}$, $y_{t+2}$).\n\nAdvantages\n   * Captures non-linear relationships and interactions.\n   * Handles missing values and outliers relatively well.\n   * Can incorporate external covariates easily.\n\nLimitations\n   * Does not naturally model time dependence like ARIMA/SARIMA.\n   * Recursive forecasts can accumulate errors (use [[Prediction Intervals]]).\n\t   * [ ] How are accumulations of errors handled?\n   * Needs careful feature engineering to capture seasonality/trends.\n\nWhen to use Random Forest for time series?\n* When the series has complex, non-linear dynamics.\n* When external features are strong drivers of the target variable.\n* When interpretability of variable importance is valuable.\n\nRelated:\n- [[ARIMA vs Random Forest in Time Series]]\n- [[Time Series Forecasting]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "ml_process",
      "time_series"
    ],
    "normalized_filename": "random_forest_for_time_series",
    "outlinks": [
      "arima",
      "time_series_forecasting",
      "supervised_learning",
      "arima_vs_random_forest_in_time_series",
      "feature_engineering",
      "random_forest",
      "sarima",
      "prediction_intervals"
    ],
    "inlinks": [
      "decomposition_in_time_series",
      "forecasting_using_lags",
      "random_forest"
    ]
  },
  {
    "category": "ML",
    "filename": "Random Forest",
    "sha": "b5ae93465929882adfdc317bcff1a0a7ee58f340",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Random%20Forest.md",
    "text": "A [[Random Forest]] is an [[Model Ensemble]] method that combines many [[Decision Tree]]s to improve accuracy and generalisation.\n\nHow does Random Forest work?\n\n* Each tree is trained on a random bootstrap sample of the training data ([[Bagging]]).\n* At each split, the tree considers only a random subset of features (commonly $\\sqrt{n}$ features if $n$ is the total number).\n* The final prediction is obtained by majority vote (classification) or averaging (regression).\n\nThis randomness reduces correlation between trees, making the overall model more robust ([[Generalisation|generalize]]).\n\nKey properties:\n* Can handle regression, classification, dimensionality reduction, and missing values.\n* Flexible, robust, and resistant to overfitting compared to a single [[Decision Tree]].\n* Works well with high-dimensional data.\n\nHyperparameters to tune:\n\n* `n_estimators`: number of trees.\n* `max_depth`: maximum depth of each tree.\n* `max_features`: number of features considered at each split (default $\\sqrt{n}$ for classification).\n* `n_jobs`: controls parallelism during training (more cores = faster training, but watch out for system slowdown).\n\nStrengths:\n* Reduces variance compared to a single tree.\n* Handles large datasets and mixed feature types well.\n* Provides feature importance estimates.\n\nWeaknesses:\n* Can still overfit with noisy or very high-dimensional data.\n* Less interpretable than a single decision tree.\n\nEvaluation:\n* Out-of-bag (OOB) error can be used as an internal validation metric (data not included in bootstrap samples).\n* Model performance can be refined by tuning [[Hyperparameter]]s.\n\nRelated:\n- [[Random Forest Regression]]\n- [[Random Forest for Time Series]]\n- [[Time Series Forecasting]]\n\n\n![[Pasted image 20240128194716.png]]\n![[Pasted image 20240118145117.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier"
    ],
    "normalized_filename": "random_forest",
    "outlinks": [
      "hyperparameter",
      "time_series_forecasting",
      "generalisation",
      "random_forest_for_time_series",
      "bagging",
      "random_forest",
      "pasted_image_20240128194716.png",
      "pasted_image_20240118145117.png",
      "decision_tree",
      "random_forest_regression",
      "model_ensemble"
    ],
    "inlinks": [
      "ada_boosting",
      "arima_vs_random_forest_in_time_series",
      "bagging",
      "decision_trees_are_fragile",
      "ds_&_ml_portal",
      "embedded_methods",
      "feature_importance",
      "feature_scaling",
      "hyperparameter_tuning",
      "imbalanced_datasets",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "machine_learning_algorithms",
      "random_forest",
      "random_forest_for_time_series",
      "random_forest_regression",
      "regularisation_of_tree_based_models",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Recall",
    "sha": "c187bb52533d989b3a79e2b07f8709f7a97d2b77",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Recall.md",
    "text": "**Recall Score** is a [[Evaluation Metrics]] used to evaluate the performance of a [[Classification]] model, focusing on the model's ability to **identify all relevant instances of the positive class**.\n\nIt answers the question: ==**How many relevant items are retrieved?**==\n\nHigh recall means that the model is effective at identifying most of the actual spam emails. This is useful in environments where missing a spam email could lead to security risks such as in corporate email systems.\n\nThe formula for recall is:\n\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n\nImportance\n- Recall is crucial in scenarios where ==**missing a positive instance is costly**==, such as in disease screening or fraud detection.\n- It helps in understanding how well the model captures all the actual positive instances.\n\nRelated Concepts\n- **Sensitivity** (also known as recall or the true positive rate) measures the proportion of actual positives that are correctly identified by the model. It indicates how well the model is at identifying positive instances.\n\n![[Pasted image 20241222091831.png]]",
    "aliases": [
      "sensitivity"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "recall",
    "outlinks": [
      "pasted_image_20241222091831.png",
      "evaluation_metrics",
      "classification"
    ],
    "inlinks": [
      "classification_report",
      "confusion_matrix",
      "ds_&_ml_portal",
      "evaluation_metrics",
      "f1_score",
      "imbalanced_datasets_smote.py",
      "model_observability",
      "precision-recall_curve",
      "precision_or_recall",
      "roc_(receiver_operating_characteristic)"
    ]
  },
  {
    "category": "ML",
    "filename": "Recommender systems",
    "sha": "867239692dc8c45636e3f0c26423adecd2d62570",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Recommender%20systems.md",
    "text": "Crab on Python\n\n- [ ] How would you make a simple one on [[Streamlit]]?\n\nA recommender system, or recommendation system, is a type of information filtering system that aims to predict the preferences or interests of users by analyzing their behavior and the behavior of similar users or items. \n\nThese systems aim to provide personalized recommendations to users.\n\nThe information is sparse therefore [[Linear Regression]], and [[Support Vector Machines|SVM]] cannot be used.\n\n### Key Components of Recommender Systems:\n\n1. User Data: Information about users, such as their preferences, ratings, purchase history, and interactions with items.\n\n2. Item Data: Information about the items being recommended, which can include attributes, descriptions, and metadata.\n\n3. Recommendation Algorithms: The methods used to generate recommendations. Common approaches include:\n   - [[Collaborative Filtering]]: This technique relies on the behavior and preferences of similar users. It can be user-based (finding similar users) or item-based (finding similar items).\n   - Content-Based Filtering: This approach recommends items based on the features of the items and the preferences of the user. For example, if a user likes action movies, the system will recommend other action movies based on their attributes.\n   - Hybrid Methods: Combining collaborative and content-based filtering to improve recommendation accuracy and overcome limitations of each method.\n\n1. [[Evaluation Metrics]]: Metrics used to assess the performance of the recommender system, such as precision, recall, F1 score, and mean average precision.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "modeling"
    ],
    "normalized_filename": "recommender_systems",
    "outlinks": [
      "linear_regression",
      "collaborative_filtering",
      "streamlit",
      "support_vector_machines",
      "evaluation_metrics"
    ],
    "inlinks": [
      "graph_neural_network",
      "k-nearest_neighbours",
      "tf-idf"
    ]
  },
  {
    "category": "ML",
    "filename": "Recurrent Neural Networks",
    "sha": "c1f5cb4f2f204c618f7df5511a04bc288d5a937f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Recurrent%20Neural%20Networks.md",
    "text": "Recurrent Neural Networks (RNNs) are a type of [[Neural network]] designed to process sequential data by maintaining a memory of previous inputs through hidden states. This makes them suitable for tasks where the order of data is needed, such as:\n\n- [[Time Series]] prediction, \n- speech recognition, \n- and [[NLP|natural language processing]] (NLP). \n\nRNNs have loops in their architecture, ==allowing information to persist across sequence steps.== However, they face challenges with long sequences due to [[vanishing and exploding gradients problem]]. To address these issues, variants like Long Short-Term Memory ([[LSTM]]) and [[GRU]] (GRU) have been developed.\n### Resources:\n[Video link](https://www.youtube.com/watch?v=TLLqsEyt8NI&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=9)\nhttps://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n\n### Key Concepts of RNNs\n\n#### Sequential Data Handling\n- RNNs maintain a hidden state that acts as memory, enabling them to model temporal dependencies. This is essential for tasks where the current output depends on both current and previous inputs.\n- At each time step, RNNs process an input, combine it with the previous hidden state, and produce an output along with an updated hidden state.\n- The hidden state carries forward information influenced by all previous inputs, theoretically allowing RNNs to remember long-term dependencies.\n\n![[Pasted image 20241219073017.png]]\n\n\n#### [[Backpropagation]] Through Time (BPTT)\n- RNNs are trained using BPTT, a variant of backpropagation. The network unrolls over time, treating each time step as a layer in a deep network.\n- Gradients are computed for each time step, and weights are updated based on cumulative error across the sequence. This allows learning of long-term dependencies but can lead to vanishing and exploding gradients in long sequences.\n#### Variants of RNNs\n- **LSTM**: Introduces gates (input, forget, output) to control information flow, addressing vanishing gradients and improving long-sequence handling.\n- **GRU**: A simpler variant of LSTM with fewer parameters, offering efficiency and ease of training while maintaining performance on sequence tasks.\n### Example Code (RNN in Python with [[PyTorch]])\n\nSee in [[ML_Tools]]see: RNN_Pytorch.py\n\n### Problem of Long Term Dependencies\n\nThe more time steps we include the less data we keep from the past.\n\nSolutions: [[LSTM]] and [[GRU]]: use gates: but are costly in computation.\n\n![[Pasted image 20241219073440.png]]\n\n### Other areas\n\n[[Use of RNNs in energy sector]]\n\n### RNNS and [[Transformer|Transformers]]\n\nWhy have [[Transformer|Transformer]]'s have replaced traditional RNN.\n\n[Inductive Reasoning, Memories and Attention.](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n\nHow to address the limitations of vanilla recurrent networks. \n\nIssues:\n- RNNs are not inductive: They memorize sequences extremely well, but don't generalise well.\n- They couple their representation size to the amount of computation per step.",
    "aliases": [
      "RNN",
      "RNNs"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "time_series"
    ],
    "normalized_filename": "recurrent_neural_networks",
    "outlinks": [
      "lstm",
      "use_of_rnns_in_energy_sector",
      "transformer",
      "pasted_image_20241219073440.png",
      "gru",
      "time_series",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation",
      "nlp",
      "pytorch",
      "ml_tools",
      "pasted_image_20241219073017.png",
      "neural_network"
    ],
    "inlinks": [
      "attention_mechanism",
      "ds_&_ml_portal",
      "feed_forward_neural_network",
      "lstm",
      "oov_words",
      "self-attention",
      "transformer",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "Regression Metrics",
    "sha": "3a7cbc1e47028e8233c0fb65654711111419e2f2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Regression%20Metrics.md",
    "text": "These metrics provide various ways to evaluate the performance of regression models.\n\n [[Mean Absolute Error]]\n [[Mean Absolute Percentage Error]]\n [[Mean Squared Error]]\n [[Root Mean Squared Error]]\n [[R squared]]\n [[Adjusted R squared]]\n [[Median Absolute Error]]\n### Evaluating Regression Models\n\nThese metric provide:\n\n1. **Comprehensive Evaluation**: Each metric provides a different perspective on model performance. For example, while MSE and RMSE give insights into the average error magnitude, MAE provides a straightforward average error measure, and R-squared indicates how well the model explains the variance in the data.\n\n2. **Sensitivity to [[uncategorised/Outliers]]**: Metrics like MSE and RMSE are sensitive to outliers due to the squaring of errors, which can be useful if you want to emphasize larger errors. In contrast, MAE and Median Absolute Error are more robust to outliers.\n\n3. **[[Interpretability]]**: RMSE is in the same units as the target variable, making it easier to interpret in the context of the data. This can be particularly useful for stakeholders who need to understand the model's performance in practical terms.\n\n4. **Model Comparison**: These metrics allow you to compare different models or configurations to determine which one performs best on your data.\n\n5. **Variance Explanation**: R-squared and Explained Variance Score provide insights into how much of the variability in the target variable is captured by the model, which is crucial for understanding the model's effectiveness.\n### Implementation\n\nSee Regression_Metrics.py",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "normalized_filename": "regression_metrics",
    "outlinks": [
      "r_squared",
      "adjusted_r_squared",
      "mean_absolute_error",
      "interpretability",
      "uncategorised/outliers",
      "mean_absolute_percentage_error",
      "mean_squared_error",
      "median_absolute_error",
      "root_mean_squared_error"
    ],
    "inlinks": [
      "adjusted_r_squared",
      "gini_impurity",
      "mean_absolute_percentage_error",
      "metric",
      "model_evaluation",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Regression",
    "sha": "ff048ce5b1735fea63246feaa04354d4f3923f60",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Regression.md",
    "text": "[[Regression]] analysis is a statistical method used to ==predict== a continuous variable based on one or more predictor variables. The most common form, [[Linear Regression]], assumes a linear relationship between the dependent variable $y$ and independent variables $x_1, x_2, \\dots, x_n$. The goal is to minimize the residual sum of squares (RSS) between observed and predicted values. Other forms, such as [[Logistic Regression]], handle classification problems.\n \n Regression models can incorporate techniques like regularization ($L_1$, $L_2$) to improve performance and prevent overfitting, especially with high-dimensional data. Advanced methods like [[Polynomial Regression]] address non-linearity, while generalized linear models (GLMs) extend regression to non-normal response variables.  \n \n **Regressor**: This is a type of model used for regression tasks, where the goal is to predict continuous values. For example, a regressor might be used to predict the price of a house based on its features, or to forecast future sales figures.\n\n\n Key Components:  \n - [[Regularisation]]: Adds $L_1$ ([[L1 Regularisation]]) or $L_2$ ([[Ridge]]) penalty to prevent overfitting in high-dimensional data.  \n - Feature transformation: [[Polynomial Regression]] and logarithmic transformations adjust for non-linearity in data.  \n - Regression is a type of [[Supervised Learning]].\n\nImportant\n - [[Multicollinearity]] can inflate variances of coefficient estimates, harming model reliability.  \n\nAttention\n - Regression assumes [[linearity]], so improper application to non-linear data can lead to biased predictions.  \n - Overfitting can occur with too many predictors, especially in small datasets. \n\nFollow up questions\n -  How can [[Polynomial Regression]] improve predictions in non-linear datasets?  \n -  What are the benefits of combining [[Linear Regression]] with [[Feature Engineering]] for complex datasets?",
    "aliases": [
      "linear models",
      "predictive regression",
      "regressive models"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "regressor",
      "statistics"
    ],
    "normalized_filename": "regression",
    "outlinks": [
      "regularisation",
      "linear_regression",
      "regression",
      "supervised_learning",
      "linearity",
      "logistic_regression",
      "polynomial_regression",
      "feature_engineering",
      "multicollinearity",
      "l1_regularisation",
      "ridge"
    ],
    "inlinks": [
      "classification",
      "ds_&_ml_portal",
      "encoding_categorical_variables",
      "energy",
      "imbalanced_datasets",
      "learning_styles",
      "logistic_regression",
      "loss_function",
      "machine_learning_algorithms",
      "parametric_vs_non-parametric_models",
      "r_squared",
      "regression",
      "supervised_learning",
      "typical_output_formats_in_neural_networks",
      "use_cases_for_a_simple_neural_network_like",
      "why_removing_outliers_may_improve_regression_but_harm_classification"
    ]
  },
  {
    "category": "ML",
    "filename": "Regularisation of Tree based models",
    "sha": "4eb508bd7e7fba177c44a007b403f9da62a19ad4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Regularisation%20of%20Tree%20based%20models.md",
    "text": "Tree models, such as Random Forests and Gradient Boosting, can also be regularized, although they don’t use L1 or L2 regularization directly. Instead, they are regularized through hyperparameters like max depth, min samples split, and learning rate to control the complexity of the trees.\n\nIn tree-based models, regularization is not applied in the same way as it is in linear models (i.e., using L1 or L2 penalties). \n\nIn tree models, [[Regularisation]] is done by controlling the growth of the trees using [[Hyperparameter]] like \n\n- `max_depth`,\n- `min_samples_split`,\n- `min_samples_leaf`.\n\nThese hyperparameters restrict the growth of the tree, preventing it from becoming too complex.\n\nFor [[Model Ensemble]] methods like Random Forests and Gradient Boosting, additional regularization techniques like \n\n- subsampling, \n- [[Bootstrap Sampling]], \n- and learning rate control\n\nto help prevent overfitting. These techniques effectively restrict the model complexity, leading to better generalization .\n\nBelow are the common regularization techniques used in tree models such as [[Decision Tree]], [[Random Forest]].\n\n### Regularization in Different Tree Models\n\n- Decision Trees: Prone to overfitting when not regularized, since they tend to grow large and complex trees. Regularization through pruning, limiting tree depth, and controlling minimum samples per split or leaf is critical.\n\n- Random Forests: Regularization is mainly achieved through the use of multiple decision trees, random [[Feature Selection]] (`max_features`), and bootstrapping (`bootstrap`). Each tree learns a different part of the data, which reduces overfitting.\n\n- Gradient Boosting Models (GBMs): Regularized by tuning the `learning_rate`, `subsample`, and controlling the tree depth and other tree-based hyperparameters like `min_samples_split`. The slower learning process with a smaller learning rate combined with these hyperparameters helps prevent overfitting.\n### Regularization Techniques for Tree Models\n\n1. Limiting Tree Depth:\n    - Max Depth (`max_depth`): This parameter restricts the maximum depth of the tree. Trees that are too deep can model complex patterns, but they are prone to overfitting. By limiting the depth, you constrain the tree's ability to learn highly specific patterns in the training data.\n    - Example: In scikit-learn, setting `max_depth` for a Decision Tree or Random Forest.\n\n    ```python\n    from sklearn.tree import DecisionTreeClassifier\n    \n    # Limit tree depth to regularize the model\n    model = DecisionTreeClassifier(max_depth=5)\n    model.fit(X_train, y_train)\n    ```\n\n2. Minimum Samples for Splitting:\n    - Min Samples Split (`min_samples_split`): This parameter specifies the minimum number of samples required to split an internal node. Increasing this value makes the tree more conservative and prevents it from splitting when there are too few samples, thus controlling its complexity.\n    - This helps avoid creating splits based on noise, which could lead to overfitting.\n\n    ```python\n    model = DecisionTreeClassifier(min_samples_split=10)\n    model.fit(X_train, y_train)\n    ```\n\n3. Minimum Samples per Leaf:\n    - Min Samples Leaf (`min_samples_leaf`): This parameter sets the minimum number of samples a node must have after a split to be a leaf. Higher values result in fewer splits, producing simpler trees that are less likely to overfit.\n    - This also encourages broader splits that require more data, reducing the sensitivity to outliers.\n\n    ```python\n    model = DecisionTreeClassifier(min_samples_leaf=4)\n    model.fit(X_train, y_train)\n    ```\n\n4. Max Number of Features:\n    - Max Features (`max_features`): This controls the number of features to consider when looking for the best split. Reducing the number of features makes the model less likely to overfit, as it limits the search space for splits. For Random Forests, this also introduces randomness that can improve generalization.\n\n    ```python\n    from sklearn.ensemble import RandomForestClassifier\n    \n    model = RandomForestClassifier(max_features='sqrt')  # Uses sqrt of total features\n    model.fit(X_train, y_train)\n    ```\n\n5. Max Leaf Nodes:\n    - Max Leaf Nodes (`max_leaf_nodes`): This parameter limits the total number of leaf nodes the tree can have. Fewer leaf nodes result in simpler trees that are less likely to overfit the training data.\n\n    ```python\n    model = DecisionTreeClassifier(max_leaf_nodes=10)\n    model.fit(X_train, y_train)\n    ```\n\n6. Subsampling (for Ensemble Methods like Random Forests and Gradient Boosting):\n    - Bootstrap Sampling (`bootstrap`): For Random Forests, regularization is achieved through bootstrapping (random sampling with replacement) during training. This introduces variability and helps prevent overfitting.\n    - Subsample (`subsample`): For Gradient Boosting, the `subsample` parameter controls the fraction of the training data used for fitting each individual tree. A value less than 1 introduces randomness and reduces the chance of overfitting, similar to how dropout works in neural networks.\n\n    ```python\n    from sklearn.ensemble import GradientBoostingClassifier\n    \n    model = GradientBoostingClassifier(subsample=0.8)  # Use 80% of data for each tree\n    model.fit(X_train, y_train)\n    ```\n\n7. Learning Rate (for Gradient Boosting Models):\n\n    - Learning Rate (`learning_rate`): This parameter controls how much each tree contributes to the overall model in Gradient Boosting. A lower learning rate slows down the learning process, requiring more trees but helping to avoid overfitting by making small adjustments at each step.\n\n    ```python\n    model = GradientBoostingClassifier(learning_rate=0.1)\n    model.fit(X_train, y_train)\n    ```\n\n8. Pruning:\n    - For Decision Trees, pruning is a post-processing regularization technique where branches that contribute little to the overall performance of the model are removed. This prevents the tree from learning noise in the data.\n    - In scikit-learn, Cost Complexity Pruning (`ccp_alpha`) is used for pruning. A larger value of `ccp_alpha` leads to more aggressive pruning, simplifying the tree.\n\n    ```python\n    model = DecisionTreeClassifier(ccp_alpha=0.01)\n    model.fit(X_train, y_train)\n    ```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "explainability",
      "optimisation"
    ],
    "normalized_filename": "regularisation_of_tree_based_models",
    "outlinks": [
      "hyperparameter",
      "regularisation",
      "bootstrap_sampling",
      "random_forest",
      "decision_tree",
      "feature_selection",
      "model_ensemble"
    ],
    "inlinks": [
      "regularisation"
    ]
  },
  {
    "category": "ML",
    "filename": "Regularisation",
    "sha": "29ef23a6e00c72465d5b978d2b2efa85ff879cd7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Regularisation.md",
    "text": "Regularization is a technique in machine learning that reduces the risk of overfitting by adding a penalty to the [[Loss function]] during model training. This penalty term restricts the magnitude of the model's parameters, thereby controlling the complexity of the model. It is especially useful in linear models but can also be applied to more complex models like neural networks.\n\n==It simplifies the model.==\n\n### Key Concepts\n\n- **$L_1$ Regularization ([[L1 Regularisation]]):** Adds the absolute value of the coefficients to the loss function, encouraging sparsity by driving some coefficients to zero, effectively selecting a subset of features.\n  \n- **$L_2$ Regularization ([[Ridge]]):** Adds the square of the coefficients to the loss function, shrinking them toward zero. It encourages smaller coefficients but does not push them exactly to zero, helping reduce overfitting by penalizing large weights.\n\n- **Elastic Net:** Combines both Lasso and Ridge regularization.\n\n### Benefits\n\n- **Prevents Overfitting:** Regularization adds a penalty term to the loss function to avoid overfitting.\n- **Feature Sparsity:** $L_1$ encourages feature sparsity, while $L_2$ reduces coefficient magnitudes.\n- **Enhanced Generalization:** Dropout enhances generalization by preventing unit co-adaptation in neural networks.\n\n### Considerations\n\n- **Underfitting Risk:** Over-penalizing parameters can lead to underfitting, where the model becomes too simplistic.\n- **Tuning $\\lambda$:** Choosing the right penalty term (i.e., $\\lambda$) is crucial for balancing bias and variance.\n\n[[When and why not to us regularisation]]\n### Questions\n- How does the balance between $L_1$ and $L_2$ regularization impact model performance in large feature spaces?\n- What are the best practices for tuning the $\\lambda$ parameter in regularization? [[Model Parameters Tuning]].\n\n### Example\n\nConsider a linear regression model with $L_2$ regularization (Ridge). The [[Loss function]] would be:\n\n$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 $$\n\nHere, $\\lambda$ controls the strength of the regularization. Higher $\\lambda$ values shrink the coefficients more.\n### Related Topics\n\n- [[Feature Selection]]: L1 regularization can zero out irrelevant features, improving model [[Interpretability]] and reducing computational costs.\n- [[Model Selection]] techniques for high-dimensional data.\n\n### Applications\n\nRegularization is widely used in linear models but is also applied in other machine learning models, particularly those prone to overfitting:\n\n- [[Neural network]]\n- [[Regularisation of Tree based models]]",
    "aliases": [
      "Regularisation techniques",
      "Regulation in ML"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "optimisation",
      "process",
      "visualization",
      "ML_Tools"
    ],
    "normalized_filename": "regularisation",
    "outlinks": [
      "model_selection",
      "loss_function",
      "neural_network",
      "when_and_why_not_to_us_regularisation",
      "model_parameters_tuning",
      "regularisation_of_tree_based_models",
      "l1_regularisation",
      "feature_selection",
      "interpretability",
      "ridge"
    ],
    "inlinks": [
      "backpropagation",
      "bias-variance_trade_off",
      "curse_of_dimensionality",
      "dropout",
      "ds_&_ml_portal",
      "embedded_methods",
      "fitting_weights_and_biases_of_a_neural_network",
      "hyperparameter_tuning",
      "lagrange_multipliers_in_optimisation",
      "lightgbm_vs_xgboost_vs_catboost",
      "orthogonalization",
      "overfitting",
      "regression",
      "regularisation_of_tree_based_models",
      "xgboost"
    ]
  },
  {
    "category": "ML",
    "filename": "Reinforcement learning",
    "sha": "28b2ebae427b8d557c42a7dac0518705d66dc821",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Reinforcement%20learning.md",
    "text": "Reinforcement Learning ( [[Reinforcement learning|RL]]) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions, and its goal is to ==maximize cumulative reward.==\n#### Current Research Problems\n\n1. Sample Efficiency: Many RL algorithms require a large number of interactions with the environment to learn effectively. Research is focused on developing methods to improve sample efficiency, such as model-based approaches and transfer learning.\n    \n2. [[Exploration vs Exploitation]]: Balancing exploration (trying new actions) and exploitation (choosing known rewarding actions) remains a challenge. New strategies, such as curiosity-driven learning and bandit algorithms, are being investigated.\n    \n3. [[Multi-Agent Reinforcement Learning]]: Extending RL to environments with multiple interacting agents introduces complexity in learning optimal strategies. Research includes coordination, competition, and communication between agents.\n    \n4. Robustness and Stability: Ensuring that RL agents perform reliably in changing or adversarial environments is a key area of study. Techniques for robust control and stability analysis are being explored.\n\n### Algorithms in Reinforcement Learning\n\n[[Q-Learning]]\n[[Deep Q-Learning]]\n[[Sarsa]]\n#### Components\n\n- Agent: An entity that interacts with the environment and learns to optimize its actions based on rewards.\n- State ($s$): The current situation in the environment, often defined by the positions and attributes relevant to the agent's decision-making.\n- Action ($a$): The available moves or decisions an agent can take.\n- Reward ($r$): A scalar value received after taking an action, representing feedback from the environment.\n- [[Policy]] ($\\pi$): A strategy that the agent follows, mapping states to actions.\n- Q-Value ($Q(s, a)$): The expected cumulative reward for taking a particular action in a given state and following the policy thereafter. The Q-values guide the agent in making decisions that maximize long-term rewards. [[Q-Learning]]\n#### Mathematical Foundations\n\n- [[Markov Decision Processes]]\n- Dynamic Programming: Techniques such as [[Bellman Equations]] equations are central to RL, as they provide a way to break down complex decision-making problems into simpler subproblems.\n- Optimization Techniques: Finding optimal [[Policy|polices]] often involves advanced optimization methods, including gradient ascent and policy iteration.\n\n### Reinforcement Learning Implementation\n\nUse import gym\n\nAction Space: What actions are available to the agent?\nObservation Space: What information is available to the agent?\nReward Envioronment: What rewards can be given to the agent?\n\nLoads the enviroemnt for examples\nenv = gym.make('LunarLander-v2')\n\nCan step though the environment dynamics\nnext_state, reward, done, info = env.step(action)\n\n```python\n# Select an action\naction = 0\n\n# Run a single time step of the environment's dynamics with the given action.\nnext_state, reward, done, info = env.step(action)\n\nwith np.printoptions(formatter={'float': '{:.3f}'.format}):\n    print(\"Initial State:\", initial_state)\n    print(\"Action:\", action)\n    print(\"Next State:\", next_state)\n    print(\"Reward Received:\", reward)\n    print(\"Episode Terminated:\", done)\n    print(\"Info:\", info)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field",
      "ml"
    ],
    "normalized_filename": "reinforcement_learning",
    "outlinks": [
      "markov_decision_processes",
      "q-learning",
      "exploration_vs_exploitation",
      "reinforcement_learning",
      "policy",
      "multi-agent_reinforcement_learning",
      "sarsa",
      "deep_q-learning",
      "bellman_equations"
    ],
    "inlinks": [
      "deep_q-learning",
      "ds_&_ml_portal",
      "energy_demand_forecasting",
      "exploration_vs_exploitation",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "industries_of_interest",
      "llm",
      "policy",
      "q-learning",
      "reinforcement_learning",
      "reward_function",
      "sarsa"
    ]
  },
  {
    "category": "ML",
    "filename": "Relationships in memory",
    "sha": "f4e40293c12dfa07a3671dea68f7d73a776a0592",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Relationships%20in%20memory.md",
    "text": "In managing the memory of a large language model (LLM), several key concepts and techniques play a crucial role in forming and maintaining relationships between data points:\n\n**[[RAG]] (Retrieval-Augmented Generation)**:\n\nThis technique enhances LLMs by integrating external data retrieval with generative capabilities. By employing chunking and reranking, RAG refines outputs, ensuring that the model can access and utilize relevant information efficiently. This process strengthens the model's ability to form and maintain relationships between different pieces of information, improving its memory and response accuracy.\n\n**Ontology and Correlating Data Points**:\n\nOntologies establish [[Semantic Relationships]] between data points by defining a structured framework of concepts and their interrelations. This structured understanding aids in memory management by providing a clear map of how different pieces of information are related. Similarly, correlating data points involves understanding and forming connections, which is essential for enhancing memory management. Together, these approaches help LLMs organize and interpret information more effectively.\n\n**Vector Store and Metadata Management**:\n\nUtilizing [[Vector Database]] vector databases allows for efficient storage and retrieval of memory representations. These databases preserve the relationships between data points, enabling LLMs to access and utilize memory more effectively. Alongside this, managing metadata is crucial for organizing, retrieving, and correlating data points. Effective metadata management helps LLMs understand the context and relationships between different pieces of information, enhancing their memory capabilities.\n\n**Structure Memory Graph**: [[GraphRAG]]\n\nOrganizing memory in graph structures allows LLMs to improve relational understanding and connection-making. Graphs provide a visual and structural representation of how information is interconnected, aiding the model's ability to form and maintain complex relationships in memory.\n\n**Cognitive Sciences**:\n\nInsights from cognitive science inform memory design and improve human-AI interaction. By integrating these insights, LLMs can mimic human-like memory processes, enhancing their ability to form, maintain, and retrieve relationships in memory, leading to more natural and effective interactions.",
    "aliases": [
      "Managing LLM memory"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "memory_management"
    ],
    "normalized_filename": "relationships_in_memory",
    "outlinks": [
      "vector_database",
      "graphrag",
      "rag",
      "semantic_relationships"
    ],
    "inlinks": [
      "llm"
    ]
  },
  {
    "category": "ML",
    "filename": "Reward Function",
    "sha": "c899eda515357da22b7b604e7ea1c00bae392754",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Reward%20Function.md",
    "text": "Mentioned as the opposite of a [[Cost Function]], typically used in [[Reinforcement learning]] to indicate the desirability of an outcome.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "reward_function",
    "outlinks": [
      "cost_function",
      "reinforcement_learning"
    ],
    "inlinks": [
      "cost_function"
    ]
  },
  {
    "category": "ML",
    "filename": "Ridge",
    "sha": "848afa86a8ce2a2d54297e6cb7be8dafd023a449",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Ridge.md",
    "text": "L2 Regularization, also known as Ridge Regularization, adds a penalty term proportional to the square of the weights to the [[Loss function]]. \n\nThis technique enhances the robustness of linear regression models (and [[Logistic Regression]]) by penalizing large coefficients, encouraging smaller weights overall, and ==distributing weight values more evenly across all features.==\n\n### Key Points\n\n- **Overfitting Mitigation**: Ridge helps mitigate [[Overfitting]], especially in high-dimensional datasets, and is effective in managing [[Multicollinearity]] among predictors.\n- **Coefficient Shrinkage**: Unlike Lasso regularization (L1), which can eliminate some features entirely by driving their coefficients to zero, Ridge reduces the magnitudes of coefficients but retains all features.\n- **Multicollinearity Handling**: Particularly useful when predictors are highly correlated, as it stabilizes estimates by shrinking the coefficients of correlated features.\n- **Feature Retention**: Ridge retains all features in the model, unlike Lasso, which can perform [[Feature Selection]]by setting some coefficients to zero.\n\n### Understanding Ridge Regularization\n\n#### 1. Purpose of Ridge Regularization\n\n- **Penalty Addition**: Adds a penalty term to the loss function, proportional to the square of the coefficients (weights), discouraging overly complex models by shrinking the coefficients.\n\n#### 2. Mathematical Formulation\n\n- The loss function for Ridge regression can be expressed as:\n  $$\\text{Loss} = \\text{SSE} + \\lambda \\sum_{j=1}^{p} b_j^2$$\n  - Where:\n    - SSE (Sum of Squared Errors) is the original loss function for [[Linear Regression]]\n    - $\\lambda$ is the regularization parameter (penalty term) that controls the strength of the penalty.\n    - $b_j$ are the coefficients of the model.\n    - $p$ is the number of predictors.\n\n#### 3. Effect of the Regularization Parameter ($\\lambda$)\n\n- **Range**: $\\lambda$ can take values from 0 to infinity.\n- **Impact**:\n  - A small $\\lambda$ (close to 0) means the model behaves similarly to ordinary least squares (OLS) regression, with minimal regularization.\n  - A large $\\lambda$ increases the penalty, leading to smaller coefficients and a simpler model.\n\n#### 4. Finding the Best $\\lambda$\n\n- Use techniques like cross-validation to determine the optimal value of $\\lambda$. By testing various values and evaluating model performance, select the one that minimizes prediction error (or variance).\n\n### Example Code\n\n```python\nfrom sklearn.linear_model import Ridge\n\n# Initialize a Ridge model\nmodel = Ridge(alpha=0.1)  # alpha is the regularization strength\nmodel.fit(X_train, y_train)\n```\n\n### Resources\n\n- [Understanding Ridge Regularization](https://www.youtube.com/watch?v=Q81RR3yKn30)\n\n---\n\n### Understanding the Content\n\n- **L2 Regularization (Ridge)**: This technique is crucial for improving model generalization by penalizing large coefficients, which helps in reducing overfitting and handling multicollinearity. The regularization parameter $\\lambda$ controls the trade-off between fitting the training data well and keeping the model coefficients small.\n\n[[Ridge]]\n### L2 Regularization (Ridge Regression): for [[Neural network]]\n\n\n\nAdds a penalty term to the loss: \\( L_{\\text{regularized}} = L + \\lambda \\cdot ||W||^2 \\). This discourages overly complex models by penalizing large weights.\n\nExample:\n\n```python\nfrom tensorflow.keras.regularizers import l2\nDense(25, activation=\"relu\", kernel_regularizer=l2(0.01))\n```",
    "aliases": [
      "L2"
    ],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "ridge",
    "outlinks": [
      "linear_regression",
      "logistic_regression",
      "overfitting",
      "loss_function",
      "neural_network",
      "multicollinearity",
      "feature_selection",
      "ridge"
    ],
    "inlinks": [
      "elastic_net",
      "embedded_methods",
      "feed_forward_neural_network",
      "fitting_weights_and_biases_of_a_neural_network",
      "optimising_a_logistic_regression_model",
      "regression",
      "regularisation",
      "ridge",
      "when_and_why_not_to_us_regularisation"
    ]
  },
  {
    "category": "ML",
    "filename": "SARIMA",
    "sha": "b137b8f328b81236e0eb26ae01d21c0df7bf9f2a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/SARIMA.md",
    "text": "Seasonal ARIMA (SARIMA) extends the standard ARIMA model by incorporating seasonal components.\nIt is expressed as:\n\n$$\\text{SARIMA}(p, d, q)(P, D, Q)_s$$\n### Components\n\n#### Non-seasonal part $(p, d, q)$\n\n(See [[ARIMA]] for details.)\n\n#### Seasonal part $(P, D, Q)_s$\n\nThese terms allow ARIMA to model repeating seasonal patterns. \n\nThe parameter $s$ denotes the seasonal period, how often the pattern repeats (e.g., $s=12$ for monthly data with yearly seasonality).\n\n* $P$ - Seasonal Autoregressive Order\n  Analogous to $p$, but applies to seasonal lags.\n  Example: If $P=1$ and $s=12$, the model uses $y_{t-12}$ (the value from one year ago) to predict $y_t$.\n\n* $D$ - Seasonal Differencing Order\n  The number of times seasonal differencing is applied to remove seasonal trends.\n  Example: $D=1$ with $s=12$ applies $y_t - y_{t-12}$.\n\n* $Q$ - Seasonal Moving Average Order\n  Analogous to $q$, but incorporates forecast errors from seasonal lags.\n  Example: $Q=1$ and $s=12$ uses the forecast error from 12 months prior.\n\n### How SARIMA Works\n\n1. Seasonal differencing ($D > 0$)\n   * Remove repeating seasonal patterns to stabilize the mean.\n   $$y'_t = y_t - y_{t-s}$$\n\n2. Non-seasonal differencing ($d > 0$)\n   * Remove overall trends to make the series stationary.\n\n3. Fit ARMA components\n\n   * Non-seasonal AR ($p$) and MA ($q$) capture short-term dependencies.\n   * Seasonal AR ($P$) and MA ($Q$) capture longer-term dependencies at seasonal lags.\n\n2. Combine predictions\n   The final forecast combines seasonal and non-seasonal components.\n\n### Determining Parameters\n\nRefer to:\n[Tsang, G. (2020). *A semi-auto way to determine parameters for SARIMA models*](https://tsanggeorge.medium.com/a-semi-auto-way-to-determine-parameters-for-sarima-model-74cdee853080)\n\n### Practical Notes\n\n* SARIMA is conceptually similar to Holt–Winters exponential smoothing, but relies on more formal statistical assumptions.\n* Before estimating $P$ and $Q$, ensure the series is seasonally stationary by applying seasonal [[Differencing in Time Series|Differencing]] if required ($D>0$).\n\n### Identifying Seasonal Orders with [[ACF Plots|ACF]] and [[PACF Plots]]\n\nTo choose $Q$ (Seasonal MA order):\n  * Examine the ACF plot for significant autocorrelations at seasonal lags (multiples of $s$).\n  * A sharp cut-off at a seasonal lag suggests the appropriate $Q$.\n\nTo choose $P$ (Seasonal AR order):\n  * Examine the PACF plot for significant spikes at seasonal lags.\n  * A sharp cut-off indicates the seasonal AR order $P$.\n\n### Related\n\n* [[AIC in Model Evaluation]]\n* [[Evolving Seasonality]]\n* [[Differencing in Time Series]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "sarima",
    "outlinks": [
      "arima",
      "aic_in_model_evaluation",
      "differencing_in_time_series",
      "evolving_seasonality",
      "acf_plots",
      "pacf_plots"
    ],
    "inlinks": [
      "aic_in_model_evaluation",
      "arima",
      "decomposition_in_time_series",
      "differencing_in_time_series",
      "evolving_seasonality",
      "random_forest_for_time_series",
      "stationary_time_series",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Sammon’s Mapping",
    "sha": "73d00f5c800b71254f8bef7d20c4ec19b8a96cc0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Sammon%E2%80%99s%20Mapping.md",
    "text": "maps a high-dimensional space to a space of lower dimensionality \n\n  \n\nnon-linear approach, PCA is linear\n\n  \n\naims to minimize the following error function, which is often referred to as Sammon's stress or Sammon's error:\n\n  \nsimilar to multidimensional scaling",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "sammon’s_mapping",
    "outlinks": [],
    "inlinks": [
      "principal_component_analysis"
    ]
  },
  {
    "category": "ML",
    "filename": "Scikit-Learn",
    "sha": "db299c02c88e0657ee82648efe053c31cf5687f3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Scikit-Learn.md",
    "text": "### Examples\n\nhttps://scikit-learn.org/stable/auto_examples/applications/index.html\n\n\n**Focus**: \n  Sci-kit Learn is a simple and efficient tool for data mining and data analysis, built on NumPy, [[Scipy]], and matplotlib. It is primarily used for traditional machine learning techniques such as classification, regression, clustering, and dimensionality reduction.\n  \n**Limitations**: \n  While excellent for classical machine learning tasks, Sci-kit Learn is not designed for deep learning or neural network architectures, which require more specialized frameworks like [[Tensorflow]] or [[PyTorch]].",
    "aliases": [
      "sklearn"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis"
    ],
    "type": null,
    "normalized_filename": "scikit-learn",
    "outlinks": [
      "tensorflow",
      "pytorch",
      "scipy"
    ],
    "inlinks": [
      "deep_learning_frameworks",
      "ds_&_ml_portal",
      "how_to_use_sklearn_pipeline",
      "lbfgs",
      "logistic_regression_statsmodel_summary_table",
      "multiprocessing",
      "optimisation_techniques",
      "optimising_a_logistic_regression_model",
      "sklearn_datasets",
      "sklearn_pipiline",
      "time_series_python_packages",
      "train-dev-test_sets",
      "transformed_target_regressor"
    ]
  },
  {
    "category": "ML",
    "filename": "Secretary Problem",
    "sha": "bb7268f19c410fccbece61e244423e5ff7867141",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Secretary%20Problem.md",
    "text": "The Secretary Problem (also called the Optimal Stopping Problem) is a classic problem in probability theory and [[Decision Theory]]. It explores the challenge of making an optimal choice when faced with uncertainty and incomplete information.\n\n### Why Care About It?\n\nThe Secretary Problem is important because it formalizes ==optimal stopping== decisions under uncertainty, which appear in:\n\n* Hiring decisions (as in the story).\n* Real estate (choosing the best property when visiting options sequentially).\n* Financial decisions (selling assets, timing investments).\n* Online algorithms (ad placement, real-time bidding).\n* Resource allocation in data centers or task scheduling.\n\nThe key insight: You can't always maximize certainty-you optimize probability given constraints.\n\n### What is the Secretary Problem?\n\n* You have \\$n\\$ applicants for a job.\n* You interview them in random order, one by one.\n* After each interview:\n\n  * You know how the candidate ranks among those seen so far (relative rank).\n  * You must accept or reject immediately (no recall).\n* Goal: Hire the single best applicant overall.\n* Question: *What strategy maximizes the probability of choosing the best applicant?*\n\n### Solution Strategy\n\nThe optimal solution involves two phases:\n\n1. Observation Phase:\n\n   * Interview and reject the first \\$r\\$ candidates (where $r \\approx \\frac{n}{e} \\approx 0.37n$).\n   * Use them as a sample to set a benchmark for \"best so far.\"\n1. Selection Phase:\n\n   * From candidate $r+1$ onward, pick the first applicant who is better than all previous ones.\n\nThis gives a maximum success probability of about:\n\n$$\nP(\\text{success}) \\approx \\frac{1}{e} \\approx 36.8\\%\n$$",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "learning",
      "optimisation",
      "probability",
      "statistics"
    ],
    "normalized_filename": "secretary_problem",
    "outlinks": [
      "decision_theory"
    ],
    "inlinks": [
      "decision_theory"
    ]
  },
  {
    "category": "ML",
    "filename": "Sentence Transformers",
    "sha": "8676c5a1fbb9497643358ae16324b413f9d4b2fe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Sentence%20Transformers.md",
    "text": "Sentence [[Transformer|Transformers]] are models built on top of transformer architectures like [[BERT]], fine-tuned to generate semantic sentence embeddings. Unlike BERT, which produces token-level outputs pooled into sentence representations, Sentence Transformers are trained to directly capture meaningful sentence-level semantics through supervised training on sentence pairs.\n\nWhy Use Sentence Transformers Instead of BERT?\n- Standard BERT embeddings are not optimized for measuring [[Semantic Relationships|semantic similarity]]. They rely on simple pooling methods (e.g. mean pooling), which treat all tokens equally and often fail to reflect sentence meaning accurately.\n\nSentence Transformers overcome this by:\n* Using a twin (siamese) architecture where sentence pairs are passed through a shared encoder.\n* Being fine-tuned on tasks like semantic similarity, paraphrase detection, and natural language [[inference]], enabling them to learn relational meaning between sentences.\n* Producing embeddings that are directly comparable using metrics like [[Cosine Similarity]].\n\nAs a result, Sentence Transformers perform significantly better on:\n* Semantic search\n* [[Vector Embedding|Embedding]] texts for semantic search and ranking\n* Clustering or grouping short texts\n* Comparing the similarity of sentence or query pairs\n\n Key Questions to Explore\n* When is fine-tuning necessary, and how does it impact embedding quality?\n* Which model variants (e.g. `all-MiniLM`, `mpnet`) are best for your task?\n* [ ] When is Sentence -BERT preferable over base BERT or other transformer models?\n\nRelated:\n- [[Sentence Transformer Workflow]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "normalized_filename": "sentence_transformers",
    "outlinks": [
      "bert",
      "vector_embedding",
      "semantic_relationships",
      "sentence_transformer_workflow",
      "transformer",
      "inference",
      "cosine_similarity"
    ],
    "inlinks": [
      "bert"
    ]
  },
  {
    "category": "ML",
    "filename": "Sklearn Pipeline",
    "sha": "7d2d8c2f292c0b79c3e6573c76f0f527302bd5d7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Sklearn%20Pipeline.md",
    "text": "You can build pipelines to test multiple models.\n\n[[Model Validation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "sklearn_pipeline",
    "outlinks": [
      "model_validation"
    ],
    "inlinks": [
      "model_deployment"
    ]
  },
  {
    "category": "ML",
    "filename": "Specificity",
    "sha": "c83ddabf4fc0e400ca145eb2324d1ab504e6da50",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Specificity.md",
    "text": "**Specificity**, also known as the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. It indicates how well the model is at identifying negative instances. Formula:\n\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\nImportance\n- Specificity is crucial in scenarios where it is important to correctly identify negative instances, such as in medical testing where a false positive could lead to unnecessary treatment.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "specificity",
    "outlinks": [],
    "inlinks": [
      "confusion_matrix",
      "evaluation_metrics",
      "roc_(receiver_operating_characteristic)"
    ]
  },
  {
    "category": "ML",
    "filename": "Spectral Clustering",
    "sha": "51562a22aa23f0819bc9b4aec0937e794e288d06",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Spectral%20Clustering.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "spectral_clustering",
    "outlinks": [],
    "inlinks": [
      "clustering"
    ]
  },
  {
    "category": "ML",
    "filename": "Supervised Learning",
    "sha": "c06975e33b7acb69537d614907458bf686ac6376",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Supervised%20Learning.md",
    "text": "Supervised learning is a type of machine learning where an algorithm learns from **==labeled data==** to make predictions or decisions. \n\nIn supervised learning, the training data consists of input-output pairs, where each input (features) is associated with a known output (label or target). \n\nThe algorithm's goal is to learn a mapping from the input to the output so that it can predict the output for new, unseen data.\n### Examples of Supervised [[Machine Learning Algorithms]]:\n- [[K-nearest neighbours]] (KNN)\n- [[Naive Bayes Classifier]]\n- [[Decision Tree]]\n- [[Linear Regression]]\n- [[Support Vector Machines]] (SVM)\n### Key Characteristics of Supervised Learning:\n1. **Labeled Data**: The training dataset contains both the input data and the corresponding correct outputs (labels).\n2. **Training Phase**: The model is trained using this labeled data to minimize errors in predicting the output.\n3. **Prediction**: After training, the model can predict the output (label) for new data based on what it learned.\n### Types of Supervised Learning Algorithms:\n\n**Supervised learning algorithms** learn from ==labeled data,== where each example is associated with a target label. \n\nLabelled data can look like the following:\n\n| Email Content                              | Label      |\n|--------------------------------------------|------------|\n| \"Congratulations! You won a free iPhone.\"  | Spam       |\n| \"Meeting scheduled for 2 PM tomorrow.\"     | Not Spam   |\n| \"Special offer: Buy 1 get 1 free!\"         | Spam       |\n| \"Please find the attached report.\"         | Not Spam   |\nLabelling can be expensive and time consuming.\n\n- **[[Classification]]**: Predicts discrete labels (e.g., categories).\n- **[[Regression]]**: Predicts continuous values.\n### Example:\nIn a **house price prediction** task:\n- The input features could be the size of the house, number of rooms, and location.\n- The output (label) would be the price of the house.\n\nThe model is trained on a dataset where the house prices are known (labeled data), and it learns to predict the price for new houses.\n#### Code implementation\n\nThat is there is a y_train, then uses to get y_pred (from X_test) and compare against y_test.\n\n\n![[Pasted image 20241012152141.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "field"
    ],
    "normalized_filename": "supervised_learning",
    "outlinks": [
      "linear_regression",
      "regression",
      "classification",
      "naive_bayes_classifier",
      "pasted_image_20241012152141.png",
      "machine_learning_algorithms",
      "decision_tree",
      "k-nearest_neighbours",
      "support_vector_machines"
    ],
    "inlinks": [
      "active_learning",
      "backpropagation",
      "classification",
      "data_transformation_in_machine_learning",
      "decision_tree",
      "ds_&_ml_portal",
      "feed_forward_neural_network",
      "k-nearest_neighbours",
      "machine_learning_algorithms",
      "random_forest_for_time_series",
      "regression",
      "transfer_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Support Vector Classifier",
    "sha": "e37a797ee0f1997d92847d0d25e3b35c91b576ca",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Support%20Vector%20Classifier.md",
    "text": "## Overview\n\n**Support Vector Classifiers (SVCs)** are a fundamental component of [[Support Vector Machines|SVM]]s, designed to find the optimal hyperplane that separates data into distinct classes. The primary objective of an SVC is to ==maximize the margin between different classes==, ensuring that the separation is as clear as possible.\n\n## Key Concepts\n\n- **Hyperplane**: A decision boundary that separates different classes in the feature space. In a two-dimensional space, this is a line; in higher dimensions, it becomes a plane or hyperplane.\n- **Support Vectors**: The data points that are closest to the hyperplane. These points are critical as they define the position and orientation of the hyperplane.\n- **Margin**: The distance between the hyperplane and the nearest data point from either class. SVC aims to maximize this margin to improve [[Classification]] robustness.\n\n## SVC vs. SVM\n\n- **SVC**: Primarily focuses on placing a hyperplane between datasets for separation. It is effective when the data is linearly separable.\n- **SVM**: Extends the concept of SVC by using kernel functions to handle cases where data is **not linearly separable** in its original space. Kernels transform the data into a higher-dimensional space where a linear separation is possible.",
    "aliases": [
      "SVC"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "classifier"
    ],
    "normalized_filename": "support_vector_classifier",
    "outlinks": [
      "support_vector_machines",
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Support Vector Machines",
    "sha": "d5ace44c95d74d59d88b5170812d5b76b6832558",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Support%20Vector%20Machines.md",
    "text": "Support Vector Machines (SVM) are a type of [supervised learning](app://obsidian.md/supervised%20learning) algorithm primarily used for [[Classification]] tasks, though they can also be adapted for regression. \n\n==The main idea is to find an optimal hyperplane that divides data into different classes by maximizing the margin between them.== \n\nThe ==support vectors== are the data points closest to the hyperplane, influencing its position and orientation.\n\nKey Features\n- Hyperplane: Finds a hyperplane that maximizes the margin between classes.\n- High-Dimensional Spaces: Robust in high-dimensional spaces, such as image and text classification.\n\nAdvantages\n- Highly effective for high-dimensional data (datasets with many features).\n- Useful for classification tasks where a clear margin of separation exists between classes.\n\nDisadvantages\n- Can be computationally expensive for large datasets and sensitive to the choice of hyperparameters.\n- Performance is highly dependent on the [[Kernelling]] choice, requiring careful tuning.\n\nVisualisation: https://dash.gallery/dash-svm/\n## How SVM Works\n\n1. Initial Space: Start in the low-dimensional space, where the data may not be linearly separable.\n2. Kernel Function: Use a [Kernelling](app://obsidian.md/Kernelling) function to move the data into a higher dimension where separation is clearer.\n3. Hyperplane Placement: Place hyperplanes (decision boundaries) between the data clusters to classify the data.\n\n## Margins\n\n- Outliers and Soft Margins: SVM allows for some miscalculations or errors in classification to handle outliers. This is part of the [Bias and variance](app://obsidian.md/Bias%20and%20variance) tradeoff, where the model is allowed to make a few mistakes to improve generalization.\n- ==Soft Margins==: Allow some data points to be within the margin or even on the wrong side of the hyperplane, enabling SVM to handle imperfect separations.\n\n![[Pasted image 20240128193726.png|700]]\n\n![[Pasted image 20240128193838.png|700]]",
    "aliases": [
      "SVM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "clustering"
    ],
    "normalized_filename": "support_vector_machines",
    "outlinks": [
      "pasted_image_20240128193838.png",
      "pasted_image_20240128193726.png",
      "kernelling",
      "classification"
    ],
    "inlinks": [
      "classification",
      "ds_&_ml_portal",
      "feature_scaling",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "kernel_machines",
      "kernelling",
      "lagrange_multipliers_in_optimisation",
      "logistic_regression",
      "machine_learning_algorithms",
      "model_parameters",
      "parametric_vs_non-parametric_models",
      "recommender_systems",
      "supervised_learning",
      "support_vector_classifier",
      "support_vector_regression",
      "unsupervised_learning"
    ]
  },
  {
    "category": "ML",
    "filename": "Tensorflow",
    "sha": "7e50dcb567a85e330792fcd671196359c63a150e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Tensorflow.md",
    "text": "Open sourced by Google\nBased on a dataflow graph\n\n[Text summarization with TensorFlow](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html)\n\nOpen-source library for numerical computation and large-scale [[Machine Learning]], focusing on static dataflow graphs.\n\nGet same code use of tensorflow example:\n\nBasic example is\n\n[[Handwritten Digit Classification]]\n\n[[Pytorch vs Tensorflow]]\n### [[Tensorflow]]\n\n**Focus**: \n  TensorFlow is a comprehensive open-source platform for machine learning. It provides a flexible and comprehensive ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML, and developers easily build and deploy ML-powered applications.\n  \n**Integration**: \n  TensorFlow can implement a wide range of machine learning algorithms, including those available in [[Sci-kit Learn]], making it versatile for various applications.\n  \n**Modularity**: \n  Its modular architecture allows users to deploy computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.\n  \n**Parallelization**: \n  TensorFlow is optimized for high-performance numerical computation, making it suitable for large-scale machine learning tasks that require parallel processing.\n  \n**Use Cases**: \n  TensorFlow is widely used in both academic research and industry for tasks such as image and speech recognition, natural language processing, and more.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "software"
    ],
    "normalized_filename": "tensorflow",
    "outlinks": [
      "handwritten_digit_classification",
      "pytorch_vs_tensorflow",
      "machine_learning",
      "sci-kit_learn",
      "tensorflow"
    ],
    "inlinks": [
      "deep_learning",
      "deep_learning_frameworks",
      "epoch",
      "keras",
      "lstm_in_time_series",
      "pytorch",
      "pytorch_vs_tensorflow",
      "scikit-learn",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "tensorflow"
    ]
  },
  {
    "category": "ML",
    "filename": "Support Vector Regression",
    "sha": "74b1b3f4060f9ea076240dea7d8d34cdf3172a87",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Support%20Vector%20Regression.md",
    "text": "Support Vector Regression use similar principles to [[Support Vector Machines|SVM]]s but for predicting continuous variables.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "regressor"
    ],
    "normalized_filename": "support_vector_regression",
    "outlinks": [
      "support_vector_machines"
    ],
    "inlinks": [
      "machine_learning_algorithms"
    ]
  },
  {
    "category": "ML",
    "filename": "Test Loss When Evaluating Models",
    "sha": "4d2b2241c7eecdb2a7a755d21a0b48e091bf2683",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Test%20Loss%20When%20Evaluating%20Models.md",
    "text": "Test loss is used for [[Model Evaluation]] to assess how well a model generalizes to unseen data, which is essential for evaluating its performance in real-world applications.\n\n## Importance of Test Loss\n\n- Test Accuracy: Indicates the percentage of correct predictions.\n- ==Test Loss: Measures the magnitude of errors in predictions, providing complementary information to accuracy.==\n- Balancing Metrics: Depending on the application, you might prioritize [[Accuracy]] (e.g., in classification tasks) or loss (e.g., when evaluating prediction confidence or calibrating probabilistic models). Balancing both is crucial for most real-world problems.\n\nTest loss is an [[Evaluation Metrics]] that uses the [[Loss function]] to measure the model's performance on new, unseen data.\n## Key Considerations\n\nBalance Between Accuracy and Error Magnitude:\n  - Accuracy reflects the percentage of correct predictions but not the degree of correctness.\n  - Loss can reveal situations where the model is confident but wrong, or struggling despite being correct in many cases, helping to understand prediction quality beyond simple accuracy.\n\n Overfitting or Underfitting Detection:\n  - High accuracy but high loss may indicate overfitting, where the model memorizes patterns rather than learning the underlying structure.\n  - Low accuracy and high loss suggest underfitting, meaning the model hasn't learned the data well enough.\n\nModel Calibration:\n  - In probabilistic models, test loss is crucial for understanding calibration.\n  - A model that’s accurate but poorly calibrated (where predicted probabilities don't match true outcomes) will have low test accuracy but high loss.\n  - For example, in classification tasks, cross-entropy loss indicates how confidently and correctly the model assigns probabilities to each class.\n\n[[Hyperparameter Tuning]]\n  - During hyperparameter tuning (e.g., learning rate, batch size), configurations might yield high accuracy but poor loss (or vice versa).\n  - Considering both metrics provides a balanced view of performance, aiding in fine-tuning the model for both high accuracy and low error.\n\nModel Comparison: [[Model Selection]]\n  - Models with similar accuracy can have significantly different losses.\n  - The model with lower test loss is generally preferred as it suggests reliability in predicting probabilities, especially in critical applications like medical diagnoses or risk assessment.\n\n Outlier Sensitivity: [[uncategorised/Outliers]]/ [[uncategorised/Outliers|Handling Outliers]]\n  - Test loss can help identify model sensitivity to outliers.\n  - A model might achieve high accuracy but perform poorly in terms of test loss if it incorrectly classifies a few outliers.\n  - Conversely, a model with low test loss might be more stable in making predictions, even for edge cases.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "test_loss_when_evaluating_models",
    "outlinks": [
      "accuracy",
      "model_selection",
      "loss_function",
      "hyperparameter_tuning",
      "uncategorised/outliers",
      "model_evaluation",
      "evaluation_metrics"
    ],
    "inlinks": [
      "model_evaluation"
    ]
  },
  {
    "category": "ML",
    "filename": "Text Classification",
    "sha": "7e21df77f64dd8a4e54a071e482df2e249e00bb0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Text%20Classification.md",
    "text": "Text [[Classification]] is an NLP (Natural Language Processing) task where the goal is to assign predefined categories or labels to a given piece of text. \n\n### What does it mean as a problem?\n\n* Input: Raw text (sentence, paragraph, document, tweet, review, etc.)\n* Output: One or more class labels (e.g., “spam” vs. “not spam”, or “positive” vs. “negative” sentiment).\n* Goal: Learn a mapping from text to classes using a model trained on labeled data.\n\nFormally, given:\n\n$$\nX = \\{\\text{text samples}\\}, \\; y = \\{\\text{corresponding labels}\\}\n$$\n\nFind a function: $f: X \\to y$.\n\n### Examples of Text Classification Tasks\n\n* Spam Detection: Classify email as *spam* or *ham*.\n* Sentiment Analysis: Positive / Negative / Neutral review classification.\n* Topic Classification: News articles → Sports, Politics, Tech.\n* Intent Detection: In chatbots, classify user query as *booking*, *cancellation*, etc.\n* Toxic Comment Detection: Safe vs. abusive language.\n\n### Why is it challenging?\n\n* Text is unstructured: needs preprocessing ([[Tokenisation|tokenization]], encoding). See [[Preprocessing Text Classification]]\n* Vocabulary is large and sparse.\n* Ambiguity and context sensitivity in language.\n* Requires [[Feature Extraction]] (e.g., Bag of Words, TF-IDF, word embeddings).\n### Typical Approaches\n\n* Classical ML: Naive Bayes, Logistic Regression, SVM with Bag-of-Words or TF-IDF features.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "classifer",
      "NLP"
    ],
    "normalized_filename": "text_classification",
    "outlinks": [
      "preprocessing_text_classification",
      "feature_extraction",
      "tokenisation",
      "classification"
    ],
    "inlinks": [
      "naive_bayes_classifier",
      "ngrams",
      "preprocessing_text_classification"
    ]
  },
  {
    "category": "ML",
    "filename": "Time Series Python Packages",
    "sha": "19fffa30d0e83c52f961de3723f2c5541e43d1a0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Time%20Series%20Python%20Packages.md",
    "text": "[[Data Visualisation]]\n- [[Plotly]]: Interactive time series charts.time series\n\nForecasting & Modeling\n- [[statsmodels]]: ARIMA, SARIMA, state-space models, [[Statistical Tests]] for stationarity and autocorrelation.\n- [[pmdarima]]: Auto-ARIMA model selection and diagnostics.\n- prophet (by Meta): Decomposable additive models, handles trend, seasonality, and holidays easily.\n- [[ruptures]]: Change point detection for time series.\n\n[[Feature Engineering]] & [[Preprocessing]]\n- tsfresh: Automatic [[Feature Extraction]] from time series for machine learning.\n- featuretools: Feature engineering (can be used with time series if structured correctly).\n- [[Scikit-Learn]]\n\n[[Anomaly Detection]]\n- [[PyOD]]: Comprehensive library for outlier and anomaly detection (can be applied to time series).\n\nOthers:\n- Darts",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection",
      "modeling"
    ],
    "normalized_filename": "time_series_python_packages",
    "outlinks": [
      "plotly",
      "data_visualisation",
      "pyod",
      "feature_extraction",
      "preprocessing",
      "feature_engineering",
      "anomaly_detection",
      "statistical_tests",
      "ruptures",
      "statsmodels",
      "pmdarima",
      "scikit-learn"
    ],
    "inlinks": [
      "time_series"
    ]
  },
  {
    "category": "ML",
    "filename": "Train-Dev-Test Sets",
    "sha": "9a7a2734d014689e7eed02496212a332b5e27a85",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Train-Dev-Test%20Sets.md",
    "text": "In [[Model Building]] , we can train the model using the prepared data to learn patterns and make predictions. The model is trained on your dataset, which is typically divided into three main subsets: training, development (dev), and test sets.\n### Purpose of Each Set\n\n- Training Set ([[training data]]) : Used to fit the model. This is where the model learns the patterns and relationships within the data. The majority of the data is allocated here to ensure the model has enough information to learn effectively.\n\n- Development (Dev) Set: Also known as the [[validation data]], it is used to tune the [[Model Parameters]] and make decisions about model architecture. It helps in preventing overfitting by providing a separate dataset to evaluate the model's performance during training.\n\n- Test Set: Used to evaluate the final model's performance/ [[Model Evaluation]]. This set is not used during the training process and provides an unbiased evaluation of the model's ability to generalize to new, unseen data.\n\n### Why do it this way\n\n**Preventing Overfitting**: By monitoring performance on the validation set, practitioners can detect overfitting early and take corrective actions, such as adjusting model complexity or applying regularization techniques.\n\n**[[Hyperparameter Tuning]]**: The validation set is crucial for tuning hyperparameters (e.g., [[Learning Rate]], regularization strength) to optimize model performance.\n\nHistorical Suggestions\n- Train-Test Sets: 70% training, 30% testing.\n- Train-Dev-Test: 60% training, 20% development, 20% testing.\n\nModern Approach\n- With larger [[Datasets]], a split of 98% training, 1% development, and 1% testing is often used. This is because modern models require more data to learn effectively, and larger datasets allow for smaller proportions to be allocated to dev and test sets while still maintaining sufficient data for evaluation.\n\nCode Example\n```python\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.3)\n```\n\nConsiderations\n- Data Setup: Be careful when setting up the training and test data to ensure they are ==representative== of the problem domain.\n- ==[[Distributions]]: Dev and test sets should be from the same distribution to ensure consistent [[Evaluation Metrics]]. Avoid having subsets that are biased, such as data from the same geographical area.==\n- [[Handling Different Distributions]]: Randomly shuffle the data before splitting to ensure that each subset is representative of the whole dataset.\n- [[Cross Validation]]: Consider using cross-validation techniques to make the most of your data, especially when the dataset is small.\n\nRelated:\n- [[Scikit-Learn]]\n\n### When we have [[Imbalanced Datasets]]\n\n```python\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,random_state=42)\n```\n\nThe `stratify` argument in `train_test_split()` is used to ensure that the **class distribution** in your training and test sets reflects the **original distribution** of the target variable $y$.\n\nIn other words, it performs a **stratified split**, which preserves the proportion of each class across both subsets.\n\nWhy stratification matters\n\nSuppose your dataset has **class imbalance**, e.g.:\n\n| Class | Count |\n| :---- | ----: |\n| 0     |   900 |\n| 1     |   100 |\n\nIf you randomly split this dataset into 80% training and 20% test **without** stratification, the test set might (by chance) end up with very few or even **no examples** of class `1`.\nThat leads to:\n\n* Biased model evaluation (performance on minority class unmeasured),\n* Training data not representative of the whole population.\n\nBy adding `stratify=y`, scikit-learn ensures:\n\n| Subset         | Class 0 | Class 1 |\n| :------------- | :------ | :------ |\n| Training (80%) | 720     | 80      |\n| Test (20%)     | 180     | 20      |\n\nBoth subsets now **preserve the 9:1 ratio** of the original dataset.\n\nCode breakdown\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    stratify=y,\n    random_state=42\n)\n```\n\nWhen to use\n\n✅ Recommended for:\n\n* **Classification tasks** (especially when the classes are imbalanced).\n* **Model evaluation**, to ensure fair representation of all classes.\n\n❌ Not needed for:\n\n* Regression tasks (continuous `y` values), because stratification only works with discrete class labels.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "train-dev-test_sets",
    "outlinks": [
      "training_data",
      "model_building",
      "model_parameters",
      "handling_different_distributions",
      "cross_validation",
      "imbalanced_datasets",
      "hyperparameter_tuning",
      "distributions",
      "model_evaluation",
      "validation_data",
      "learning_rate",
      "datasets",
      "evaluation_metrics",
      "scikit-learn"
    ],
    "inlinks": [
      "model_building"
    ]
  },
  {
    "category": "ML",
    "filename": "Transfer Learning",
    "sha": "5084280cbc3dda8361560bcc5948ed9a420ea3d1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Transfer%20Learning.md",
    "text": "Transfer learning is a technique in machine learning that ==leverages knowledge gained from one setting (source domain) to improve performance on a different but related setting (target domain).== \n\nThe core idea is to train a model on a large dataset in the source domain, learning rich feature representations that capture general patterns and relationships in the data.== These learned representations can then be transferred to the target domain, where they can be fine-tuned with a smaller dataset to achieve good performance on the target task.\n\nTransfer learning makes sense when:\n- It makes sense to do when there is lots of examples for basic layers and training, and there are few of the specialised data set.\n- Task A and B have the same input x.\n- You have a lot more data for Task A than Task B.\n- Low level features from A could be helpful for learning B.\n- When labelled data is scarce.\n\nWe can use pretrained models (i.e. from [[Hugging Face]], [[Keras]] applications, [[PyTorch]] pretrained models, model zoo).\n\nExamples of Transfer Learning\n- **Image Recognition:** A model trained on a large dataset of labelled images (e.g., ImageNet) can learn features like edges, shapes, and textures that are useful for recognising a wide variety of objects. These features can then be transferred to a different image recognition task with a smaller dataset, such as classifying medical images or identifying different species of plants.\n\t- Pretraining - training on image recognition. Fine-training - retraining on radiology. \n\t  \n- **Natural Language Processing:** A language model trained on a massive text corpus can learn word embeddings that capture semantic relationships between words. These embeddings can then be transferred to tasks like sentiment analysis or machine translation, where they can improve performance, even with limited labelled data in the target language.\n\nTypes of Transfer Learning\n- **Unsupervised Pretraining for [[Supervised Learning]]:** The sources describe how unsupervised pretraining with models like denoising autoencoders can be used to learn useful representations that can be transferred to supervised learning tasks.\n- **Cross-Domain Transfer Learning:** This involves transferring knowledge between domains with different input distributions but the same task. \n- **[[Performance Drift]]:** This is a form of transfer learning where the data distribution changes gradually over time. The model needs to adapt to these changes to maintain good performance.\n\nBenefits of Transfer Learning\n- **Improved Generalisation:** By leveraging knowledge from a larger dataset, transfer learning can help models generalise better to new data, especially when the target dataset is small.\n- **Reduced Data Requirements:** Transfer learning can significantly ==reduce the amount of labelled data needed to train a model in the target domain.== This is particularly beneficial for tasks where labelled data is expensive or time-consuming to obtain.\n- **Faster Training:** Fine-tuning a pretrained model on a smaller dataset is typically faster than training a model from scratch.\n### Follow up questions\n\n- Why might fine-tuning a pre-trained model like GPT yield better results than training from scratch\n### Practical Implementation\n\nIn [[ML_Tools]] see: [[transfer_learning.py]]\n\n### Links\n- [Video Overview](https://www.youtube.com/watch?v=yofjFQddwHE&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=19)\n- [Deep Learning Video](https://www.youtube.com/watch?v=DyPW-994t7w&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=15)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "transfer_learning",
    "outlinks": [
      "supervised_learning",
      "hugging_face",
      "pytorch",
      "performance_drift",
      "ml_tools",
      "keras",
      "transfer_learning.py"
    ],
    "inlinks": [
      "bert",
      "deep_learning",
      "distillation",
      "imbalanced_datasets",
      "llm",
      "transfer_learning.py"
    ]
  },
  {
    "category": "ML",
    "filename": "Transformer",
    "sha": "09dd6eac051f5cbbb754afbd35dceb16e982bd4f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Transformer.md",
    "text": "A transformer in machine learning (ML) refers to a deep learning model architecture designed to process sequential data, such as natural language processing ([[NLP]]). It was introduced in the paper \"[[uncategorised/Attention Is All You Need]]\" and has since become a cornerstone in NLP tasks.\n\nTransformers excel at handling sequence-based data and are particularly known for their self-attention mechanisms [[Attention mechanism]], which allow them to process long-range dependencies in data.\n\n### Key Concepts of a Transformer\n\n1. Architecture Overview:\n    - A transformer model consists of an encoder and a decoder, although some models use only the encoder (like [[BERT]] only consists of encoders) or only the decoder (like GPT3). Each of these components is made up of layers that include mechanisms for attention and [[Feed Forward Neural Network]].\n    - Encoder learns the context, decoder does the task.\n\n2. Self-[[Attention mechanism]]:\n    - The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence relative to each other. This is crucial for understanding context and relationships in language. See [[Attention mechanism]].\n    - Scaled Dot-Product Attention: For each word in a sentence, the model computes attention scores with every other word. These scores are used to create a weighted representation of the input, emphasizing relevant words and de-emphasizing less relevant ones.\n\n3. [[Multi-head attention]]\n    - Instead of having a single attention mechanism, transformers use multiple attention heads. Each head learns different aspects of the relationships between words, allowing the model to capture various linguistic features.\n\n4. Positional Encoding:\n    - Since transformers do not inherently understand the order of words (unlike [[Recurrent Neural Networks|RNNs]]), they use positional encoding to inject information about the position of each word in the sequence.\n\n5. Feed-Forward Neural Network:\n    - After the attention mechanism, the output is passed through a feed-forward neural network, which is applied independently to each position.\n      \n6. Layer Normalization and Residual Connections:\n    - Transformers use layer normalization and residual connections to stabilize training and help with gradient flow, making it easier to train deep networks.\n\n7. Training and Applications:\n    - Transformers are trained on large corpora of text data using [[Unsupervised Learning|unsupervised]] or semi-supervised learning techniques. They are used for a variety of NLP tasks, including translation, summarization, and question answering.\n\n### Additional Concepts\n\n- Encoder-Decoder Structure:\n    - The encoder processes the input sequence to build a representation (maintaining [[Semantic Relationships]]), while the decoder takes this representation and generates the output sequence (next word prediction).\n\n- Parallelization:\n    - Unlike Recurrent Neural Networks ([[Recurrent Neural Networks]]), transformers do not require sequential processing, making them more efficient, especially when training large datasets.\n\nFollow-up questions:\n- [[Transformers vs RNNs]]",
    "aliases": [
      "Transformers"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "normalized_filename": "transformer",
    "outlinks": [
      "bert",
      "semantic_relationships",
      "attention_mechanism",
      "unsupervised_learning",
      "feed_forward_neural_network",
      "recurrent_neural_networks",
      "multi-head_attention",
      "transformers_vs_rnns",
      "uncategorised/attention_is_all_you_need",
      "nlp"
    ],
    "inlinks": [
      "attention_mechanism",
      "bert",
      "ds_&_ml_portal",
      "hugging_face",
      "llm",
      "lstm",
      "mathematical_reasoning_in_transformers",
      "nltk",
      "positional_encoding",
      "rag",
      "recurrent_neural_networks",
      "self-attention",
      "sentence_transformers",
      "tokenisation",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "word2vec.py"
    ]
  },
  {
    "category": "ML",
    "filename": "Transformed Target Regressor",
    "sha": "5fd40ae4e2b16b09fd0d0a50af0a4b13dc294349",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Transformed%20Target%20Regressor.md",
    "text": "[[Scikit-Learn]]\n\nThe `TransformedTargetRegressor` is a utility class in `scikit-learn` that applies a transformation to the target values in a regression problem. This can be useful in several scenarios:\n\n1. **Non-normal target distribution**: Many regression algorithms assume that the target variable is normally distributed. If your target variable has a skewed distribution, applying a transformation (like a [[Log transformation]]) can help improve the performance of the model.\n    \n2. **Heteroscedasticity**: This is a situation where the variance of the error terms in a regression model is not constant. In such cases, applying a transformation to the target variable can help stabilize the variance and improve the model's performance.\n    \n3. **Non-linear relationships**: If the relationship between the predictors and the target variable is not linear, a transformation can help capture the non-linearity.\n\nThe `TransformedTargetRegressor` applies the transformation before training the model and automatically applies the inverse transformation when making predictions. This makes it easier to work with transformed target variables, as you don't have to manually apply the inverse transformation every time you want to make a prediction.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "regressor",
      "transformation"
    ],
    "normalized_filename": "transformed_target_regressor",
    "outlinks": [
      "log_transformation",
      "scikit-learn"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Transformers vs RNNs",
    "sha": "d8517273d9ce7efb324a58d7d50760245872384f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Transformers%20vs%20RNNs.md",
    "text": "[[Transformer|Transformers]] and Recurrent Neural Networks ([[Recurrent Neural Networks]]) are both deep learning architectures ==used for processing sequential data==, but they differ significantly in structure, operation, and performance.\n\nWhile RNNs have been essential for sequence modeling, transformers have become the dominant architecture in ML due to their ability to handle large-scale data and long-range dependencies more efficiently. \n\nRNNs still have use cases, especially for tasks where memory constraints are critical ==or for smaller datasets==, but transformers are the go-to solution for most modern ML applications.\n\n### Summary Table:\n\n| Aspect                      | RNNs                               | Transformers                     |\n| --------------------------- | ---------------------------------- | -------------------------------- |\n| Architecture            | Sequential (step-by-step)          | Parallel (process all at once)   |\n| Attention               | Implicit through hidden states     | Explicit via self-attention      |\n| Parallelization         | Not parallelizable                 | Fully parallelizable             |\n| Handling Long Sequences | Struggles with long dependencies   | Excellent with long dependencies |\n| Efficiency              | Slower training                    | Faster due to parallelization    |\n| Scalability             | Poor scalability to long sequences | Scalable but memory-intensive    |\n| Use Cases               | Time-series, small datasets        | NLP, large datasets, vision      |\n### 1. Architecture\n- RNNs: \n  - RNNs ==process data sequentially,== one time step at a time. They maintain a ==hidden state== that is updated as the model processes each token in the sequence, making them suitable for time-dependent tasks.\n  - Common variants include [[LSTM]] (Long Short-Term Memory) and [[GRU]] ([[Gated Recurrent Units]]), which are designed to capture ==long-term dependencies== more effectively.\n\n- [[Transformer]]:\n  - Transformers do not process data sequentially. Instead, they ==process the entire sequence in parallel==, allowing them to ==model relationships between tokens regardless of their position==. This is achieved through the self-attention mechanism.\n  - Transformers include positional encodings to account for the order of tokens, since their architecture doesn't have an inherent understanding of sequence order.\n\n### 2. Processing Mechanism\n- RNNs:\n  - RNNs depend on the previous hidden state to process the next token, which means they inherently process information sequentially.\n  - The ==hidden state is updated at each time step==, which can lead to issues like [[vanishing and exploding gradients problem]], especially in long sequences, making it ==difficult for RNNs to capture long-range dependencies==.\n\n- Transformers:\n  - Transformers use [[Attention mechanism]] to allow each token to interact directly with every other token in the sequence. This allows transformers to capture long-range dependencies more effectively and efficiently.\n  - The self-attention mechanism enables ==parallelization== of the computation for all tokens in the sequence, which speeds up training and inference.\n\n### 3. Parallelization and Efficiency\n- RNNs:\n  - Since RNNs must process sequences one step at a time, they ==cannot be easily parallelized==. This makes them less efficient, especially for long sequences.\n  - RNNs are also slower to train because of this sequential dependency.\n\n- Transformers:\n  - Transformers can process entire sequences at once, making it easier to parallelize computation, especially on GPUs. This leads to much faster training times compared to RNNs.\n  - This parallelization is a major reason transformers have become the preferred model in large-scale tasks.\n\n### 4. Handling Long-Term Dependencies\n- RNNs:\n  - RNNs often struggle with capturing long-term dependencies because the information must be passed through multiple time steps, which can lead to forgetting or corruption of information over long sequences.\n  - ==LSTMs and GRUs were developed to mitigate this problem, but they are still not as effective as transformers for capturing long-range relationships.==\n\n- Transformers:\n  - The self-attention mechanism in transformers allows the model to directly connect tokens from distant parts of the sequence. This makes transformers much better at modeling long-range dependencies.\n  - Transformers can also model relationships across sequences regardless of their length, leading to better performance on tasks requiring a global understanding of the data.\n\n### 5. Memory and Scalability\n- RNNs:\n  - RNNs are relatively ==more memory-efficient for shorter sequences== but become inefficient for longer ones due to their sequential nature and the need to store hidden states at each time step.\n  - They also scale poorly to long sequences or large datasets because of the need to compute one step at a time.\n\n- Transformers:\n  - Transformers, while faster, require more memory due to the computation of attention matrices, which scale quadratically with the sequence length. This can be a bottleneck for very long sequences or resource-constrained environments.\n  - New transformer variants (e.g., [[Longformer]] or [[Reformer]]) have been introduced to improve memory efficiency for longer sequences.\n\n### 6. Use Cases\n- RNNs:\n  - Traditionally used for [[time-series data]], speech recognition, and sequence generation tasks.\n  - They are useful when the order of data is crucial and when handling smaller datasets or shorter sequences.\n\n### 7. Performance\n- RNNs:\n  - While effective in small-scale, low-latency tasks, RNNs often perform worse than transformers on complex tasks that involve large-scale data or long-range dependencies.\n\n- Transformers:\n  - Transformers have significantly outperformed RNNs in most tasks requiring sequential data processing, particularly in NLP. Pre-trained models like [[BERT]], GPT, and T5 are based on the transformer architecture and have set state-of-the-art results in many benchmarks.\n\nTransformer-based models like [[BERT]] and GPT outperform traditional RNNs in NLP tasks for several key reasons:\n\n1. Parallelization: Unlike [[Recurrent Neural Networks|RNNs]], which process sequences sequentially (one time step at a time), transformers can process entire sequences in parallel. This significantly speeds up training and allows for more efficient use of computational resources.\n\n2. Self-[[Attention mechanism]]: Transformers utilize a self-attention mechanism that enables them to weigh the importance of different words in a sentence relative to each other. This allows the model to capture long-range dependencies and relationships between words more effectively than RNNs, which often struggle with long-term dependencies due to their sequential nature.\n\n3. Handling Long Sequences: RNNs, especially vanilla ones, can suffer from issues like vanishing and exploding gradients, making it difficult to learn from long sequences. Transformers, on the other hand, can directly connect tokens from distant parts of the sequence, making them much better at modeling long-range dependencies.\n\n4. Multi-Head Attention: Transformers employ multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously. Each attention head can learn different aspects of the relationships between words, enhancing the model's ability to understand context and meaning.\n\n5. Positional Encoding: Since transformers do not inherently understand the order of words, they use positional encoding to inject information about the position of each word in the sequence. This allows them to maintain the sequential nature of language while still benefiting from parallel processing.\n\n6. Scalability and Performance: Transformers have shown to be more scalable and perform better on large datasets, which is crucial for many NLP tasks. Pre-trained models like BERT and GPT have set state-of-the-art results in various benchmarks due to their architecture and training methodologies.\n\nThe combination of parallel processing, self-attention mechanisms, and the ability to handle long-range dependencies makes transformer-based models like BERT and GPT significantly more effective than traditional RNNs in NLP tasks. \n\nFor further reading, you can refer to the note on [[Transformers vs RNNs]] for a detailed comparison of their architectures and performance.\n\n#### Sources:\n- [Transformer](obsidian://open?vault=content&file=Transformer)\n- [Transformers vs RNNs](obsidian://open?vault=content&file=Transformers%20vs%20RNNs)\n- [BERT](obsidian://open?vault=content&file=BERT)\n- [LSTM](obsidian://open?vault=content&file=LSTM)\n- [Attention mechanism](obsidian://open?vault=content&file=Attention%20mechanism)\n- [Mathematical Reasoning in Transformers](obsidian://open?vault=content&file=Mathematical%20Reasoning%20in%20Transformers)\n- [Recurrent Neural Networks](obsidian://open?vault=content&file=Recurrent%20Neural%20Networks)\n- [Transfer Learning](obsidian://open?vault=content&file=Transfer%20Learning)\n- [Multi-head attention](obsidian://open?vault=content&file=Multi-head%20attention)\n- [BERT Pretraining of Deep Bidirectional Transformers for Language Understanding](obsidian://open?vault=content&file=BERT%20Pretraining%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding)\n- [LLM](obsidian://open?vault=content&file=LLM)\n- [Evaluating Language Models](obsidian://open?vault=content&file=Evaluating%20Language%20Models)\n- [Bert Pretraining](obsidian://open?vault=content&file=Bert%20Pretraining)\n- [Reasoning tokens](obsidian://open?vault=content&file=Reasoning%20tokens)\n- [NLP](obsidian://open?vault=content&file=NLP)\n- [Hugging Face](obsidian://open?vault=content&file=Hugging%20Face)\n- [Questions](obsidian://open?vault=content&file=Questions)\n- [Neural network](obsidian://open?vault=content&file=Neural%20network)\n- [Boosting](obsidian://open?vault=content&file=Boosting)\n- [Named Entity Recognition](obsidian://open?vault=content&file=Named%20Entity%20Recognition)\n- [Deep Learning](obsidian://open?vault=content&file=Deep%20Learning)\n- [Language Model Output Optimisation](obsidian://open?vault=content&file=Language%20Model%20Output%20Optimisation)\n- [Small Language Models](obsidian://open?vault=content&file=Small%20Language%20Models)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "transformers_vs_rnns",
    "outlinks": [
      "lstm",
      "bert",
      "reformer",
      "attention_mechanism",
      "transformer",
      "time-series_data",
      "gru",
      "recurrent_neural_networks",
      "gated_recurrent_units",
      "transformers_vs_rnns",
      "vanishing_and_exploding_gradients_problem",
      "longformer"
    ],
    "inlinks": [
      "transformer",
      "transformers_vs_rnns"
    ]
  },
  {
    "category": "ML",
    "filename": "Type I Error (False Positive)",
    "sha": "65ac372e5254c020467ab6622df2a403805af176",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Type%20I%20Error%20(False%20Positive).md",
    "text": "### Type I Error\n- **Definition**: A Type I error occurs when the model incorrectly predicts the positive class. In other words, it identifies a negative instance as positive.\n- **Example**: If a model predicts that an email is spam (positive) when it is actually not spam (negative), this is a Type I error.\n- **Consequences**: Type I errors can lead to unnecessary actions or consequences, such as misclassifying legitimate emails as spam, which may result in important messages being missed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "type_i_error_(false_positive)",
    "outlinks": [],
    "inlinks": [
      "statistics",
      "why_type_1_and_type_2_matter"
    ]
  },
  {
    "category": "ML",
    "filename": "Type II Error (False Negative)",
    "sha": "dbcb7505ff7f0ab8fe786603a496db901db7c879",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Type%20II%20Error%20(False%20Negative).md",
    "text": "### Type II Error (False Negative)\n- **Definition**: A Type II error occurs when the model incorrectly predicts the negative class. This means it fails to identify a positive instance.\n- **Example**: If a model predicts that an email is not spam (negative) when it is actually spam (positive), this is a Type II error.\n- **Consequences**: Type II errors can lead to missed opportunities or risks, such as allowing spam emails to clutter the inbox or failing to detect a disease in a medical diagnosis scenario.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation"
    ],
    "normalized_filename": "type_ii_error_(false_negative)",
    "outlinks": [],
    "inlinks": [
      "why_type_1_and_type_2_matter"
    ]
  },
  {
    "category": "ML",
    "filename": "Types of Neural Networks",
    "sha": "0d1a462fcb563630d61be577d93a19bd6209e3da",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Types%20of%20Neural%20Networks.md",
    "text": "Types of [[Neural network]]:\n\n[[Feed Forward Neural Network]]\n\n[[Convolutional Neural Networks]]\n\n[[Recurrent Neural Networks]]\n\n[[Generative Adversarial Networks]]\n\n[[Transformer]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml"
    ],
    "normalized_filename": "types_of_neural_networks",
    "outlinks": [
      "transformer",
      "feed_forward_neural_network",
      "recurrent_neural_networks",
      "generative_adversarial_networks",
      "convolutional_neural_networks",
      "neural_network"
    ],
    "inlinks": [
      "neural_network"
    ]
  },
  {
    "category": "ML",
    "filename": "Typical Output Formats in Neural Networks",
    "sha": "07df28f61d2fde3837e1ccd592e0d01e73dcaa51",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Typical%20Output%20Formats%20in%20Neural%20Networks.md",
    "text": "The output format of a [[Neural network]] is largely determined by the specific task it is designed to perform.\n## Classification\n\n### [[Binary Classification]]\n\n Single Output Node: This involves a single output node with a value between 0 and 1, representing the probability of the input belonging to the positive class.\n\nExample: A spam classifier might output a value close to 1 for a spam email and a value close to 0 for a legitimate email.\n\n### Multiclass [[Classification]]\n\n Multiple Output Nodes: Each class has its own output node, with values typically between 0 and 1, representing the probability of the input belonging to that class. These probabilities often sum to 1.\n\nExample: An image classifier for different types of animals (cat, dog, bird) might output a vector like [0.2, 0.7, 0.1], indicating a 70% probability of the image being a dog.\n\n## [[Regression]]\n\nSingle Output Node: This involves a single output node representing a continuous value.\n\nExample: A neural network predicting house prices would output a single value representing the predicted price.\n\n## Sequence to Sequence Tasks\n\n Sequence of Outputs: The output is often represented as a list or a tensor.\n \nExample: A neural machine translation model would output a sequence of words or subword units in the target language.\n\nExample Applications\n- **Machine Translation:** Converts a sentence from one language to another.\n- **Text Summarization:** Generates a concise summary from a longer text.\n- **Speech Recognition:** Transcribes spoken language into written text.\n## Generative Tasks (e.g., Image Generation, Music Composition)\n\nData in the Same Format as the Input: The output is typically in the same format as the input data.\n \nExample: An image generation model might output a tensor representing a generated image.\n\nSee [[Generative AI]]\n## Key Considerations\n\n[[Activation Function]]: The choice of activation function in the output layer can significantly influence the output format. \n\nLoss Functions: The [[Loss function]] used during training also guides the output format. For example, binary crossentropy is commonly used for binary classification, while mean squared error is often used for regression.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "algorithm",
      "deep_learning",
      "exploration"
    ],
    "normalized_filename": "typical_output_formats_in_neural_networks",
    "outlinks": [
      "regression",
      "binary_classification",
      "generative_ai",
      "loss_function",
      "activation_function",
      "neural_network",
      "classification"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "UMAP",
    "sha": "5e751cba343186b4e63fd6d2b670b6698a91bcf2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/UMAP.md",
    "text": "UMAP is a [[Dimensionality Reduction]] technique, similar to [[t-SNE]].\n\nCan be used to visualise the embeddings by applying UMAP to see whether any [[Clustering]] exists within the dataset.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "visualization"
    ],
    "normalized_filename": "umap",
    "outlinks": [
      "t-sne",
      "clustering",
      "dimensionality_reduction"
    ],
    "inlinks": [
      "evaluate_embedding_methods",
      "t-sne"
    ]
  },
  {
    "category": "ML",
    "filename": "Unsupervised Learning",
    "sha": "112ba0b6e15d15b853064fa714c2923d2df46d32",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Unsupervised%20Learning.md",
    "text": "Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or predefined outputs. \n\nUnsupervised learning involves discovering hidden patterns in data without predefined labels. It is valuable for exploratory data analysis, [[Clustering]], and [[Isolated Forest]].\n\nThe goal is to find hidden patterns, relationships, or structures in the data. Unlike supervised learning, which uses labeled input-output pairs, unsupervised learning relies solely on input data, allowing the algorithm to uncover insights independently.\n\n### Key Concepts\n\n1. No Labeled Data: There is no ground truth or correct output associated with the input data.\n2. Data Patterns: The algorithm identifies inherent structures, clusters, or associations within the dataset.\n3. Objective: The primary objective is to explore the data and organize it to reveal underlying patterns.\n\n### Common Types of Unsupervised Learning\n\n#### [[Clustering]]\n\nDescription: The algorithm groups similar data points together based on their features.\n\nExample: Customer segmentation in marketing, where a clustering algorithm divides customers into groups based on purchasing behavior, demographics, or browsing history.\n\nPopular Algorithms:\n  - [[K-means]]: Divides the data into \\( k \\) clusters, where each data point belongs to the nearest cluster.\n  - Hierarchical Clustering\n  - [[DBSCAN]]\n  - [[Support Vector Machines]]\n  - [[K-nearest neighbours]]\n\n#### [[Dimensionality Reduction]]\n\nDescription: Reduces the number of input variables (features) while preserving as much information as possible. This is helpful for high-dimensional data, where visualization and analysis become challenging.\n\nPopular Algorithms:\n  - [[Principal Component Analysis]] \n\n#### [[Isolated Forest]]\n\nDescription: Identifies [[uncategorised/Outliers]] or unusual data points that don’t conform to the expected pattern in the dataset.\n\nExample: Detecting fraudulent credit card transactions by identifying transactions that deviate significantly from typical spending patterns.\n\nMechanism: Works by randomly partitioning the data and identifying [[uncategorised/Outliers|anomalies]] as points that can be isolated quickly.",
    "aliases": [
      "unsupervised"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "#clustering",
      "field"
    ],
    "normalized_filename": "unsupervised_learning",
    "outlinks": [
      "isolated_forest",
      "clustering",
      "k-means",
      "principal_component_analysis",
      "uncategorised/outliers",
      "dimensionality_reduction",
      "support_vector_machines",
      "k-nearest_neighbours",
      "dbscan"
    ],
    "inlinks": [
      "clustering",
      "ds_&_ml_portal",
      "k-means",
      "latent_dirichlet_allocation",
      "learning_styles",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "transformer"
    ]
  },
  {
    "category": "ML",
    "filename": "Use Cases for a Simple Neural Network Like",
    "sha": "037b32b85484b31050f77d36785f5454a2341128",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Use%20Cases%20for%20a%20Simple%20Neural%20Network%20Like.md",
    "text": "Scenarios where a simple [[Neural network|Neural Network]] work like this might be useful:\n\n**[[Regression]] with Multiple Features**\nIf you have multiple input features and you want to predict a continuous output, this network can learn the appropriate weights for each feature. For instance:\n- Predicting **fuel efficiency** of a car based on features like engine size, horsepower, and weight.\n- Predicting **sales** based on multiple factors like marketing spend, seasonality, and economic indicators.\n\n**[[Binary Classification]]**\nWith slight modification (e.g., adding a **Sigmoid activation** to the output layer), you could use this network for binary classification tasks. For example:\n- Classifying whether an email is **spam** or not based on features like word frequency and sender information.\n  \n**Multi-Feature [[Time Series Forecasting]]**\nIf you have time series data with multiple variables, you can feed it into this simple network to predict future values based on past trends. For instance:\n- Predicting **stock prices** based on multiple features like historical prices, trading volume, and economic data.\n\n**Training and Optimization (Next Steps)**\nThe provided code only defines the network and performs a **forward pass**, but to use this model for real-world tasks, you would need to:\n- **Define a loss function** (e.g., [[Mean Squared Error]] for regression or Cross-Entropy Loss for classification).\n- **Train the network** using an optimizer like **[[Stochastic Gradient Descent]] (SGD)**, **[[Adam Optimizer]]**, or another optimization algorithm.\n- **Backpropagate** the gradients to update the model’s weights using gradient descent.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "use_cases_for_a_simple_neural_network_like",
    "outlinks": [
      "time_series_forecasting",
      "regression",
      "binary_classification",
      "adam_optimizer",
      "mean_squared_error",
      "stochastic_gradient_descent",
      "neural_network"
    ],
    "inlinks": [
      "pytorch"
    ]
  },
  {
    "category": "ML",
    "filename": "Variability in linear models",
    "sha": "20a894f3e0e62e45e0d3f135b11af1e1cc350021",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Variability%20in%20linear%20models.md",
    "text": "Variability in linear models\n\nMathematically, SSE is a [[Loss function]] given by:\n\n$$SSE = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n\nwhere $N$ is the number of observations, $y_i$ is the actual value of the dependent variable for observation $i$, and $\\hat{y}_i$ is the predicted value based on the linear regression model.\n\nThe formula for the Regression Sum of Squares (SSR) in the context of [[Linear Regression]] is:\n\n$$SSR = \\sum_{i=1}^{N} (\\hat{y}_i - \\bar{y})^2 $$\n\nWhere:\n- $\\hat{y}_i$ is the predicted value of the dependent variable for observation $i$ based on the linear regression model.\n- $\\bar{y}$ is the mean of the observed values of the dependent variable.\n- $N$ is the total number of observations.\n\nSSR measures the amount of variability in the dependent variable that is explained by the independent variables in the model. It ==reflects how well the regression model captures the relationship between the independent and dependent variables.==\n\nTotal Sum of Squares (SST) represents the total variability in the dependent variable $y$. The relationship between SST, SSR, and SSE is given by:\n\n $SST = SSR + SSE$ \n\nThis equation reflects the decomposition of total variability into explained variability (SSR) and unexplained variability (SSE) due to errors.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "variability_in_linear_models",
    "outlinks": [
      "linear_regression",
      "loss_function"
    ],
    "inlinks": [
      "linear_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Variance in ML",
    "sha": "fddb8a31786aa439031ef49f629c1cf6bc1c91a0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Variance%20in%20ML.md",
    "text": "Variance measures how much a model’s predictions change when trained on different subsets of the training data. It reflects the model’s sensitivity to the specific data used for training.\n\n#### High Variance\n\n* The model is too sensitive to the training data.\n* Captures noise as well as signal; poor generalization.\n* Symptom: Good performance on training data, poor performance on test data (overfitting).\n#### Bias–Variance Context\n\n* Variance = Error due to instability in the model (changes drastically with small variations in training data).\n* High flexibility; higher variance.\n* Low variance models = more stable but less flexible.\n\n#### Intuition\n\nIf you slightly change the training data and the predictions change a lot; high variance.\nIf predictions remain almost the same; low variance.\n\n#### Key Point\n\nVariance is about model sensitivity and stability:\n\n$$\n\\text{High Variance} \\implies \\text{Overfitting}, \\quad \\text{Low Variance} \\implies \\text{More stable but risk of underfitting}.\n$$\nRelated:\n- [[Bias in ML]]\n- [[Bias-Variance Trade Off]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "variance_in_ml",
    "outlinks": [
      "bias_in_ml",
      "bias-variance_trade_off"
    ],
    "inlinks": [
      "bias_in_ml",
      "decision_trees_are_fragile",
      "variance"
    ]
  },
  {
    "category": "ML",
    "filename": "WCSS and elbow method",
    "sha": "4b795589e91dc375f2e0f288c63ee58f6aceef02",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/WCSS%20and%20elbow%20method.md",
    "text": "USE: WCSS (within-cluster sum of squares)\n\nWCSS is a measure developed within the [[ANOVA]] framework. It gives a very good idea about the different distance between different clusters and within clusters, thus providing us a rule for deciding the appropriate number of clusters.\n\nThe plot will resemble an \"elbow,\" and the goal is to find the point where the decrease in WCSS slows down, forming an elbow-like shape.\n\nElbow numbers are the point where the rate of decrease in WCSS starts to flatten out\n\nThe rationale behind the elbow method is that \n\nRationale: as you increase the number of clusters (K), the WCSS will generally decrease because each cluster becomes smaller. However, there is a point where the addition of more clusters provides diminishing returns in terms of reducing WCSS. The elbow point represents a good balance between capturing the variance in the data and avoiding excessive fragmentation.\n\n## Code\n\n\n```python\n\n# Use WCSS and elbow method\n# number of clusters\nwcss=[]\nstart=2\nend=10\n# Create all possible cluster solutions with a loop\nfor i in range(start,end):\n    # Cluster solution with i clusters\n    kmeans = KMeans(i)\n    # Fit the data\n    kmeans.fit(df_scaled)\n    # Find WCSS for the current iteration\n    wcss_iter = kmeans.inertia_\n    # Append the value to the WCSS list\n    wcss.append(wcss_iter)\n\n  \n\n# Create a variable containing the numbers from 1 to 6, so we can use it as X axis of the future plot\n\nnumber_clusters = range(start,end)\n\n# Plot the number of clusters vs WCSS\n\nplt.plot(number_clusters,wcss)\n\n# Name your graph\n\nplt.title('The Elbow Method')\n# Name the x-axis\nplt.xlabel('Number of clusters')\n# Name the y-axis\nplt.ylabel('Within-cluster Sum of Squares')\n\n# Identify the elbow numbers (there may be more than one thats best)\nelbow_nums=[4,5,6,7,8]\n```\n\nplotting\n```python\n# function to give scatter for each elbow number\n    \ndef scatter_elbow(X, elbow_num, var1, var2):\n    \"\"\"\n    Apply clustering with elbow method and plot a scatter plot with cluster information.\n\n    Parameters:\n    - X: DataFrame, input data for clustering\n    - elbow_num: int, number of clusters determined by elbow method\n    - var1, var2: str, names of the variables for the scatter plot\n\n    Returns:\n    None (plots the scatter plot)\n    \"\"\"\n    # Apply [[clustering]] with elbow number\n    kmeans = KMeans(elbow_num)\n    kmeans.fit(X)\n\n    # Add cluster information\n    identified_clusters = kmeans.fit_predict(X)\n    X['Cluster'] = identified_clusters\n\n    # Plot\n    plt.scatter(X[var1], X[var2], c=X['Cluster'], cmap='rainbow')\n    plt.xlabel(var1)\n    plt.ylabel(var2)\n    plt.title(f\"{elbow_num}-Clustering for {var1}-{var2}\")\n    plt.show()\n# Example usage:\n# scatter_elbow(data, elbow_num, 'var1', 'var2')\nfor elbow_num in elbow_nums:\n    scatter_elbow(df, elbow_num, var1, var2)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "clustering"
    ],
    "normalized_filename": "wcss_and_elbow_method",
    "outlinks": [
      "anova",
      "clustering"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters",
      "inertia_k_means_cost_function",
      "k-means"
    ]
  },
  {
    "category": "ML",
    "filename": "Vector Embedding",
    "sha": "7c6c24afc0562f98a2b2750f155977c941da6485",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Vector%20Embedding.md",
    "text": "Vector Embedding is a technique used in machine learning and [[NLP]] to represent data in a continuous vector space. This representation captures the [[Semantic Relationships]] of data, such as words or sentences, allowing similar items to be positioned close to each other in the vector space.\n### Key Concepts\n\n- Data Compression: Embeddings compress data into a lower-dimensional space, making it easier to process and analyze. This is particularly useful for high-dimensional data like text or images.\n  \n- Semantic Similarity: In the embedding space, similar items are positioned close to each other. This proximity reflects semantic similarity, meaning that items with similar meanings or characteristics have similar vector representations.\n\n1. [[Dimensionality Reduction]]: Words are represented in a lower-dimensional space compared to traditional methods like one-hot encoding, resulting in more efficient computations.\n\n2. [[Semantic Relationships]]: Words with similar meanings or contexts are located close to each other in the vector space. For example, \"king\" and \"queen\" might be closer to each other than \"king\" and \"apple.\"\n\n![[Pasted image 20241015211934.png]]\n\n4. Contextual Understanding: (Vector) Word embeddings capture the context in which words appear, allowing models to understand nuances and relationships in language.\n\nPopular methods for generating vector (word) embeddings include:\n- [[Word2vec]],\n- GloVe, \n- FastText.\n- [[spaCy]]\n\n### Types of Similarity Measures\n\n- Euclidean Distance\n- [[Cosine Similarity]]\n\n### Applications\n\n- [[Language Models]]: Vector embeddings are widely used in language models to represent words, phrases, or sentences, enabling models to understand and generate human language more effectively.\n- [[Attention mechanism]]: Embeddings are often used with attention mechanisms to enhance model performance in tasks like translation, summarization, and question answering.\n\n### Example Use Cases\n\n- Word Embeddings: Techniques like Word2Vec and GloVe create word embeddings that capture semantic relationships between words, enabling tasks like word similarity and analogy solving.\n- Sentence Embeddings: Models like [[BERT]] and Sentence Transformers generate embeddings for entire sentences, facilitating tasks like sentiment analysis and semantic search.\n\n### Visualizations\n\n- [[t-SNE]]: A technique for visualizing high-dimensional data, often used to display word embeddings in a two-dimensional space.\n\n\n\n\n![[Pasted image 20241015211844.png]]\n## Implementation\n\nHow to do vector embeddings in [[PyTorch]] that show [[Semantic Relationships]] between terms.\n## Articles\n\nhttps://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85\n\n## Related Terms\n\n- [ ] [[How to search within a graph]]\n- [[How would you decide between using TF-IDF and Word2Vec for text vectorization]]\n- [[embeddings for OOV words]]",
    "aliases": [
      "embedding",
      "word embedding"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "math",
      "ML_Tools"
    ],
    "normalized_filename": "vector_embedding",
    "outlinks": [
      "bert",
      "t-sne",
      "how_would_you_decide_between_using_tf-idf_and_word2vec_for_text_vectorization",
      "semantic_relationships",
      "attention_mechanism",
      "spacy",
      "embeddings_for_oov_words",
      "how_to_search_within_a_graph",
      "cosine_similarity",
      "pasted_image_20241015211844.png",
      "dimensionality_reduction",
      "pasted_image_20241015211934.png",
      "nlp",
      "language_models",
      "pytorch",
      "word2vec"
    ],
    "inlinks": [
      "bert",
      "embeddings_for_oov_words",
      "evaluate_embedding_methods",
      "faiss",
      "how_llms_store_facts",
      "how_to_search_within_a_graph",
      "llm",
      "nlp",
      "positional_encoding",
      "sentence_similarity",
      "sentence_transformer_workflow",
      "sentence_transformers",
      "similarity_search",
      "vector_database",
      "word2vec"
    ]
  },
  {
    "category": "ML",
    "filename": "Weak Learners",
    "sha": "5c375c022874b3983192bfc8784714c883699098",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Weak%20Learners.md",
    "text": "Weak learners are simple models that perform slightly better than random guessing. They are often used as the building blocks in [[Model Ensemble]] methods to create a strong predictive model.\n\n## Characteristics\n\n- **Simplicity:** Weak learners are typically simple models, such as [[Decision Tree]] stumps, which split the data based on a single feature.\n- **Performance:** Individually, they may not perform well, but when combined, they can produce a powerful ensemble model.\n\n## Role in Model Ensembling\n\nWeak learners are a crucial component of [[Model Ensemble]] techniques, such as boosting and bagging, where multiple weak learners are combined to improve overall model performance.\n\n## Learning Rate\n\n- The [[Learning Rate]] is a [[Hyperparameter]]that controls the contribution of each weak learner to the final ensemble model.\n- A smaller learning rate means that each weak learner has a smaller impact, often requiring more learners to achieve good performance.\n\n## Examples\n\n- AdaBoostClassifier",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "weak_learners",
    "outlinks": [
      "decision_tree",
      "learning_rate",
      "hyperparameter",
      "model_ensemble"
    ],
    "inlinks": [
      "boosting",
      "gradient_boosting"
    ]
  },
  {
    "category": "ML",
    "filename": "When and why not to us regularisation",
    "sha": "c6b06fb57d934c98fc43d00ac423f179f811f386",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/When%20and%20why%20not%20to%20us%20regularisation.md",
    "text": "While regularization is tool to combat overfitting, it is not a always useful. It is necessary to consider the model's \n- complexity,\n- the quality and \n- quantity of data, \n- and the appropriateness of the regularization parameters\n\nto ensure effective performance on validation data. If your model is performing well on [[training data]] but poorly on [[validation data]], regularization might not always solve this issue for several reasons:\n\n1. Underfitting: While regularization aims to reduce overfitting, it can also lead to underfitting if the penalty is too strong. This occurs when the model becomes too simplistic and fails to capture the underlying patterns in the data, resulting in poor performance on both training and validation datasets.\n\n2. Model Complexity: Regularization primarily addresses the complexity of the model. If the model architecture itself is not suitable for the task (e.g., too simple or inappropriate for the [[Data Distribution]]), regularization won't help improve performance. The model may still struggle to learn the necessary features, leading to poor validation performance.\n\n3. Insufficient Data: If the training dataset is small or not representative of the validation dataset, regularization may not compensate for the lack of data. The model might learn noise or irrelevant patterns from the training data, which regularization cannot correct.\n\n4. Improper Regularization Parameter ($\\lambda$): The effectiveness of regularization depends on the choice of the regularization parameter $\\lambda$. If $\\lambda$ is set too high, it can overly penalize the model's parameters, leading to underfitting. Conversely, if it's too low, it may not sufficiently reduce overfitting.\n\n5. Feature Interaction: Regularization techniques like [[L1 Regularisation|L1]] and [[Ridge|L2]] may not effectively capture complex interactions between features. If the relationships in the data are intricate, regularization alone may not improve the model's ability to generalize.\n\n6. Validation Set Issues: The validation set itself may not be representative of the problem space, or it may contain noise or outliers that affect the model's performance. Regularization won't address these issues if the validation data is flawed.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_quality",
      "exploration",
      "optimisation"
    ],
    "normalized_filename": "when_and_why_not_to_us_regularisation",
    "outlinks": [
      "training_data",
      "l1_regularisation",
      "data_distribution",
      "validation_data",
      "ridge"
    ],
    "inlinks": [
      "regularisation"
    ]
  },
  {
    "category": "ML",
    "filename": "Why Removing Outliers May Improve Regression but Harm Classification",
    "sha": "a8447335badfd9ec6422b4aa0dfa710994daaffd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Why%20Removing%20Outliers%20May%20Improve%20Regression%20but%20Harm%20Classification.md",
    "text": "Removing [[uncategorised/Outliers]] May Improve Regression but Harm Classification\n\n### Impact on Regression Model:\n\nRegression models, particularly linear [[Regression]], are sensitive to outliers because they attempt to minimize the sum of squared errors. By removing outliers, the model can better capture the underlying trend of the data, leading to improved performance metrics such as R-squared and reduced [[Mean Squared Error]].\n\n### Impact on [[Classification]] Models\n\n- Class Boundary Distortion: Classification models, such as decision trees or support vector machines, rely on the [[Distributions|distribution]] of data points to define class boundaries. ==Outliers can provide valuable information about the variability within classes.==\n\n- Loss of Information: Removing outliers may lead to the loss of important data points that could help in distinguishing between classes, potentially resulting in a less accurate model. For example, an outlier might represent a rare but important class that the model needs to learn from.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "anomaly_detection"
    ],
    "normalized_filename": "why_removing_outliers_may_improve_regression_but_harm_classification",
    "outlinks": [
      "regression",
      "uncategorised/outliers",
      "distributions",
      "mean_squared_error",
      "classification"
    ],
    "inlinks": [
      "outliers"
    ]
  },
  {
    "category": "ML",
    "filename": "Why Type 1 and Type 2 matter",
    "sha": "07c380f2887a33b0051121f056c7289d8bed647a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Why%20Type%201%20and%20Type%202%20matter.md",
    "text": "Type I and Type II errors are used in evaluating the performance of [[Classification]] models, and understanding their differences is essential for interpreting model results effectively.\n\n- [[Type I Error (False Positive)]]\n- [[Type II Error (False Negative)]]\n\n### Why Both Errors Matter\n1. **Impact on Decision-Making**: The consequences of Type I and Type II errors can vary significantly depending on the context. In some applications, such as medical diagnoses, a Type II error (failing to detect a disease) may be more critical than a Type I error (false alarm). Conversely, in fraud detection, a Type I error may lead to unnecessary investigations.\n\n2. **Balancing Precision and Recall**: Understanding these errors helps in balancing precision (the proportion of true positives among all positive predictions) and recall (the proportion of true positives among all actual positives). Depending on the application, one may be prioritized over the other, influencing model tuning and evaluation.\n\n3. **[[Model Evaluation]]**: Both types of errors are essential for a comprehensive evaluation of a model's performance. Metrics such as precision, recall, and the F1 score incorporate these errors to provide a more nuanced view of how well the model is performing.\n\n4. **Risk Management**: By analyzing the trade-offs between Type I and Type II errors, practitioners can make informed decisions about model thresholds and operational strategies, ensuring that the model aligns with business or clinical objectives.\n\n\n![[Pasted image 20250312064809.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "classifier",
      "evaluation"
    ],
    "normalized_filename": "why_type_1_and_type_2_matter",
    "outlinks": [
      "pasted_image_20250312064809.png",
      "type_i_error_(false_positive)",
      "model_evaluation",
      "classification",
      "type_ii_error_(false_negative)"
    ],
    "inlinks": [
      "evaluation_metrics"
    ]
  },
  {
    "category": "ML",
    "filename": "Why does increasing the number of models in a ensemble not necessarily improve the accuracy",
    "sha": "119c1c609e1f0b03ae02894efa89efb4ecc5dc14",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Why%20does%20increasing%20the%20number%20of%20models%20in%20a%20ensemble%20not%20necessarily%20improve%20the%20accuracy.md",
    "text": "Increasing the number of models in an ensemble ([[Model Ensemble]]) does not always lead to improved accuracy due to several limiting factors:\n\n- **Convergence of Predictions**: Additional models may lead to similar predictions, resulting in minimal changes to the overall output.\n- **Limited Data Representation**: If the dataset is noisy or incomplete, more models will only aggregate existing noise without capturing new patterns.\n- **Diminishing Returns**: Each new model contributes less unique information, and performance is ultimately limited by the irreducible error in the data.\n- **Increased Complexity**: More models increase computational costs and training times without necessarily improving accuracy.\n- **Overfitting Risk**: Adding complex models can lead to overfitting, where the ensemble learns noise instead of underlying patterns.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy",
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "Why does the Adam Optimizer converge",
    "sha": "2e80cbe130b7c6546b776803a2dd22f3e0183377",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Why%20does%20the%20Adam%20Optimizer%20converge.md",
    "text": "### Why the [[Adam Optimizer]] Converges\n\nThe Adam optimizer is able to efficiently handle sparse gradients and adaptively adjust learning rates. The convergence of Adam, often observed as a flattening of the [[Cost Function]], can be attributed to several factors inherent to its design and the characteristics of the dataset being used.\n\nThe convergence of the Adam optimizer, resulting in a stable cost value, is a product of its adaptive learning rate, regularization effects, numerical stability mechanisms, and the dataset's characteristics. \n#### 1. Convergence to Local Minimum or Saddle Point\n\n**Adaptive [[Learning Rate]]:** Adam adjusts the learning rate for each parameter individually, allowing it to quickly converge towards a local minimum or saddle point. This adaptability helps in navigating complex cost landscapes but may also cause the optimizer to plateau at a local minimum rather than reaching the global minimum. This plateauing effect can result in a stable final cost value, such as 0.146.\n\n**Gradient Behaviour:** As Adam converges, the gradients often become small or near zero, slowing down the learning process and leading to a flattened cost curve.\n\n#### 2. Learning Rate\n\n**Impact of Learning Rate (\\(\\alpha\\)):** The choice of learning rate significantly influences convergence speed. A larger [[Learning Rate]] might cause overshooting, while a smaller one might lead to slow convergence. Common values like 0.001, 0.005, 0.01, and 0.1 are used to balance these effects.\n\n**Stability in Suboptimal Regions:** If the learning rate is not optimal, Adam might get stuck in a suboptimal region, such as a local minimum or saddle point, resulting in a stable cost value.\n\n#### 3. Regularization\n\n**L2 Regularization:** The inclusion of L2 regularization helps prevent overfitting but also affects the optimization process by slightly increasing the final cost value. The observed cost value is a balance between error reduction and the regularization penalty.\n\n#### 4. Numerical Stability\n\n**Moment Estimates:** Adam uses first and second moment estimates (mean of gradients and squared gradients) to update parameters. As these estimates improve, the magnitude of updates decreases, leading to smaller changes in the cost function and eventual flattening.\n\n**Epsilon for Stability:** The epsilon parameter ensures numerical stability by preventing division by very small values, which can also lead to reduced update steps when squared gradients are small.\n\n#### 5. Dataset Characteristics\n\n**Simplicity of the Dataset:** The characteristics of the dataset, such as its simplicity or complexity, can influence convergence. In a simple dataset with few features, the optimizer might reach a plateau quickly due to the limited complexity of the problem.\n\n#### 6. Final Cost Comparison\n\n**Reasonable Solution:** The stable cost value, such as 0.146, indicates that Adam has found a reasonable solution given the dataset and optimizer settings. It reflects a balance between minimizing error and applying regularization.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "why_does_the_adam_optimizer_converge",
    "outlinks": [
      "cost_function",
      "learning_rate",
      "adam_optimizer"
    ],
    "inlinks": [
      "adam_optimizer"
    ]
  },
  {
    "category": "ML",
    "filename": "Why standardise features",
    "sha": "5052a3bb5bc5250d84e828a90c24d97a5fe4bc1a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Why%20standardise%20features.md",
    "text": "Standardizing ([[Standardisation]]) features is a common preprocessing step in machine learning and statistical modeling. It transforms each feature to have mean $0$ and standard deviation $1$.\n\n### 1. Equal Feature Contribution\n\nWhen features are on different scales, models that rely on distance metrics or gradient-based optimization may be biased toward features with larger numeric ranges.\n\nExample:\n* `SAT`: 400–1600\n* `Rand 1,2,3`: 1–5\n\nWithout standardization, `SAT` may dominate the model simply due to its larger scale.\n\nStandardization ensures all features contribute proportionally.\n\n### 2. Interpretability of Coefficients\n\nIn a linear model, standardized features allow coefficients to represent the change in the target (in standard deviations) per one standard deviation increase in the predictor.\n\nThis allows direct comparison of feature importance:\n* Larger coefficients imply stronger influence on the prediction.\n\n### 3. Improved Numerical Stability\n\nGradient-based optimizers (e.g. in neural networks, logistic regression) converge faster and more reliably when inputs are on comparable scales.\n\nThis prevents inefficient steps due to scale imbalance:\n\n* Large features → overshooting\n* Small features → slow progress\n\n### 4. Required by Certain Algorithms\n\nSome algorithms are scale-sensitive and may require standardization for proper functioning:\n\n* Distance-based: k-NN, k-means, SVM\n* Regularized models: Ridge, Lasso\n* Projection methods: PCA, [[Latent Dirichlet Allocation|LDA]]\n\nWithout scaling, these models may behave unpredictably or suboptimally.\n### 5. Consistent Treatment of Future Data\n\nStandardization ensures that new observations are transformed using the same scaling parameters as the training data.\n\nThe same `StandardScaler` used during training must be applied to new inputs during inference to maintain model validity.\n\n### When *Not* to Standardize\n\n* When features are already on the same scale (e.g. proportions, dummy variables)\n* With tree-based models like:\n  * Decision Trees\n  * Random Forests\n  * XGBoost\n\nThese models are scale-invariant and do not benefit from standardization.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "ml",
      "preprocessing"
    ],
    "normalized_filename": "why_standardise_features",
    "outlinks": [
      "latent_dirichlet_allocation",
      "standardisation"
    ],
    "inlinks": [
      "feature_importance",
      "linear_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "Wrapper Methods",
    "sha": "e6191b07ab5465d94dab67304e46dc00918da300",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Wrapper%20Methods.md",
    "text": "Used in [[Feature Selection]]. Wrapper methods are powerful because they directly optimize the performance of the machine learning model by selecting the most informative subset of features. \n\n1. Iterative Approach: Unlike [[Filter Methods]], which assess the relevance of features based on statistical properties, wrapper methods ==directly involve the machine learning algorithm in the feature selection process.==\n   \n2. Subset Selection: Wrapper methods work by creating different subsets of features from the original dataset and training a model on each subset. These subsets can be combinations of different features or a subset of all features.\n\n3. [[Model Evaluation]]: After training a model on each subset of features, the performance of each model is evaluated using a performance metric, such as accuracy, precision, recall, or F1-score, depending on the problem type (classification or regression).\n\n4. Optimization Criterion: The goal of wrapper methods is to find the subset of features that maximizes the performance of the machine learning model. This can be achieved by selecting the subset that yields the highest performance metric on a validation set or through cross-validation.\n\n5. Computational Intensity: Wrapper methods are computationally intensive because they involve training multiple models for each possible combination of features. As a result, they can be slower and require more computational resources compared to filter methods.\n\n## Examples of Wrapper Methods:\n\n   - Forward Selection: Starts with an empty set of features and iteratively adds one feature at a time, selecting the feature that improves model performance the most.\n    \n   - Backward Elimination: Begins with all features and iteratively removes one feature at a time, selecting the feature whose removal improves model performance the least.\n     \n   - Recursive Feature Elimination (RFE): Iteratively removes features from the full feature set based on their importance, as determined by a specified machine learning algorithm.\n     \n   - Selection Criteria: The choice of performance metric and optimization criterion depends on the specific machine learning task and dataset characteristics. It's essential to select a metric that aligns with the goals of the project and to validate the selected subset of features on unseen data.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "wrapper_methods",
    "outlinks": [
      "filter_methods",
      "feature_selection",
      "model_evaluation"
    ],
    "inlinks": [
      "embedded_methods",
      "feature_selection"
    ]
  },
  {
    "category": "ML",
    "filename": "XGBoost",
    "sha": "7954fc0f219bbe7f52c60ba5b1a0c14dbcc79317",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/XGBoost.md",
    "text": "XGBoost (eXtreme Gradient Boosting) is a highly efficient and flexible implementation of [[Gradient Boosting]] that is widely used for its accuracy and performance in machine learning tasks.\n\n### How does XGBoost work\n\nIt works by building an [[Model Ensemble]] - ensemble of decision trees, where each tree is trained to correct the errors made by the previous ones. Here's a breakdown of how XGBoost works:\n### Key Concepts\n\n1. Gradient Boosting Framework:\n   - XGBoost is based on the gradient boosting framework, which builds models sequentially. Each new model aims to reduce the errors (residuals) of the combined ensemble of previous models.\n\n2. Decision Trees:\n   - XGBoost typically uses decision trees as the base learners. These trees are added one at a time, and existing trees in the model are not changed.\n\n3. Objective Function:\n   - The objective function in XGBoost consists of two parts: the loss function and a regularization term.\n   - [[Loss function]]: Measures how well the model fits the training data. For regression, this might be [[Mean Squared Error]]; for classification, it could be logistic loss.\n   - [[Regularisation]]: Helps prevent overfitting by penalizing complex models. XGBoost supports both L1 (Lasso) and L2 (Ridge) regularization.\n\n4. Additive Training:\n   - XGBoost adds trees to the model sequentially. Each tree is trained to minimize the loss function, taking into account the errors made by the previous trees.\n\n5. [[Gradient Descent]]\n   - The model uses gradient descent to minimize the loss function. It calculates the gradient of the loss function with respect to the model's predictions and uses this information to update the model.\n\n6. [[Learning Rate]] ($\\eta$):\n   - A parameter that scales the contribution of each tree. A smaller learning rate requires more trees but can lead to better performance.\n\n7. Tree Pruning:\n   - XGBoost uses a technique called \"max depth\" to control the complexity of the trees. It also employs a \"max delta step\" to ensure that the updates are not too aggressive.\n\n8. [[Handling Missing Data]]\n   - XGBoost can handle missing data internally by learning the best direction to take when a value is missing.\n\n9. Parallel and Distributed Computing:\n   - XGBoost is designed to be highly efficient and can leverage parallel and distributed computing to speed up training.\n\nKey Features:\n- Tree Splitting: Builds [[Decision Tree]] in a level-wise manner, leading to balanced trees and efficient computation.\n- Parameters: Key parameters include `eta` (learning rate) and `max_depth` (maximum depth of a tree), which control the model's complexity and learning process.\n\n### Workflow\n\n1. Initialization:\n   - Start with an initial prediction, often the mean of the target values for regression or a uniform probability for classification.\n\n2. Iterative Training:\n   - For each iteration, compute the gradient of the loss function with respect to the current predictions.\n   - Fit a new decision tree to the negative gradient (residuals).\n   - Update the model by adding the new tree, scaled by the learning rate.\n\n3. Model Output:\n   - The final model is a weighted sum of all the trees, where each tree contributes to the final prediction.\n\nAdvantages:\n- Accuracy: Known for its high accuracy and robustness across various machine learning tasks.\n- [[Regularisation]]: Supports L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n- Flexibility: Offers a wide range of hyperparameters for fine-tuning models.\n\nUse Cases:\n- Structured Data: Particularly effective for structured data and tabular datasets.\n- [[Interpretability]]: Suitable when model interpretability is important.\n- [[Hyperparameter Tuning]]: Ideal for scenarios where extensive hyperparameter tuning is feasible.\n### Implementing XGBoost in Python\n\n#### Step 2: Import Necessary Libraries\n\n```python\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n```\n#### Step 3: Prepare Your Data\n\nSplit your dataset into training and testing sets:\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n#### Step 4: Convert Data to DMatrix\n\nConvert the data into DMatrix, the optimized data structure used by XGBoost:\n\n```python\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n```\n\n#### Step 5: Set Parameters\n\nDefine the parameters for the XGBoost model:\n\n```python\nparams = {\n    'max_depth': 6,\n    'eta': 0.1,\n    'objective': 'binary:logistic',  # Use 'reg:squarederror' for regression tasks\n    'eval_metric': 'logloss'\n}\n```\n\n#### Step 6: Train the Model\n\nTrain the XGBoost model using the training data:\n```python\nnum_rounds = 100\nbst = xgb.train(params, dtrain, num_rounds)\n```\n\n#### Step 7: Make Predictions and Evaluate\nMake predictions on the test set and evaluate the model's performance:\n\n```python\ny_pred = bst.predict(dtest)\ny_pred_binary = [1 if y > 0.5 else 0 for y in y_pred]\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```\n\n# Notes\n\nSet up an example of XGBoost. Plot the paramater space slices \"Min_Samples_split\", \"Max_Depth\" vs accuracy.\n\n```python\nxgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\nxgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)\nxgb_model.best_itersation\n```",
    "aliases": [
      "XGM"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "optimisation"
    ],
    "normalized_filename": "xgboost",
    "outlinks": [
      "gradient_descent",
      "regularisation",
      "gradient_boosting",
      "loss_function",
      "learning_rate",
      "handling_missing_data",
      "hyperparameter_tuning",
      "decision_tree",
      "mean_squared_error",
      "interpretability",
      "model_ensemble"
    ],
    "inlinks": [
      "boosting",
      "ds_&_ml_portal",
      "feature_importance",
      "gradient_boosting",
      "lightgbm_vs_xgboost_vs_catboost",
      "optuna",
      "time_series_forecasting"
    ]
  },
  {
    "category": "ML",
    "filename": "Xaiver",
    "sha": "80f410c117bf202cb46010db975db3201a245cc1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/Xaiver.md",
    "text": "Xavier initialization (also known as Glorot initialization) is a weight initialization technique designed to improve the training of deep neural networks. It maintains stable variance of activations and gradients across layers, helping prevent both the vanishing and exploding gradient problems. Proposed by Xavier Glorot and Yoshua Bengio in their 2010 paper _\"Understanding the difficulty of training deep feedforward neural networks.\"_\n## Purpose\n\nTo ensure that the variance of the activations and gradients remains consistent across layers during both forward and backward propagation.\n\nThis helps with [[vanishing and exploding gradients problem]].\n## Role in Training Stability\n\n[[Xavier]] initialization helps stabilize training by setting the initial weights to values that are neither too small nor too large. This supports:\n\n- Better gradient flow\n- Faster and more stable convergence\n- Improved learning in deeper networks\n    \n\n### Related Notes\n- [[initialization methods]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "optimisation"
    ],
    "normalized_filename": "xaiver",
    "outlinks": [
      "initialization_methods",
      "vanishing_and_exploding_gradients_problem",
      "xavier"
    ],
    "inlinks": [
      "initialization_methods",
      "vanishing_and_exploding_gradients_problem"
    ]
  },
  {
    "category": "ML",
    "filename": "conceptual data model",
    "sha": "e658c3a1d1a89f544c3cdc579118d0eb4fc6f374",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/conceptual%20data%20model.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling"
    ],
    "normalized_filename": "conceptual_data_model",
    "outlinks": [],
    "inlinks": [
      "database_schema"
    ]
  },
  {
    "category": "ML",
    "filename": "emergent behavior",
    "sha": "38023f55e49d7bb5b2716bfd9ccf8a62752fdf60",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/emergent%20behavior.md",
    "text": "### Related Papers\n\n[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "emergent_behavior",
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling"
    ]
  },
  {
    "category": "ML",
    "filename": "f-regression",
    "sha": "62fb53fbeff58953688ace620f52e8a439275868",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/f-regression.md",
    "text": "`f_regression` is a statistical method provided by `sklearn.feature_selection` to evaluate the linear relationship between each independent variable in X and a continuous target variable y. It is a univariate [[Feature Selection]] method based on the [[F-statistic]] from simple linear regression.\n\nSpecifically:\n\n* For each feature, `f_regression` fits a simple [[Linear Regression]] model (i.e. one feature at a time).\n* It computes:\n  * The F-statistic, which tests whether there is a linear relationship between the feature and the target.\n  * The corresponding p-value, which helps assess the statistical significance of that relationship.\n\n### Statistical Assumptions\n* The relationship between each feature and the target is assumed to be linear.\n* Errors are assumed to be normally distributed with constant variance.\n* Features are assessed independently — mutual influence or [[Multicollinearity]] is ignored.\n\n### Limitations\n\n* `f_regression` does not account for feature interactions or joint effects.\n* It cannot capture non-linear dependencies.\n* It is not applicable to models outside the linear regression framework (e.g., tree-based models or SVMs).\n\n## When to Use `f_regression`\n\nUse `f_regression` when:\n\n* You're performing linear regression and want to evaluate individual feature relevance.\n* You want a fast, interpretable [[Filter Methods]] for selecting features before training.\n* You assume no or limited multicollinearity between features.\n\nDo not use `f_regression` when:\n\n* Your model is non-linear or [[non-parametric]].\n* Feature interactions are essential to the model’s behavior.\n* You're working with [[Classification]] tasks — instead, use `f_classif`.\n\n## Example: Computing P-values with `f_regression`\n\n```python\nfrom sklearn.feature_selection import f_regression\n\n# Compute F-statistics and p-values\nf_stats, p_values = f_regression(X, y)\n\nprint(p_values)\n```\n\nExample Output:\n\n```python\n(array([56.04804786, 0.17558437]), array([7.19951844e-11, 6.76291372e-01]))\n```\n\n* Feature 1 is statistically significant (very small p-value).\n* Feature 2 is not statistically significant.\n\nDocumentation:\n[scikit-learn f\\_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "statistics"
    ],
    "normalized_filename": "f-regression",
    "outlinks": [
      "linear_regression",
      "filter_methods",
      "f-statistic",
      "non-parametric",
      "multicollinearity",
      "feature_selection",
      "classification"
    ],
    "inlinks": [
      "linear_regression"
    ]
  },
  {
    "category": "ML",
    "filename": "granularity",
    "sha": "fc9e5cadec21c254ae62baa6f2a3dac5ae15c613",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/granularity.md",
    "text": "Definition of Grain in [[Dimensional Modelling]]\n   - The grain of a [[Fact Table]] defines what a single row in the table represents. It is the level of detail captured by the fact table.\n   - Declaring the grain is essential because it sets the foundation for the entire dimensional model. It determines how detailed the data will be.\n\nImportance of Grain Declaration:\n   - The grain must be established before selecting [[Dimensions]] and [[Facts]] because all dimensions and facts must align with the grain.\n   - This alignment ensures consistency across the data model, which is critical for the performance and usability of [[business intelligence]] applications.\n\nBalancing Granularity:\n   - In the transformation layer, you need to decide the level of aggregation. For instance, you might aggregate hourly data into daily data to save storage space.\n   - Adding dimensions increases the number of rows exponentially, so it's important to carefully choose which dimensions to include.\n\nSemantic Layer:\n   - A [[semantic layer]] sits on top of transformed data in a data warehouse, providing flexibility and enabling ad-hoc analysis without needing to store every possible data representation.\n   - This is akin to [[OLAP]] cubes, where you can perform complex queries (slice-and-dice) on large datasets without pre-storing all combinations.\n\n## Choosing the level of granularity\n\nGranularity, or grain, refers to the ==level of detail== represented by a single row in a fact table within a data warehouse. \n\nThe choice of granularity depends on the business requirements and the types of analyses you want to support. Finer granularity (e.g., [[Transaction]]-level) provides more detailed insights but requires more storage and processing power. Coarser granularity (e.g., monthly product-level) reduces storage needs and can improve query performance but may limit the depth of analysis.\n\nBy clearly defining the grain, you ensure that all dimensions and facts in the data model are consistent and aligned with the intended analytical use cases.\n\n### Example: Retail Sales Data\n\nImagine you are designing a data warehouse for a retail company that tracks sales transactions. You need to decide the granularity of the sales fact table. Here are a few possible options:\n\n1. Transaction-Level Granularity:\n   - Grain: Each row represents a single sales transaction.\n   - Example: A row might include details such as transaction ID, date and time of sale, store location, product sold, quantity, and total sale amount.\n   - Use Case: This level of granularity is useful for detailed analysis, such as examining individual customer purchases or identifying specific transaction patterns.\n\n2. Daily Store-Level Granularity:\n   - Grain: Each row represents the total sales for a specific store on a specific day.\n   - Example: A row might include the store ID, date, total sales amount, and total number of transactions for that day.\n   - Use Case: This granularity is suitable for analyzing daily sales trends across different stores, comparing store performance, or identifying peak sales days.\n\n3. Monthly Product-Level Granularity:\n   - Grain: Each row represents the total sales for a specific product across all stores for a specific month.\n   - Example: A row might include the product ID, month, total sales amount, and total units sold.\n   - Use Case: This level is ideal for tracking product performance over time, identifying best-selling products, or planning inventory and supply chain logistics.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "database",
      "modeling"
    ],
    "normalized_filename": "granularity",
    "outlinks": [
      "transaction",
      "facts",
      "dimensions",
      "dimensional_modelling",
      "olap",
      "semantic_layer",
      "fact_table",
      "business_intelligence"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters",
      "dashboards",
      "fact_table",
      "grain",
      "joining_time_series",
      "mysql",
      "querying_time_series",
      "rollup",
      "transaction"
    ]
  },
  {
    "category": "ML",
    "filename": "inference versus prediction",
    "sha": "85ca2d7524d3346c17c55938018bc591473ab1d9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/inference%20versus%20prediction.md",
    "text": "[[inference]] is similar to prediction, ==but in the context of [[Generative AI]],== it is more specific to the application of a pre-trained model to ==produce an output from new input data==. \n\nWhile [[prediction]] often refers to tasks like classification or regression, inferencing in Gen AI refers to generating novel outputs, such as text, images, or audio, based on learned patterns.\n\nThe key distinction is that in generative models, inferencing not only predicts but ==creates new data== (like text or images) rather than assigning categories or predicting numerical values, as in traditional machine learning models. For example:\n- In a language model [[LLM]], inferencing is generating the next word or sentence in a text.\n- In a text-to-image model, inferencing produces an image based on a textual description.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "ml"
    ],
    "normalized_filename": "inference_versus_prediction",
    "outlinks": [
      "llm",
      "prediction",
      "inference",
      "generative_ai"
    ],
    "inlinks": [
      "inference"
    ]
  },
  {
    "category": "ML",
    "filename": "inference",
    "sha": "f5d03c85617baaa5de6ddd4165ba41dac5d847ac",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/inference.md",
    "text": "Inferencing involves prediction, but the output is more generative and creative in nature.\n\n[[inference versus prediction]]",
    "aliases": [
      "inferencing"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "ml"
    ],
    "normalized_filename": "inference",
    "outlinks": [
      "inference_versus_prediction"
    ],
    "inlinks": [
      "hypothesis_testing",
      "inference_versus_prediction",
      "sentence_transformers",
      "small_language_models"
    ]
  },
  {
    "category": "ML",
    "filename": "initialization methods",
    "sha": "31266075e8a10581a52f6a2a86270c1e6dea5436",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/initialization%20methods.md",
    "text": "A weight initialization method sets the initial values of a [[Neural network]]’s weights *before* training begins. This choice significantly affects how well and how fast the network learns.\n\n## Why Initialization Matters\n\nDuring training, weights are updated via [[Gradient Descent]]. If weights start too small or too large, gradients can:\n\n* Vanish (approach zero), halting learning in earlier layers.\n* Explode (grow very large), causing unstable updates.\n\nProper initialization ensures that:\n\n* The input signal maintains a consistent scale as it propagates forward.\n* The gradient signal maintains a consistent scale as it propagates backward.\n## Main Goals of Initialization\n\n1. Prevent vanishing/exploding gradients.\n2. Maintain similar variance of activations and gradients across layers.\n3. Speed up convergence.\n\n## Common Initialization Methods\n\n* Xavier (Glorot): Scales weights based on the number of input and output units to maintain variance.\n* He: Optimized for [[Relu]] activations, focuses on variance preservation from the input side.\n* Uniform / Normal: Basic methods that are prone to instability in deep networks.\n\n## Related Notes\n\n* [[Xaiver]]\n* [[initialization methods]]\n* [[vanishing and exploding gradients problem]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "optimisation"
    ],
    "normalized_filename": "initialization_methods",
    "outlinks": [
      "gradient_descent",
      "relu",
      "xaiver",
      "initialization_methods",
      "vanishing_and_exploding_gradients_problem",
      "neural_network"
    ],
    "inlinks": [
      "initialization_methods",
      "vanishing_and_exploding_gradients_problem",
      "xaiver"
    ]
  },
  {
    "category": "ML",
    "filename": "interoperable",
    "sha": "aa0a8b52e8fca7ee54624566d0bdf753de46a3f1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/interoperable.md",
    "text": "The able to exchange and make use of information",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability"
    ],
    "normalized_filename": "interoperable",
    "outlinks": [],
    "inlinks": [
      "multi-level_index"
    ]
  },
  {
    "category": "ML",
    "filename": "lambda architecture",
    "sha": "8d2d4814e7a49a23f04382a8d21ab8b35ae4ddf5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/lambda%20architecture.md",
    "text": "Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. [[Data Streaming]]\n\nThis approach to architecture attempts to balance ==latency, throughput, and fault tolerance== using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. \n\nThe two view outputs may be joined before the presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of [MapReduce](term/map%20reduce.md).\n\nLambda architecture is a [[design pattern]] for processing large volumes of data by combining both batch and stream processing methods. It aims to provide a comprehensive and efficient way to handle big data by addressing the needs for low-latency data processing, high throughput, and fault tolerance. \n\n[[Batch Processing]]\n\nLambda architecture is particularly useful in scenarios where ==both historical== and ==real-time data insights== are crucial, such as in financial services, telecommunications, and online retail. It allows organizations to leverage the strengths of both batch and stream processing to meet diverse data processing needs.\n\nHow it works:\n1. All data entering the system is dispatched to both the batch layer and the speed layer for processing.\n2. The batch layer has two functions: (i) managing the master dataset (an immutable, append-only set of raw data), and (ii) to pre-compute the batch views.\n3. The serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way.\n4. The speed layer compensates for the high latency of updates to the serving layer and deals with recent data only.\n5. Any incoming query can be answered by merging results from batch views and real-time views.\n\n### Components of Lambda Architecture\n\n1. Batch Layer:\n   - Purpose: To process large sets of historical data in batches.\n   - Functionality: It computes results from all available data, ensuring accuracy and completeness.\n   - Tools: Often uses distributed processing frameworks like Hadoop or Spark for batch processing.\n   - Output: Produces a batch view, which is a complete and accurate dataset that can be queried.\n\n1. Speed Layer:\n   - Purpose: To process real-time data streams with low latency.\n   - Functionality: It provides immediate insights by processing data as it arrives.\n   - Tools: Utilizes stream processing frameworks like Apache Storm, Apache Flink, or Spark Streaming.\n   - Output: Generates a real-time view that reflects the most recent data.\n\n1. Serving Layer:\n   - Purpose: To merge and serve the results from both the batch and speed layers.\n   - Functionality: It combines the batch view and real-time view to provide a unified, queryable dataset.\n   - Tools: Databases or data stores optimized for fast reads, such as Cassandra or HBase, are often used.\n\n### How Lambda Architecture Works\n\n- Data Ingestion: Data is ingested into both the batch and speed layers simultaneously.\n- Batch Processing: The batch layer processes data in large volumes, typically with higher latency, to ensure accuracy and completeness.\n- Stream Processing: The speed layer processes data in real-time, providing low-latency updates.\n- Data Serving: The serving layer combines outputs from both layers, allowing users to query the most up-to-date and accurate data.\n\n### Benefits of Lambda Architecture\n\n- Fault Tolerance: By separating batch and real-time processing, the architecture can handle failures more gracefully.\n- Scalability: It can scale to handle large volumes of data by leveraging distributed processing frameworks.\n- Flexibility: Supports both historical and real-time data processing, making it suitable for a wide range of applications.\n\n### Challenges\n\n- Complexity: Maintaining two separate processing paths (batch and speed) can increase system complexity.\n- Data Consistency: Ensuring consistency between batch and real-time views can be challenging.\n- Maintenance: Requires more effort to maintain and update due to its dual-layer nature.\n\n![[paste-3345779523926.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "orchestration"
    ],
    "normalized_filename": "lambda_architecture",
    "outlinks": [
      "design_pattern",
      "data_streaming",
      "batch_processing",
      "paste-3345779523926.png"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "model-agnostic feature importance",
    "sha": "4f6146fb89eea94444e804cb4b7203d9ae79aa33",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/model-agnostic%20feature%20importance.md",
    "text": "model-agnostic [[Feature Importance]]\n### Alternative for Non-Linear Models\n\nIf you're working with non-linear models or want a approach, consider:\n\n* **Mutual Information:** `mutual_info_regression` ([[non-parametric]])\n* **Permutation Importance:** Model-agnostic\n* **[[SHapley Additive exPlanations|SHAP]] or [[Local Interpretable Model-agnostic Explainations|LIME]]:** For [[Model Interpretability]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "modeling"
    ],
    "normalized_filename": "model-agnostic_feature_importance",
    "outlinks": [
      "shapley_additive_explanations",
      "non-parametric",
      "local_interpretable_model-agnostic_explainations",
      "feature_importance",
      "model_interpretability"
    ],
    "inlinks": []
  },
  {
    "category": "ML",
    "filename": "objective function",
    "sha": "f6159daa8b125c86da036354b5a53ca42029df38",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/objective%20function.md",
    "text": "#### Objective Function\n\n* A general term for any function that we aim to maximize or minimize in an optimization problem.\n* In machine learning, the objective function represents the overall goal (e.g., minimize error, maximize likelihood).\n* It may include primary loss plus regularization terms:\n\n  $$\n  \\text{Objective} = \\text{Loss} + \\text{Regularization}\n  $$\n\nExamples:\n\n* In linear regression: minimize Mean Squared Error (MSE).\n* In logistic regression: minimize cross-entropy loss.\n* In regularized models: minimize (Loss + \\$\\lambda\\$ \\* penalty).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "objective_function",
    "outlinks": [],
    "inlinks": [
      "loss_function",
      "model_optimisation"
    ]
  },
  {
    "category": "ML",
    "filename": "semi-structured data",
    "sha": "291b7375d293f5cf5d8b11c778fa5fe22bd41023",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/semi-structured%20data.md",
    "text": "Semi-structured data is data that lacks a rigid structure and that does not conform directly to a data model, but that has tags, metadata, or elements that describe the data. \n\nExamples of semi-structured data are JSON or [[XML]] files. \n\n==Semi-structured data often contains enough information that it can be relatively easily converted into == [[structured data]]. \n\n[[Json]] data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet.\n\n|          | **data**                        |\n| -------- | ------------------------------- |\n| Record 1 | \\\"{'id': 1, 'name': 'Mary X'}\\\" |\n| Record 2 | \\\"{'id': 2, 'name': 'John D'}\\\" |\n\n\nIt is often relatively straightforward to convert semi-structured data into structured data. Converting semi-structured data into structured data is often done during the [Data Transformation](Data%20Transformation.md) stage in an [ETL](ETL.md) or [ELT](term/elt.md) process.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "storage"
    ],
    "normalized_filename": "semi-structured_data",
    "outlinks": [
      "json",
      "structured_data",
      "xml"
    ],
    "inlinks": [
      "snowflake"
    ]
  },
  {
    "category": "ML",
    "filename": "vanishing and exploding gradients problem",
    "sha": "14764009a51e1e2030288cbb2329b8fcb86fc25f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/machine-learning/vanishing%20and%20exploding%20gradients%20problem.md",
    "text": "The vanishing gradient problem arises when gradients of the loss with respect to early layer weights approach zero, hindering learning in those layers. This typically occurs due to activation functions like sigmoid or tanh and deep architectures.\n\n## Description\n\nThe vanishing gradient problem occurs when gradients become extremely small in early layers during [[Backpropagation]]. This leads to:\n\n- Slow convergence\n- Poor learning of long-term dependencies\n## Causes\n\n- Activation functions: Sigmoid and tanh functions saturate for large inputs, resulting in gradients near zero. These small gradients, when propagated backward, diminish layer by layer.\n- Deep architectures: As depth increases, gradient signals can diminish before reaching early layers.\n- Ineffective optimization: Poor initialization or unsuitable learning rates can worsen the problem.\n## Symptoms\n\n- Loss remains constant over epochs.\n- Weight values do not change significantly during training.\n\nThese symptoms can be diagnosed using:\n\n- Loss monitoring tools in frameworks like [[Keras]]\n- Plots of weight values across epochs\n## Mathematical Background\n\nThe vanishing gradient problem is linked to the product of Jacobian matrices across layers. When the Jacobian matrices have eigenvalues with magnitudes less than 1, the gradient norm can shrink exponentially with the number of layers due to the chain rule.\n## Related\n- [[Batch Normalisation]]\n- [[Xaiver]]\n- [[Relu]] or [[initialization methods]]\n- [[Recurrent Neural Networks|RNN]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "ml",
      "optimisation"
    ],
    "normalized_filename": "vanishing_and_exploding_gradients_problem",
    "outlinks": [
      "relu",
      "xaiver",
      "initialization_methods",
      "recurrent_neural_networks",
      "batch_normalisation",
      "backpropagation",
      "keras"
    ],
    "inlinks": [
      "backpropagation",
      "batch_normalisation",
      "ds_&_ml_portal",
      "forward_propagation",
      "initialization_methods",
      "lstm",
      "recurrent_neural_networks",
      "relu",
      "transformers_vs_rnns",
      "xaiver"
    ]
  },
  {
    "category": "LANG",
    "filename": "AI Agents Memory",
    "sha": "b20eac17defe893c3cb04f33be7c5156f995c198",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/AI%20Agents%20Memory.md",
    "text": "How to give memory to long-running AI agents. That is how do we enable agents to **maintain**, **retrieve**, and **update** salient conversation memories over extended conversations.\n\nContext:\n* LLMs tend to forget information within and across sessions due to fixed context windows.\n* Forgetting over sessions can cause user frustration.\n* Large contexts increase both **cost** and **latency**.\n* Memory systems should not depend solely on the size of the input prompt.\n\n### Challenges\n\n* **Context overflow**: exceeding the token window.\n* **Irrelevant history**: retaining unused or low-value information.\n* **Memory management**: removing stale or outdated memories without losing critical data.\n\n### Use Cases\n\n* Personalised learning assistants.\n* Customer support bots.\n* Financial advisory agents.\n### Architecture Overview\n\n1. **Store** memories in a database.\n2. **Retrieve** relevant memories via semantic \\[\\[similarity search]].\n3. **Update** process: add, update, delete, or NOOP (no operation).\n4. Optional **graph memory** layer\n\nMetrics to evaluate:\n  * Memory quality.\n  * F1 score.\n  * BLEU-1 score.\n  * Token consumption analysis.\n\nKey Takeaways\n* Persistent long-term memory improves **performance**, reduces **cost**, and increases **speed**.\n* Reasoning is essential during the memory update phase to maintain accuracy and coherence.\n\n### Example Engineering Questions\n\n#### Architecture & Scale\n\n* What are the biggest engineering challenges in maintaining long-term memory at scale, especially with respect to latency, consistency, and cost?\n* How should **mem0** be integrated into production with a self-hosted setup (e.g., Docker with MCP)?\n* Which API calls are required for summarisation and retrieval, and at which stages?\n\n#### Cost & Retrieval\n\n* How can cost be controlled while maintaining contextual memory?\n* What trade-offs exist between retrieval speed and memory depth?\n* Does marking graph relationships as invalid (instead of deleting them) cause memory bloat and higher cost?\n\n#### Memory Quality & Versioning\n\n* How is memory versioned as the agent or model changes?\n* Are there mechanisms for detecting **memory drift** or **bias accumulation**?\n\n#### API Behaviour & Retrieval Logic\n\n* How many API calls are required for create/update and retrieval?\n* Does retrieval fetch top-N similar memories (e.g., top 10 via \\[\\[cosine similarity]])?\n* Can the number of historical messages retrieved for fact generation be configured?\n* Differences between open-source and enterprise versions of mem0, especially in fact generation.\n\n#### Candidate Facts & Domain Adaptation\n\n* Difference between *candidate facts* and *summary*.\n* Must developers define candidate facts per domain?\n* How are nodes selected?\n* Should mem0 or mem0g be chosen per domain, or can mem0g be applied universally?\n\n#### Evaluation\n\n* Evaluation metrics considered: F1 score, BLEU-1, memory quality.\n* How does Mem0 quantitatively evaluate long-term memory effectiveness in retrieval accuracy, relevance, and downstream task performance over time?\n* For fine-tuning memory quality for specific applications, what parameters can be adjusted?\n\n### Resources\nPaper: Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\nRelated: [[LLM Memory]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "evaluation",
      "language_models",
      "NLP",
      "optimisation"
    ],
    "normalized_filename": "ai_agents_memory",
    "outlinks": [
      "llm_memory"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Attention mechanism",
    "sha": "7c7758757f03430821d8ad1f02958ea5169cbf37",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Attention%20mechanism.md",
    "text": "The attention mechanism is inspired by how humans read: we don’t give equal focus to every word-we ==concentrate on those most relevant to understanding the context==. Neural networks apply the same principle, dynamically weighting parts of the input sequence based on relevance.\n\nOriginally introduced to overcome the limitations of models like [[Recurrent Neural Networks]]s and [[LSTM]]s, attention mechanisms significantly improve the handling of long-range dependencies in sequence tasks. They are now central to many modern [[NLP]] applications, including machine translation, text generation, and language understanding.\n### Why Attention Matters\n\nIn traditional sequence models, all information must be compressed into a single fixed-size vector, which leads to loss of context-especially for long inputs. Attention allows the model to:\n\n* Focus selectively on relevant input tokens\n* Dynamically adjust what it \"attends\" to at each prediction step\n* Better capture dependencies across distant positions in a sequence\n\n### How Attention Works (Simplified)\n\n1. Score Calculation: Compute how relevant each token is to a given query token (e.g., using dot product).\n2. Weighting: Apply softmax to get attention weights (a probability distribution).\n3. Context Vector: Take the weighted sum of value vectors to produce a context-specific representation.\n\nThis mechanism enables the model to emphasize important tokens and de-emphasize irrelevant ones during prediction.\n### See Also\n\n* [[Self-Attention]]\n* [[Self-Attention vs Multi-Head Attention]]\n* [[Key Components of Attention and Formula]]\n* [[Transformer]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "attention_mechanism",
    "outlinks": [
      "lstm",
      "self-attention_vs_multi-head_attention",
      "transformer",
      "recurrent_neural_networks",
      "key_components_of_attention_and_formula",
      "self-attention",
      "nlp"
    ],
    "inlinks": [
      "ai_engineer",
      "bert",
      "ds_&_ml_portal",
      "language_model_output_optimisation",
      "llm",
      "lstm",
      "multi-head_attention",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "BERTScore",
    "sha": "58e48f26172efc5a46afe96384b98202bf295074",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/BERTScore.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "bertscore",
    "outlinks": [],
    "inlinks": [
      "assessing_gen_ai_generated_content"
    ]
  },
  {
    "category": "LANG",
    "filename": "BERT",
    "sha": "6389c89e8b49e6539575c441806f8907a576b9d3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/BERT.md",
    "text": "### Overview\n* [[BERT]] (Bidirectional Encoder Representations from [[Transformer]]) is a [[Transformer]]-based model developed by [[Google]] in 2018 (transformers are often better than traditional [[NLP]] methods.\n* It is built on the [[Transformer]] architecture and uses a bidirectional context representation—capturing the meaning of words based on both their left and right context.\n* Introduced in the paper: \"[[BERT Pretraining of Deep Bidirectional Transformers for Language Understanding]]\".\n\n### Pretraining and [[Transfer Learning]]\n\nPre-trained on large corpora using two main objectives:\n  * Masked Language Modeling (MLM): Predict randomly masked words.\n  * Next Sentence Prediction (NSP): Predict whether one sentence follows another.\nEnables [[Transfer Learning]] through task-specific fine-tuning.\n### Input [[Vector Embedding|Embeddings]]\n\n* [[Tokenisation]]: Representation of each word or token. Then embeds these tokens as [[Vector Embedding]].\n* Sentence Embeddings/ [[Sentence Transformers]]: Capture relationships between entire sentences.\n* [[Positional Encoding]]: Adds information about the position of words to handle order.\n\n### Applications of BERT\n1. Text Classification – Sentiment analysis, topic classification.\n2. [[Named Entity Recognition|NER]] – Extraction of entities like names, places, etc.\n3. Question Answering – Find answers based on a passage.\n4. Text [[Summarisation]] – Create concise summaries of documents.\n5. Language Translation – Assist with machine translation.\n6. [[Sentence Similarity]] – Evaluate semantic similarity between sentences.\n\n### Limitations of BERT with Large Datasets\n\nBERT generates contextual embeddings for each word in a sentence, which are typically pooled—using methods like mean pooling—to form a single sentence embedding (see [[Sentence Transformers]]). However, such pooling treats all words equally, regardless of their importance to the sentence’s overall meaning. This limits BERT’s ability to capture fine-grained semantic relationships.\n\nWhile fine-tuning BERT on sentence pairs can help produce embeddings that better reflect relational meaning, this process is computationally intensive and does not scale well to large datasets.\n### Resources\n* [What is BERT and how does it work? | A Quick Review](https://www.youtube.com/watch?v=6ahxPTLZxU8&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl&index=12)\n### Exploratory Questions\n\n* [ ] What does [[BERT]] learn about syntax vs semantics? ⏬\n* [ ] How do [[Attention mechanism]] heads contribute to sentence meaning?\n* [ ] What are the limitations of BERT for [[Sentence Similarity]] and sentence clustering?\n\n### Variants\n* BERT-base: 12 layers, 110M parameters.\n* BERT-large: 24 layers, 340M parameters.\n* Optimized alternatives for specific tasks:\n\t* [[Sentence Similarity]]: Use [[Sentence-BERT]] instead of BERT for better performance on semantic similarity.\n\n### Related\n- [[Word2vec]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "language_models",
      "NLP"
    ],
    "normalized_filename": "bert",
    "outlinks": [
      "bert",
      "vector_embedding",
      "attention_mechanism",
      "transfer_learning",
      "transformer",
      "sentence_similarity",
      "positional_encoding",
      "summarisation",
      "sentence-bert",
      "tokenisation",
      "google",
      "nlp",
      "sentence_transformers",
      "word2vec",
      "named_entity_recognition",
      "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
    ],
    "inlinks": [
      "bert",
      "ds_&_ml_portal",
      "lstm",
      "positional_encoding",
      "rag",
      "search",
      "self-attention",
      "sentence_similarity",
      "sentence_transformer_workflow",
      "sentence_transformers",
      "small_language_models",
      "tokenisation",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding",
      "word2vec.py"
    ]
  },
  {
    "category": "LANG",
    "filename": "Bag of words",
    "sha": "6e052b6b36a510bcc7d99b64c51659ddee675cc5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Bag%20of%20words.md",
    "text": "In the context of natural language processing (NLP), the Bag of Words (BoW) model is a simple and commonly used ==method for text representation==. It converts text data into numerical form by treating each ==document as a collection of individual words, disregarding grammar and word order==. Here's how it works:\n\n1. Vocabulary Creation: A vocabulary is created from the entire corpus, which is a list of all unique words appearing in the documents.\n\n2. Vector Representation: Each document is represented as a vector, where each element corresponds to a word in the vocabulary. The value of each element is typically the count of occurrences of the word in the document.\n\n3. Simplicity and Limitations: While BoW is easy to implement and useful for tasks like text classification, it has limitations. It ignores word order and context, and can result in large, sparse vectors for large vocabularies.\n\nDespite its simplicity, BoW can be effective for certain NLP tasks, especially when combined with other techniques like [[TF-IDF]] to weigh the importance of words.\n\nTakes key terms of a text in normalised ==unordered== form.\n\n`CountVectorizer` from scikit-learn to convert a collection of text documents into a matrix of token counts.\n\n```python\n#Need normalize_document\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Using CountVectorizer with the custom tokenizer\nbow = CountVectorizer(tokenizer=normalize_document)\nbow.fit(corpus)  # Fitting text to this model\nprint(bow.get_feature_names_out())  # Key terms\n```\n\nRepresent each sentence by a vector of length determined by get_feature_names_out. representing the tokens contained.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "ML_Tools"
    ],
    "normalized_filename": "bag_of_words",
    "outlinks": [
      "tf-idf"
    ],
    "inlinks": [
      "nlp",
      "oov_words",
      "preprocessing_text_classification",
      "tf-idf",
      "word2vec"
    ]
  },
  {
    "category": "LANG",
    "filename": "Chain of thought",
    "sha": "9bdb17c7378283ded70bdfc7aade6e99c5b02fb9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Chain%20of%20thought.md",
    "text": "**Chain of Thought (CoT) reasoning**\n\nAsking sequenced questions that guide someone (or yourself) through a reasoning path is a core technique in problem-solving and teaching. Examples:\n\n- \"What is the known information?\"\n- \"What is being asked?\"\n- \"What patterns can we observe?\"\n- \"What similar problems have we solved before?\"\n\nUsed in in AI systems is a cognitive-inspired framework that improves the performance of large [[Language Models]] (LLMs) by explicitly guiding the AI through intermediate reasoning steps.\n\nAdvantages of Chain of Thought:\n- **Improved [[Interpretability]]**: Since the model outputs intermediate steps, it's easier for humans to understand how the final answer was reached.\n- **Better Performance on Complex Tasks**: CoT allows the model to handle multi-step reasoning more effectively.\n- **Easier Debugging**: If there's an error in reasoning, it can be spotted at a specific step in the chain, which aids in model fine-tuning and debugging.\n\nRelated to:\n- [[Model Ensemble]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "chain_of_thought",
    "outlinks": [
      "interpretability",
      "model_ensemble",
      "language_models"
    ],
    "inlinks": [
      "agent-based_modelling",
      "asking_questions",
      "llm"
    ]
  },
  {
    "category": "LANG",
    "filename": "Claude",
    "sha": "841f427669686787ce1fa3d57da83dc51563985e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Claude.md",
    "text": "Claude is better for code and uses Artifact for tracking code changes.\n\nClaude is crazy see: https://youtu.be/RudrWy9uPZE?t=473\n\nArtefacts exist",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI"
    ],
    "normalized_filename": "claude",
    "outlinks": [],
    "inlinks": [
      "evaluating_language_models"
    ]
  },
  {
    "category": "LANG",
    "filename": "ChatGPT",
    "sha": "58e48f26172efc5a46afe96384b98202bf295074",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/ChatGPT.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "chatgpt",
    "outlinks": [],
    "inlinks": [
      "reasoning_tokens",
      "thinking_systems"
    ]
  },
  {
    "category": "LANG",
    "filename": "Comparing LLMs",
    "sha": "e8e38fc94304fe64165f7c8278754c1cd7d59cbe",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Comparing%20LLMs.md",
    "text": "Use lmarena.ai as a bench marking tool. \nweb dev arena\ntext to image leader board\n\nRelated:\n[[LLM]]\n[[Hugging Face]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "comparing_llms",
    "outlinks": [
      "llm",
      "hugging_face"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Distillation",
    "sha": "d13f053d1bba5fb16275782bab78aff262c9a2db",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Distillation.md",
    "text": "Training smaller models with larger.\n\n[[Transfer Learning]]\n[[Small Language Models]]\n\n![[Pasted image 20250130074219.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "language_models",
      "ml"
    ],
    "normalized_filename": "distillation",
    "outlinks": [
      "transfer_learning",
      "small_language_models",
      "pasted_image_20250130074219.png"
    ],
    "inlinks": [
      "llm",
      "scaling_agentic_systems",
      "small_language_models"
    ]
  },
  {
    "category": "LANG",
    "filename": "ElasticSearch",
    "sha": "f91c56cec872c257f013f65bff7300b815d67ab6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/ElasticSearch.md",
    "text": "Elasticsearch is an open source distributed [[Search]] and analytics engine, often used to store and [[Search]] through text data (e.g., logs, documents, articles). It's commonly integrated with NLP workflows for:\n\n - Storing extracted named entities or keywords\n - Enabling full-text search over processed corpora\n - Ranking documents based on custom scoring\n\nUse Cases:\n\n - Search systems over preprocessed corpora\n - Document similarity lookup\n - Named entity indexing\n\nIntegration Example:\n\n - Use [[spaCy]] to extract keywords or metadata\n - Store results in Elasticsearch index\n - Use query interface to retrieve matching or related docs\n\nExploratory Questions:\n\n - How does spaCy output map to ElasticSearch indexing fields?\n - Can entity relationships or dependency trees be indexed effectively?\n - How can [[TF-IDF]] or vector search (e.g., via Elastic’s k-NN or OpenSearch) be layered in?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "tool"
    ],
    "normalized_filename": "elasticsearch",
    "outlinks": [
      "spacy",
      "tf-idf",
      "search"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Embedded Methods",
    "sha": "e5261ae34d1bfc2256dd8cefd6a53166de9bc551",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Embedded%20Methods.md",
    "text": "Embedded methods for [[Feature Selection]] ==integrate [[Feature Selection]] directly into the model training process.==\n\nEmbedded methods provide a convenient and efficient approach to feature selection by integrating it into the model training process, ultimately leading to models that are more parsimonious and potentially more interpretable.\n\n1. Incorporated into Model Training: Unlike [[Filter Methods]] and [[Wrapper Methods]], which involve feature selection as a separate step from model training, embedded methods perform feature selection simultaneously with model training. This means that feature importance or relevance is determined within the context of the model itself.\n\n2. Regularization Techniques: Embedded methods commonly use [[Regularisation]] techniques to penalize the inclusion of unnecessary features during model training. \n\n3. Automatic Feature Selection: Embedded methods automatically select the most relevant features by learning feature importance during the training process. The model adjusts the importance of features iteratively based on their contribution to minimizing the [[Loss function]].\n\n4. Examples of Embedded Methods:\n   - [[L1 Regularisation]] (L1 Regularization):\n   - [[Elastic Net]]: Elastic Net combines L1 ([[L1 Regularisation]]) and L2 ([[Ridge]]) regularization .\n   - Tree-based Methods: [[Decision Tree]] and ensemble methods like [[Random Forest]] and [[Gradient Boosting]] inherently perform feature selection during training by selecting the most informative features at each split node of the tree.\n   - [[CART]]\n\n5. Advantages:\n   - Simplicity: Embedded methods simplify the feature selection process by integrating it into model training, reducing the need for additional preprocessing steps.\n   - Efficiency: Because feature selection is performed during model training, embedded methods can be more computationally efficient compared to wrapper methods, which require training multiple models.\n\n6. Considerations:\n   - [[Hyperparameter Tuning]]: Tuning regularization parameters or other model-specific parameters may be necessary to optimize feature selection performance.\n   - Model [[Interpretability]]: While embedded methods can automatically select features, interpreting the resulting model may be challenging, especially for complex models like ensemble methods.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "selection"
    ],
    "normalized_filename": "embedded_methods",
    "outlinks": [
      "regularisation",
      "decision_tree",
      "filter_methods",
      "elastic_net",
      "l1_regularisation",
      "gradient_boosting",
      "loss_function",
      "random_forest",
      "cart",
      "hyperparameter_tuning",
      "wrapper_methods",
      "feature_selection",
      "interpretability",
      "ridge"
    ],
    "inlinks": [
      "feature_selection"
    ]
  },
  {
    "category": "LANG",
    "filename": "Evaluate Embedding Methods",
    "sha": "df2af76170aeea897ab1fbc6f3719d6ba1bb7be3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Evaluate%20Embedding%20Methods.md",
    "text": "#### How to Evaluate Embedding Methods\n\n##### Semantic Relationship Fidelity\n\nA good [[Vector Embedding|embedding]] should place semantically similar sentences or words closer together in the embedding space. You can test this using:\n\n a) [[Cosine Similarity]]\n\n* Compute the [[Cosine Similarity]] between embedding vectors.\n* Higher similarity between semantically related pairs (e.g., *\"Paris is the capital of France\"* vs *\"France’s capital is Paris\"*) indicates better embedding quality.\n* Compare scores across methods:\n\n  ```python\n  from sklearn.metrics.pairwise import cosine_similarity\n  similarity = cosine_similarity([vec1], [vec2])\n  ```\n\n b) Analogy or Word Arithmetic\n\n* Test whether embeddings support compositional reasoning.\n* Example:\n  $\\text{Embedding}(\\text{king}) - \\text{Embedding}(\\text{man}) + \\text{Embedding}(\\text{woman}) \\approx \\text{queen}$\n* This shows if semantic and syntactic dimensions are meaningfully encoded ([[syntactic relationships]]).\n\n c) Clustering Consistency\n\n* Cluster the embeddings (e.g. via k-means) and evaluate whether related texts group together.\n* Measure cluster cohesion and separation (e.g. using Silhouette Score).\n##### Information Content & Sparsity\n\n a) Use [[TF-IDF]] as Baseline\n\n* TF-IDF scores highlight the most important words in a text.\n* Evaluate how well dense embeddings retain the importance structure identified by TF-IDF.\n* For example, check whether high TF-IDF words receive higher attention in models like BERT (via attention weights) or influence sentence embedding directions.\n\n##### Downstream Task Performance\n\nTrain simple classifiers (e.g. [[Logistic Regression]]) on embeddings to predict:\n  * Sentiment\n  * Topic\n  * Semantic similarity class (entailment, contradiction, etc.)\n\nBetter embeddings typically yield better accuracy/F1 on such tasks.\n\n##### Visual Inspection\n\n a) [[Dimensionality Reduction]]\n* Use [[Principal Component Analysis|PCA]], [[t-SNE]], or [[UMAP]] to project embeddings into 2D.\n* Visually inspect whether semantically similar items form coherent clusters.\n\n#### Guiding Questions\n\n* Do the embeddings distinguish fine-grained semantic shifts (e.g., “bank” as a financial institution vs riverbank)?\n* Are word or sentence embeddings stable across paraphrased or reordered text?\n* Do similar sentences result in embeddings with high cosine similarity?\n* How well do embeddings handle [[OOV words]] or rare terms?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "evaluation",
      "nlp"
    ],
    "normalized_filename": "evaluate_embedding_methods",
    "outlinks": [
      "syntactic_relationships",
      "vector_embedding",
      "t-sne",
      "umap",
      "oov_words",
      "logistic_regression",
      "cosine_similarity",
      "principal_component_analysis",
      "dimensionality_reduction",
      "tf-idf"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Generative AI From Theory to Practice",
    "sha": "22a1e5f59243cdec152f3bde1e1326455f00a3d1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Generative%20AI%20From%20Theory%20to%20Practice.md",
    "text": "### Objective:\n\n How do LLMs work and operate.  \n Enabling [[LLM]]'s at scale:\n Explore recent AI and [[Generative AI]] language models \n\n\n### Steps\n\nMath on words: Turn words into coordinates.\nStatistics on words: Given context what is the probability of whats next.\nVectors on words. [[Cosine Similarity]]\nHow train: Use [[Markov chain]] for prediction of the next [[Tokenisation]]\n\n\nTokeniser: map from token to number\n\n1. Pre-training: tokenise input using [[NLP]] techqinues\n2. [[LLM]] looks at context: nearby tokens, in order to predict\n\ndifferent implmentationg for differnet languages. Differnet tokenisers or translating after.\n\nJourney to scale:\n\n1. Demos, POC (plan to scale): understand limitations\n2. Beyond experiments and before production: \n3. Enterprise level: translate terms so they can use governess techniques.\n\nBuilding:\n\n![[Pasted image 20240524130607.png]]\n\n### [[Software Development Life Cycle]]\n\nFor GenAI: Building an applicaiton with GenAi features\n\n1. Plan: use case: prompts : archtecture: cloud or on site\n2. Build: vector database\n3. Test: Quality and responsible ai. \n\n### [[call summarisation]]\n\ntake transcript - > summariser -> summarise\n\nSource: human labeled transcripts to check summariser. \n\n![[Pasted image 20240524131311.png|500]]\n\n[[Ngrams]] analysis - when specific words realy matter\n\n\n### [[RAG]]\n\nUse relvant data to make response better:\n\n![[Pasted image 20240524131603.png]]\n\n## [[GAN]]\n\nFor image models.\n\nExamples: midjourney,stable diffusion,dall-e 3\n\nimage model techniques:\n- text to image\n- image to image\n## Notes: \n\nUse [[LLM]]'s to get short info, then cluster.\nGoing round training data : called a Epochs",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI"
    ],
    "normalized_filename": "generative_ai_from_theory_to_practice",
    "outlinks": [
      "llm",
      "markov_chain",
      "generative_ai",
      "pasted_image_20240524130607.png",
      "pasted_image_20240524131311.png",
      "ngrams",
      "gan",
      "cosine_similarity",
      "software_development_life_cycle",
      "tokenisation",
      "pasted_image_20240524131603.png",
      "nlp",
      "call_summarisation",
      "rag"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Fuzzywuzzy",
    "sha": "9accbf4ff18a821c246d8be6fbd0a9a2f1b6e333",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Fuzzywuzzy.md",
    "text": "Tool used for correcting spelling with [[Pandas]].\n\n[[Data Cleansing]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "fuzzywuzzy",
    "outlinks": [
      "data_cleansing",
      "pandas"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Generative AI",
    "sha": "a64bdb75a7dabba9638de660b55e33a28b3e5ed2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Generative%20AI.md",
    "text": "Tools that generate content.\n\nMainly:\n- Text\n- Images",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI"
    ],
    "normalized_filename": "generative_ai",
    "outlinks": [],
    "inlinks": [
      "assessing_gen_ai_generated_content",
      "generative_ai_from_theory_to_practice",
      "guardrails",
      "how_businesses_use_gen_ai",
      "how_to_reduce_the_need_for_gen_ai_responses",
      "inference_versus_prediction",
      "knowledge_graph",
      "typical_output_formats_in_neural_networks"
    ]
  },
  {
    "category": "LANG",
    "filename": "Grammar method",
    "sha": "19b7d8c2f58bd5004eec11fdb6545b07181b7f82",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Grammar%20method.md",
    "text": "Can understand the Grammar as a method for acceptable sentences.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "grammar_method",
    "outlinks": [],
    "inlinks": [
      "nlp"
    ]
  },
  {
    "category": "LANG",
    "filename": "Guardrails",
    "sha": "36dfa86c0ad7149123de39629a7b6558169aedc1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Guardrails.md",
    "text": "Controlling a [[Generative AI]] in business through the use of [[Guardrails]] ensures that the AI remains aligned with specific business goals and avoids unintended or harmful outputs. Guardrails are essential for maintaining security, compliance, and reliability in AI systems. Here's an outline based on your notes:\n\n### 1. Input Guardrails\n\n   - Prompt Injection Control: [[Prompts]] To prevent users from prompting the AI in ways that could result in harmful or inappropriate responses, filtering or validating inputs can be essential. This reduces the risk of the model being \"jailbroken\" (i.e., forced to generate outputs outside its intended use case).\n   - Topic Restriction: Limit the AI’s inputs to specific business-relevant topics. For instance, if the AI is designed for customer support, it should ignore inputs about unrelated topics (e.g., entertainment or politics).\n   - User Authentication: Depending on business needs, certain input guardrails can restrict access to specific features or sensitive information based on user credentials or roles.\n\n### 2. Output Guardrails\n\n   - Content Moderation: Post-processing can be applied to outputs to ensure they align with business values, compliance regulations, or safety standards. For example, any harmful or offensive language can be filtered out.\n   - Pre-defined Boundaries: Limit the AI’s responses to fall within specific domains. For instance, when the AI is asked questions outside its scope, it can respond with a predefined message, such as \"I am not programmed to handle that topic.\"\n   - Compliance and Ethical Constraints: Outputs can be regulated to ensure the model adheres to legal, ethical, and regulatory constraints, which is especially important in industries like finance or healthcare.\n\n### 3. Jailbreaking Concerns\n\n   - Jailbreaking occurs when a user manipulates the system to bypass these guardrails, leading to undesirable outputs. This depends on the business context—some may tolerate more flexible AI behavior, while others, like legal or healthcare firms, need strict controls.\n\n### 4. Business-Specific Use Cases\n\n   - Tailor the AI to address specific business needs. For example, a generative AI for a legal firm should stick to legal advice and documentation, whereas a customer service chatbot should handle predefined topics like returns and product support.\n   - [[Data Observability|monitoring]] / Monitoring and Logging: Keep track of input and output interactions to ensure that the AI’s performance remains within its intended boundaries.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "GenAI"
    ],
    "normalized_filename": "guardrails",
    "outlinks": [
      "guardrails",
      "prompts",
      "data_observability",
      "generative_ai"
    ],
    "inlinks": [
      "guardrails"
    ]
  },
  {
    "category": "LANG",
    "filename": "How LLMs store facts",
    "sha": "3604c494cd1f8a9d0d4ced7bce3abd2d806cbef9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/How%20LLMs%20store%20facts.md",
    "text": "[How might LLMs store facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8&list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&index=6)\n\nNot solved\n\nHow do [[Multilayer Perceptrons]] store facts?\n\nDifferent directions encode information in [[Vector Embedding]] space.\n\nMLP's are blocks of vectors, these are acted on my the context matrix \n\n[[Johnson–Lindenstrauss lemma]]\n\nSparse Autoencoder - used in [[Interpretability]] of [[LLM]] responses\n\nSee [[Anthropic]] posts\n- https://transformer-circuits.pub/2022/toy_model/index.html#adversarial\n- https://transformer-circuits.pub/2023/monosemantic-features",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "agents",
      "NLP"
    ],
    "normalized_filename": "how_llms_store_facts",
    "outlinks": [
      "vector_embedding",
      "llm",
      "anthropic",
      "johnson–lindenstrauss_lemma",
      "multilayer_perceptrons",
      "interpretability"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "How businesses use Gen AI",
    "sha": "a8a8dd0062406490320b64e142fe1afecacab935",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/How%20businesses%20use%20Gen%20AI.md",
    "text": "Businesses leverage [[Generative AI]] to transform various operations, using models like OpenAI, Gemini (Google Cloud), Anthropic, and Meta models. These models provide services through cloud providers, making them accessible via APIs. Key use cases include:\n\n1. **Content Creation**: Generative AI can produce text, images, code, and even videos, enhancing marketing, design, and communication efforts.\n2. **Customer Support**: AI chatbots and assistants automate customer interactions, reducing response times and improving service quality.\n3. **Data Analysis & Insights**: Models help businesses analyze large datasets, enabling predictive analytics and trend forecasting.\n4. **Customization**: Personalization of products and services, such as tailored recommendations or [[transactional journeys]]/customer experiences, is powered by generative AI.\n5. **Multi-Model Access**: Enterprises use AI gateways to integrate multiple generative models, allowing them to choose the best model for specific tasks based on performance or cost efficiency.\n\nCloud providers like **Google Cloud (Gemini)** or **Microsoft Azure (OpenAI)** offer easy integration of these models into business workflows through APIs, streamlining deployment for large-scale applications\n\n## AI Gateway?\n\nAn AI Gateway is a middleware platform that simplifies and secures interactions between AI models and applications. In this context, businesses use AI gateways to streamline the integration, management, and deployment of generative AI models like those provided by OpenAI, Google (Gemini), and Anthropic. AI gateways provide the following key benefits:\n\n1. **Model Access and Management**: They centralize access to multiple AI models via APIs, making it easier for businesses to switch between or utilize multiple AI models for different tasks.\n2. **Security and Governance**: AI gateways add layers of security, enabling compliance with regulations and protecting proprietary data when using external AI services [1] . [2]\n3. **Performance Optimization**: By handling the AI model interactions efficiently, gateways can reduce latency and improve [[model performance]] in business applications [3]\n## 🌐 Sources\n1. [konghq.com - What is an AI Gateway? Concepts and Examples](https://konghq.com/blog/enterprise/what-is-an-ai-gateway)\n2. [ibm.com - How an AI Gateway provides leaders with greater control](https://www.ibm.com/blog/announcement/how-an-ai-gateway-provides-greater-control-and-visibility-into-ai-services/)\n3. [traefik.io - AI Gateway: What Is It? How Is It Different From API Gateway?](https://traefik.io/glossary/ai-gateway/)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "GenAI"
    ],
    "normalized_filename": "how_businesses_use_gen_ai",
    "outlinks": [
      "model_performance",
      "generative_ai",
      "transactional_journeys"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "How to reduce the need for Gen AI responses",
    "sha": "d98cf67b3e79b7342efa72c45db3601ff7a281e8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/How%20to%20reduce%20the%20need%20for%20Gen%20AI%20responses.md",
    "text": "Reducing the need for frequent [[Generative AI]] (Gen AI) responses can be done by leveraging techniques such as [[caching]] and setting up predefined [[transactional journeys]]. Here's a breakdown:\n\n1. **Caching AI Responses**: Caching allows storing frequently requested AI responses and reusing them. This reduces the number of queries to the AI model, thus lowering both response time and cost. For example, common queries like \"How do I reset my password?\" can be cached for quick reuse without engaging the AI model each time (1).\n\n2. **Predefined Transactional Journeys**: For repetitive tasks (e.g., \"I want to close my account\"), predefined ==workflows== or \"journeys\" can be set up. These automate processes without requiring AI interaction. This is ideal for tasks like bill payments, account management, or order cancellations, where responses can be scripted or handled by traditional logic, bypassing AI.\n\n### Examples of User Journeys:\n- **Account Closure**: Guiding users through the steps to close an account without involving AI.\n- **Password Reset**: Automating the reset process with predefined steps.\n- **Order Tracking**: Providing real-time updates using existing tracking systems.\n## 🌐 Sources\n1. [medium.com - How Cache Helps in Generative AI Response and Cost Optimization](https://medium.com/@punya8147_26846/how-cache-helps-in-generative-ai-response-and-cost-optimization-9a6c9be058bb)\n2. [medium.com - Slash Your AI Costs by 80%](https://medium.com/gptalk/slash-ai-costs-by-80-the-game-changing-power-of-prompt-caching-d44bcaa2e772)\n3. [botpress.com - How to Optimize AI Spend Cost in Botpress](https://botpress.com/blog/how-to-optimize-ai-spend-cost-in-botpress)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "GenAI"
    ],
    "normalized_filename": "how_to_reduce_the_need_for_gen_ai_responses",
    "outlinks": [
      "caching",
      "generative_ai",
      "transactional_journeys"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "How would you decide between using TF-IDF and Word2Vec for text vectorization",
    "sha": "4570fe8a676195a91201c015aaa2fad1839b7bde",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/How%20would%20you%20decide%20between%20using%20TF-IDF%20and%20Word2Vec%20for%20text%20vectorization.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "how_would_you_decide_between_using_tf-idf_and_word2vec_for_text_vectorization",
    "outlinks": [],
    "inlinks": [
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "In NER how would you handle ambiguous entities",
    "sha": "a77df0dc8a57adea8fda7e133ef1fd8ac89a7348",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/In%20NER%20how%20would%20you%20handle%20ambiguous%20entities.md",
    "text": "Handling ambiguous entities in [[Named Entity Recognition]] (NER) can be quite challenging. Here are some strategies that can be employed:\n\n1. **Contextual Analysis**: Utilize the surrounding ==context== of the ambiguous entity to determine its correct classification. For example, the word \"Apple\" could refer to the fruit or the company, but the context in which it appears can help disambiguate its meaning.\n\n2. **Disambiguation Models**: Implement additional models specifically designed for entity disambiguation. These models can leverage knowledge bases or ontologies to determine the most likely entity based on context.\n\n3. **Multi-label Classification**: Instead of forcing a single label, allow for multiple possible labels for ambiguous entities. This can be useful in cases where an entity might belong to more than one category.\n\n4. **Training Data**: Ensure that the training dataset includes examples of ambiguous entities in various contexts. This can help the model learn to recognize and differentiate between them.\n\n5. **User Feedback**: Incorporate user feedback mechanisms to refine the model's predictions. If users can correct or confirm entity classifications, this can improve the model over time.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "in_ner_how_would_you_handle_ambiguous_entities",
    "outlinks": [
      "named_entity_recognition"
    ],
    "inlinks": [
      "named_entity_recognition"
    ]
  },
  {
    "category": "LANG",
    "filename": "Key Components of Attention and Formula",
    "sha": "d924297f712d70d8bc2a4e204bdc163b5d438e38",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Key%20Components%20of%20Attention%20and%20Formula.md",
    "text": "### Key Components of Attention and Formula\n\n1. Query: Represents the current word or position that requires attention: $Q$\n2. Key: Represents each word in the input sequence: $K$\n3. Value: Represents the actual content or information in the input sequence: $V$\n4. Attention Scores: The attention mechanism computes the relevance between the query and each key by computing a similarity score (such as dot-product or other scoring methods).\n5. Softmax: These scores are then passed through a softmax function to form a probability distribution, which gives us the attention weights.\n6. Context Vector: A weighted sum of the values ($V$), using the attention weights, is computed. This context vector is what the model uses to generate the output token.\n\nGiven a query matrix, key matrix, and value matrix, attention is calculated as:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nWhere:\n- $Q$,$K$, and $V$are matrices of query, key, and value vectors.\n- $d_k$ is the dimension of the keys.\n- The softmax is applied row-wise to produce attention weights.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "NLP"
    ],
    "normalized_filename": "key_components_of_attention_and_formula",
    "outlinks": [],
    "inlinks": [
      "attention_mechanism"
    ]
  },
  {
    "category": "LANG",
    "filename": "Knowledge graph vs RAG setup",
    "sha": "23bbbef60a51699c3bfedf16f7a71df01141fe9e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Knowledge%20graph%20vs%20RAG%20setup.md",
    "text": "### Comparison: Knowledge Graph vs. RAG Setup\n\n- ==**Knowledge Graphs** are structured representations of entities and their relationships, designed primarily for querying, reasoning, and storing factual information.==\n- ==**RAG setups** enhance generative models by retrieving external knowledge (from unstructured or semi-structured data) and integrating it into the generation process.==\n\nWhile not the same, these two concepts can be used together to build systems that combine structured knowledge retrieval with the natural language generation capabilities of RAG models.\n\nWhile **knowledge graphs** and **RAG** are distinct, they can be integrated to improve certain systems:\n- ==A **RAG model** could use a **knowledge graph** as the retrieval source.== Instead of retrieving unstructured text documents, the RAG model could retrieve structured, factual triples from a knowledge graph and incorporate this into the generation process. This would improve the accuracy of fact-based questions and answers.\n\nA [[Knowledge Graph]] and a **Retrieval-Augmented Generation ([[RAG]])** setup are related but distinct concepts, particularly in how they handle knowledge representation and retrieval. While they can complement each other in certain applications, they serve different purposes and operate in different ways.\n\n| Aspect                      | Knowledge Graph                                                                         | RAG Setup                                                                             |\n| --------------------------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n| **Purpose**                 | Stores and organizes knowledge for querying and reasoning                               | Combines retrieval of external information with ==text generation==                   |\n| **Data Structure**          | ==Highly structured== (graph with nodes and edges)                                      | Unstructured or semi-structured (documents, text snippets)                            |\n| **Retrieval Mechanism**     | Queries are made through graph traversal or SPARQL-like languages                       | Information is retrieved via search mechanisms (e.g., dense embeddings)               |\n| **Usage**                   | Often used for querying factual data, answering structured queries, [[Semantic Relationships]] | Used to enhance the factual accuracy of generative models by retrieving external data |\n| **Reasoning and Inference** | Capable of logical reasoning based on relationships                                     | Does not perform reasoning; it retrieves and integrates relevant text                 |\n| **Scalability**             | Requires careful design to manage large, complex graphs                                 | Can handle large text corpora, but retrieval quality affects the final generation     |\n| **Generative Capabilities** | Not generative (focused on querying existing knowledge)                                 | [[Generative]] (synthesizes and generates natural language responses)                 |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "data_structure",
      "GenAI",
      "memory_management"
    ],
    "normalized_filename": "knowledge_graph_vs_rag_setup",
    "outlinks": [
      "rag",
      "knowledge_graph",
      "semantic_relationships",
      "generative"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "LLM",
    "sha": "0d7d1fe2b156865896b6e20f983d734101e2ef9c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/LLM.md",
    "text": "A Large Language Model (LLM) is a type of language model designed for language understanding and generation. They can perform a variety of tasks, including:\n\n- Text generation\n- Machine translation\n- Summary writing\n- Image generation from text\n- Machine coding\n- Chatbots or Conversational AI\n# Questions\n\n- [[How do we evaluate of LLM Outputs]]\n- [[LLM Memory|What is LLM memory]]\n- [[Relationships in memory|Managing LLM memory]]\n- [[Mixture of Experts]]: having multiple experts instead of one big model.\n- [[Distillation]]\n- [[Mathematics]] on the parameter usage [[Attention mechanism]]\n- Use of [[Reinforcement learning]] in training [[Chain of thought]] methods in LLM's (deepseek)\n\n## How do Large Language Models (LLMs) Work?\n\nLarge [[Language Models]] (LLMs) are a type of artificial intelligence model that is designed to understand and generate human language. Key aspects of how they work include:\n\n- Word Vectors: LLMs represent words as long lists of numbers, known as word vectors ([[Vector Embedding|word embedding]]).\n- Neural Network Architecture: They are built on a neural network architecture known as the [[Transformer]]. This architecture enables the model to identify relationships between words in a sentence, irrespective of their position in the sequence.\n- [[Transfer Learning]]: LLMs are trained using a technique known as transfer learning, where a pre-trained model is adapted to a specific task.\n\n## Characteristics of LLMs\n\n- ==Non-Deterministic:== LLMs are non-deterministic, meaning the types of problems they can be applied to are of a probabilistic nature (==temperature==).\n- Data Dependency: The performance and behaviour of LLMs are heavily influenced by the data they are trained on.",
    "aliases": [
      "Large Language Models",
      "LLMs"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "llm",
    "outlinks": [
      "llm_memory",
      "vector_embedding",
      "reinforcement_learning",
      "attention_mechanism",
      "transfer_learning",
      "transformer",
      "chain_of_thought",
      "how_do_we_evaluate_of_llm_outputs",
      "mixture_of_experts",
      "language_models",
      "relationships_in_memory",
      "mathematics",
      "distillation"
    ],
    "inlinks": [
      "agent-based_modelling",
      "comparing_llms",
      "data_analysis",
      "deep_learning",
      "evaluating_language_models",
      "generative_ai_from_theory_to_practice",
      "how_do_we_evaluate_of_llm_outputs",
      "how_llms_store_facts",
      "inference_versus_prediction",
      "johnson–lindenstrauss_lemma",
      "langchain",
      "language_models",
      "language_models_large_(llms)_vs_small_(slms)",
      "llm_evaluation_metrics",
      "neural_scaling_laws",
      "prompt_engineering",
      "rag",
      "scaling_agentic_systems",
      "small_language_models",
      "vector_database"
    ]
  },
  {
    "category": "LANG",
    "filename": "LLM Memory",
    "sha": "d4f2b6adab3ee19ae7f660b2f0950a8bf2d4390a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/LLM%20Memory.md",
    "text": "Memory in large [[Language Models]] (LLMs) involves managing context windows to enhance reasoning capabilities without the high costs associated with traditional training methods. The goal of [[LLM Memory]] is to address challenges like \"forgetting,\" where LLMs struggle to retain context across interactions.\n## Key Concepts:\n\nForgetting Context:\nUnderstanding how and why LLMs lose context, especially in multi-turn dialogues, and its impact on response accuracy. Forgetting occurs due to the limitations of fixed context windows, manifesting differently in single-turn (immediate forgetting) versus multi-turn interactions (progressive loss of context).\n\nPrioritization of Context:\nTechniques for determining which parts of the context are most relevant and need to be retained, optimizing memory usage.\n\nTime Length of Memory:\nBalancing how long memory should be maintained to ensure it remains useful and relevant over time.\n\nDynamic Memory Management:\nAdapting memory structures in real-time to accommodate evolving knowledge and interactions.\n\nIn-Context Memory:\nMemory tied to specific interactions, making it more relevant and easier to apply in particular scenarios.\n\nMulti-turn Interactions:\nAddressing context retention across multiple interactions, emphasizing the importance of maintaining coherence over extended conversations.\n## Types of Memory:\n\nSemantic Memory:\nFocuses on the meaning and [[Semantic Relationships]] between concepts, which is crucial for improving LLM reasoning and context understanding.\n\nHierarchical Memory:\nBalances immediate retrieval with long-term storage of information, enabling better performance in various applications.\nSupports evolving and persistent memory systems tailored to specific tasks.",
    "aliases": [
      "context",
      "What is LLM memory"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "llm_memory",
    "outlinks": [
      "llm_memory",
      "semantic_relationships",
      "language_models"
    ],
    "inlinks": [
      "ai_agents_memory",
      "faiss",
      "generators_in_python",
      "langchain",
      "llm",
      "llm_memory"
    ]
  },
  {
    "category": "LANG",
    "filename": "Language Model Output Optimisation",
    "sha": "a8505b6b71ef267d789f83a575aecfd7b921db26",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Language%20Model%20Output%20Optimisation.md",
    "text": "What techniques from [[information theory]] can be used to measure and optimize the amount of information conveyed by an language model?\n\nIn information theory, several techniques can be applied to measure and optimize the amount of information conveyed by an [[Language Models]].\n\n1. Entropy: Entropy measures the uncertainty or unpredictability of a random variable. In the context of language models, it can be used to quantify the uncertainty in predicting the next word in a sequence. Lower entropy indicates more predictable and informative outputs.\n\n2. [[Cross Entropy]]: This measures the difference between two probability distributions. For language models, cross-entropy can be used to evaluate how well the predicted distribution of words matches the actual distribution in the data. Minimizing cross-entropy during training helps optimize the model's predictions.\n\n3. Perplexity: Perplexity is a common metric for evaluating language models. It is the exponentiation of the cross-entropy and represents the model's uncertainty in predicting the next word. Lower perplexity indicates a better-performing model.\n\n4. Mutual Information: This measures the amount of information shared between two variables. In language models, it can be used to assess how much information about the input is retained in the output, helping to optimize the model's ability to convey relevant information.\n\n5. KL Divergence: Kullback-Leibler divergence measures how one probability distribution diverges from a second, expected probability distribution. It can be used to optimize language models by minimizing the divergence between the predicted and true distributions.\n\n6. Information Bottleneck: This technique involves finding a balance between compressing the input data and preserving relevant information for the task. It can be used to optimize models by focusing on the most informative features.\n\n7. Rate-Distortion Theory: This involves finding the trade-off between the fidelity of the information representation and the amount of compression. It can be applied to optimize language models by balancing the complexity of the model with the quality of the information conveyed.\n\n8. [[Attention mechanism]]: While not strictly an information theory concept, attention mechanisms in neural networks can be seen as a way to dynamically allocate information processing resources, focusing on the most informative parts of the input.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "optimisation"
    ],
    "normalized_filename": "language_model_output_optimisation",
    "outlinks": [
      "attention_mechanism",
      "information_theory",
      "language_models",
      "cross_entropy"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Language Models Large (LLMs) vs Small (SLMs)",
    "sha": "12d3fbd14b8254b01aa5badcf48a18adda64e598",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Language%20Models%20Large%20(LLMs)%20vs%20Small%20(SLMs).md",
    "text": "### Overview\nLanguage models can be categorized into large language models ([[LLM]]) and small language models ([[SLM]]). While LLMs boast extensive general-purpose knowledge and capabilities, SLMs offer distinct advantages in certain scenarios, particularly when it comes to efficiency, resource constraints, and task-specific environments.\n\n### Key Differences\n\n| Aspect             | LLMs                                              | SLMs                                                 |\n|--------------------|---------------------------------------------------|------------------------------------------------------|\n| Accuracy        | Higher accuracy across broad tasks due to large datasets and extensive training. | Comparable performance in domain-specific tasks after fine-tuning. |\n| Efficiency      | Computationally expensive; requires significant resources for training and inference. | More resource-efficient; suited for edge devices and real-time applications. |\n| [[Interpretability]]| Often a \"black box\"; difficult to explain decision-making. | More interpretable due to simpler architecture. |\n| Generality      | General-purpose; capable of handling a wide range of tasks. | Task-specific; excels in specific domains and structured data. |",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "language_models_large_(llms)_vs_small_(slms)",
    "outlinks": [
      "llm",
      "slm",
      "interpretability"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Language Models",
    "sha": "165a2f26347fe2f9b536963bd9fee6e088d06af1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Language%20Models.md",
    "text": "A language model is a machine learning model that is designed to understand, generate, and predict human language. \n\nIt does this by analyzing large amounts of text data to learn the patterns, structures, and relationships between words and phrases. \n\nThey work by assigning probabilities to sequences of words, allowing them to predict the next word in a sentence or generate coherent text based on a given prompt.\n\nRelated to:\n[[LLM]]\n[[Small Language Models|SLM]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "language_models",
    "outlinks": [
      "llm",
      "small_language_models"
    ],
    "inlinks": [
      "chain_of_thought",
      "curse_of_dimensionality",
      "knowledge_graph",
      "langchain",
      "language_model_output_optimisation",
      "llm",
      "llm_memory",
      "small_language_models",
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "Local LLM use cases",
    "sha": "5cb42584c7bb8433d086dae576494ec6d32c8719",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Local%20LLM%20use%20cases.md",
    "text": "- [ ] Can you load a local model onto a SD card and run it on a raspberry pi?\n\nWhy small models work:\n- They’re faster, cheaper, and can run on consumer hardware (laptops, even Raspberry Pi-level devices with optimisations) while giving acceptable quality for narrow, well-defined tasks. Pairing them with a vector store (e.g., Chroma, [[Weaviate]], Milvus) for [[RAG]] can dramatically boost usefulness without increasing model size.\n### Use cases\n\n#### Text Processing & Automation\n* Template filling – e.g., generating structured responses, filling in report fields.\n* Summarisation – condensing meeting transcripts or local documents without sending data to the cloud.\n* Classification – tagging or categorising requests, tickets, or files.\n* Text cleaning – grammar correction, standardising language for logs or reports.\n\n#### Domain-Specific Models\n* Fine-tune a small LLM for:\n  * Industry jargon translation (e.g., maintenance logs → plain English).\n  * Technical troubleshooting guides.\n  * Incident classification in operations or engineering.\n* Works well when paired with RAG (Retrieval-Augmented Generation) from a local knowledge base.\n#### Edge & Offline Scenarios\n* Field work in remote areas (e.g., utilities, scientific expeditions).\n* IoT devices with natural language interfaces.\n* Portable knowledge assistants for technicians, inspectors, or surveyors.\n#### Educational & Training Tools\n* Interactive Q\\&A tutors for company onboarding.\n* Scenario-based training simulations where the model plays a role.\n\n#### Related:\n- [[Small Language Models]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "local_llm_use_cases",
    "outlinks": [
      "small_language_models",
      "rag",
      "weaviate"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Mathematical Reasoning in Transformers",
    "sha": "4fd17b645b8cca36f3796777c8914c836d485f26",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Mathematical%20Reasoning%20in%20Transformers.md",
    "text": "**transformer-based models** that address mathematical reasoning either through pretraining, hybrid systems, or fine-tuning on specific mathematical tasks\n\n- **Challenges**: General-purpose transformers [[Transformer|Transformer]] are trained primarily on large corpora of text, which include mathematical problems but lack systematic and rigorous math-specific training. This results in limited capabilities for handling complex calculations or abstract algebraic problems.\n\n- **Grokking in Mathematical Reasoning**: This is an area of research where models are trained on small datasets of synthetic math problems to encourage **grokking**, a phenomenon where the model suddenly achieves near-perfect performance after extended training. Researchers are interested in how transformers might be able to **\"[[grok]]\"** math concepts after seeing many examples.\n\nmath data sets: MATH dataset,Aristo\n\nPretrained transformers on math specific data.\n\n[[GPT-f]] represents a significant advancement in the use of **transformer-based models for mathematical reasoning**,",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "#question"
    ],
    "normalized_filename": "mathematical_reasoning_in_transformers",
    "outlinks": [
      "transformer",
      "gpt-f",
      "grok"
    ],
    "inlinks": [
      "reasoning_tokens",
      "symbolic_computation"
    ]
  },
  {
    "category": "LANG",
    "filename": "Mixture of Experts",
    "sha": "a2470bbd11a24e1383a7959c9456d4354654c774",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Mixture%20of%20Experts.md",
    "text": "Different parts of the network focusing on parts of the questions\n\nRouting, distribution\n\nactivating",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "mixture_of_experts",
    "outlinks": [],
    "inlinks": [
      "llm"
    ]
  },
  {
    "category": "LANG",
    "filename": "Multi-head attention",
    "sha": "28b04d6b985eb8273f46db3a2f0b4c3a189355e0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Multi-head%20attention.md",
    "text": "Multi-head attention extends the standard [[Attention mechanism]] by enabling the model to attend to different parts of an input sequence simultaneously, capturing diverse relationships-both local and global.\n\n#### Why Use Multi-Head Attention?\n\n* Multiple Focus Areas: Each head attends to different parts of the sequence. Some capture short-range (syntactic) relationships, others long-range (semantic) dependencies.\n* Diverse Representations: Each head operates in a distinct learned subspace, allowing the model to represent the same input in multiple ways.\n* Richer Contextual Understanding: By aggregating these views, the model gains a more expressive and nuanced understanding of the input.\n\n#### How It Works (Simplified Steps)\n\n1. Linear Projections: Input tokens are projected into queries ($Q$), keys ($K$), and values ($V$) separately for each head.\n2. Independent Attention: Each head computes attention scores and outputs a context vector.\n3. Concatenation: Outputs from all heads are concatenated.\n4. Final Projection: A linear transformation combines the multi-head output into a single vector.\n\n#### Example Applications\n\nIn language translation, heads might focus on:\n  * Aligning subject-verb structures\n  * Resolving pronoun references\n  * Handling grammatical reordering between source and target languages\n\nIn semantic tasks, they can disambiguate words (e.g., “bank” as riverbank or financial institution) by attending to context.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "normalized_filename": "multi-head_attention",
    "outlinks": [
      "attention_mechanism"
    ],
    "inlinks": [
      "self_attention_vs_multi-head_attention",
      "transformer"
    ]
  },
  {
    "category": "LANG",
    "filename": "Model Cascading",
    "sha": "58e48f26172efc5a46afe96384b98202bf295074",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Model%20Cascading.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "model_cascading",
    "outlinks": [],
    "inlinks": [
      "small_language_models"
    ]
  },
  {
    "category": "LANG",
    "filename": "NER Implementation",
    "sha": "3fa63aa81ba43ee3cf9a0de62f46400eddb810e9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/NER%20Implementation.md",
    "text": "```python\nimport spacy\n# Load spaCy model for NER\n!python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n# ===============================================\n# 6. EXTRACT COMPANY NAMES (NER)\n# ===============================================\ndef extract_companies(text):\n    doc = nlp(text)\n    return [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n\ndf['companies'] = df['headline'].apply(extract_companies)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "ner_implementation",
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ]
  },
  {
    "category": "LANG",
    "filename": "NLP Portal",
    "sha": "91c895fde73847254a7c736db4b6f999f7f0e2a0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/NLP%20Portal.md",
    "text": "",
    "aliases": [],
    "date modified": "28-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "nlp_portal",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "NLP",
    "sha": "71bab842e722f19e37ec9af7c2dcfabb96a142f8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/NLP.md",
    "text": "Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It encompasses various techniques and models to process and analyze large amounts of natural language data.\n\n## Key Concepts\n\n### [[Preprocessing]]\n- [[Normalisation of Text]]: The process of converting text into a standard format, which may include lowercasing, removing punctuation, and stemming or [[lemmatization]].\n- [[Part of speech tagging]]: Assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text.\n- Understanding a sentence: Participants and actions.\n\n### Models\n- [[Bag of Words]]: Represents text data by counting the occurrence of each word in a document, ignoring grammar and word order. It takes key terms of a text in normalized unordered form.\n- [[TF-IDF]]: Stands for Term Frequency-Inverse Document Frequency. It improves on Bag of Words by considering the importance of a word in a document relative to its frequency across multiple documents.\n- Vectorization: Converting text into numerical vectors. Techniques like Bag of Words, TF-IDF, or [[Vector Embedding]] (e.g., Word2Vec, GloVe) are used to represent text data numerically.\n\n### Analysis\n- [[One-hot encoding]]: Converts categorical data into a binary vector representation, indicating the presence or absence of a word from a list in the given text.\n\n### Methods\n- [[Ngrams]]: Creates tokens from groupings of words, not just single words. Useful for capturing context and meaning in text data.\n- [[Grammar method]]: Involves analyzing the grammatical structure of sentences to extract meaning and relationships between words.\n\n### Actions\n- [[Summarisation]]: The process of distilling the most important information from a text to produce a concise version.\n\n## Tools and Libraries\n\n### General Imports\n\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\n\nporter_stemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n```\n\n- [[nltk]]: A leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n  - punkt: An unsupervised trainable model for tokenizing text into sentences and words.\n  - [[stopwords]]: Commonly used words (such as \"the\", \"is\", \"in\") that are often removed from text data because they do not carry significant meaning.\n  - wordnet: A lexical database for the English language that groups words into sets of synonyms and provides short definitions and usage examples.\n  - re: Regular expressions for pattern matching and text manipulation.",
    "aliases": [
      "Natural Language Processing"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "nlp",
    "outlinks": [
      "vector_embedding",
      "lemmatization",
      "bag_of_words",
      "one-hot_encoding",
      "stopwords",
      "preprocessing",
      "summarisation",
      "normalisation_of_text",
      "part_of_speech_tagging",
      "ngrams",
      "nltk",
      "grammar_method",
      "tf-idf"
    ],
    "inlinks": [
      "attention_mechanism",
      "bert",
      "curse_of_dimensionality",
      "faiss",
      "generative_ai_from_theory_to_practice",
      "generators_in_python",
      "latent_dirichlet_allocation",
      "lstm",
      "named_entity_recognition",
      "ngrams",
      "nltk",
      "non-negative_matrix_factorization",
      "normalisation",
      "oov_words",
      "rag",
      "recurrent_neural_networks",
      "sentence_similarity",
      "spacy",
      "symbolic_computation",
      "t-sne",
      "tokenisation",
      "transformer",
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "Named Entity Recognition",
    "sha": "8645cd860ced7d1c0ab86839714e3e7c148712bd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Named%20Entity%20Recognition.md",
    "text": "Named Entity Recognition (NER) is a subtask of [[NLP|Natural Language Processing]] (NLP) that involves identifying and classifying key entities in text into predefined categories such as names, organizations, locations.\n\nThe process typically employs algorithms like Conditional Random Fields (CRFs) or deep learning models such as Bi-directional [[LSTM]] (Long Short-Term Memory) networks.\n\nMathematically, NER can be framed as a sequence labeling problem where the goal is to assign a label $y_i$ to each token $x_i$ in a sentence. The model learns from annotated datasets, optimizing parameters to maximize the likelihood $P(y|x)$ using techniques like [[Backpropagation]].\n\nNER has significant implications in information extraction, search engines, and automated customer support systems.\n\n### Important\n - NER transforms unstructured text into [[structured data]] for analysis.\n - The choice of model significantly impacts the accuracy of entity recognition.\n\n### Example\n An example of NER is identifying \"Apple Inc.\" as an organization in the sentence: \"Apple Inc. released a new product.\"\n\n### Follow up questions\n - [[How does the choice of training data affect the performance of NER models]]\n - [[What are the challenges of NER in multilingual contexts]]\n - [[Why is named entity recognition (NER) a challenging task]]\n - [[In NER how would you handle ambiguous entities]]\n - [[NER Implementation]]\n\n### Related Topics\n - Text classification in [[NLP]]  \n - Information extraction techniques",
    "aliases": [
      "Entity Recognition",
      "NER"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "modeling",
      "NLP"
    ],
    "normalized_filename": "named_entity_recognition",
    "outlinks": [
      "lstm",
      "what_are_the_challenges_of_ner_in_multilingual_contexts",
      "why_is_named_entity_recognition_(ner)_a_challenging_task",
      "structured_data",
      "ner_implementation",
      "backpropagation",
      "nlp",
      "how_does_the_choice_of_training_data_affect_the_performance_of_ner_models",
      "in_ner_how_would_you_handle_ambiguous_entities"
    ],
    "inlinks": [
      "bert",
      "graphrag",
      "in_ner_how_would_you_handle_ambiguous_entities",
      "nltk",
      "spacy"
    ]
  },
  {
    "category": "LANG",
    "filename": "Ngrams",
    "sha": "34640f1e71621b015d1947a5f79524f23bf797ed",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Ngrams.md",
    "text": "N-grams are used in NLP that allow for the analysis of text data by breaking it down into smaller, manageable sequences. \n\nAn **N-gram** is a contiguous sequence of *n* items (or tokens) from a given sample of text or speech. In the context of natural language processing ([[NLP]]) and text analysis, these items are typically words or characters. \n\nN-grams are used to analyze and ==model the structure of language==, and they can help in various tasks such as [[Text Classification]].\n### Types of N-grams\n- **Unigram**: An N-gram where *n = 1*. It represents individual words or tokens. For example, in the sentence \"I love AI\", the unigrams are [\"I\", \"love\", \"AI\"].\n\n- **Bigram**: An N-gram where *n = 2*. It represents pairs of consecutive words. For the same sentence, the bigrams would be [\"I love\", \"love AI\"].\n\n- **Higher-order N-grams**: These can go beyond three words, such as 4-grams (quadgrams) or 5-grams, and so on.\n### Code implementations:\n\nThis can be does through kwargs in CountVectorizer.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "ngrams",
    "outlinks": [
      "nlp",
      "text_classification"
    ],
    "inlinks": [
      "curse_of_dimensionality",
      "generative_ai_from_theory_to_practice",
      "nlp"
    ]
  },
  {
    "category": "LANG",
    "filename": "Non-negative Matrix Factorization",
    "sha": "44d10c2b4bd95e9f02d5cfc71d883a4d9035d28e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Non-negative%20Matrix%20Factorization.md",
    "text": "Non-negative Matrix Factorization (NMF) is a matrix decomposition technique that factors a non-negative matrix $V$ into two non-negative matrices $W$ and $H$ such that:\n\n$$\nV \\approx W \\cdot H\n$$\n\nIn [[NLP]], NMF is often applied to document-term matrices for [[topic modeling]] and [[Feature Extraction]].\n\n### How NMF Works in NLP\n\n1. Construct a document-term matrix $V$:\n\n   * Rows = documents\n   * Columns = terms/words\n   * Entries = term frequency (TF) or TF-IDF.\n\n1. Decompose $V$ into:\n\n   * $W$ (document-topic matrix): Each row represents the distribution of topics for a document.\n   * $H$ (topic-term matrix): Each row represents the distribution of terms for a topic.\n\n3. Interpret topics:\n\n   * Each topic is represented by a set of high-weight words from $H$.\n   * Each document is represented by a mixture of topics from $W$.\n\n### Application to Topic Importance Indicators\n\n* Topic Importance for Documents: Look at $W$ to see which topics dominate a document.\n* Key Words for Topics: Look at $H$ to find top terms per topic, which serve as indicators of the topic’s content or importance.\n* Ranking Features: Terms with higher weights in $H$ are more important for defining a topic.\n### Benefits\n\n* Produces interpretable topics because all entries are non-negative.\n* Works well with sparse and high-dimensional NLP data.\n* Can complement feature importance analysis in text classification and clustering.\n### Example (Python, using TF-IDF)\n\n```python\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = [\"I love NLP\", \"Machine learning is fun\", \"NLP and ML are related\"]\nvectorizer = TfidfVectorizer()\nV = vectorizer.fit_transform(documents)\n\nnmf = NMF(n_components=2, random_state=42)\nW = nmf.fit_transform(V)  # document-topic matrix\nH = nmf.components_       # topic-term matrix\n```\n\n* Rows of $H$ → important words per topic (topic indicators)\n* Rows of $W$ → importance of topics per document\n\n### Image\n\n![[Pasted image 20250823094439.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "selection"
    ],
    "normalized_filename": "non-negative_matrix_factorization",
    "outlinks": [
      "nlp",
      "topic_modeling",
      "feature_extraction",
      "pasted_image_20250823094439.png"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "NotebookLM",
    "sha": "b807c055bf0360b7f6c6134beb2751b03f941ae0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/NotebookLM.md",
    "text": "https://www.youtube.com/watch?v=EOmgC3-hznM\n\nkey topics \n\nchat interface takes into account resources.\n\nsave to note- to dave. \n\nhow to select and folders - from obsidian [[Data Archive]] for this ? A getter of some kind\n\ncan convert muiltple notes into a single note.\n\nCan add website as source.\n\nproject context - similar projects notes\n\nFocus knowledge retrieval\n- get info from sources (folders)\n\nFAQ \n\nNote: can help with file extraction rem utils function (for [[NotebookLM]])",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "tool"
    ],
    "normalized_filename": "notebooklm",
    "outlinks": [
      "data_archive",
      "notebooklm"
    ],
    "inlinks": [
      "notebooklm"
    ]
  },
  {
    "category": "LANG",
    "filename": "Pandas Dataframe Agent",
    "sha": "a22c11603993d5005e62cf97158e513834d20773",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Pandas%20Dataframe%20Agent.md",
    "text": "Example:\nhttps://github.com/AssemblyAI/youtube-tutorials/tree/main/pandas-dataframe-agent\n\nFollow:\n\nhttps://www.youtube.com/watch?v=ZIfzpmO8MdA&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=7\n\nCan as pandas questions to a dataframe. \n\nTypes of questions:\n- what is the max value of \"col1\"",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "agents"
    ],
    "normalized_filename": "pandas_dataframe_agent",
    "outlinks": [],
    "inlinks": [
      "langchain"
    ]
  },
  {
    "category": "LANG",
    "filename": "OOV words",
    "sha": "3d1a968f206cbce9ee533c39a176b1df66f2b1c8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/OOV%20words.md",
    "text": "In [[NLP|Natural Language Processing]] (NLP), OOV words refers to:\n\n> OOV words are terms not seen during training or not present in a model’s vocabulary. They pose challenges for text understanding, especially in traditional NLP approaches. Modern tokenization strategies (e.g., subwords) greatly reduce—but do not entirely eliminate—the issue.\n### Modern NLP Models\n\nModern transformer-based models (e.g., BERT, GPT) almost eliminate the concept of OOV by using subword [[Tokenisation]]:\n- Any word, even if unseen, can be broken into known subword units.\n- However, this still has semantic implications: e.g., rare words may be split into unintuitive or ambiguous fragments.\n\n### Context and Meaning\n\nMany NLP models (e.g., classical models like [[Bag of Words]], or early word embeddings like [[Word2vec]]) rely on a fixed vocabulary that was built from a training corpus. Any word not seen during training is considered out-of-vocabulary.\n### Why OOV Words Matter\n\n1. Loss of information: If a model cannot represent or process a word (e.g., \"microservices\" or a new slang term), it cannot reason about its meaning.\n2. Performance degradation: In text classification, machine translation, or entity recognition tasks, frequent OOV words reduce the model’s accuracy.\n3. Domain adaptation challenges: OOV words often appear in domain-specific or noisy data (e.g., medical, legal, user-generated content).\n### Strategies to Handle OOV Words\n\n| Strategy                   | Description                                                                        | Common Usage                         |\n| -------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------ |\n| UNK token                  | Map all unknown words to a special token like `<UNK>`                              | Basic [[Recurrent Neural Networks|RNN]] models, early NLP          |\n| Subword tokenization       | Break OOV words into smaller known parts (e.g., BPE, ==WordPiece==, SentencePiece) | Used in BERT, GPT, RoBERTa           |\n| Character-level models     | Process input character-by-character, avoiding fixed vocabulary                    | Useful in noisy or multilingual text |\n| Dynamic vocabulary updates | Re-train or extend embeddings on new corpora                                       | Custom applications                  |\n### Example\n\nAssume a vocabulary contains:\n\n```python\n[\"cat\", \"dog\", \"runs\", \"the\", \"fast\"]\n```\n\nNow, the sentence:\n\n```text\n\"The cheetah runs fast\"\n```\n\n “cheetah” is not in the vocabulary → it's an OOV word.\n \n A model may:\n\n   - Replace it with `<UNK>`: `\"the <UNK> runs fast\"`\n   - Use subwords: `\"the chee ##tah runs fast\"` (WordPiece-style)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "oov_words",
    "outlinks": [
      "bag_of_words",
      "recurrent_neural_networks",
      "tokenisation",
      "nlp",
      "word2vec"
    ],
    "inlinks": [
      "embeddings_for_oov_words",
      "evaluate_embedding_methods"
    ]
  },
  {
    "category": "LANG",
    "filename": "Part of speech tagging",
    "sha": "45ee59affce9338daa843ce7f84dd3ec789e7f13",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Part%20of%20speech%20tagging.md",
    "text": "Part of speech tagging : assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text\n\nPart-of-speech tagging involves assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text\n```python\nfrom nltk import pos_tag\npos_tag(temp[:20])\n```\nwill get outputs such as [('history', 'NN'), ('poland', 'NN'), ('roots', 'NNS'), ('early', 'JJ').",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "part_of_speech_tagging",
    "outlinks": [],
    "inlinks": [
      "nlp",
      "nltk"
    ]
  },
  {
    "category": "LANG",
    "filename": "Prompt Engineering",
    "sha": "53e0a72c4065fbec61366ff28d489047a51e9259",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Prompt%20Engineering.md",
    "text": "Prompt engineering is a technique in the field of natural language processing (NLP), particularly when working with [[LLM|large language models]] (LLMs). \n\nIt involves designing and optimizing input [[Prompts]] to get the most relevant and accurate responses from these models. \n\nTechniques like [[prompt retrievers]], which include systems like UPRISE and DaSLaM, enhance the ability to retrieve and generate contextually appropriate prompts.\n\nPrompt engineering aims to ==guide LLMs toward producing desired outputs while minimizing ambiguity.== \n\n### Key Takeaways\n\n- Prompt engineering optimizes input to improve LLM responses.\n- Techniques like prompt retrievers (e.g., UPRISE, DaSLaM) enhance prompt effectiveness.\n- Quality prompts reduce ambiguity and guide model outputs.\n- Applications span multiple industries, enhancing user interaction and content generation.\n\n### Key Components Breakdown\n\nMethods: \n  - Prompt Design: Crafting specific, clear prompts to guide model responses.\n  - Prompt Retrieval: Utilizing systems like UPRISE and DaSLaM to find effective prompts based on context.\n  \nConcepts:\n  - Contextualization: Understanding the context in which prompts are used to improve relevance.\n  - Iterative Testing: Continuously refining prompts based on model performance.\n\nAlgorithms:\n  - Retrieval-Augmented Generation (RAG): Combines retrieval of relevant documents with generative responses.\n  - Few-Shot Learning: Providing examples within prompts to guide model behavior.\n\n### Concerns, Limitations, or Challenges\n- Ambiguity: Poorly designed prompts can lead to vague or irrelevant responses.\n- Dependence on Training Data: LLMs may produce biased or inaccurate outputs based on their training data.\n- Complexity: Designing effective prompts requires a deep understanding of both the model and the task.\n\n### Example\nFor instance, if a user wants to generate a summary of a scientific article, a poorly constructed prompt like \"Summarize this\" may yield unsatisfactory results. In contrast, a well-engineered prompt such as \"Provide a concise summary of the key findings and implications of the following article on climate change\" is likely to produce a more relevant and informative response.\n\n### Follow-Up Questions\n1. [[Evaluating the effectiveness of prompts]]\n2. [[How can prompt engineering be integrated into existing NLP workflows to enhance performance]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "prompt_engineering",
    "outlinks": [
      "evaluating_the_effectiveness_of_prompts",
      "llm",
      "prompts",
      "how_can_prompt_engineering_be_integrated_into_existing_nlp_workflows_to_enhance_performance",
      "prompt_retrievers"
    ],
    "inlinks": [
      "agent-based_modelling",
      "how_do_we_evaluate_of_llm_outputs"
    ]
  },
  {
    "category": "LANG",
    "filename": "Prompts",
    "sha": "e80f4837e66db41cd68e2be55d7c2d80ea40d590",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Prompts.md",
    "text": "Pre set prompts are most useful when they are easily accessible.\n\nIn obsidian copilot can select prompts with \"/\".\n#### How to write prompts\n\n[Link](https://www.youtube.com/watch?v=jC4v5AS4RIM)\n\nUse a formula to design a prompt.\n\n1) Persona\n2) Context\n3) Task\n4) Format\n\n![[Pasted image 20240910072458.png| 500]]\n\n### Prompts to Ask Better Questions\n\n- What are the underlying assumptions here?\n- Is there another way to frame this?\n- What’s missing from this picture?\n- If this were false, what else would be true?\n- What would X say about this (e.g., an expert, a critic, a novice)?\n- What’s the smallest next step to test this idea?\n\n### Related\n- [[Asking questions]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "prompt"
    ],
    "normalized_filename": "prompts",
    "outlinks": [
      "pasted_image_20240910072458.png",
      "asking_questions"
    ],
    "inlinks": [
      "ai_engineer",
      "asking_questions",
      "evaluating_language_models",
      "guardrails",
      "langchain",
      "prompt_engineering",
      "rag"
    ]
  },
  {
    "category": "LANG",
    "filename": "Pyright",
    "sha": "9977486433002c1d15b809eb9cd5cd88c0f26f37",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Pyright.md",
    "text": "Pyright is a ==static type checker== for Python that enhances code reliability by enforcing type constraints ==at compile-time.==\n\nIt utilizes type hints to identify potential errors, such as type mismatches, before runtime, thereby improving code robustness. \n\nPyright significantly reduces runtime errors by enforcing type constraints at compile-time.\n\nThe use of type hints in Pyright improves code readability and [[Maintainability]], serving as [[Documentation & Meetings]] for function signatures.\n\n### Related Topics\n\n- Type inference in programming languages\n- The role of type systems in [[functional programming]]\n- [[Debugging]]\n- [[Maintainable Code]]\n- [[type checking]]\n\n### Follow up questions\n\n- How does the inclusion of Pyright impact the performance of large-scale Python applications?\n- What are the trade-offs between using Pyright and other static type checkers in terms of accuracy and speed?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "prompt"
    ],
    "normalized_filename": "pyright",
    "outlinks": [
      "maintainable_code",
      "debugging",
      "maintainability",
      "functional_programming",
      "type_checking",
      "documentation_&_meetings"
    ],
    "inlinks": [
      "maintainable_code",
      "pyright_vs_pydantic"
    ]
  },
  {
    "category": "LANG",
    "filename": "RAG",
    "sha": "208626c5167510ef365a41a77bf0a2b81c0d53f5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/RAG.md",
    "text": "Rag is a framework the help [[LLM]] be more up to date.\n\nRAG grounds the Gen AI in external data.\n\nGiven a question sometimes the answer given is wrong, issue with [[LLM]] is no source of data and is out of date. RAG is a specific architecture used in natural language processing ([[NLP]]), where a retrieval mechanism is combined with a generative model ([[Generative]]) (often a [[Transformer]] like GPT). RAG systems are designed to ==enhance the ability of a generative model to answer questions or generate content by incorporating factual knowledge retrieved from external data sources== (such as documents, databases, or knowledge repositories). RAG is the connection of [[LLM]]'s with external databases. \n\n Example of a RAG System:\n - A user asks: *\"What is the capital of France?\"*\n - The retrieval module fetches a relevant document (e.g., from Wikipedia) that contains the information about France’s capital.\n - The generation module synthesizes the response: \"The capital of France is Paris.\"\n\n### [[LLM]] Challenges\n- Responses are sometimes no sources and out of date.\n- LLM's are trained on some store of data (static). We want this store to be updated.\n### Key characteristics of RAG:\n\n![[Pasted image 20240928194559.png|500]]\n\nBased on a [[Prompts]].\n\n1. Retrieval Component:\n   - This module fetches relevant documents (and up to date) or information from an external corpus based on the query or input. It may use traditional search methods like dense vector retrieval (e.g., using embeddings) or keyword-based retrieval.\n   - Retriever should be good enough to give the most truthful information based on the store\n   \n1. Generative Component:\n   - After retrieving relevant documents, the [[Generative]] model (such as GPT or [[BERT]]-based models) synthesizes the final response, integrating both the input query and the retrieved information to generate more accurate and contextually informed outputs.\n   \n1. Augmentation with External Knowledge:\n   - Instead of solely relying on pre-trained internal knowledge (as in traditional language models), RAG setups use the external knowledge source for augmenting generation, thus improving factual accuracy and reducing the risk of hallucinations (incorrect or fabricated responses).\n\nModel should be able to saay \"I dont know\" instead of [[hallucinating]]\n\n### Resources\n\nProblems\n1. LLMs struggle with memorization > \"LLMs may struggle with\ntasks that require domain-specific expertise or up-to-date\ninformation.\n2. LLMs struggle with generating factually inaccurate content\n(hallucinations)\nSolution\n3. A lightweight [[retriever]] (SM) to extract relevant document\nfragments from external knowledge bases, document collections,\nor other tools\n\nTypes of retrievers: \n Different RaG techniques:\n Sparse retrievers, \n BM25, \n dense retrievers.\nUse Bert for similarity matching:",
    "aliases": [],
    "date modified": "27-09-2025",
    "resources": "https://www.youtube.com/watch?v=T-D1OfcDW1M",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "rag",
    "outlinks": [
      "bert",
      "hallucinating",
      "retriever",
      "llm",
      "transformer",
      "prompts",
      "generative",
      "pasted_image_20240928194559.png",
      "nlp"
    ],
    "inlinks": [
      "assessing_gen_ai_generated_content",
      "ds_&_ml_portal",
      "generative_ai_from_theory_to_practice",
      "graphrag",
      "knowledge_graph_vs_rag_setup",
      "local_llm_use_cases",
      "relationships_in_memory",
      "small_language_models"
    ]
  },
  {
    "category": "LANG",
    "filename": "Scaling Agentic Systems",
    "sha": "32563424eeee6d296bf4a5a9769c0dbd952fc49e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Scaling%20Agentic%20Systems.md",
    "text": "[[Agentic Solutions]] propose an improvement over traditional Large Language Model ([[LLM]]) usage by employing networks of Small Language Models (SLMs). These systems aim to strike a balance between scalability, control, and performance, addressing specific tasks with precision while maintaining overall system adaptability.\n\nIdeas from MLOPs talk by MaltedAI.\n\nAgentic solutions represent a pragmatic approach to AI systems by focusing on modularity, task-specific efficiency, and the thoughtful integration of human expertise. These architectures show promise for enhancing scalability, control, and adaptability in real-world applications.\n## Contrasting SLMs and LLMs\n\n[[Small Language Models|SLM]] (Small Language Models):\n    - Intent-based conversations and decision trees.\n    - Controlled systems, harder to build features but easier to execute.\n    - Task-specific and efficient in offline environments.\n\nLLMs (Large Language Models):\n    - Flexible and natural in handling diverse queries.\n    - Suitable for general-purpose scenarios and exploratory tasks.\n    - High computational and scaling costs.\n\n### Combined Approach:\n\n- Use [[Small Language Models|SLM]] for inference and LLMs for training.\n- Shift the focus from making models larger to solving real-world problems effectively.\n## Key Concepts in Agentic Solutions\n\n1. Neural Decision Parser:\n    - Acts as the \"brain\" of the system, determining the appropriate action given user input.\n    - SLMs interpret user utterances to express code aligned with system intent.\n\n1. Phased Policy:\n    - Distinguishes between contextual and general-purpose questions.\n    - Ensures deliberate task execution in stages for clarity and efficiency.\n\n1. Knowledge Graphs and Interaction Models:\n    - Complex graph structures enable intelligent conversations between models.\n    - RAG setups leverage teacher-student frameworks for effective task distribution.\n\n1. [[Distillation]] Networks of SLMs:\n    - SMEs (Subject Matter Experts) guide teacher models that distill their expertise into student SLMs.\n    - Enhances task scalability while controlling costs.\n\n1. Scaling with Distillation:\n    - Leverage teacher-student frameworks for high-quality, scalable data.\n    - Allow teacher models to handle hard-to-scale aspects.\n\n1. Knowledge Discovery:\n    - Extract SME knowledge effectively while filtering irrelevant data.\n    - Build high-quality datasets for task-specific applications.\n\n## Applications of SLM Networks\n\n1. Task-Specific Systems:\n    - Offline processing, task search, and targeted QA.\n    - Optimized embedding spaces for domain-specific applications.\n\n1. Swarm Intelligence:\n    - Decision-making through deliberation among SLMs.\n    - Aggregates diverse inputs (HR, tech, CEO) for robust conclusions.\n\n1. Business Process Models:\n    \n    - Search and page ranking systems.\n    - Smaller, focused systems tailored to specific business needs.\n\n\n\n## Designing Agentic Solutions\n\n1. Role of SMEs:\n    \n    - Define tasks and input structures.\n    - Guide model development with domain knowledge.\n2. Data Preparation:\n    \n    - Comprehensive sampling of the problem space ensures generalization.\n    - Data variability is critical for robust models.\n3. Evaluation and Responsiveness:\n    \n    - Measure system performance to enable continuous improvement.\n    - Focus on responsive, real-time processing.\n4. Tool Integration:\n    \n    - Use LLMs with Python engines or computational tools like Wolfram for data analysis and complex interactions.\n\n\n\n## Advantages of SLM Networks\n\n- Precision: Models perform only what they are designed for.\n- Efficiency: Smaller models are scalable and cost-effective.\n- Focused Applications: Avoids the complexity of embedding spaces for entire businesses.\n\n\n## Future Directions",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "language_models"
    ],
    "normalized_filename": "scaling_agentic_systems",
    "outlinks": [
      "llm",
      "small_language_models",
      "agentic_solutions",
      "distillation"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Self attention vs multi-head attention",
    "sha": "c5d25ffb9c21f46790f47ec61fff444b729bdefc",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Self%20attention%20vs%20multi-head%20attention.md",
    "text": "https://www.youtube.com/shorts/Muvjex0nkes\n\n[[Self-Attention]]: Take every word pays attention to every other word to capture context by:\n1. take input word vectors,\n2. break words into Q,K,V vectors,\n3. compute attention matrix\n4. generate final word vectors.\n\n[[Multi-head attention]]: perform self attention in parallel.\n1. take word vectors,\n2. break words into Q,K,V vectors,\n\t1. Break each Q,K,V vector into the number of heads parts\n3. compute attention matrix for each head\n4. generate final word vectors for each head\n5. Combine back together\n\nThese have better understanding of the context.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "self_attention_vs_multi-head_attention",
    "outlinks": [
      "multi-head_attention",
      "self-attention"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Self-Attention",
    "sha": "8ea1563b55d3fbd942e70e87052b1710a4512f28",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Self-Attention.md",
    "text": "In this mechanism, the model applies attention to itself. This means each word in the input sequence attends to all other words in the sequence, including itself. Self-attention is used in models like [[Transformer]] to capture dependencies within a sentence.\n\n[[Self-Attention]]\n* Each token in a sequence considers all others when computing its representation.\n* This enables rich, context-aware embeddings, even for long inputs.\n* Unlike [[Recurrent Neural Networks]], Transformers allow parallel processing, making them more efficient and scalable.\n\nSelf-attention is the core of models like [[BERT]]& [[GPT]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "architecture",
      "devops",
      "NLP"
    ],
    "normalized_filename": "self-attention",
    "outlinks": [
      "gpt",
      "bert",
      "transformer",
      "recurrent_neural_networks",
      "self-attention"
    ],
    "inlinks": [
      "attention_mechanism",
      "self-attention",
      "self_attention_vs_multi-head_attention"
    ]
  },
  {
    "category": "LANG",
    "filename": "Semantic Relationships",
    "sha": "c33479e254935dde43764d6840968f975cbf138b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Semantic%20Relationships.md",
    "text": "Semantic relationships (Semantic Similarity) refer to the connections and associations between words and concepts based on their meanings. \n\nUnderstanding these relationships can enhance various natural language processing tasks, such as information retrieval, text analysis, and sentiment analysis.\n\n### Lexical Resources like [[WordNet]]\n\nOne of the key resources for exploring semantic relationships is [[WordNet]]\n\nYou can use WordNet to find synonyms or related concepts for important words (those with high [[TF-IDF]] scores) in your documents. If different documents contain synonyms or words related in the WordNet hierarchy, this may indicate a semantic relationship between them, even if the exact words differ.\n\nWordNet also provides measures of semantic similarity between **synsets** based on their paths in the hypernym hierarchy. These measures can be explored to quantify the semantic relatedness of key terms in your documents. The Natural Language Toolkit ([[nltk]]) offers an interface to access WordNet.\n\n### Sentiment Analysis with SentiWordNet\n\nAnother resource is SentiWordNet, which extends WordNet by **assigning sentiment scores** (positive, negative, objective) to different senses of words. While your primary goal may be to explore semantic relationships, analyzing the sentiment expressed in your documents based on important words can provide an additional layer of understanding. \n\nDocuments discussing similar topics might also share similar sentiments, strengthening the case for a semantic link. NLTK provides access to SentiWordNet, allowing you to incorporate sentiment analysis into your exploration of semantic relationships.\n\n### Related\n- [[Sentence Similarity]]\n- [[Smart Connections]]\n-",
    "aliases": [
      "semantic similarity"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "semantic_relationships",
    "outlinks": [
      "sentence_similarity",
      "smart_connections",
      "wordnet",
      "nltk",
      "tf-idf"
    ],
    "inlinks": [
      "knowledge_graph",
      "knowledge_graph_vs_rag_setup",
      "latent_dirichlet_allocation",
      "llm_memory",
      "relationships_in_memory",
      "semantic_search",
      "sentence_transformers",
      "syntactic_relationships",
      "transformer",
      "vector_database",
      "vector_embedding",
      "word2vec"
    ]
  },
  {
    "category": "LANG",
    "filename": "Semantic search",
    "sha": "6c2476c85185c2fcdbdb6a9cdac0f3ca237683f2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Semantic%20search.md",
    "text": "[[Semantic Relationships]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI"
    ],
    "normalized_filename": "semantic_search",
    "outlinks": [
      "semantic_relationships"
    ],
    "inlinks": [
      "faiss",
      "vector_database"
    ]
  },
  {
    "category": "LANG",
    "filename": "Sentence Similarity",
    "sha": "6f86f58ce9e4193a4134716d33fb519dbcc29370",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Sentence%20Similarity.md",
    "text": "Sentence similarity refers to the degree to which two sentences are alike in meaning. It is a crucial concept in natural language processing ([[NLP]]) tasks such as information retrieval, text summarization, and paraphrase detection. Measuring sentence similarity involves comparing the semantic content of sentences to determine how closely they relate to each other.\n\nThere are several methods to measure sentence similarity:\n\n1. **Lexical Similarity**: This involves comparing the words in the sentences directly. Common techniques include:\n   - **Jaccard Similarity**: Measures the overlap of words between two sentences.\n   - **[[Cosine Similarity]]**: Represents sentences as vectors (e.g., using [[TF-IDF]]) and measures the cosine of the angle between them.\n\n2. **Syntactic Similarity**: This considers the structure of the sentences, using techniques like:\n   - **Parse Trees**: Comparing the syntactic trees of sentences to see how similar their structures are.\n\n3. **Semantic Similarity**: This goes beyond surface-level word matching to understand the meaning of sentences:\n   - **Word Embeddings** ([[Vector Embedding]]): Using models like [[Word2vec]], GloVe, or FastText to represent words in a continuous vector space, then averaging these vectors to represent sentences.\n   - **Sentence Embeddings**: Using models like Universal Sentence Encoder, BERT, or Sentence-[[BERT]] to directly obtain embeddings for entire sentences, which can then be compared using [[Cosine Similarity]] or other distance metrics.\n\n4. **Neural Network Models**: Advanced models like BERT, RoBERTa, or GPT can be fine-tuned on specific tasks to directly predict similarity scores between sentence pairs.\n\n\n\nEach method has its strengths and weaknesses, and the choice of method often depends on the specific requirements of the task and the available computational resources.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "sentence_similarity",
    "outlinks": [
      "vector_embedding",
      "bert",
      "cosine_similarity",
      "nlp",
      "word2vec",
      "tf-idf"
    ],
    "inlinks": [
      "bert",
      "semantic_relationships"
    ]
  },
  {
    "category": "LANG",
    "filename": "Sentence Transformer Workflow",
    "sha": "9084a1fca708a20a6d356bb480c5e0031127bc5f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Sentence%20Transformer%20Workflow.md",
    "text": "### Sentence Transformer Workflow \n\n#### Step 1: Input Sentence Pair\n\n* Input consists of two sentences: A and B.\n* Both are processed independently using the same [[BERT]] model (a twin/siamese network).\n\n#### Step 2: [[Vector Embedding|Embedding]] Extraction\n\n* Sentences A and B are passed separately through BERT.\n* Each yields a fixed-size embedding vector: $a = \\text{Embed}(A), b = \\text{Embed}(B)$.\n\n#### Step 3: Compute Difference and Combine\n\n* Compute absolute difference: |a - b|.\n* Form a combined vector: $\\[a; b; |a - b|].\n\n#### Step 4: Feedforward Neural Network ([[Feed Forward Neural Network|FFNN]])\n\n* Pass the combined vector through a two-layer FFNN.\n* Output is a set of raw logits (real-valued scores for each class).\n\n#### Step 5: [[Classification]] via Softmax\n\n* Apply softmax to logits to get class probabilities.\n* The class with the highest probability is selected.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "process"
    ],
    "normalized_filename": "sentence_transformer_workflow",
    "outlinks": [
      "vector_embedding",
      "bert",
      "feed_forward_neural_network",
      "classification"
    ],
    "inlinks": [
      "sentence_transformers"
    ]
  },
  {
    "category": "LANG",
    "filename": "Similarity Search",
    "sha": "e6524c83ffa4c84fd9c852107269cc561a4b7f54",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Similarity%20Search.md",
    "text": "Given an input text, [[Search]] a [[Vector Embedding]] space to related text.\n\n**Similarity search** retrieves items that are **close in meaning, content, or structure** to a given [[Querying|query]], typically using a **vector space model**. It is a foundation of modern search, recommendation, and information retrieval systems.\n\nQuery: \"How to schedule a task?\"\nMatch: \"Creating [[Cron jobs]] in Linux\"\nSimilar in meaning, found via vector similarity",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "similarity_search",
    "outlinks": [
      "vector_embedding",
      "querying",
      "cron_jobs",
      "search"
    ],
    "inlinks": [
      "faiss",
      "vector_database"
    ]
  },
  {
    "category": "LANG",
    "filename": "Small Language Models",
    "sha": "737c742059fe32ea0c180c70c36597a6d88496d9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Small%20Language%20Models.md",
    "text": "[[LLM|LLMs]] dominate many general-purpose NLP tasks, small [[Language Models]] have their own place in specialized tasks, where they excel due to computational efficiency, [[Interpretability]], and task-specific fine-tuning. \n\nSLMs remain highly relevant for [[Edge ML]] and edge computing, ==domain-specific tasks==, and applications requiring [[Interpretability]], making them a crucial tool in the NLP landscape.\n\n### Use Cases for Small Language Models (SLMs)\n\n- [[Contrastive Decoding]]: Improve the quality of generated content by filtering out low-quality outputs, by having a SLM guide and critique a LLM or other way ([[inference]])\n\t- Mitigate hallucinations\n\t- Augmented Reasoning\n\t  \n- [[Distillation]]: Transfer the knowledge from a larger model to a smaller one, retaining performance but reducing computational requirements (see [[BERT]] Teacher model).\n  \n- Data Synthesis: Generate or augment datasets in scenarios with limited data.\n  \n- [[Model Cascading]]: Use a combination of smaller models and larger models in a cascading architecture, where simpler tasks are handled by SLMs and more complex ones by LLMs. Model cascading and routing allow SMs to handle simpler tasks, reducing computational overhead. Or the other way first do a general search with a LLM then refine to domain specific small model which is more [[Interpretability|interpretable]] and specific.\n  \n- Domain specific & Limited Data Availability: SMs, however, can be ==effectively fine-tuned== on smaller, ==domain-specific datasets== and outperform general LLMs in tasks with limited data availability.\n  \n- [[RAG]] (Retrieval Augmented Generation): Lightweight ==retrievers== (SMs) can support LLMs in finding relevant external information.\n\n### Advantages of SLMs\n\n- Require less computational power and are faster in [[inference]] compared to LLMs.\n- [[Interpretability]]\n- Accessible for those without resources in power and data",
    "aliases": [
      "SLM"
    ],
    "date modified": "27-09-2025",
    "resources": "https://arxiv.org/pdf/2409.06857",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "small_language_models",
    "outlinks": [
      "bert",
      "edge_ml",
      "llm",
      "contrastive_decoding",
      "inference",
      "language_models",
      "model_cascading",
      "interpretability",
      "rag",
      "distillation"
    ],
    "inlinks": [
      "agentic_solutions",
      "distillation",
      "language_models",
      "local_llm_use_cases",
      "scaling_agentic_systems"
    ]
  },
  {
    "category": "LANG",
    "filename": "Stemming",
    "sha": "e8ba095a5e8f46d550276ff660e29646d5af068c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Stemming.md",
    "text": "Shorting words to the key term.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "stemming",
    "outlinks": [],
    "inlinks": [
      "nltk",
      "normalisation_of_text",
      "preprocessing_text_classification"
    ]
  },
  {
    "category": "LANG",
    "filename": "Summarisation",
    "sha": "3e6ed976b5bf28f30dc6070555de3095831af654",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Summarisation.md",
    "text": "## Summarization in NLP\n\nSummarization in natural language processing (NLP) is the process of condensing a text document into a shorter version while retaining its main ideas and key information. There are two primary forms of summarization:\n\nThe unsupervised summarization process involves ==splitting text, tokenizing sentences, assigning scores based on importance, and selecting top sentences==. Effective scoring methods include calculating sentence ==similarity== and analyzing ==word frequencies== to ensure that the summary captures the essence of the original text.\n\n[[Extraction]]:\n- This method involves selecting specific words or sentences directly from the original text to create a summary. It focuses on identifying and pulling out the most important parts of the text without altering the original wording.\n[[Abstraction]]:\n- The abstraction method generates a summary that may include new words and phrases not present in the original text. This approach is more complex as it requires understanding the content and rephrasing it, often using techniques like paraphrasing.\n\n### Unsupervised Summarization Process\n\nThe basic idea behind unsupervised summarization involves the following steps:\n\n1. **Split Text into Sentences**: The text is divided into individual sentences for analysis.\n  \n2. **Tokenize Sentences**: Each sentence is tokenized into separate words, allowing for detailed examination of word usage.\n\n3. **Assign Scores to Sentences**: Sentences are evaluated based on their importance, which is a crucial step in the summarization process.\n\n4. **Select Top Sentences**: The highest-scoring sentences are selected and displayed in their original order to form the summary.\n\n### Methods for Assigning Scores\n\nThe main point of summarization is effectively assigning scores to sentences. Here are some common methods for doing this:\n\n- **==Similarity== Calculation**: Calculate the similarity between each pair of sentences and select those that are most similar to the majority of sentences. This helps identify sentences that capture the central themes of the text.\n\n- **Word Frequencies**: Analyze word frequencies to identify the most common words in the text. Sentences that contain a higher number of these frequent words are then selected for the summary.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "summarisation",
    "outlinks": [
      "extraction",
      "abstraction"
    ],
    "inlinks": [
      "bert",
      "nlp"
    ]
  },
  {
    "category": "LANG",
    "filename": "TF-IDF Implementation",
    "sha": "d747b3e41e383c964e292c90402ea65f1b88ab4c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/TF-IDF%20Implementation.md",
    "text": "## TF-IDF Implementation \n\n### Python Script (scikit-learn version)\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# Step 1: Tokenize and vectorize using Bag of Words\nbow = CountVectorizer(tokenizer=normalize_document)\nX_counts = bow.fit_transform(corpus)\n\n# Step 2: Apply TF-IDF transformation\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)\n\n# Optional: View TF-IDF scores per document\nfor doc_id in range(len(corpus)):\n    print(f\"Document {doc_id}: {corpus[doc_id]}\")\n    print(\"TF-IDF values:\")\n    tfidf_vector = X_tfidf[doc_id].T.toarray()\n    for term, score in zip(bow.get_feature_names_out(), tfidf_vector):\n        if score > 0:\n            print(f\"{term.rjust(10)} : {score[0]:.4f}\")\n```\n\n### Python Script (custom TF-IDF implementation)\n\n```python\nimport math\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.util import bigrams, trigrams\n\nstop_words = stopwords.words('english')\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndef tokenize(text):\n    tokens = tokenizer.tokenize(text.lower())\n    tokens = [t for t in tokens if len(t) > 2 and t not in stop_words]\n    return tokens + [' '.join(b) for b in bigrams(tokens)] + [' '.join(t) for t in trigrams(tokens)]\n\ndef tf(term, doc_tokens):\n    return doc_tokens.count(term) / len(doc_tokens)\n\ndef idf(term, docs_tokens):\n    doc_count = sum(1 for doc in docs_tokens if term in doc)\n    return math.log(len(docs_tokens) / (1 + doc_count))\n\ndef compute_tfidf(docs):\n    docs_tokens = [tokenize(doc) for doc in docs]\n    all_terms = set(term for doc in docs_tokens for term in doc)\n    tfidf_scores = []\n    for tokens in docs_tokens:\n        tfidf = {}\n        for term in all_terms:\n            if term in tokens:\n                tfidf[term] = tf(term, tokens) * idf(term, docs_tokens)\n        tfidf_scores.append(tfidf)\n    return tfidf_scores\n```",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "tf-idf_implementation",
    "outlinks": [],
    "inlinks": [
      "tf-idf"
    ]
  },
  {
    "category": "LANG",
    "filename": "TF-IDF",
    "sha": "594d3e691fb7e3d3b31332517bed03197fea8afb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/TF-IDF.md",
    "text": "TF-IDF is a statistical technique used in text analysis to determine the importance of a word in a document relative to a collection of documents (corpus). It balances two ideas:\n\n- Term Frequency (TF): Captures how often a term occurs in a document.\n- Inverse Document Frequency (IDF): Discounts terms that appear in many documents.\n\nHigh TF-IDF scores indicate terms that are frequent in a document but rare in the corpus, making them useful for distinguishing between documents in tasks such as information retrieval, document classification, and recommendation.\n\nTF-IDF combines local and global term [[Statistics]]:\n- TF gives high scores to frequent terms in a document\n- IDF reduces the weight of common terms across documents\n- TF-IDF identifies terms that are both frequent and distinctive\n\nCan be used to give an initial snapshot of a notes themes and topic.\n### Equations\n\n#### Term Frequency\n\n$TF(t, d)$ measures how often a term $t$ appears in a document $d$, normalized by the total number of terms in $d$:\n\n$$\nTF(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}}\n$$\n\nWhere:\n- $f_{t,d}$ is the raw count of term $t$ in document $d$  \n- $\\sum_k f_{k,d}$ is the total number of terms in $d$ (i.e. the document length)\n\n#### Inverse Document Frequency\n\nIDF assigns lower weights to frequent terms:\n\n$$\nIDF(t, D) = \\log \\left( \\frac{N}{1 + |\\{d \\in D : t \\in d\\}|} \\right)\n$$\n\nWhere:\n- $N$ is the number of documents in the corpus $D$  \n- $|\\{d \\in D : t \\in d\\}|$ is the number of documents containing term $t$  \n- Adding 1 to the denominator avoids division by zero\n\n#### TF-IDF Score\n\nThe final score is:\n\n$$\nTF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D)\n$$\n\n### Related Notes\n\n- [[Bag of Words]]\n- [[Tokenisation]]\n- [[Clustering]]\n- [[Search]]\n- [[Recommender systems]]\n- [[nltk]]\n- [[TF-IDF Implementation]] <-\n\n### Exploratory Ideas\n- Can track TF-IDF over time (e.g., note evolution)\n- Can cluster or classify the documents using TF-IDF?",
    "aliases": [
      "TFIDF"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "NLP",
      "preprocessing"
    ],
    "normalized_filename": "tf-idf",
    "outlinks": [
      "bag_of_words",
      "clustering",
      "statistics",
      "tf-idf_implementation",
      "tokenisation",
      "search",
      "recommender_systems",
      "nltk"
    ],
    "inlinks": [
      "bag_of_words",
      "cosine_similarity",
      "elasticsearch",
      "evaluate_embedding_methods",
      "feature_extraction",
      "nlp",
      "preprocessing_text_classification",
      "search",
      "semantic_relationships",
      "sentence_similarity"
    ]
  },
  {
    "category": "LANG",
    "filename": "Text2Cypher",
    "sha": "b9397b5a37b470ca2f5a84c723a00b5c952c9b6a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Text2Cypher.md",
    "text": "Text2Cypher is a concept that allows users to convert natural language queries into Cypher queries, which are used to interact with [[GraphRAG|graph database]] like [[neo4j]]. This functionality enables users to ask questions in a more intuitive/[[Interpretability|interpretable]], conversational manner, rather than needing to know the specific syntax of [[Cypher]].\n\nAllows the user to ask vague questions.\nAllows for multihop queries on the graph\n\nOverall, Text2Cypher aims to simplify the interaction with graph databases, making it accessible to users who may not be familiar with query languages.\n### Key Features of Text2Cypher:\n\n1. **Natural Language Processing**: It utilizes natural language processing (NLP) techniques to understand user queries and translate them into structured Cypher queries.\n\n2. **Flexibility**: Users can ask vague or complex questions that may not directly relate to the underlying data structure, making it easier to retrieve information from a graph database.\n\n3. **Traversal Queries**: Text2Cypher can generate traversal queries that navigate through the graph, allowing for multi-hop queries that explore relationships between entities.\n\n4. **Explainability**: By converting natural language into Cypher, it helps provide a clearer understanding of how the data is structured and how the queries are executed, enhancing interpretability.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "querying"
    ],
    "normalized_filename": "text2cypher",
    "outlinks": [
      "graphrag",
      "neo4j",
      "interpretability",
      "cypher"
    ],
    "inlinks": [
      "graphrag",
      "how_to_search_within_a_graph"
    ]
  },
  {
    "category": "LANG",
    "filename": "Tokenisation",
    "sha": "b3468c7eba015ab322fa6a0195723cc2d00cb737",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Tokenisation.md",
    "text": "Tokenisation is a core step in natural language processing ([[NLP]]) where raw text is split into smaller units known as tokens. These tokens can be words, sentences, or subwords, depending on the level of analysis. Tokenisation prepares text for downstream tasks like embedding, classification, or parsing.\n\nThere are different kinds: [[spaCy]] or [[Hugging Face]] tokenisers.\n\n### Types of Tokenisation\n\n#### 1. Word Tokenisation\n\nSplits text into individual words and retains punctuation.\n\n```python\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(text_original)\nprint(tokens)\n```\n\n#### 2. Sentence Tokenisation\n\nSegments text into full sentences.\n\n```python\nfrom nltk.tokenize import sent_tokenize\nsentences = sent_tokenize(text_original)\nprint(sentences)\n```\n\n#### 3. Custom Tokenisation with Cleaning\n\nFor cleaner preprocessing (e.g. lowercasing, removing non-alphanumerics and stopwords):\n\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntemp = text_original.lower()\ntemp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)      # keep only letters and numbers\ntemp = re.sub(r\"[[0-9]+\\]\", \"\", temp)         # remove bracketed numbers\ntokens = word_tokenize(temp)\ntokens_clean = [t for t in tokens if t not in stopwords.words(\"english\")]\nprint(tokens_clean)\n```\n\n### Special Tokens in [[Transformer]] Models\n\nModern models like [[BERT]] use special tokens during tokenisation to preserve input structure:\n* [CLS]: Marks the start of the input sequence; used for classification tasks.\n* [SEP]: Separates different segments (e.g. question from context in QA tasks).\n\n### Related:\n- [[nltk]]",
    "aliases": [
      "tokenization"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "NLP",
      "preprocessing"
    ],
    "normalized_filename": "tokenisation",
    "outlinks": [
      "bert",
      "hugging_face",
      "spacy",
      "transformer",
      "nlp",
      "nltk"
    ],
    "inlinks": [
      "bert",
      "generative_ai_from_theory_to_practice",
      "nltk",
      "normalisation_of_text",
      "oov_words",
      "preprocessing_text_classification",
      "text_classification",
      "tf-idf"
    ]
  },
  {
    "category": "LANG",
    "filename": "Vectorisation",
    "sha": "92ef3d4154191b97896521347d52a5bef591d501",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Vectorisation.md",
    "text": "### Vectorisation in [[Python]]\n\nVectorisation refers to the practice of replacing explicit loops with array operations, typically using libraries like [[Numpy]]. This leads to faster and more efficient code execution.\n\n#### Why is NumPy vectorisation faster than a `for` loop?\n\n* NumPy operations like `np.dot()` are implemented in compiled C and optimised for parallel execution.\n* They utilise SIMD (Single Instruction, Multiple Data) and can leverage multi-threading and GPU acceleration (with appropriate backends).\n* In contrast, `for` loops in Python are interpreted sequentially, adding overhead and limiting performance.\n#### Example: Dot Product\n\n```python\nimport numpy as np\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Vectorised\nnp.dot(a, b)\n\n# Manual loop\nsum([x*y for x, y in zip(a, b)])\n```\n\n> Vectorised code runs simultaneously across elements, whereas loops run sequentially.\n### Resources\n\nRelated:\n- [[Numpy]]\n- [[Pandas]]\n\n[Link](https://www.youtube.com/watch?v=uvTL1N02f04&list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI&index=24)\n\n![[Pasted image 20241217204829.png|500]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "software"
    ],
    "normalized_filename": "vectorisation",
    "outlinks": [
      "numpy",
      "pandas",
      "python",
      "pasted_image_20241217204829.png"
    ],
    "inlinks": []
  },
  {
    "category": "LANG",
    "filename": "Why is named entity recognition (NER) a challenging task",
    "sha": "6a06144ae5e0bcbd3e2b76b261000746a34d9fa0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Why%20is%20named%20entity%20recognition%20(NER)%20a%20challenging%20task.md",
    "text": "Named Entity Recognition (NER) is considered a challenging task for several reasons:\n\n1. **Ambiguity**: Entities can be ambiguous, meaning the same word or phrase can refer to different entities depending on the context. For example, \"Washington\" could refer to a city, a state, or a person. Disambiguating these entities requires a deep understanding of context.\n\n2. **Variability in Language**: Natural language is highly variable and can include slang, idioms, and different syntactic structures. This variability makes it difficult for NER models to consistently identify entities across different texts.\n\n3. **Named Entity Diversity**: Entities can take many forms, including names, organizations, locations, dates, and more. Each type may have different characteristics, requiring the model to adapt to various patterns.\n\n4. **Lack of Annotated Data**: High-quality annotated datasets are crucial for training NER models. However, creating such datasets can be time-consuming and expensive, leading to limited training data for certain domains or languages.\n\n5. **Multilingual Challenges**: NER systems often struggle with multilingual texts, where the same entity may be represented differently in different languages. This adds complexity to the recognition process.\n\n6. **Nested Entities**: In some cases, entities can be nested within each other (e.g., \"The University of California, Berkeley\"). Recognizing such nested structures can be particularly challenging for NER systems.\n\n7. **Domain-Specific Language**: Different domains (e.g., medical, legal, technical) may have specific terminologies and entities that general NER models may not recognize effectively without domain-specific training.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "why_is_named_entity_recognition_(ner)_a_challenging_task",
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ]
  },
  {
    "category": "LANG",
    "filename": "Word2vec",
    "sha": "ed3e63af938ef1e73fb39c4be5dc69651f1baaa9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/Word2vec.md",
    "text": "Word2Vec is a technique for generating vector representations of words. Developed by researchers at Google, it uses a shallow [[Neural network]] to produce [[Vector Embedding|word embedding]] that capture [[Semantic Relationships]] and [[syntactic relationships]]. Word2Vec has two main architectures:\n\n1. CBOW (Continuous [[Bag of Words]]):\n    - Predicts a target word given its context (neighboring words).\n    - Efficient for smaller datasets.\n      \n2. Skip-Gram:\n    - Predicts the context words given a target word.\n    - Performs better on larger datasets.\n\nWord2Vec generates dense, continuous vector representations where words with similar meanings are close to each other in the embedding space. For example:\n\n- `vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")`\n\nUses Negative [[Resampling]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "python",
      "ML_Tools"
    ],
    "normalized_filename": "word2vec",
    "outlinks": [
      "syntactic_relationships",
      "vector_embedding",
      "bag_of_words",
      "semantic_relationships",
      "resampling",
      "neural_network"
    ],
    "inlinks": [
      "bert",
      "oov_words",
      "sentence_similarity",
      "vector_embedding",
      "word2vec.py"
    ]
  },
  {
    "category": "LANG",
    "filename": "WordNet",
    "sha": "912d6dae6b035227d3c8477ebaf3d23fdc27d2cb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/WordNet.md",
    "text": "WordNet, a lexical database that groups words into sets of **cognitive synonym**s called synsets. These synsets are linked together in a hierarchy based on semantic relations, including:\n\n- Hypernymy: Represents an \"is-a\" relationship (e.g., \"dog\" is a hypernym of \"beagle\").\n- Hyponymy: Represents a more specific type (e.g., \"beagle\" is a hyponym of \"dog\").",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "wordnet",
    "outlinks": [],
    "inlinks": [
      "nltk",
      "semantic_relationships"
    ]
  },
  {
    "category": "LANG",
    "filename": "embeddings for OOV words",
    "sha": "db6db0f012d5ae87338d3813cf24051da9c379c2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/embeddings%20for%20OOV%20words.md",
    "text": "Can you find words in a [[Vector Embedding|word embedding]] that where not used to creates the embedding? These are [[OOV words]].\n\nYes, but with important caveats. If a word is not in the [[spaCy]] model’s vocabulary with a vector, then:\n\n### What you can do\n\n#### Option 1: Filter out words without vectors (what you're doing now)\nThis is the cleanest option:\n```python\nif token.has_vector:\n    embeddings.append(token.vector)\n    valid_words.append(word)\n```\n\n#### Option 2: Fallback to character-level embeddings (optional)\nIf you're using `en_core_web_lg`, spaCy sometimes provides approximate vectors for out-of-vocabulary (OOV) words using subword features. But with `en_core_web_md`, OOV words truly lack vector meaning.\n\n#### Option 3: Use a different embedding model\nUse FastText or transformer-based models (e.g., Sentence Transformers), which can produce [[embeddings for OOV words]] based on subword information or context.\n\nExample with [[FastText]] (using gensim):\n```python\nfrom gensim.models import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\")  # or download from FastText\nembedding = model.get_vector(\"unseenword\")  # FastText will synthesize it\n```\n\n### Summary\n\n| Approach                     | Handles OOV? | Notes |\n|-----------------------------|--------------|-------|\n| spaCy `en_core_web_md`      | ❌            | Skips words without vectors (recommended) |\n| spaCy `en_core_web_lg`      | ⚠️ Sometimes  | May infer vectors using subword info |\n| FastText / GloVe            | ✅            | Good for unseen words |\n| Sentence Transformers (BERT)| ✅            | Contextualized, ideal for phrases/sentences |",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "optimisation"
    ],
    "normalized_filename": "embeddings_for_oov_words",
    "outlinks": [
      "vector_embedding",
      "oov_words",
      "spacy",
      "embeddings_for_oov_words",
      "fasttext"
    ],
    "inlinks": [
      "embeddings_for_oov_words",
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "lemmatization",
    "sha": "d77a88b4bdbd4ee1223f86d73b29ddefbc3d5a76",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/lemmatization.md",
    "text": "Lemmatization is the process of ==reducing a word to its base or root== form, known as the \"lemma.\" \n\nUnlike stemming, which simply cuts off word endings, lemmatization considers the context and morphological analysis of the words. \n\nIt ensures that the root word is a valid word in the language. ==For example, the words \"running,\" \"ran,\" and \"runs\" would all be lemmatized to \"run.\"== \n\nThis process helps in normalizing text data for natural language processing tasks by grouping together different forms of a word.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "lemmatization",
    "outlinks": [],
    "inlinks": [
      "nlp",
      "nltk",
      "normalisation_of_text",
      "preprocessing_text_classification",
      "spacy"
    ]
  },
  {
    "category": "LANG",
    "filename": "nltk",
    "sha": "1669035e83b0f39a64e2007c96dfe53e66cea626",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/nltk.md",
    "text": "A package for natural language processing toolkit.\n\nNLTK (Natural Language Toolkit) is a Python library for working with human language data. It provides tools for text processing, linguistic analysis, and building natural language processing ([[NLP]]) models. \n\nNLTK is an accessible toolkit for classical NLP tasks. While more modern libraries like [[spaCy]] or [[Transformer]]s are preferred for production systems, NLTK remains valuable for learning, prototyping, and linguistic exploration.\n\n### Key Features:\n- [[Tokenisation]]: breaking text into words or sentences.\n- Stopwords removal: filtering out common non-informative words.\n- [[Stemming]] and [[lemmatization]]: reducing words to base/root forms.\n- [[Part of speech tagging]]: identifying parts of speech (e.g., noun, verb).\n- [[Named Entity Recognition]] (NER)\n- Parsing and Treebanks\n- Access to many corpora (e.g., Gutenberg texts, [[WordNet]])",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP"
    ],
    "normalized_filename": "nltk",
    "outlinks": [
      "stemming",
      "lemmatization",
      "spacy",
      "transformer",
      "part_of_speech_tagging",
      "tokenisation",
      "wordnet",
      "nlp",
      "named_entity_recognition"
    ],
    "inlinks": [
      "nlp",
      "semantic_relationships",
      "tf-idf",
      "tokenisation"
    ]
  },
  {
    "category": "LANG",
    "filename": "prompt retrievers",
    "sha": "58e48f26172efc5a46afe96384b98202bf295074",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/prompt%20retrievers.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "prompt_retrievers",
    "outlinks": [],
    "inlinks": [
      "prompt_engineering"
    ]
  },
  {
    "category": "LANG",
    "filename": "spaCy",
    "sha": "5228a3665432f9649de3e2a066ebeb7cfa2d3cb0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/spaCy.md",
    "text": "#### What is spaCy and how is it best used within Python? \n\nspaCy is a fast, production-ready [[NLP]] library in Python, commonly used for tasks such as:\n\n - Tokenization\n - [[Named Entity Recognition]] (NER)\n - Part-of-speech tagging\n - Dependency parsing\n - Sentence segmentation\n - [[lemmatization]]\n\nIt is designed to work efficiently on large volumes of text and offers:\n\n - Pretrained pipelines (e.g., `en_core_web_sm`, `en_core_web_trf`)\n - Seamless integration with deep learning frameworks (e.g., [[PyTorch]], Transformers via `spacy-transformers`)\n\nBest practices for using spaCy:\n\n - Process documents as streams (e.g., use `nlp.pipe` with generators)\n - Avoid processing documents one-by-one unless debugging\n - Use spaCy's `DocBin` for serialized storage of processed data\n - Combine with custom pipelines (e.g., text cleaning → spaCy → downstream classification)\n\nExample:\n\n```python\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ntexts = [\"This is sentence one.\", \"Here is another sentence.\"]\n\n# Efficient [[batch processing]] using nlp.pipe\nfor doc in nlp.pipe(texts):\n    print([ent.text for ent in doc.ents])\n```\n\nExploratory Questions:\n\n - How does spaCy performance scale with document size and number of texts?\n - How can we integrate spaCy with a [[Data Pipeline]] (e.g., stream from disk/database)?\n - What are use cases where rule-based patterns outperform pretrained models?\n\n#### Using Generators with spaCy\n\nspaCy is designed to work well with generators—especially in `nlp.pipe`, which supports any iterable:\n\n```python\ndef text_stream(file_path):\n    with open(file_path, \"r\") as f:\n        for line in f:\n            yield line.strip()\n\ntexts = text_stream(\"documents.txt\")\nfor doc in nlp.pipe(texts, batch_size=32):\n    yield doc  # or process and write to file\n```\n\nThis approach is significantly more efficient than looping over `nlp(text)` one-by-one.\n\nExploratory Questions:\n - What are good strategies to pair `nlp.pipe()` with result-saving (e.g., JSONL or database)?\n - How do you monitor progress over long-running generators?\n - How do `batch_size` and `n_process` affect spaCy throughput?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "NLP",
      "python"
    ],
    "normalized_filename": "spacy",
    "outlinks": [
      "lemmatization",
      "data_pipeline",
      "pytorch",
      "nlp",
      "batch_processing",
      "named_entity_recognition"
    ],
    "inlinks": [
      "elasticsearch",
      "embeddings_for_oov_words",
      "nltk",
      "tokenisation",
      "vector_embedding"
    ]
  },
  {
    "category": "LANG",
    "filename": "stopwords",
    "sha": "72db7145603dac7f2441fc221013fe8d4e30ac07",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/stopwords.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "stopwords",
    "outlinks": [],
    "inlinks": [
      "nlp"
    ]
  },
  {
    "category": "LANG",
    "filename": "syntactic relationships",
    "sha": "5c4792c98aacfd57a7bde4e9a07f240b6bd64321",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/syntactic%20relationships.md",
    "text": "Syntactic relationships refer to the structural connections between words or phrases in a sentence, focusing on grammar and the arrangement of words. They determine how words combine to form phrases, clauses, and sentences, following the rules of syntax.\n\n[[Semantic Relationships]], on the other hand, deal with the meaning and interpretation of words and phrases. They focus on how words relate to each other in terms of meaning, such as synonyms, antonyms, and hierarchical relationships like hypernyms and hyponyms.\n\nThe key difference is that syntactic relationships are concerned with the form and structure of language, while semantic relationships are concerned with meaning and interpretation.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models"
    ],
    "normalized_filename": "syntactic_relationships",
    "outlinks": [
      "semantic_relationships"
    ],
    "inlinks": [
      "evaluate_embedding_methods",
      "word2vec"
    ]
  },
  {
    "category": "LANG",
    "filename": "topic modeling",
    "sha": "1432588739e54421961660930ec94407dbdbae47",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/natural-language/topic%20modeling.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "language_models",
      "NLP"
    ],
    "normalized_filename": "topic_modeling",
    "outlinks": [],
    "inlinks": [
      "latent_dirichlet_allocation",
      "non-negative_matrix_factorization"
    ]
  },
  {
    "category": "PM",
    "filename": "1-on-1 Template",
    "sha": "7989311042a920d7c1a2dc91bcb9fc3d4bf0beee",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/1-on-1%20Template.md",
    "text": "- Decisions\n    - [Your name] _add decisions that need to be made_\n    - [Other person's name] _add decisions that need to be made_\n        \n- Action items\n    - [Your name] _add next steps from the discussion_\n    - [Other person's name] _add next steps from the discussion_\n        \n- Topics to discuss (bi-directional)\n    - [Your name] _add topics or questions to discuss together_\n    - [Other person's name] _add topics or questions to discuss together_\n        \n- Updates (uni-directional - no action needed)\n    - [Your name] _add updates with no action needed_\n    - [Other person's name] _add updates with no action needed_\n\nRelated:\n- [[1-to-1's with a Line Manager]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "1-on-1_template",
    "outlinks": [
      "1-to-1's_with_a_line_manager"
    ],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "1-to-1's with a Line Manager",
    "sha": "dc8d264d49707a6cf978a0d309db6edf9f9d15bd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/1-to-1's%20with%20a%20Line%20Manager.md",
    "text": "Purpose: Regular 1-to-1s are key for alignment, feedback, support, and professional growth. To be effective, meetings should be structured, consistent, and focused on outcomes.\n#### Best Practices\n\nPrepare in Advance\n* Review past notes and actions\n* Reflect on achievements, blockers, and questions\n* Set a clear agenda\n\nUse a Shared Tracker. Maintain a living document (e.g. [[Google Sheets]]) to record:\n* Discussion points\n* Actions and owners\n* Status updates\n* Long-term goals and milestones\n\nBenefits:\n* Maintains shared context and visibility\n* Tracks progress and accountability\n* Enables asynchronous updates when meetings are delayed or missed\n\nFocus Areas\n* Current work updates\n* Bidirectional feedback\n* Career development discussions\n* Support or escalation needs\n* Planning and prioritisation",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "career",
      "communication"
    ],
    "normalized_filename": "1-to-1's_with_a_line_manager",
    "outlinks": [
      "google_sheets"
    ],
    "inlinks": [
      "1-on-1_template"
    ]
  },
  {
    "category": "PM",
    "filename": "Asking questions",
    "sha": "2f01748a2a62dea8b2ca4e5009c7232625602be2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Asking%20questions.md",
    "text": "### Why Ask Better Questions?\n\nAsking better questions enhances thinking, learning, problem-solving, and communication. Good questions:\n\n- Guide inquiry and exploration.\n- Clarify assumptions.\n- Elicit insights or novel responses.\n- Enable self-reflection and deeper understanding.\n\n### What Makes a Good Question?\n\nA good question:\n- **Elicits a novel or thoughtful response** — not just a fact or yes/no.\n- **Opens possibilities** rather than closing them.\n- **Reveals assumptions** or **forces re-evaluation** of mental models.\n- **Matches the context and audience** — good questions for brainstorming differ from those for debugging.\n- **Fosters chain-of-thought reasoning**, helping others articulate how they arrive at conclusions.\n### Types of Questions\n\nQuestions can be classified by their **function**, **depth**, or **structure**.\n\n#### 1. By Function\n\n| Type        | Purpose                                   | Example                             |\n|-------------|-------------------------------------------|-------------------------------------|\n| Clarifying  | Understand what's being said              | \"What do you mean by X?\"           |\n| Probing     | Dig deeper into reasoning or logic        | \"Why do you think that?\"           |\n| Exploratory | Generate ideas or new perspectives        | \"What if we reversed the problem?\" |\n| Reflective  | Encourage self-awareness                  | \"What assumption am I making here?\"|\n| Critical    | Test or challenge statements              | \"What evidence supports that?\"     |\n\n#### 2. By Depth\n- **Surface-level:** \"What is X?\"\n- **Mid-level:** \"How does X relate to Y?\"\n- **Deep-level:** \"Why does X matter?\" or \"What are the implications of X?\"\n\n#### 3. By Structure\n\n- **Open-ended:** Encourage elaboration.  \n    → _\"How might we design this differently?\"_\n    \n- **Closed:** Seek a specific answer.  \n    → _\"Is this implementation correct?\"_\n    \n- **Leading:** Suggest an answer.  \n    → _\"Wouldn't you agree that…?\"_\n    \n- **Falsifiable/Testable:** Can be proven right or wrong.  \n    → _\"Does increasing X always decrease Y?\"_\n### Characteristics of Good Questions\n\n| Characteristic | Description                                                       |\n|----------------|-------------------------------------------------------------------|\n| **Purposeful** | Serves a clear goal in the conversation or inquiry                |\n| **Contextual** | Relevant to the topic or the respondent                           |\n| **Open/Expansive** | Invites multiple viewpoints or lines of reasoning             |\n| **Challenging** | Pushes beyond defaults or surface-level answers                  |\n| **Precise**    | Minimizes ambiguity while leaving room for elaboration           |\n| **Sequenced**  | Ordered to build thought step-by-step ([[Chain of thought]])        |\n\n### Related Concepts\n- [[Chain of thought]]\n- [[Design Thinking Questions]]\n- [[Prompts]]\n\n### Questions:\n- [ ] How do LLMs generate or refine questions using [[Prompts]] or [[Chain of thought]] approaches?",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "communication",
      "GenAI",
      "learning",
      "NLP"
    ],
    "normalized_filename": "asking_questions",
    "outlinks": [
      "chain_of_thought",
      "prompts",
      "design_thinking_questions"
    ],
    "inlinks": [
      "project_management_portal",
      "prompts"
    ]
  },
  {
    "category": "PM",
    "filename": "Change Management",
    "sha": "c4cf6aa9af47210966f2964daf4ee138397cbd63",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Change%20Management.md",
    "text": "Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a desired future state. It involves\n- preparing, \n- supporting,\n- and helping people to adopt change in order to drive organizational success and outcomes. \n\nEffective change management helps\n- minimize resistance, \n- improves engagement, \n- and increases the likelihood of successful outcomes.\n\nThe process typically includes:\n\n1. **Planning**: Identifying the need for change, defining the change, and developing a strategy to implement it.\n\n2. **Communication**: Clearly explaining the reasons for the change, the benefits, and the impact on the organization and its people.\n\n3. **Training and Support**: Providing the necessary training and resources to help employees adapt to the change.\n\n4. **Knowledge Sharing**: Provide training and resources to help teams understand best practices for data quality.\n\n5. **Implementation**: Executing the change plan while managing any resistance or challenges that arise.\n\n6. **Monitoring and Evaluation**: Assessing the effectiveness of the change and making adjustments as needed to ensure successful adoption.\n\n7. **Sustainability**: Ensuring that the change is maintained over time and becomes integrated into the organization's culture and operations.\n\nWhy change fails:\n- Change is hard, identify the pain points.  \n- Resistance is why change fails, due to loss aversion, uncertainty, unexpected change when not bought in.  \n\nHow we can accomplish change:\n- Story telling will help.  \n- Introduce a hook i.e. can we reduce the processing time for tasks by X amount.\n- Put ourselves in a better position for tomorrow.",
    "aliases": [
      "Change Program"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "business"
    ],
    "normalized_filename": "change_management",
    "outlinks": [],
    "inlinks": [
      "data_literacy",
      "data_quality",
      "digital_transformation",
      "prevention_is_better_than_the_cure"
    ]
  },
  {
    "category": "PM",
    "filename": "Communication Techniques",
    "sha": "fce526b06f528f2c63f9c9b155c9ef6e13937ce1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Communication%20Techniques.md",
    "text": "Using these structured communication bridges can enhance clarity and engagement, especially in spontaneous or high-stakes discussions or [[Project Management Portal]].\n\nTips for Using Communication Bridges\n1. Start Small: Begin by integrating 2-3 bridges that feel natural to you.\n2. Observe Reactions: Notice how listeners respond when you clarify changes, summarize key points, or highlight actions.\n3. Practice Consistency: Make these bridges a regular part of your speaking style.\n\n[Speak More Clearly: 8 Precise Steps to Improve Communication](https://www.youtube.com/watch?v=Tc5dCLE_GP0)\n\n### 1. Context Bridge\n\n- Purpose: Aligns everyone by setting the context before diving into details.\n- How to Use: Start with phrases like:\n  - \"At a high level...\"\n  - \"This is our goal...\"\n  - ==\"The main problem is...\"==\n- Effect: Helps to focus thoughts and prevents initial rambling.\n\n### 2. Change Bridge\n\n- Purpose: Emphasizes shifts, trends, or significant moments in the discussion.\n- How to Use: Use phrases that highlight changes, such as:\n  - \"Here's the before, and here's the after...\"\n  - \"We’re shifting from X to Y...\"`\n  - \"We are at a tipping point...\"\n- Effect: Grabs attention by making the change clear.\n\n### 3. Insight Bridge\n\n- Purpose: Shares deeper insights or unique perspectives, creating \"aha\" moments.\n- How to Use: Key phrases include:\n  - \"Counterintuitively, ...\"\n  - \"Here's what most people miss...\"\n  - \"The deeper insight is...\"\n  - \"The key point here is...\"\n- Effect: Signals that you’ve thought deeply, which moves the conversation forward.\n\n### 4. Analysis Bridge\n\n- Purpose: Anchors discussion in evidence, keeping it grounded.\n- How to Use: Reference specific data points or comparisons with:\n  - \"The evidence shows...\"\n  - \"The data indicates...\"\n  - \"When we compared X and Y...\"\n- Effect: Focuses on facts, minimizing loss of direction.\n\n### 5. Logical Transition Bridge\n\n- Purpose: Provides a clear flow in the conversation, avoiding confusion.\n- How to Use: Classic transitions include:\n  - \"First, second, third...\"\n  - \"This leads to...\"\n  - \"On the other hand...\"\n- Effect: Helps listeners follow along without losing the thread.\n\n### 6. Summary Bridge\n\n- Purpose: Ensures that key messages stay clear, especially in long discussions.\n- How to Use: Frequently summarize main points with phrases like:\n  - \"The bottom line is...\"\n  - ==\"If you remember one thing, it’s this...\"==\n  - \"To bring it back to the goal...\"\n- Effect: Reinforces the main message throughout the discussion.\n\n### 7. Refinement Bridge\n\n- Purpose: Allows for clarification or expansion of ideas as needed.\n- How to Use: Rephrase or elaborate with:\n  - \"Let me break this down further...\"\n  - \"Another way of looking at it is...\"\n  - \"A useful analogy might be...\"\n- Effect: Clarifies complex points, helping everyone understand the core message.\n\n### 8. Action Bridge\n\n- Purpose: Concludes with actionable steps, defining the next moves.\n- How to Use: Conclude with statements like:\n  - ==\"Our immediate priority is...\"==\n  - \"Here’s what we’ll do next...\"\n  - \"The deliverables are...\"\n- Effect: Ends discussions with clear direction and accountability.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "communication"
    ],
    "normalized_filename": "communication_techniques",
    "outlinks": [
      "project_management_portal"
    ],
    "inlinks": [
      "project_management_portal"
    ]
  },
  {
    "category": "PM",
    "filename": "Communication principles",
    "sha": "ba47cbab39a3ec4e3686aa6b1fd69286dbbb3025",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Communication%20principles.md",
    "text": "[[Project Management Portal]]\n\n![[Pasted image 20240916075433.png]]\n\n![[Pasted image 20240916075439.png]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication"
    ],
    "normalized_filename": "communication_principles",
    "outlinks": [
      "pasted_image_20240916075439.png",
      "project_management_portal",
      "pasted_image_20240916075433.png"
    ],
    "inlinks": [
      "project_management_portal"
    ]
  },
  {
    "category": "PM",
    "filename": "Conceptual Model",
    "sha": "0ff2d65855d260af90590d38dc4ce08217b4d509",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Conceptual%20Model.md",
    "text": "Conceptual Model\n   - Entities: Customer, Order, Book\n   - Relationships: Customers place Orders, Orders include Books\n\nConceptual Model\n   - Focuses on high-level business requirements.\n   - Defines important data entities and their relationships.\n   - Tools: [[ER Diagrams]], ER Studio, DbSchema.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication"
    ],
    "normalized_filename": "conceptual_model",
    "outlinks": [
      "er_diagrams"
    ],
    "inlinks": [
      "data_modeling",
      "gartner_hype_cycle"
    ]
  },
  {
    "category": "PM",
    "filename": "Communication with Stakeholders",
    "sha": "b838bb751ecd6a7b0cc680c7f47870567d473f3a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Communication%20with%20Stakeholders.md",
    "text": "Purpose\n* Ensure stakeholders understand the relevance and impact of the solution.\n* Align expectations and build trust throughout the project.\n* Facilitate collaboration to maximize business outcomes and ROI.\n#### Key Principles\n\nSolution Relevance\n\n   * Explain the “So what?”: why the results matter to the business.\n   * Highlight benefits, potential impact, and alignment with business objectives.\n\nExpectation Management\n\n   * Set clear, realistic goals and timelines.\n   * Emphasize early wins and continuous improvement.\n   * Maintain transparency and build trust.\n\nCollaboration\n\n   * Translate business problems into measurable metrics.\n   * Prioritize objectives and decisions based on business impact.\n   * Encourage regular feedback, questions, and co-development.\n\nCommunication Tools\n   * Use visualizations to illustrate progress and insights.\n   * Present findings in actionable formats for stakeholders.\n   * Pilot tests can demonstrate value and secure buy-in.\n\n#### Outcome\n* Stakeholders understand the value and limitations of the work.\n* Continuous alignment ensures practical, deployable solutions.\n* Early feedback and collaboration increase adoption and success.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "communication_with_stakeholders",
    "outlinks": [],
    "inlinks": [
      "good_enough_principle_in_data_projects",
      "project_management_portal",
      "working_with_smes"
    ]
  },
  {
    "category": "PM",
    "filename": "Documentation",
    "sha": "e141c02c7e1ebda87bfa5ead56bf5c6a9d55e076",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Documentation.md",
    "text": "How you transfer ideas.\n\nMarkdown book maker: [mdBook](https://rust-lang.github.io/mdBook/guide/installation.html)\n\nAlso see: https://marp.app/",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "documentation"
    ],
    "normalized_filename": "documentation",
    "outlinks": [],
    "inlinks": [
      "github_actions",
      "html",
      "jupyter_book",
      "scientific_method"
    ]
  },
  {
    "category": "PM",
    "filename": "Education and Training",
    "sha": "93344a9c3051e3f8450cb529d4a7ac53e7b7f444",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Education%20and%20Training.md",
    "text": "Adaptive Learning Systems\n\n- **Overview**: Adaptive learning systems use technology to tailor educational experiences to individual student needs. RL is instrumental in personalizing these systems.\n- **Applications**:\n    - **Personalized Learning Paths**: RL algorithms can create customized learning paths for students based on their performance, preferences, and engagement levels, adapting content delivery in real-time.\n    - **Feedback and Assessment**: Adaptive systems can provide immediate feedback based on student responses, reinforcing concepts through targeted exercises and adjusting difficulty levels as needed.\n    - **Engagement Strategies**: By analyzing student interactions, RL can suggest motivational strategies, such as gamification elements or timely reminders, to keep students engaged and motivated.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "deep_learning"
    ],
    "normalized_filename": "education_and_training",
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ]
  },
  {
    "category": "PM",
    "filename": "Experiment Plan Template",
    "sha": "b05ca7e40c3059da0d532308a8068ae6fbb45e0c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Experiment%20Plan%20Template.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "experiment_plan_template",
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "Feedback Template",
    "sha": "5e7461430da6cb1f652e386bb5ada0a11c6c2723",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Feedback%20Template.md",
    "text": "- Praise: I really appreciate your work on this\n    \n    - _add here_\n        \n- FYI: It's really not a big deal, but I'm letting you know just in case.\n    \n    - _add here_\n        \n- Suggestion: I’m fairly confident this would help, but I can live without it\n    \n    - _add here_\n        \n- Recommendation: This could be holding you back\n    \n    - _add here_\n        \n- Plea: It’s almost at the breaking point if it’s not already there.\n    \n    - _add here_",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "feedback_template",
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "Fishbone diagram",
    "sha": "7e0d771be241c6fcd0e00d3bdce98482d1bfbb9c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Fishbone%20diagram.md",
    "text": "Fishbone diagram\n[[Documentation & Meetings]]\nRoot cause analysis: [[Documentation & Meetings]]\n- 5 Y's\n- Fishbone diagram: start at issue at head\n- ![[Pasted image 20250312162034.png]]\n- People and ownership: Who is entering the data: the source data",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "documentation"
    ],
    "normalized_filename": "fishbone_diagram",
    "outlinks": [
      "pasted_image_20250312162034.png",
      "documentation_&_meetings"
    ],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "How to do git commit messages properly",
    "sha": "d1dfa24f913de7c3804a68a62ff27c545b8ef175",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/How%20to%20do%20git%20commit%20messages%20properly.md",
    "text": "## Structure of a goof [[Git]] Commit Message\n\n1. **Subject Line**\n   - Keep it short (50 characters or less).\n   - Use the imperative mood (e.g., \"Fix bug\" instead of \"Fixed bug\").\n   - Capitalize the first letter.\n   - Do not end with a period.\n\n2. **Body (Optional)**\n   - Separate from the subject line with a blank line.\n   - Explain the \"what\" and \"why\" of the changes, not the \"how\".\n   - Wrap text at 72 characters.\n\n3. **Footer (Optional)**\n   - Include references to issues or pull requests (e.g., \"Closes\").\n   - Add any additional notes or metadata.\n\n### **Good Examples**:\n\n1. **Fix a Bug**  \n   ```\n   Fix incorrect login redirection\n   The login redirection was leading to an unauthorized page after successful login. \n   This fix ensures users are redirected to their dashboard upon successful authentication.\n   ```\n   \n2. **Add a New Feature**  \n   ```\n   Add search functionality to the user dashboard\n   Introduced a search bar in the user dashboard, allowing users to quickly find relevant information within their profile.\n   ```\n   \n3. **Update Documentation**  \n   ```\n   Update README to include new API endpoints\n   Added details about the newly added API endpoints for user registration and password recovery in the README file.\n   ```\n   \n4. **Refactor Code**  \n   ```\n   Refactor data fetching logic in the dashboard\n   The data fetching logic was consolidated into a reusable service to improve [[maintainability]] and reduce duplication.\n   ```\n   \n5. **Add Unit Tests**  \n   ```\n   Add unit tests for the authentication service\n   Implemented unit tests for the login and registration methods to ensure robust coverage of authentication functionality.\n   ```\n\n### **Bad Examples**:\n\n1. **Too Vague**  \n   ```\n   Update\n   ```\n   *This commit message doesn’t provide any meaningful context.*\n\n2. **Incomplete Explanation**  \n   ```\n   Fix bug\n   ```\n   *This doesn’t explain the *what* or *why*, making it unclear to someone reviewing the code.*\n\n3. **Too General**  \n   ```\n   Changed stuff\n   ```\n   *“Changed stuff” is not informative and doesn’t provide clear insight into what was actually modified.*\n\n4. **No Context**  \n   ```\n   Fixed issue\n   ```\n   *No context is provided about what the issue was, making it difficult for others to understand the change.*\n\n5. **Overly Short**  \n   ```\n   Remove unused variable.\n   ```\n   *The message could be expanded to include more context about why the variable was removed and what impact it had.*\n\n### **Tips Expanded**:\n\n- **Be Descriptive**: Commit messages should give a clear understanding of the changes. Describe what was changed, *why* it was changed, and *how* (if necessary).\n- **Focus on One Change**: Avoid including unrelated changes in a single commit. Each commit should represent one logical unit of work.\n- **Use Active Voice**: Avoid passive voice. Focus on the subject doing something (`Add`, `Fix`, `Implement`).\n- **Wrap Text Appropriately**: Ensure lines don’t exceed 72 characters for better readability in Git log and discussions.\n- **Be Concise**: Subject lines should be short (preferably under 50 characters), clear, and to the point without unnecessary detail.\n\n### **Common Pitfalls** to Avoid:\n\n- **Too Vague**: Commit messages like \"Update\" or \"Fix bug\" provide no actionable information.\n- **Too Long**: Descriptive, yes, but avoid overly lengthy messages that become difficult to scan quickly.\n- **No Context**: A good commit message should allow anyone reviewing it to understand the change without needing additional context.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "process"
    ],
    "normalized_filename": "how_to_do_git_commit_messages_properly",
    "outlinks": [
      "maintainability",
      "git"
    ],
    "inlinks": [
      "git"
    ]
  },
  {
    "category": "PM",
    "filename": "Jupyter Book",
    "sha": "847625c6aa86ac3010b2a51c73ef954a58aab69f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Jupyter%20Book.md",
    "text": "Jupyter Book is an open-source tool that lets you build publication-quality [[Documentation]] or books from Jupyter notebooks (`.ipynb`), Markdown files, and other content.\n\nExamples: https://python.quantecon.org/intro.html\n\nLink: https://jupyterbook.org/en/stable/intro.html\n## Key Features:\n\n* Converts notebooks and Markdown into static HTML or PDF.\n* Supports code execution, outputs, math (LaTeX), and interactive widgets.\n* Integrates with Sphinx (for extensions, cross-referencing, citations).\n* Ideal for data science documentation, course materials, and research reports.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "documentation"
    ],
    "normalized_filename": "jupyter_book",
    "outlinks": [
      "documentation"
    ],
    "inlinks": [
      "ipynb"
    ]
  },
  {
    "category": "PM",
    "filename": "Jobs to be done",
    "sha": "12077d4621e4258fafa22ffac4ce436df3d8a8d0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Jobs%20to%20be%20done.md",
    "text": "## Jobs to Be Done (JTBD) Framework\n\nAt its core, the JTBD framework asks:\n> *“What job is the customer hiring a product or service to do?”*\n\nFocusing instead on progress the customer seeks to make in a specific situation. A \"Job\" can have three dimensions:\n\n| Dimension    | Meaning | Example |\n|:-------------|:--------|:--------|\n| Physical     | Tangible actions or bodily experiences | A runner buys lightweight shoes to reduce foot strain. |\n| Emotional    | Feelings, status, identity concerns | A luxury car purchase to feel powerful or successful. |\n| Functional   | Practical tasks to accomplish | Buying accounting software to automate expense reports. |\n## Pains and Gains\n\nRelated to JTBD is mapping pains and gains:\n\n| Aspect | Definition                                                    | Example                                                    |\n| :----- | :------------------------------------------------------------ | :--------------------------------------------------------- |\n| Pains  | What annoys, frustrates, or creates friction for the customer | Bookkeeping is time-consuming and stressful.               |\n| Gains  | Positive outcomes or benefits the customer desires            | Want faster, easier financial reporting to feel organized. |\nUnderstanding pains and gains helps you design solutions that reduce pains and create gains.\n\n## Value Proposition Design and Value Map\n\nThe Value Proposition Canvas from Strategyzer formalizes this:\nhttps://www.strategyzer.com/library/the-value-proposition-canvas\n\nThe idea is \"fit\" — your solution should tightly align with the customer's pains, gains, and jobs.\n\nExample:  \nIf small business owners \"hire\" a service to handle taxes (functional), feel relief (emotional), and save time (physical), a fractional CFO service could offer that — if positioned correctly.\n\n![[Pasted image 20250427074650.png]]\n\nRelated:\n- [[Project Management Portal]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "communication"
    ],
    "normalized_filename": "jobs_to_be_done",
    "outlinks": [
      "pasted_image_20250427074650.png",
      "project_management_portal"
    ],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "Managing Data Science Teams",
    "sha": "f335c87e02facf95b2f655a0697e5ed45fbef351",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Managing%20Data%20Science%20Teams.md",
    "text": "Data wrangling is the biggest job\n\nHow to justify the costs of a new data scientist:\n- Whats the trajectory of the end goal of hte business case.\n- Not ROI approach\n- Mapping person to the road map and what is need to deliver.\n\nDeliverables:\n- Problem framing document:\n- Business problem,\n- Proposed solution\n- Required resources\n- Examples\n- Governance process",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "managing_data_science_teams",
    "outlinks": [],
    "inlinks": [
      "managing_people"
    ]
  },
  {
    "category": "PM",
    "filename": "Modern data team",
    "sha": "58cb400faf43dbc8a2b8fe16744bedfce4d9f469",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Modern%20data%20team.md",
    "text": "Modern data team\n- data engineer\n- analytics engineer\n- data analyst\n- data team of one.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "modern_data_team",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "One Pager Template",
    "sha": "c850f1d3693b268197e7ca28ab1a1d673d023ff2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/One%20Pager%20Template.md",
    "text": "# Proposal: [Project name]\n\n# About this doc\n\n_Metadata about this document. Describe the scope and current status._\n\nThis doc is a proposal for [feature or change]. Upon approval, we will look to have this prioritized as a project and do a full Technical Design Document.\n\n|   |   |\n|---|---|\n|Sign off deadline|_Date_|\n|Status|_Draft_|\n|Author(s)|_Name 1, Name 2_|\n\nSign offs\n\n- *Name 1*\n    \n- *Name 2*\n    \n- Add your name here to sign off\n    \n\n# Problem\n\n_What is the problem being solved? What are the pain points? What is the current solution and why is not good enough?_\n\n# High level goal\n\n_Why should we do this? Answer this in metrics ideally but otherwise a sentence or two is okay._\n\n# What will happen if we don’t solve this?\n\n_Make it clear the downsides of what will happen if we don’t invest the time into this._\n\n# Proposed solution: [Option name]\n\n_State the option you suggest and explain your reasoning. What benefits will we get from this approach? Time, money, risk, convenience, etc._\n\n# Alternatives\n\n_A table or summary of the other options to achieve the goal. Also, consider adding this to an Appendix to keep the doc focused too._\n\n- Option 1: …\n    \n    - Pros: …\n        \n    - Cons: …\n        \n- Option 2: …\n    \n    - Pros: …\n        \n    - Cons: …\n        \n- …\n    \n\n# Risks\n\n_What can go wrong with the proposed approach? How are you mitigating that?_\n\n- _Risk 1_\n    \n- _Risk 2_\n    \n- _…_\n    \n\n# Open Questions [optional]\n\n_Anything still being figured out that you could use some additional eyes or thoughts on._",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "one_pager_template",
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "Process for prototyping",
    "sha": "0253cf1e67c4cb3e108e0381ec0bdb22a4f9dcae",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Process%20for%20prototyping.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "process"
    ],
    "normalized_filename": "process_for_prototyping",
    "outlinks": [],
    "inlinks": [
      "managing_people"
    ]
  },
  {
    "category": "PM",
    "filename": "Problem Definition",
    "sha": "dbe910019985a20bf37be9ee4b5aeab3630b8fd2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Problem%20Definition.md",
    "text": "# What is involved:\n\nClearly articulate the problem you're trying to solve and the outcomes you expect.\n\n## Follow up questions\nWhat assumption can we make based on the problem?\n\n# What kind of questions are good to ask?\n\n**Business Context:**\n\n- What are the desired outcomes and how would success be measured?\n- What are the limitations and feasibility of using machine learning in this context?\n\n**2. Data Availability and Quality:**\n\n- What data is available in quantity and quality and relevant to the problem?\n- What is the format and structure of the data?\n\n**3. Feature Engineering and Model Selection:**\n\n- What are the key features or variables that might be predictive of the desired outcome?\n- What type of machine learning model might be best suited for this problem (e.g., classification, regression, [[Clustering]])?\n\n**4. Evaluation and Deployment:**\n\n- How will we evaluate the performance of the machine learning model?\n- What metrics will be used to measure success?",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "communication"
    ],
    "type": "process",
    "normalized_filename": "problem_definition",
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "scientific_method"
    ]
  },
  {
    "category": "PM",
    "filename": "Project Management Portal",
    "sha": "3405872008db4f6d4cb30f67e1c8af6f4661422f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Project%20Management%20Portal.md",
    "text": "How to manage clients and get jobs done:\n- [[Communication Techniques]]\n- [[Communication principles]]\n- [[Asking questions]]\n- [[Communication with Stakeholders]]\n- [[Good Enough Principle in Data Projects]]\n- [[Managing People]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal"
    ],
    "normalized_filename": "project_management_portal",
    "outlinks": [
      "communication_principles",
      "communication_with_stakeholders",
      "asking_questions",
      "communication_techniques",
      "managing_people",
      "good_enough_principle_in_data_projects"
    ],
    "inlinks": [
      "communication_principles",
      "communication_techniques",
      "jobs_to_be_done"
    ]
  },
  {
    "category": "PM",
    "filename": "Pull Request Template",
    "sha": "535043a7d3342ba838e98f904ef2edd2b84dec0e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Pull%20Request%20Template.md",
    "text": "## Tl;dr\n\n_1-liner if the context of the change is long_\n\n## Context\n\n_A few sentences on the high level context for the change. Link to relevant design docs or discussion._\n\n## This Change\n\n_What this change does in the larger context. Specific details to highlight for review. Include UI change screenshots or videos if applicable:_\n\n- _Callout 1_\n    \n- _Callout 2_\n    \n- _Callout 3_\n    \n\n## Test Plan\n\n_Go over how you plan to test it. Your test plan should be more thorough the riskier the change is. For major changes, I like to describe how I E2E tested it and will monitor the rollout._\n\n## Links\n\n- _link to ticket_\n    \n- _link to design doc_\n    \n- _link to design_\n    \n\n## Checklist\n\n- Pull request title is succinct with [tiny] if it’s extra small\n    \n- Describes the problem\n    \n- Describes the solution (screenshots included if UI changes)\n    \n- Has a test plan\n    \n- Contains links to any context (Slack, Figma, JIRA ticket, etc.)\n    \n- Code is self reviewed for readability, approach, and edge cases\n    \n- Lines changed that may require additional explanation are annotated with an explanation\n    \n- Change is ideally < 500 lines if possible. < 150 is ideal.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "pull_request_template",
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "RACI",
    "sha": "957d23cb6a2ba9e36d63f7b8e466044a3c384e8d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/RACI.md",
    "text": "RACI is a [[project management]] and organizational tool used to define and clarify the roles and responsibilities of individuals within a project or a process. \n\nRACI stands for : \n\n- Responsible (R)\n- Accountable (A)\n- Consulted (C)\n- Informed (I)",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "raci",
    "outlinks": [
      "project_management"
    ],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "Remaining useful life models",
    "sha": "edb125c5e1b33b843b6630b6284952bfe03460d4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Remaining%20useful%20life%20models.md",
    "text": "A type of [[Machine Learning]] problem.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "remaining_useful_life_models",
    "outlinks": [
      "machine_learning"
    ],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "Return of Experience Form",
    "sha": "e6118dc58bcc73a906b631233ec3afc6e8c0104f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Return%20of%20Experience%20Form.md",
    "text": "This document is a \"Return of Experience\" (REX) form from (Business Support & Performance) Department, focusing on the a project delivered.\n  \nIts general purpose is to:\n- Document and share a specific use case:\n- Highlight benefits and improvements: \n- Identify challenges and solutions:\n- Provide recommendations and best practices: \n- Serve as an internal knowledge-sharing tool:",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "return_of_experience_form",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "Reveal.js",
    "sha": "8b1665bc6e21ab4fb1f71480e5b29d21762e0875",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Reveal.js.md",
    "text": "[[html]] presentation framework for making slides.\n\nCourse:\n- https://www.youtube.com/watch?v=a6ioNtv2H-E&ab_channel=CodinginPublic\n\nSee: https://github.com/hakimel/reveal.js/tree/master?tab=readme-ov-file\n\nRelated:\n- [[nbconvert slideshows]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication"
    ],
    "normalized_filename": "reveal.js",
    "outlinks": [
      "nbconvert_slideshows",
      "html"
    ],
    "inlinks": [
      "ipynb",
      "nbconvert",
      "nbconvert_slideshows"
    ]
  },
  {
    "category": "PM",
    "filename": "Technical Debt",
    "sha": "e123e9398de3cefb38d3d4caee1f73c60f882a8c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Technical%20Debt.md",
    "text": "Technical debt refers to the concept in software development where developers take shortcuts or make suboptimal decisions to spped up the delivery of a project. \n\nThese shortcuts can lead to increased complexity and potential issues in the codebase, which may require additional effort to address in the future.\n\nJust like financial debt, technical debt incurs \"interest,\" meaning that the longer it remains unaddressed, the more costly it becomes to fix.\n\n### How Can Businesses Reduce Technical Debt?\n\n1. Automate [[Testing]] and Code Quality Checks: \n   - Implement automated tests to ensure code quality and catch issues early. Tools like [[tool.ruff]], mypy, and fixit can help enforce coding standards and identify potential problems.\n   - Use type checkers and automated checks for coding conventions to maintain consistency and reduce errors.\n\n1. Track Technical Debt:\n   - Use dashboards to monitor and visualize technical debt. This helps in identifying areas that need attention and prioritizing them accordingly.\n\n1. Code Refactoring and \"Spa Days\":\n   - Schedule regular \"spa days\" for the codebase, where the focus is on cleaning and refactoring specific areas. This helps in gradually reducing technical debt without impacting ongoing development.\n\n1. Empower Developers:\n   - Allow developers to identify and address technical debt as they work on the codebase. They are often best positioned to recognize areas that need improvement.\n\n1. Prioritize and Plan:\n   - Make technical debt reduction a part of the project planning process. Prioritize tasks that address high-impact debt and allocate time for refactoring in each development cycle.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "business",
      "documentation",
      "software"
    ],
    "normalized_filename": "technical_debt",
    "outlinks": [
      "tool.ruff",
      "testing"
    ],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "UML",
    "sha": "3afa549473cf8d82b52f39852cdd21d39e4dce50",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/UML.md",
    "text": "Universal modeling language.\n\nSometimes diagrams are good enough.\n\nhttps://www.drawio.com/\n\nhttps://www.reddit.com/r/SoftwareEngineering/comments/133iw7n/is_there_any_free_handy_tool_to_create_uml/\n\nhttps://plantuml.com/",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "modeling"
    ],
    "normalized_filename": "uml",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "PM",
    "filename": "html",
    "sha": "706faf8c5d1733e4953dbf4e50fbdaa3cdc4e7f1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/html.md",
    "text": "An HTML file is the foundational format for structuring content on the web. \n\n#### Display Content in Web Browsers\n\n* Structure text, headings, images, links, tables, and lists. Embed media (e.g. videos, audio). Create forms for user input\n* Combine with Markdown or LaTeX tools to render complex documents\n\n#### Create Web Applications\n* Serve as the frontend for web apps\n* Work with backend languages (e.g. Python, [[Node.JS]], PHP) to render dynamic content\n\n#### Data Presentation & Visualization\n* Embed interactive charts using libraries like Chart.js, D3.js, or [[Plotly]]\n\n#### Offline Use\n* Use HTML for standalone [[Documentation]], dashboards, or reports\n* Convert Jupyter Notebooks or Markdown into HTML for distribution\n\n#### Custom Tools and Prototypes\n\n* Quickly mock up UI prototypes\n* Design interactive reports or data stories",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "html",
    "outlinks": [
      "plotly",
      "documentation",
      "node.js"
    ],
    "inlinks": [
      "epub",
      "reveal.js"
    ]
  },
  {
    "category": "PM",
    "filename": "Why use ER diagrams",
    "sha": "aaf3af6e9832eac588442f238ecfc65888344a5f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/Why%20use%20ER%20diagrams.md",
    "text": "Cleaning a dataset before creating an [[ER Diagrams]] is crucial for ensuring accuracy and reliability in your database design\n\n1. [[Data Quality]]: Cleaning the dataset helps identify and rectify errors, inconsistencies, and missing values. This ensures that the data accurately represents the real-world entities and relationships you intend to model.\n\n2. [[Normalised Schema]]: Before creating an ER diagram, it's essential to normalize the data, which involves organizing it efficiently to reduce redundancy and dependency. Cleaning the dataset beforehand allows you to identify redundant information and eliminate it, leading to a more streamlined ER diagram.\n\n3. Entity Identification: Through data cleaning, you can properly identify the entities within your dataset. This involves determining which attributes belong to which entity, as well as identifying any composite or derived attributes. Proper entity identification is fundamental to creating an accurate ER diagram.\n\n4. Relationship Clarity: Cleaning the dataset helps clarify the relationships between entities. By ensuring that the data accurately reflects the relationships between different entities, you can create a more precise ER diagram that accurately represents the connections between various elements.\n\n5. Data Consistency: [[Data Cleansing]] ensures consistency across the dataset, which is essential for maintaining integrity in the ER diagram. Consistent data allows for clearer identification of relationships and attributes, leading to a more effective database design.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "documentation"
    ],
    "normalized_filename": "why_use_er_diagrams",
    "outlinks": [
      "data_cleansing",
      "er_diagrams",
      "data_quality",
      "normalised_schema"
    ],
    "inlinks": [
      "er_diagrams"
    ]
  },
  {
    "category": "PM",
    "filename": "pdoc",
    "sha": "12942f12408aa825a9b4ba865cfbfda7d897a3a1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/pdoc.md",
    "text": "## **pdoc Overview**\n\n[pdoc](https://pdoc.dev/) is a lightweight and automatic documentation generator for Python projects. It converts your module’s **docstrings** into clean, modern **HTML** pages that can be viewed locally or hosted online.\n\n### **Key Features**\n\n* **Automatic Documentation:** Generates documentation directly from Python docstrings.\n* **Markdown Support:** Allows Markdown formatting inside docstrings.\n* **Customizable Output:** Offers configuration options for templates, themes, and output directories.\n* **Easy Integration:** Can be integrated into build pipelines or run manually from the command line.\n* **Modern Design:** Produces clean, readable, and navigable documentation.\n\n## **Basic Usage**\n\n### **1. Generate HTML documentation**\n\nTo document a file or package and save the output as HTML:\n\n```bash\npdoc your_module_or_file.py --output-dir docs\n```\n\nThis creates a `docs/` folder containing the generated documentation:\n\n```\ndocs/\n ├── index.html\n └── your_module_or_file.html\n```\n\n### **2. Live Preview**\n\nTo preview documentation locally without saving files:\n\n```bash\npdoc your_module_or_file.py\n```\n\npdoc starts a local server (usually at `http://localhost:8080`) for interactive viewing.\n## **Documenting a Directory**\n\nTo generate documentation for a local folder (e.g., `scripts`):\n\n```bash\npdoc -o docs ./scripts\n```\n\nThis command creates documentation for all Python files in `scripts/` and outputs them to `docs/`.\n\n## **Next Steps**\n\n1. **View Locally:** Open `docs/index.html` in your browser.\n2. **Host Online:** Upload the `docs/` folder to a web server, [[GitHub Pages]], or [[Read the Docs]].\n3. **Include in Repository:** Commit the `docs/` folder so collaborators can access it easily.\n4. **Automate:** Add the `pdoc` command to your build or CI workflow.\n\n### **Common Commands**\n\n| Purpose         | Command                                                |\n| --------------- | ------------------------------------------------------ |\n| Generate HTML   | `pdoc your_module.py --output-dir docs`                |\n| Force overwrite | `pdoc --html --force --output-dir docs your_module.py` |\n| Live preview    | `pdoc your_module.py`                                  |",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "documentation",
      "ML_Tools",
      "architecture"
    ],
    "normalized_filename": "pdoc",
    "outlinks": [
      "github_pages",
      "read_the_docs"
    ],
    "inlinks": [
      "documentation_&_meetings"
    ]
  },
  {
    "category": "PM",
    "filename": "nbconvert slideshows",
    "sha": "040a9d4a0879fe8170237f3008948a59e4eab36a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/nbconvert%20slideshows.md",
    "text": "This tool converts [[ipynb]] into a slideshow.\n\n[[nbconvert]]\n\nExamples:\n- https://digitalhumanities.hkust.edu.hk/tutorials/turn-your-jupyter-notebook-into-interactive-presentation-slides-using-anaconda/\n- https://nordicesmhub.github.io/NEGI-Abisko-2019/topics/report/presentation.html\n- https://www.youtube.com/watch?v=EOpcxy0RA1A&ab_channel=JamesChurch\n\nIn [[DE_Tools]] see:\n- Explorations\\Other\\Nbconvert\n\njupyter nbconvert main.ipynb --to slides --post serve\n\njupyter nbconvert presentation.ipynb --to slides --no-prompt --TagRemovePreprocessor.remove_input_tags={\\\"to_remove\\\"} --SlidesExporter.reveal_theme=sky\n\nUse tags to remove cells.\n\nRelated: Better?\n- https://github.com/damianavila/RISE\n\nUse [[Reveal.js]]\n- Open the notebook in **Jupyter Notebook** (not Lab).\n- Go to `View > Cell Toolbar > Slideshow`\n- Set each cell's `Slide Type` as described above. (search within vscode for switch slide type)\n\nOther valid values for \"slide_type\":\n\"slide\" — starts a new slide\n\"subslide\" — nested slide\n\"fragment\" — incremental appearance\n\"notes\" — speaker notes\n\"skip\" — cell excluded from presentation\n\n\n![[Pasted image 20250726211647.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "documentation"
    ],
    "normalized_filename": "nbconvert_slideshows",
    "outlinks": [
      "reveal.js",
      "de_tools",
      "ipynb",
      "pasted_image_20250726211647.png",
      "nbconvert"
    ],
    "inlinks": [
      "nbconvert",
      "reveal.js"
    ]
  },
  {
    "category": "PM",
    "filename": "project management",
    "sha": "eaf08fb23bc33000c3e1619d202e59b9d67be563",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/project-management/project%20management.md",
    "text": "",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "project_management",
    "outlinks": [],
    "inlinks": [
      "raci"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "ANOVA",
    "sha": "67a80adcb039fa20ff7e7a201defc56c92b25195",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/ANOVA.md",
    "text": "Analysis of Variance (ANOVA)\n\nANOVA evaluates whether the **means** of a **numerical target variable** differ significantly across groups defined by a **categorical feature**.\n\n* Computes the [[F-statistic]] and associated [[p values]].\n* Determines if group-level differences are statistically significant.\n* Useful for [[Feature Selection]] in classification tasks where categorical features may influence the target.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "anova",
    "outlinks": [
      "f-statistic",
      "p_values",
      "feature_selection"
    ],
    "inlinks": [
      "central_limit_theorem_&_small_sample_sizes",
      "feature_selection",
      "filter_methods",
      "general_linear_regression",
      "statistical_assumptions",
      "wcss_and_elbow_method"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Addressing Multicollinearity",
    "sha": "0b717e774dffe4b5ffea1be9808ba51a2fc74984",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Addressing%20Multicollinearity.md",
    "text": "Multicollinearity can impact the performance and [[Interpretability]] of regression models by causing instability in coefficient estimates and complicating the analysis of variable significance. Techniques like PCA can help by transforming correlated variables into uncorrelated principal components, thereby improving model stability and interpretability.\n\n[[Principal Component Analysis]] (PCA) is a [[Dimensionality Reduction]] technique that can help address [[Multicollinearity]] in regression models.\n\n1. **Combining Correlated Variables**: PCA transforms the correlated independent variables into a set of uncorrelated variables called principal components. These components capture the majority of the variance in the data while reducing redundancy.\n\n2. **[[Dimensionality Reduction]]**: By selecting a smaller number of principal components that explain most of the variance, PCA can simplify the model. This reduces the complexity and potential overfitting associated with having too many correlated predictors.\n\n3. **Improving Model Stability**: By using principal components instead of the original correlated variables, the regression model can achieve greater stability and reliability in coefficient estimates, as the issues caused by multicollinearity are mitigated.\n\n4. **Enhanced Interpretability**: While the principal components may not have a direct interpretation in terms of the original variables, they can still provide insights into the underlying structure of the data and the relationships among variables.\n### Example Code\n\n```python\n\n# edit this to explore how to address multi collinusing pca \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport pandas as pd\n\nvariables = data_cleaned[['var1', 'var2', 'var3']]\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"features\"] = variables.columns\n\n# Drop feature with VIF > 10\ndata_no_multicollinearity = data_cleaned.drop(['Year'], axis=1)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "ML_Tools"
    ],
    "normalized_filename": "addressing_multicollinearity",
    "outlinks": [
      "'var1',_'var2',_'var3'",
      "multicollinearity",
      "principal_component_analysis",
      "dimensionality_reduction",
      "interpretability"
    ],
    "inlinks": [
      "multicollinearity"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Assumption of Normality",
    "sha": "640aa336eaf7f7d2016abd352b807185ab8fddb8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Assumption%20of%20Normality.md",
    "text": "[[Statistical Assumptions]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "assumption_of_normality",
    "outlinks": [
      "statistical_assumptions"
    ],
    "inlinks": [
      "central_limit_theorem_&_small_sample_sizes",
      "statistical_assumptions"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Bernoulli",
    "sha": "efbf5bb631cb0f80d82b52dfd2cfb3eae594d61e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Bernoulli.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "bernoulli",
    "outlinks": [],
    "inlinks": [
      "distributions"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Bootstrap Sampling",
    "sha": "ab7ab28e881182642e5f829dc37d3b667e47bd98",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Bootstrap%20Sampling.md",
    "text": "[[Resampling]] with replacement from an original dataset.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "bootstrap_sampling",
    "outlinks": [
      "resampling"
    ],
    "inlinks": [
      "bagging",
      "regularisation_of_tree_based_models",
      "resampling",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Casual Inference",
    "sha": "1138a84e9999c8c574d9def2c31337c71b9a49f2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Casual%20Inference.md",
    "text": "missing data problem",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "casual_inference",
    "outlinks": [],
    "inlinks": [
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Central Limit Theorem & Small Sample Sizes",
    "sha": "145e4f68e5b38216b55fe5a0c4a9faffd44370ff",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Central%20Limit%20Theorem%20&%20Small%20Sample%20Sizes.md",
    "text": "The [[Central Limit Theorem]] (CLT) is particularly important for data scientists working with small sample sizes. It enables the use of various statistical methods, and helps in making valid inferences about the population from limited data.\n\n1. **[[Assumption of Normality]]**: The CLT states that the sampling [[Distributions|distribution]] of the sample means will approximate a normal distribution, regardless of the underlying population distribution, as long as the sample size is sufficiently large. \n   \n2. This is crucial for data scientists because many statistical methods and tests (such as t-tests, [[ANOVA]], and regression analysis) rely on the [[Assumption of Normality]]. Even with small sample sizes, the CLT provides a foundation for making inferences about the population.\n\n3. **Confidence Intervals and [[Hypothesis testing]]**: The CLT enables data scientists to construct confidence intervals and perform hypothesis tests even when the sample size is small. By using the sample mean and the standard error (which is derived from the sample size), data scientists can estimate the range within which the true population mean is likely to fall, and test hypotheses about population parameters.\n\n4. **Reduction of Variability**: The variance of the sampling distribution decreases as the sample size increases, which means that larger samples provide more reliable estimates of the population mean. For small sample sizes, the CLT helps data scientists understand the potential variability in their estimates and make more informed decisions based on their data.\n\n5. **Practical Application**: In many real-world scenarios, obtaining large samples may not be feasible due to time, cost, or logistical constraints. The CLT allows data scientists to work with smaller samples while still applying statistical techniques that assume normality, thus broadening the scope of analysis.\n\n6. **Robustness of Results**: The CLT provides a theoretical justification for the robustness of statistical methods. Even if the original data is not normally distributed, the means of sufficiently large samples will tend to be normally distributed, allowing for more reliable conclusions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "central_limit_theorem_&_small_sample_sizes",
    "outlinks": [
      "central_limit_theorem",
      "anova",
      "hypothesis_testing",
      "assumption_of_normality",
      "distributions"
    ],
    "inlinks": [
      "central_limit_theorem"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Central Limit Theorem",
    "sha": "89074db6c847adc43ded5c1c4b95a740eb8ebf34",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Central%20Limit%20Theorem.md",
    "text": "The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the original distribution.\n\n[[Central Limit Theorem & Small Sample Sizes]]\n\n### Key Points\n\n- **Mean of [[Resampling]] Distribution:** The mean of the sampling distribution is equal to the mean of the original population.\n- **Variance of Sampling Distribution:** The variance of the sampling distribution is the population variance divided by the sample size (\\(n\\)), making it \\(n\\) times smaller.\n- **Applicability:** The CLT applies when calculating the sum or average of many variables, such as the sum of rolled numbers when rolling dice.\n\n### Importance\n\n- The CLT allows us to assume normality for various variables, which is crucial for:\n  - Confidence intervals\n  - Hypothesis testing\n  - Regression analysis\n\n[[Central Limit Theorem]]\n**Explain the concept of the Central Limit Theorem.**;; \n\n\n<!--SR:!2024-01-26,3,250-->",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "central_limit_theorem",
    "outlinks": [
      "resampling",
      "central_limit_theorem_&_small_sample_sizes",
      "central_limit_theorem"
    ],
    "inlinks": [
      "central_limit_theorem",
      "central_limit_theorem_&_small_sample_sizes",
      "statistics",
      "z-test"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Chi-Squared Test",
    "sha": "116d244ee824b6a62ed7bd77fd40830d4095e581",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Chi-Squared%20Test.md",
    "text": "## Chi-Squared Test\n\nThe Chi-squared test is used to determine if there is a significant association between categorical variables. It assesses whether the observed frequencies in a contingency table differ from the expected frequencies, assuming the data is independent.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "chi-squared_test",
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Confidence Interval",
    "sha": "355e721858eec6b442c5d340e3e1b5ff2aa07848",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Confidence%20Interval.md",
    "text": "A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with a confidence level, such as 95%, indicating the probability that the interval captures the true parameter.\n\nKey Points\n- **Confidence Level:** The likelihood that the interval includes the true parameter (e.g., 95%).\n- **Purpose:** Quantifies the uncertainty of an estimate, providing a range rather than a single value.\n### Example\n- A 95% confidence interval for a mean of (50, 60) suggests that, in repeated sampling, 95% of such intervals would contain the true mean.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "confidence_interval",
    "outlinks": [],
    "inlinks": [
      "data_analysis",
      "model_interpretability",
      "out-of-sample_rolling_forecast_evaluation",
      "prediction_intervals_vs_confidence_interval",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Correlation vs Causation",
    "sha": "bed7678f8117067fab1a1ff76756c169d1edf684",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Correlation%20vs%20Causation.md",
    "text": "What is the meaning of [[Correlation]] does not imply causation?\n\nCorrelation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. \n\n\n- **Correlation**: Indicates an association between variables but does not imply that changes in one variable cause changes in the other.\n- **Causation**: Suggests a direct cause-and-effect relationship between variables, requiring experimentation to establish.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "statistics"
    ],
    "normalized_filename": "correlation_vs_causation",
    "outlinks": [
      "correlation"
    ],
    "inlinks": [
      "correlation",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Cosine Similarity",
    "sha": "5fda15e4efc073429de3cabfc069cae1528d25de",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Cosine%20Similarity.md",
    "text": "Cosine similarity is a [[Metric]] used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from -1 to 1.where 1 indicates identical orientation, 0 indicates orthogonality, and -1 indicates opposite orientation. \n\nCosine similarity is commonly used in\n- text analysis, \n- information retrieval, \n- recommendation systems to compare document similarity, user preferences, or item features.\n\nIn [[Binary Classification]], cosine similarity can be used as a feature to help distinguish between two classes. For instance, in text classification tasks, you might represent documents as vectors using techniques like [[TF-IDF]]. \n\nCosine = \"How aligned are two vectors in direction?\"",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "NLP"
    ],
    "normalized_filename": "cosine_similarity",
    "outlinks": [
      "binary_classification",
      "tf-idf",
      "metric"
    ],
    "inlinks": [
      "evaluate_embedding_methods",
      "generative_ai_from_theory_to_practice",
      "sentence_similarity",
      "sentence_transformers",
      "vector_database",
      "vector_embedding",
      "word2vec.py"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Correlation",
    "sha": "f9c2fba465e9984ffa5f1edad466650020ce1284",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Correlation.md",
    "text": "Use in understanding relationships between variables in data analysis. \n\nWhile it helps identify associations, it's important to remember that ==correlation does not imply causation.== \n\nVisualization tools like heatmaps and clustering can aid in identifying and interpreting these relationships effectively.\n\n- What is Correlation?: A measure of the strength and direction of the relationship between two variables.\n### Description\n\n- Correlation measures the relationship between two variables, indicating how they change together. It ranges from -1 to 1:\n\n  - -1: Perfect negative correlation\n  - 0: No correlation\n  - 1: Perfect positive correlation\n\n### Key Points\n\n- [[Correlation vs Causation]]: Correlation does not imply causation. While correlation highlights associations, causation establishes a direct influence.\n- Significance: Correlation values < -0.5 or > +0.5 are considered significant.\n- Impact of Outliers: [[uncategorised/Outliers]] can distort correlation results.\n- Standardization: Correlation is a standardized version of [[Covariance]].\n\n### Model Preparation\n\n[[Feature Selection]]:\n  - Identify features correlated with the target. If all are correlated, keep all.\n  - For features correlated with each other, consider dropping one to avoid redundancy.\n  - If two features are highly correlated with the target, both can be retained.\n\nIf two variables are strongly positively correlated, it often makes sense to drop one of them to simplify the model. This is because ==highly correlated variables can introduce redundancy==, leading to [[Multicollinearity]] in regression models.\n\nBy removing one of the correlated variables, you can:\n\n1. Reduce Complexity: Simplifying the model by reducing the number of predictors can make it easier to interpret and manage.\n2. Improve Stability: Reducing multicollinearity can lead to more stable and reliable coefficient estimates.\n3. Enhance Performance: In some cases, removing redundant features can improve the model's predictive performance by reducing overfitting.\n\nHowever, it's important to ensure that the variable you choose to keep is the one that is more relevant or has a stronger theoretical justification for inclusion in the model. \n\n### Viewing Correlations\n\n- Use [[Heatmap]] or [[Clustering]] to visualize correlations between features.\n\n### Example Code\n\nTo find the correlation between two features:\n\n```python\ndf[['var1', 'target']].groupby(['var1'], as_index=False).mean().sort_values(by='target', ascending=False)\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "correlation",
    "outlinks": [
      "clustering",
      "'var1',_'target'",
      "multicollinearity",
      "correlation_vs_causation",
      "covariance",
      "uncategorised/outliers",
      "heatmap",
      "feature_selection"
    ],
    "inlinks": [
      "autocorrelation",
      "clustering",
      "correlation_vs_causation",
      "covariance",
      "data_selection_in_ml",
      "ds_&_ml_portal",
      "eda",
      "feature_evaluation",
      "feature_selection",
      "filter_methods",
      "heatmap",
      "multicollinearity",
      "pca_principal_components",
      "spearman_vs_pearson_correlation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Covariance vs Correlation",
    "sha": "3c0c7549fc93b9d5a0c67a96ff442b826d5a4869",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Covariance%20vs%20Correlation.md",
    "text": "[[Covariance]] provides a basic measure of how two variables move together, correlation offers a more [[Interpretability]] and standardized way to understand their relationship.\n\nDefinition:\n   - Covariance measures the degree to which two variables change together. It can take any value, which makes it difficult to interpret the strength of the relationship.\n   - Correlation, on the other hand, is a standardized measure of the relationship between two variables, ranging from -1 to 1. This standardization allows for easier interpretation of the strength and direction of the relationship.\n\nFormula:\n   - Correlation is derived from covariance. The formula for the correlation coefficient $r$between two variables $X$ and $Y$ is:\n   $$\n   r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n   $$\n   where $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively. This formula shows that correlation is essentially the covariance normalized by the product of the standard deviations of the two variables.\n\nInterpretation:\n   - While covariance can indicate the direction of the relationship (positive or negative), it does not provide information about the strength of that relationship. \n   - Correlation, being bounded between -1 and 1, allows for a clearer understanding of how strongly the two variables are related.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "covariance_vs_correlation",
    "outlinks": [
      "interpretability",
      "covariance"
    ],
    "inlinks": [
      "covariance"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Covariance",
    "sha": "a2c49a1768dfb507fbb755a1d7853d9984122147",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Covariance.md",
    "text": "In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between the variables. Specifically, covariance can be defined as follows:\n\n- **Positive Covariance**: If the covariance is positive, it means that as one variable increases, the other variable tends to also increase. Conversely, if one variable decreases, the other variable tends to decrease as well.\n  \n- **Negative Covariance**: If the covariance is negative, it indicates that as one variable increases, the other variable tends to decrease, and vice versa.\n\n- **Zero Covariance**: A covariance close to zero suggests that there is no linear relationship between the two variables.\n\nThe formula for calculating the covariance between two random variables $X$ and $Y$ is given by:\n\n$$\n\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\n$$\n\nwhere:\n- $X_i$ and $Y_i$ are the individual sample points,\n- $\\bar{X}$ and $\\bar{Y}$ are the means of $X$ and $Y$ respectively,\n- $n$ is the number of data points.\n\nCovariance is used in:\n- in the calculation of [[Correlation]] coefficients \n- and in multivariate statistics, such as in [[Gaussian Mixture Models]] where it helps describe the shape and orientation of the data distribution.\n\nRelated:\n- [[Covariance vs Correlation]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "statistics"
    ],
    "normalized_filename": "covariance",
    "outlinks": [
      "covariance_vs_correlation",
      "correlation",
      "gaussian_mixture_models"
    ],
    "inlinks": [
      "correlation",
      "covariance_structures",
      "covariance_vs_correlation",
      "gaussian_mixture_models"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Cryptography",
    "sha": "7bfcb35c9b4c432cce905956c5521f79e15e3f5b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Cryptography.md",
    "text": "Cryptography is the foundation of digital [[Data Security]], enabling privacy and secure communication over the internet.\n\nExamples are implemented in [[Node.JS]] (using `crypto` module) and are written in [[JavaScript]].\n\nResources:\n- [7 Cryptography Concepts EVERY Developer Should Know](https://www.youtube.com/watch?v=NuyzuNBFWxQ)\n- https://fireship.io/lessons/node-crypto-examples/\n## [[Hash]] (Chop and mix)\n\nA hashing function takes an input of any length and outputs a fixed-length value, ensuring:\n\n- The same input always produces the same output.\n- It is computationally expensive to reverse the hash.\n- It has a low [[Probability]] of collisions.\n\n### Create a Hash in Node.js\n\n```javascript\nconst { createHash } = require('crypto');\n\nfunction hash(str) {\n    return createHash('sha256').update(str).digest('hex');\n}\n\nlet password = 'hi-mom!';\nconst hash1 = hash(password);\nconsole.log(hash1);\n\npassword = 'hi-mom';\nconst hash2 = hash(password);\nconsole.log(hash1 === hash2 ? '✔️ Good password' : '❌ Password does not match');\n```\n\n## Salting\n\nSalting strengthens hashes by appending a random string before hashing, preventing attacks using precomputed hash tables.\n\n### Password Salt with Scrypt in Node.js\n\n```javascript\nconst { scryptSync, randomBytes, timingSafeEqual } = require('crypto');\n\nfunction signup(email, password) {\n    const salt = randomBytes(16).toString('hex');\n    const hashedPassword = scryptSync(password, salt, 64).toString('hex');\n    users.push({ email, password: `${salt}:${hashedPassword}` });\n}\n\nfunction login(email, password) {\n    const user = users.find(v => v.email === email);\n    if (!user) return 'login fail!';\n    \n    const [salt, key] = user.password.split(':');\n    const hashedBuffer = scryptSync(password, salt, 64);\n    const match = timingSafeEqual(hashedBuffer, Buffer.from(key, 'hex'));\n    return match ? 'login success!' : 'login fail!';\n}\n\nconst users = [];\nsignup('foo@bar.com', 'pa$$word');\nconsole.log(login('foo@bar.com', 'password'));\n```\n\n## HMAC (Hash-based Message Authentication Code)\n\nHMAC combines a hash with a secret key, ensuring authenticity and integrity.\n\n### HMAC in [[Node.JS]]\n\n```javascript\nconst { createHmac } = require('crypto');\n\nconst password = 'super-secret!';\nconst message = '🎃 hello jack';\n\nconst hmac = createHmac('sha256', password).update(message).digest('hex');\nconsole.log(hmac);\n```\n\n## Symmetric Encryption\n\nSymmetric encryption uses the same key to encrypt and decrypt data.\n\n### Symmetric Encryption in Node.js\n\n```javascript\nconst { createCipheriv, randomBytes, createDecipheriv } = require('crypto');\n\nconst message = 'i like turtles';\nconst key = randomBytes(32);\nconst iv = randomBytes(16);\nconst cipher = createCipheriv('aes256', key, iv);\nconst encryptedMessage = cipher.update(message, 'utf8', 'hex') + cipher.final('hex');\n\nconst decipher = createDecipheriv('aes256', key, iv);\nconst decryptedMessage = decipher.update(encryptedMessage, 'hex', 'utf-8') + decipher.final('utf8');\nconsole.log(`Decrypted: ${decryptedMessage}`);\n```\n\n## Keypairs\n\nKeypairs consist of a public key (shared) and a private key (kept secret) for secure communication.\n\n### Generate an RSA Keypair in Node.js\n\n```javascript\nconst { generateKeyPairSync } = require('crypto');\n\nconst { privateKey, publicKey } = generateKeyPairSync('rsa', {\n  modulusLength: 2048,\n  publicKeyEncoding: { type: 'spki', format: 'pem' },\n  privateKeyEncoding: { type: 'pkcs8', format: 'pem' },\n});\n\nconsole.log(publicKey);\nconsole.log(privateKey);\n```\n\n## Asymmetric Encryption\n\nAsymmetric encryption encrypts with a public key and decrypts with a private key, securing communication over networks.\n\n### RSA Encryption in Node.js\n\n```javascript\nconst { publicEncrypt, privateDecrypt } = require('crypto');\nconst { publicKey, privateKey } = require('./keypair');\n\nconst secretMessage = 'Confidential message';\nconst encryptedData = publicEncrypt(publicKey, Buffer.from(secretMessage));\nconsole.log(encryptedData.toString('hex'));\n\nconst decryptedData = privateDecrypt(privateKey, encryptedData);\nconsole.log(decryptedData.toString('utf-8'));\n```\n\n## Signing\n\nSigning verifies the authenticity of a message by hashing it and encrypting the hash with a private key.\n\n### RSA Signing in Node.js\n\n```javascript\nconst { createSign, createVerify } = require('crypto');\nconst { publicKey, privateKey } = require('./keypair');\n\nconst data = 'this data must be signed';\nconst signer = createSign('rsa-sha256');\nsigner.update(data);\nconst signature = signer.sign(privateKey, 'hex');\nconsole.log(signature);\n\nconst verifier = createVerify('rsa-sha256');\nverifier.update(data);\nconst isVerified = verifier.verify(publicKey, signature, 'hex');\nconsole.log(isVerified);\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "code_snippet",
      "math",
      "security"
    ],
    "normalized_filename": "cryptography",
    "outlinks": [
      "javascript",
      "probability",
      "node.js",
      "hash",
      "data_security"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "Differentation",
    "sha": "164af5e00d951ae05ecc0dc0334777acd39240e0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Differentation.md",
    "text": "# Forward Mode Automatic Differentiation\n\nuses dual numbers\n\nimplemented in tensor flow\n\nsee also Reverse Mode Automatic Differentiation\n\nFast,Flexible,Exact",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "differentation",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "Distributions",
    "sha": "abedc21c693043c71324bef24765e523a83fe021",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Distributions.md",
    "text": "### Discrete Distributions\nThese distributions have probabilities concentrated on specific values.\n\n- [[Uniform]] Distribution: All outcomes are equally likely. Example: Drawing a card from a shuffled deck. A boxplot can be meaningful if there’s variation in the distribution. Since the values are discrete, the boxplot will show the range and quartiles effectively.\n- [[Bernoulli]] Distribution: Represents two possible outcomes. Example: Coin flip (heads or tails), true/false scenarios. A bar chart or frequency plot would be better for visualizing the proportions. or rolling a dice.\n- [[Binomial]] Distribution: Represents the number of successes in a sequence of Bernoulli trials. Example: Number of heads in 10 coin flips,\n- [[Poisson]] Distribution: Models the frequency of events in a fixed interval. Example: Number of website visits per hour. A boxplot is suitable for this distribution, showing central tendencies, spread, and potential outliers.\n\n#### Continuous Distributions\nThese distributions have probabilities spread over a continuous range.\n\n- [[Gaussian Distribution]]: Characterized by a bell-shaped curve, symmetric with thin tails. Example: Heights, exam scores.\n- T Distribution: Similar to the normal distribution but with fatter tails, useful with limited data.\n- Chi-squared Distribution: Asymmetric and non-negative, commonly used in [[Hypothesis testing]].\n- Exponential Distribution: Models the time between events. Example: Time between website traffic hits, radioactive decay.\n- Logistic Distribution: S-shaped curve, often used in forecasting and modeling growth.\n  \n  ![[Pasted image 20250308191945.png]]\n\n#### Practical Applications\n\nFeature Distribution: Understanding the distribution of numerical/ categortical feature values across samples can provide insights into data characteristics.\n\n  - Observation: Analyze the spread and central tendency of data.\n  - Decision: Determine appropriate statistical methods or transformations.\n\n### Related Notes\n\n- [[Violin plot]]\n- [[Boxplot]]\n- [[Q-Q Plot]]",
    "aliases": [
      "Distribution"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "ML_Tools"
    ],
    "normalized_filename": "distributions",
    "outlinks": [
      "binomial",
      "violin_plot",
      "boxplot",
      "hypothesis_testing",
      "uniform",
      "poisson",
      "gaussian_distribution",
      "bernoulli",
      "pasted_image_20250308191945.png",
      "q-q_plot"
    ],
    "inlinks": [
      "anomaly_detection",
      "boxplot",
      "central_limit_theorem_&_small_sample_sizes",
      "covariance_structures",
      "data_analyst",
      "data_selection_in_ml",
      "ds_&_ml_portal",
      "eda",
      "feature_selection",
      "fitting_weights_and_biases_of_a_neural_network",
      "gaussian_distribution",
      "gaussian_mixture_models",
      "gini_impurity_vs_cross_entropy",
      "handling_different_distributions",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "kmeans_vs_gmm",
      "precision-recall_curve",
      "resampling",
      "seaborn",
      "spearman_vs_pearson_correlation",
      "standard_deviation",
      "statistics",
      "t-test",
      "train-dev-test_sets",
      "variance",
      "why_removing_outliers_may_improve_regression_but_harm_classification"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "EM Algorithm",
    "sha": "6993982947fc5400a46758c482a707a056cebbcd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/EM%20Algorithm.md",
    "text": "For when you have [[Missing Data]]\n\n[[Maximum Likelihood Estimation]]\n\nResources: https://www.youtube.com/watch?v=3AZDaTN2sCI&ab_channel=VeryNormal\n\nWhen it is not feasible to wait for more data.\n\nLets you do [[Maximum Likelihood Estimation]] when data is missing.\n\nGoal: Create a educated guess at a parameter.\n\nCondition expectations? \n[[Statistical Assumptions]]: independence \n\nSteps to the cycles: will converge\n- Estimation: Derive data\n- Maximisation:",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "em_algorithm",
    "outlinks": [
      "missing_data",
      "maximum_likelihood_estimation",
      "statistical_assumptions"
    ],
    "inlinks": [
      "maximum_likelihood_estimation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Factor Analysis",
    "sha": "9d9a197940f274913086522b5f0a7089804e5110",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Factor%20Analysis.md",
    "text": "Factor Analysis (FA) is a statistical method used for:\n- [[Dimensionality Reduction]],\n- [[EDA]]\n- or latent variable detection\n\nIt identifies underlying relationships between observed variables by modeling them as linear combinations of a smaller number of ==unobserved latent factors==.\n\nIn simpler terms, it helps reduce a large number of variables into fewer factors while retaining the core information and structure of the data. It assumes that observed variables are influenced by some common latent factors and unique errors.\n\n### Key Features of Factor Analysis:\n\n1. Latent Factors: These are unobserved variables that capture the shared variance among observed variables.\n2. Variance Decomposition: FA splits the total variance of observed variables into:\n    - Common variance: Shared by latent factors.\n    - Unique variance: Specific to each observed variable.\n\nIn [[ML_Tools]] see: [[Factor_Analysis.py]]\n\n### Next Steps:\n\n1. Would you like to visualize the factors to understand how the data clusters in the new latent space?\n2. Should we explore the relationships between the factors and target classes (e.g., species in the Iris dataset)?",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "statistics"
    ],
    "normalized_filename": "factor_analysis",
    "outlinks": [
      "factor_analysis.py",
      "ml_tools",
      "eda",
      "dimensionality_reduction"
    ],
    "inlinks": [
      "covariance_structures"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Gaussian Distribution",
    "sha": "954e251d51e37b24bf1c954d28f61c8ee1f85777",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Gaussian%20Distribution.md",
    "text": "Common assumption for a [[Distributions]].",
    "aliases": [
      "normally distributed data"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "gaussian_distribution",
    "outlinks": [
      "distributions"
    ],
    "inlinks": [
      "anomaly_detection",
      "distributions",
      "standardisation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Graph Theory",
    "sha": "c03db6c5f25a2c6c181ec76743d2b264fd3b9b70",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Graph%20Theory.md",
    "text": "[[Graph Theory Community]]\n\n[[Page Rank]]\n\n[[PyGraphviz]]\n\n[[networkx]]\n\n[[Plotly]] for graphs\nhttps://plotly.com/python/network-graphs/",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "graph",
      "math"
    ],
    "normalized_filename": "graph_theory",
    "outlinks": [
      "graph_theory_community",
      "plotly",
      "networkx",
      "pygraphviz",
      "page_rank"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "Grouped plots",
    "sha": "e3beb001ffc6eb69858e118092e5bb224bd3df7f",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Grouped%20plots.md",
    "text": "Related:\n- pairplots\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Facet Grid Example\ng = sns.FacetGrid(tips, col=\"sex\", row=\"time\")\ng.map_dataframe(sns.histplot, x=\"total_bill\", bins=20)\n\nplt.show()\n```\n\n![[Pasted image 20250402212849.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "visualization"
    ],
    "normalized_filename": "grouped_plots",
    "outlinks": [
      "pasted_image_20250402212849.png"
    ],
    "inlinks": [
      "data_visualisation",
      "melt"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Handling Different Distributions",
    "sha": "bc22ef7902c60fdc33015bc00eceb0304f245745",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Handling%20Different%20Distributions.md",
    "text": "Handling different [[Distributions]] is needed for developing robust, fair, and accurate machine learning models that can adapt to a wide range of data environments.\n\n## Importance of Handling Different Distributions\n\n1. [[Model Robustness]]: Ensures models generalize well to new, unseen data.\n2. Bias Mitigation: Prevents bias in predictions by accommodating diverse data types.\n3. Improved [[Accuracy]]: Fine-tunes models for better accuracy across varied [[Datasets]].\n4. Maintains model effectiveness across different data sources.\n5. Decision Making: Informs [[Preprocessing]], [[Model Selection]], and evaluation strategies.\n\n## Resources\n\nVideo: [Training and Testing on Different Distributions](https://www.youtube.com/watch?v=sfk5h0yC67o&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=16)\n\n## Example Scenario\n\nHigh-resolution photos (many) vs. amateur photos (small number) exhibit different distributions.\n\n## Strategy for Handling Distributions\n\nCode Example: See `Handling_Different_Distributions.py` in [[ML_Tools]]\n\nIn this script:\n- **Data Generation:** Creates two mock datasets with different distributions.\n- **Data Splitting:** Combines and splits the data into train, dev, and test sets.\n- **Model Tuning:** Uses `GridSearchCV` to find the best hyperparameters for a RandomForest model.\n- **Model Training and Evaluation:** Trains the model on the training set and evaluates it on the dev and test sets.\n- **Visualization:** Uses `matplotlib` to plot the distribution of a feature from both datasets and the model's accuracy on the dev and test sets.\n\n### Follow up questions\n\nHow best to combine the datasets?\nHow should we shuffle and split based on the distributions?\nHow do we pick the dev set?\n\n1. **Combining Datasets:**\n    - The script combines two datasets (`dataset1` and `dataset2`) that may have different distributions. This step ensures that the model is exposed to a variety of data during training.\n    \n1. **Random Shuffling and Splitting:**\n    - By shuffling and splitting the combined dataset into train, dev, and test sets, the script ensures that each set contains a mix of data from both distributions. This helps the model learn from the diversity in the data.\n\n1. **Model Tuning with Diverse Data:**\n    - The model tuning process uses the dev set, which contains data from both distributions. This helps in finding hyperparameters that work well across different data characteristics.\n\n## Related Topics\n- [[Preprocessing]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "management",
      "statistics"
    ],
    "normalized_filename": "handling_different_distributions",
    "outlinks": [
      "accuracy",
      "preprocessing",
      "model_selection",
      "model_robustness",
      "distributions",
      "datasets",
      "ml_tools"
    ],
    "inlinks": [
      "data_cleansing",
      "train-dev-test_sets"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Hypothesis testing",
    "sha": "51b211c4b548387af39e3f2d8ce3c0faa4de6c26",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Hypothesis%20testing.md",
    "text": "Used to draw inferences about population parameters based on sample data. The process involves the formulation of two competing hypotheses: the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$). \n\n### Key Concepts\n- Null Hypothesis ($H_0$): The hypothesis that there is no effect or no difference, which we seek to test.\n- Alternative Hypothesis ($H_1$): The hypothesis that indicates the presence of an effect or a difference.\n- P-value: A measure that helps determine the strength of the evidence against $H_0$. A small p-value (typically $< 0.05$) suggests that we reject $H_0$, indicating that the observed effect is statistically significant.\n### Decision Making\n- Accepting $H_0$: This means there is insufficient evidence to support the alternative hypothesis, suggesting that any observed effect could be due to random chance.\n- Rejecting $H_0$: This indicates that there is enough statistical evidence to conclude that the status quo does not represent the truth.\n### Limitations\nHypothesis testing is subject to Type I errors (false positives) and Type II errors (false negatives). A small p-value does not guarantee practical significance or causation, and results can be influenced by sample variability.\n\n### Example\nAn example of hypothesis testing is conducting a [[T-test]] to compare the means of two groups. The null hypothesis states that the means are equal ($H_0: \\mu_1 = \\mu_2$), while the alternative hypothesis states they are not equal ($H_1: \\mu_1 \\neq \\mu_2$).\n\n### Important Notes\n- Hypothesis testing relies on the formulation of $H_0$ and $H_1$, and the decision to accept or reject $H_0$ is based on the [[p values]].\n- A small p-value indicates statistical significance but does not imply practical relevance or causation.\n\n### Follow-up Questions and Answers\n\n##### In hypothesis testing, why might a very small p-value still lead to incorrect conclusions?  \n\nA very small p-value might lead to incorrect conclusions due to several factors, including:\n   - Sample Size: With large sample sizes, even trivial effects can yield small p-values, leading to the rejection of $H_0$ for effects that are not practically significant.\n   - Multiple Comparisons: Conducting multiple tests increases the risk of Type I errors, where we incorrectly reject $H_0$.\n   - Misinterpretation: A small p-value does not imply that the effect is large or important; it merely ==indicates that the observed data is unlikely under $H_0$.==\n\n##### How does the inclusion of effect size metrics improve the interpretation of hypothesis testing results?  \n\nIncluding ==effect size metrics== provides a quantitative measure of the magnitude of the observed effect, allowing researchers to assess the practical significance of their findings. While p-values indicate whether an effect exists, ==effect sizes help determine how meaningful that effect is== in real-world terms.\n\n##### What are the implications of multiple testing on the validity of p-values in hypothesis testing?  \n\nMultiple testing increases the likelihood of encountering false positives (Type I errors). When multiple hypotheses are tested simultaneously, the probability of incorrectly rejecting at least one true null hypothesis rises. This necessitates adjustments to p-values (e.g., Bonferroni correction) to maintain the overall error rate.\n\n### Related Topics\n- Bayesian statistics and its approach to hypothesis testing\n- The role of confidence intervals in statistical [[inference]]\n- [[Statistics]]\n- [[Testing]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "hypothesis_testing",
    "outlinks": [
      "t-test",
      "inference",
      "statistics",
      "p_values",
      "testing"
    ],
    "inlinks": [
      "central_limit_theorem_&_small_sample_sizes",
      "data_analysis",
      "data_analyst",
      "distributions",
      "statistical_assumptions",
      "statistical_tests",
      "statistics",
      "t-test",
      "testing"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Interquartile Range (IQR) Detection",
    "sha": "8bf114a9028ead1de72c1a4a2e21519e62d2f11e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Interquartile%20Range%20(IQR)%20Detection.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_IQR.py\n\nContext:  \n\nThe IQR method is a robust and widely used statistical technique for identifying outliers, especially in [[univariate data]]. It is based on the distribution of data and is less sensitive to extreme values compared to methods reliant on mean and standard deviation.\n\nSteps:\n- Compute the IQR:\n    - The IQR is the range within which the central 50% of the data lies.\n    - Formula:  \n        $\\text{IQR} = Q3 - Q1$  \n        where:\n        - $Q1$: The first quartile (25th percentile)\n        - $Q3$: The third quartile (75th percentile).\n          \n- Determine the bounds:\n    - Define lower and upper bounds to detect potential outliers:  \n        $\\text{Lower Bound} = Q1 - 1.5 \\cdot \\text{IQR}$  \n        $\\text{Upper Bound} = Q3 + 1.5 \\cdot \\text{IQR}$\n        \n- Identify anomalies:\n    - Any data point outside the lower or upper bounds is flagged as an anomaly.\n\nApplications:\n- Best suited for non-Gaussian distributions.\n- Commonly used in boxplots for visualizing outliers.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "process",
      "statistics"
    ],
    "normalized_filename": "interquartile_range_(iqr)_detection",
    "outlinks": [
      "univariate_data"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "boxplot"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Johnson–Lindenstrauss lemma",
    "sha": "9587979edd6ec366c8dab19657198bd8546a87dd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Johnson%E2%80%93Lindenstrauss%20lemma.md",
    "text": "## Johnson–Lindenstrauss lemma\n \n\nhttps://youtu.be/9-Jl0dxWQs8?list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&t=1125\n\nThe number of vectors that can be fit into a spaces grows exponentially.\n\nUseful for [[LLM]] in storing ideas. \n\nPlotting M>N almost orthogonal vectors in N-dim space\n\nOptimisation process that nudges then towards being perpendicular between 89-91 degrees\n\nStates there exists a linear mapping from a higher dimensional space into a sufficiently high-dimensional subspace that will preserve approximately the distances between points, up to a small amount of distortion. \n\nIn other words, states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.\n\nThe limits of the project depend on error rate and number of points (not the number of dimensions).\n\n```python\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# List of vectors in some dimension, with many\n# more vectors than there are dimensions\nnum_vectors = 10000\nvector_len = 100\nbig_matrix = torch.randn(num_vectors, vector_len)\nbig_matrix /= big_matrix.norm(p=2, dim=1, keepdim=True)\nbig_matrix.requires_grad_(True)\n\n# Set up an optimization loop to create nearly-perpendicular vectors\noptimizer = torch.optim.Adam([big_matrix], lr=0.01)\nnum_steps = 250\n\nlosses = []\n\ndot_diff_cutoff = 0.01\nbig_id = torch.eye(num_vectors, num_vectors)\n\nfor step_num in tqdm(range(num_steps)):\n    optimizer.zero_grad()\n\n    dot_products = big_matrix @ big_matrix.T\n    # Punish deviation from orthogonality\n    diff = dot_products - big_id\n    loss = (diff.abs() - dot_diff_cutoff).relu().sum()\n\n    # Extra incentive to keep rows normalized\n    loss += num_vectors * diff.diag().pow(2).sum()\n\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n# Plot loss curve\nplt.plot(losses)\nplt.grid(True)\nplt.show()\n\n# Compute angle distribution\ndot_products = big_matrix @ big_matrix.T\nnorms = torch.sqrt(torch.diag(dot_products))\nnormed_dot_products = dot_products / torch.outer(norms, norms)\nangles_degrees = torch.rad2deg(torch.acos(normed_dot_products.detach()))\n\n# Use this to ignore self-orthogonality\nself_orthogonality_mask = ~(torch.eye(num_vectors, num_vectors).bool())\nplt.hist(angles_degrees[self_orthogonality_mask].numpy().ravel(), bins=1000, range=(0, 180))\nplt.grid(True)\nplt.show()\n\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "johnson–lindenstrauss_lemma",
    "outlinks": [
      "llm"
    ],
    "inlinks": [
      "how_llms_store_facts",
      "mathematics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Markov chain",
    "sha": "dbaaffa04490f29b58bc8b13265785522ed2288b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Markov%20chain.md",
    "text": "Is a stochastic model that describes a sequence of events in which the [[Probability]] of each event depends only on the state attained in the previous event.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "statistics"
    ],
    "normalized_filename": "markov_chain",
    "outlinks": [
      "probability"
    ],
    "inlinks": [
      "generative_ai_from_theory_to_practice",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Mathematics",
    "sha": "373bddc2a1edc357bd0efd93a48ea4338e475168",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Mathematics.md",
    "text": "[[Johnson–Lindenstrauss lemma]]\n\n[[Big O Notation]]\n\n[[Directed Acyclic Graph (DAG)]]\n\n[[information theory]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "portal"
    ],
    "normalized_filename": "mathematics",
    "outlinks": [
      "information_theory",
      "big_o_notation",
      "directed_acyclic_graph_(dag)",
      "johnson–lindenstrauss_lemma"
    ],
    "inlinks": [
      "llm",
      "symbolic_computation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Mean Absolute Error",
    "sha": "6061ffbb8ad01601c6d604cbb3554979a87eb2c4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Mean%20Absolute%20Error.md",
    "text": "Definition: MAE measures the average absolute differences between predicted and actual values.\n\nInterpretation: Lower values indicate better model performance, as it reflects fewer errors in predictions. Same unit as initial series.\n\nFormula: \n   $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n   - Where:\n     - $n$ = number of observations\n     - $y_i$ = actual value\n     - $\\hat{y}_i$ = predicted value",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "mean_absolute_error",
    "outlinks": [],
    "inlinks": [
      "evaluating_time_series_forecasts",
      "mean_absolute_percentage_error",
      "model_evaluation",
      "model_optimisation",
      "regression_metrics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Mean Squared Error",
    "sha": "0e81450f6fa98286a1485133bc5311cf5fd19158",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Mean%20Squared%20Error.md",
    "text": "==Measures numerical proximity.==\n#### Mean Squared Error\n   - Definition: MSE calculates the average of the squares of the errors (the differences between predicted and actual values).\n   - Interpretation: Like MAE, lower values are better. However, MSE is more sensitive to outliers due to the squaring of errors, which can disproportionately affect the metric. Greater error values are exaggerated.\n   - Formula: \n   $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n   - Where:\n     - $n$ = number of observations\n     - $y_i$ = actual value\n     - $\\hat{y}_i$ = predicted value",
    "aliases": [
      "MSE"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "mean_squared_error",
    "outlinks": [],
    "inlinks": [
      "cost_function",
      "cross_entropy.py",
      "evaluating_time_series_forecasts",
      "fitting_weights_and_biases_of_a_neural_network",
      "loss_function",
      "regression_metrics",
      "use_cases_for_a_simple_neural_network_like",
      "why_removing_outliers_may_improve_regression_but_harm_classification",
      "xgboost"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Multicollinearity",
    "sha": "82d6bcf21ad5cbe78488d81b4981b05303ae30e0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Multicollinearity.md",
    "text": "When two or more regressors are in [[Correlation]]\n\nMulticollinearity refers to the ==instability== of a model due to ==highly correlated independent variables.==\n\nIt occurs when two or more independent variables in a regression model are highly correlated, which can make it difficult to determine the individual effect of each variable on the dependent variable.\n\nMulticollinearity affects regression models primarily because it leads to instability in the estimated coefficients of the independent variables.\n\nAlso see:\n- [[Addressing Multicollinearity]]\n- [[Impact of multicollinearity on model parameters]]\n\nRelated:\n- Multicollinearity hurts your hypothesis test\n\t- Correlation increases bias in the estimated parameters\n\t- Decreases power via exploded standard errors\n\nResults of Multicollinearity:\n1. **Difficulty in Estimating Coefficients**: When independent variables are highly correlated, it becomes challenging to isolate the individual effect of each variable on the dependent variable. This can result in large standard errors for the coefficients, making them unreliable.\n    \n2. **Inflated [[Variance]]**: The presence of multicollinearity inflates the variance of the coefficient estimates, which can lead to less precise estimates. This means that small changes in the data can lead to large changes in the estimated coefficients.\n    \n3. **Misleading Significance Tests**: Multicollinearity can cause some variables to appear statistically insignificant when they might actually be significant. This can lead to incorrect conclusions about the importance of predictors in the model.\n    \n4. **Model Interpretation**: The interpretation of the coefficients becomes complicated, as the effect of one variable may be confounded with the effect of another correlated variable.\n### Key Points\n\n- Assumption: The multicollinearity assumption suggests that ==independent variables should not be collinear.==\n- Detection: Use tools like [[Heatmap]] or [[Clustering]] to visualize [[Correlation]] and identify multicollinearity.\n- Variance Inflation Factor (VIF): High VIF values (typically greater than 10) indicate a high degree of multicollinearity. ==Features with high VIF should be dropped to improve model stability.==",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "type": null,
    "normalized_filename": "multicollinearity",
    "outlinks": [
      "correlation",
      "clustering",
      "impact_of_multicollinearity_on_model_parameters",
      "variance",
      "heatmap",
      "addressing_multicollinearity"
    ],
    "inlinks": [
      "addressing_multicollinearity",
      "correlation",
      "data_selection_in_ml",
      "ds_&_ml_portal",
      "dummy_variable_trap",
      "f-regression",
      "heatmap",
      "regression",
      "ridge",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Odds vs Probability",
    "sha": "52fd021ffcba7f12a8475b0289a1ddf1670743e5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Odds%20vs%20Probability.md",
    "text": "The terms [[Odds]] and [[Probability]] are related but distinct concepts used to describe the likelihood of events.\n\n| Concept     | Definition                  | Formula                     | Range         |\n| ----------- | --------------------------- | --------------------------- | ------------- |\n| Probability | Likelihood of event         | $P = \\frac{a}{a + b}$       | $[0, 1]$      |\n| Odds        | Ratio of success to failure | $\\text{Odds} = \\frac{a}{b}$ | $[0, \\infty)$ |\n\nOdds are commonly used in gambling and logistic regression, while probabilities are more general across statistics and decision theory. From probability to odds:\n $$\n  \\text{Odds} = \\frac{P}{1 - P}\n  $$",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "statistics"
    ],
    "normalized_filename": "odds_vs_probability",
    "outlinks": [
      "odds",
      "probability"
    ],
    "inlinks": [
      "logistic_regression_does_not_predict_probabilities"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Odds",
    "sha": "06714e9d10a29d0bd69fe10bd7e10c3a42d8f5b4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Odds.md",
    "text": "### Odds\n\n* Definition: The ratio of favorable outcomes to unfavorable outcomes.\n* Formula:\n\n  $$\n  \\text{Odds in favor of } A = \\frac{P(A)}{1 - P(A)}\n  $$\n\n  $$\n  \\text{Odds against } A = \\frac{1 - P(A)}{P(A)}\n  $$\n* Can be expressed as:\n\n  * Odds Ratio (numeric): e.g., 1.5\n  * Odds Format (a\\:b): e.g., 3:2\n\n#### Example:\n\nProbability of drawing a red card from a deck = $\\frac{26}{52} = 0.5$\nOdds in favor = $\\frac{0.5}{1 - 0.5} = 1$ → 1:1 odds",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "odds",
    "outlinks": [],
    "inlinks": [
      "odds_vs_probability"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Parametric tests",
    "sha": "748b246af079505f4c4f93222ecd31637466ed96",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Parametric%20tests.md",
    "text": "",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "test"
    ],
    "normalized_filename": "parametric_tests",
    "outlinks": [],
    "inlinks": [
      "parametric_vs_non-parametric_tests"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Prediction Intervals",
    "sha": "f90216b657a70d61319338baa51c4c3d5cf90b32",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Prediction%20Intervals.md",
    "text": "Prediction intervals estimate the range within which a future observation from the same distribution is likely to fall, with a specified confidence level.\n\n**Formula**:\n\n$$\\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n}}$$\n\n**Where**:\n- $\\bar{x}$: Sample mean\n- $s$: Sample standard deviation\n- $n$: Sample size\n- $t_{\\alpha/2, n-1}$: t-critical value for the chosen confidence level\n\n**Notes**:\n- Prediction intervals are always **wider** than a confidence interval for the mean.\n- They use the t-distribution due to sample uncertainty.\n- The interval is centered around $\\bar{x}$ but accounts for:\n    - **Estimation error** of the mean\n    - **Natural variability** of new values\n\n**Use Cases**:\n- Forecasting where a new measurement is likely to fall.\n- Risk assessment and operational thresholds.\n\nRelated:\n- [[Probability]]\n- [[Prediction Intervals vs Confidence Interval]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "prediction_intervals",
    "outlinks": [
      "prediction_intervals_vs_confidence_interval",
      "probability"
    ],
    "inlinks": [
      "prediction_intervals_vs_confidence_interval",
      "random_forest_for_time_series",
      "z-scores_vs_prediction_intervals"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Probability",
    "sha": "70f66b4b2cde9222de245aa7241b611d2a2bd285",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Probability.md",
    "text": "### Probability\n\n* Definition: The chance that an event occurs, expressed as a proportion of the total outcomes.\n* Formula:\n\n  $$\n  P(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of outcomes}}\n  $$\n* Range: $0 \\leq P(A) \\leq 1$\n\n#### Example:\n\nIf a die is rolled, the probability of rolling a 4 is:\n$P(4) = \\frac{1}{6}$",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "probability",
    "outlinks": [],
    "inlinks": [
      "cryptography",
      "markov_chain",
      "odds_vs_probability",
      "prediction_intervals"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Proportion Test",
    "sha": "cfb203eff8c3ab8810c58422ed48c604700a535c",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Proportion%20Test.md",
    "text": "The proportion test is used to compare proportions between groups. It can be categorized into:\n- **One-Sample Proportion Test**: Compares the proportion of successes in a single sample to a known population proportion.\n- **Two-Sample Proportion Test**: Compares the proportions of successes between two independent samples.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "test"
    ],
    "normalized_filename": "proportion_test",
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Q-Q Plot",
    "sha": "a6427bae90fd1e8705a5195e14545237534a3cd2",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Q-Q%20Plot.md",
    "text": "#### Q-Q plot\n\nA Q-Q (quantile-quantile) plot is a graphical tool used to compare the distribution of a dataset against a theoretical distribution (e.g., normal, logistic, exponential). It helps assess how well a given distribution fits the data.\n\nHow Q-Q Plots Work:\n\n1. Sort your dataset → Compute the sample quantiles (percentiles).\n2. Compute the theoretical quantiles → Take the same number of points from the theoretical distribution (e.g., normal, logistic).\n3. Plot sample quantiles vs. theoretical quantiles:\n    - If the points lie on a straight diagonal line, the data follows the theoretical distribution.\n    - If the points deviate significantly, the data does not fit that distribution.\n\nInterpreting a Q-Q Plot:\n\n- Straight diagonal line → Data follows the chosen distribution.\n- Curved S-shape → Data has skewness.\n    - Upward curve (right tail high) → Right-skewed.\n    - Downward curve (left tail high) → Left-skewed.\n- Heavy tails (outliers) → Points at the ends deviate from the line.\n- Light tails (thin-tailed distribution) → Points at the ends fall below the line.\n\nReferences:\n- https://www.youtube.com/watch?v=okjYjClSjOg\n\n![[Pasted image 20250427083434.png]]",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "visualization"
    ],
    "normalized_filename": "q-q_plot",
    "outlinks": [
      "pasted_image_20250427083434.png"
    ],
    "inlinks": [
      "anomaly_detection",
      "data_visualisation",
      "distributions",
      "evaluating_time_series_forecasts"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "R squared",
    "sha": "8883e37c0526786641f6ee54d183239cd517ee1e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/R%20squared.md",
    "text": "R², or the coefficient of determination, ==measures the proportion of variance in the dependent variable that is explained by the independent variables== in a [[Regression]] model.\n\n**Interpretation**:  \n- R² values range from 0 to 1.\n- A value of 1 indicates perfect predictions, meaning the model explains all the variability of the response data around its mean.\n- Higher R² values signify a better fit of the model to the data. However, it can be misleading when adding more predictors, as R² will never decrease when more variables are added to a model. See [[Adjusted R squared]].\n\n**Formula**:  \n$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n\nWhere:\n- $y_i$ = actual values\n- $\\hat{y}_i$ = predicted values\n- $\\bar{y}$ = mean of the actual values\n\n**Example**:  \nAn R² of 0.60 indicates that 60% of the variability observed in the target variable is explained by the regression model.\n\n#### Follow up Questions\n- [[R-squared metric not always a good indicator of model performance in regression]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "r_squared",
    "outlinks": [
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "regression",
      "adjusted_r_squared"
    ],
    "inlinks": [
      "adjusted_r_squared",
      "linear_regression",
      "regression_metrics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "R-squared metric not always a good indicator of model performance in regression",
    "sha": "5246c90c51f418f3b62c89317cd7a89d43fb261d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/R-squared%20metric%20not%20always%20a%20good%20indicator%20of%20model%20performance%20in%20regression.md",
    "text": "R-squared (R²) is a commonly used metric for assessing the performance of regression models, but it is not always a reliable indicator of model quality. It should not be the sole criterion for evaluating model performance. It is essential to consider other metrics, such as [[Adjusted R squared]], [[Cross Validation]] results, and the overall context of the analysis.\n\n1. **Increased Complexity**: R² will never decrease when more predictors are added to a model, even if those predictors do not have a meaningful relationship with the dependent variable. This can lead to overfitting, where the model captures noise rather than the underlying data pattern.\n\n2. **Lack of Context**: A high R² value does not necessarily imply that the model is appropriate for prediction. ==It only indicates the proportion of variance explained==. A model with a high R² might still have poor predictive performance if it does not generalize well to new data.\n\n3. **Non-linearity**: R² assumes a linear relationship between the independent and dependent variables. If the true relationship is non-linear, R² may provide a false sense of model adequacy.\n\n4. **Ignoring Model Assumptions**: R² does not account for whether the assumptions of the regression model (such as homoscedasticity, independence, and normality of residuals) are met. A model may have a high R² but still violate these assumptions, leading to unreliable results.\n\n5. **Adjusted R²**: To address some of these issues, Adjusted R² is often used, which adjusts the R² value based on the number of predictors in the model. It provides a more accurate measure of model performance when comparing models with different numbers of predictors.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
    "outlinks": [
      "adjusted_r_squared",
      "cross_validation"
    ],
    "inlinks": [
      "r_squared"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "R",
    "sha": "824f4ca12e843bfb397e5e0bc6e00b19544af6e0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/R.md",
    "text": "Programming language mainly used for [[Statistics]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "r",
    "outlinks": [
      "statistics"
    ],
    "inlinks": [
      "jupytext",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Reasoning tokens",
    "sha": "d4c28738ad32d4c17c5fc5f33f7cfa864e9ce8b4",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Reasoning%20tokens.md",
    "text": "Transformers rely on pattern recognition and language-based reasoning.\n\n**Reasoning tokens serve as a mechanism for token-based logical progression**, allowing models like [[ChatGPT]] to simulate math insights by leveraging pattern recognition, token relationships, and sequential reasoning, even without explicit [[Symbolic computation]] or mathematical processing built into the model itself (see [[Mathematical Reasoning in Transformers]]).\n\nIn the context of models like ChatGPT, **reasoning tokens** refer to the individual pieces of language that contribute to the step-by-step logical process used by the model to solve problems, including mathematical ones.\n\n**Logical Continuity and Error Correction**:\n\nReasoning tokens enable the model to maintain **logical continuity**, allowing it to backtrack or adjust outputs based on the sequence of previously generated tokens. For example, if the model makes a mistake in an earlier step (like a miscalculation), it can revise its response as it generates subsequent tokens that recognize the inconsistency.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "GenAI",
      "math"
    ],
    "normalized_filename": "reasoning_tokens",
    "outlinks": [
      "symbolic_computation",
      "chatgpt",
      "mathematical_reasoning_in_transformers"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "Resampling",
    "sha": "07678a9b4a7eb686ac850af7ab62f3e73d8d8656",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Resampling.md",
    "text": "**Resampling** refers to the process of repeatedly drawing samples from an existing dataset (or population) to estimate the variability of a statistic or to generate new sample [[distributions]].\n\nIt is not about collecting new data but **reusing existing data** to make inferences or improve model [[Generalisation|robustness]].\n### Common Uses\n\n* **[[Bootstrap sampling]]:** Drawing random samples *with replacement* to estimate confidence intervals or standard errors.\n* **[[Cross Validation]]:** Repeatedly splitting data into training and test sets to evaluate model performance.\n* **Permutation tests:** Randomly shuffling data labels to test hypotheses without parametric assumptions.\n\n### In Time Series\n\nResampling can also refer to **changing the frequency of observations**, such as aggregating daily data into monthly means or interpolating to higher frequency - see [[Time sampling]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "resampling",
    "outlinks": [
      "bootstrap_sampling",
      "time_sampling",
      "generalisation",
      "cross_validation",
      "distributions"
    ],
    "inlinks": [
      "bagging",
      "bootstrap_sampling",
      "central_limit_theorem",
      "data_reduction",
      "estimator",
      "smote_(synthetic_minority_over-sampling_technique)",
      "word2vec"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Root Mean Squared Error",
    "sha": "212025036e5722b38d4dd90ea017489d81e30ee8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Root%20Mean%20Squared%20Error.md",
    "text": "- Definition: RMSE is the square root of MSE, providing an error metric in the same units as the target variable.\n   - Interpretation: Lower RMSE values indicate better model performance, and it also emphasizes larger errors due to the squaring process. Easier to interpreted ([[Interpretability]]), back to the same scale as the input.\n   - Formula: \n   $$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\nLimitations:\n- RMSE gives extra weight to large errors, which could skew results if there are outliers.",
    "aliases": [
      "RMSE"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "root_mean_squared_error",
    "outlinks": [
      "interpretability"
    ],
    "inlinks": [
      "evaluating_time_series_forecasts",
      "linear_regression",
      "mean_absolute_percentage_error",
      "model_evaluation",
      "model_optimisation",
      "regression_metrics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Spearman vs Pearson Correlation",
    "sha": "49e9681b8c4240d66c580a639691e50107076ca5",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Spearman%20vs%20Pearson%20Correlation.md",
    "text": "Spearman [[Correlation]] is a rank-based measure of association, whereas Pearson correlation quantifies the strength of a linear relationship between two continuous variables.\n### Key Differences\n\n1. Data Type and Distribution\n   * Pearson assumes:\n     * Continuous variables\n     * Linear relationships\n     * Normally distributed variables (or large enough samples for CLT)\n\n   * Spearman can be applied to:\n     * Discrete, ordinal, or continuous variables\n     * Non-normal [[Distributions]]\n     * Nonlinear but monotonic relationships\n\n1. Sensitivity to [[storage/utils/file_getter/selected_files/Outliers|Outliers]]\n   * Pearson is sensitive to outliers, as it directly uses raw values.\n   * Spearman works on ranks, making it more robust to extreme values or skewed distributions.\n\n1. Functional Form of Relationship\n   * Pearson measures linear correlation (i.e., whether $y$ changes linearly with $x$).\n   * Spearman measures monotonic correlation (i.e., whether $y$ tends to increase or decrease as $x$ increases, regardless of the exact form).\n### Summary\n\n> Spearman correlation is better suited for ordinal, skewed, or discrete data, especially when the relationship is monotonic but non-linear, or when outliers are present. Pearson is preferable when the relationship is expected to be linear and homoscedastic between two continuous variables.\n\n### Example: Visualising the Difference\n\nWe create a synthetic dataset with:\n\n* A non-linear but monotonic relationship between $x$ and $y$\n* Some outliers to test robustness\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr, spearmanr\n\nnp.random.seed(0)\n\n# Simulate x (discrete low-cardinality variable)\nx = np.random.choice([0, 1, 2, 3, 4], size=100, p=[0.2, 0.3, 0.3, 0.15, 0.05])\n\n# Simulate y: nonlinear monotonic trend with noise\ny = x2 + np.random.normal(0, 2, size=100)\n\n# Inject outliers\ny[::15] += np.random.randint(10, 20, size=7)\n\n# Correlations\npearson_corr, _ = pearsonr(x, y)\nspearman_corr, _ = spearmanr(x, y)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.scatter(x, y, alpha=0.7)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Non-linear Relationship with Outliers\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Pearson correlation: {pearson_corr:.3f}\")\nprint(f\"Spearman correlation: {spearman_corr:.3f}\")\n```\n\n### Interpretation of Results\n\nTypical output (may vary):\n\n```\nPearson correlation: 0.68\nSpearman correlation: 0.83\n```\n\n* Pearson is reduced due to:\n  * Nonlinearity (e.g., quadratic growth)\n  * Outliers inflating variance\n\n* Spearman remains strong:\n  * Preserves rank order\n  * Diminishes influence of outliers\n### Conclusion\n\nThis example highlights that Spearman correlation is more robust and flexible in many real-world settings where:\n\n* Variables are discrete, ranked, or not normally distributed\n* The relationship is monotonic but not linear\n* Outliers may distort direct measurements of association\n\nFor rigorous statistical analysis, it’s important to examine data characteristics and the underlying [[Statistical Assumptions]] before selecting a correlation metric.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "analysis",
      "statistics"
    ],
    "normalized_filename": "spearman_vs_pearson_correlation",
    "outlinks": [
      "distributions",
      "statistical_assumptions",
      "correlation",
      "storage/utils/file_getter/selected_files/outliers"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "Standard deviation",
    "sha": "483ad236a233482bb2a0d6fcf02c7a35b0956f35",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Standard%20deviation.md",
    "text": "Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a set of data values. It indicates how much individual data points deviate from the mean (average) of the dataset.\n\n## Formula\n\nFor a dataset with $n$ observations $X_1, X_2, \\ldots, X_n$, the standard deviation $\\sigma$ is calculated using the formula:\n\n$$\n\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu)^2}\n$$\n\nWhere:\n- $\\sigma$ = standard deviation\n- $n$ = number of observations\n- $X_i$ = each individual observation\n- $\\mu$ = mean of the dataset, calculated as:  \n$$\n\\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n$$\n\n## Why Standard Deviation is Preferred Over [[Variance]]\n\n1. **Same Units as Data**  \n   Standard deviation is expressed in the same units as the original data, making it more [[Interpretability|interpretable]].  \n   - **Example:** If you measure height in centimeters, the standard deviation will also be in centimeters.  \n   - **Contrast:** Variance is expressed in squared units (e.g., square centimeters), which can be less intuitive to understand.\n\n2. **Direct Interpretation**  \n   Standard deviation provides a direct measure of the average distance of data points from the mean.  \n   - A **small standard deviation** indicates that the data points are close to the mean.  \n   - A **large standard deviation** suggests that the data points are more spread out.\n\n3. **Normal [[Distributions|Distribution]] Context**  \n   In the context of a normal distribution, standard deviation helps in understanding the spread of data:  \n   - Approximately **68%** of the data falls within **one standard deviation** of the mean.  \n   - About **95%** falls within **two standard deviations**.  \n   - About **99.7%** falls within **three standard deviations** (known as the empirical rule).  \n   This property is particularly useful for identifying [[uncategorised/Outliers]].\n\n4. **Ease of Communication**  \n   Standard deviation is more intuitive and easier to communicate to a broader audience, including those without a strong statistical background. Its direct relation to the data makes it a preferred choice for explaining variability.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "communication",
      "math"
    ],
    "normalized_filename": "standard_deviation",
    "outlinks": [
      "distributions",
      "interpretability",
      "variance",
      "uncategorised/outliers"
    ],
    "inlinks": [
      "eda",
      "t-test",
      "z-test"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Standardisation",
    "sha": "93c24889f6d7c5d07d1fd613fcd8590757817ee6",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Standardisation.md",
    "text": "Standardisation is a [[Preprocessing|data preprocessing]] technique used to [[Data Transformation]]. Centers data with zero mean and unit [[Variance]], suitable for algorithms sensitive to variance.\n\nDefinition: Standardisation involves rescaling the features of your data so that they have ==a mean of 0 and a standard deviation of 1==. This is achieved by subtracting the mean of the feature from each data point and then dividing by the standard deviation.\n\nPurpose: \n- Useful for algorithms that assume the data is normally distributed ([[Gaussian Distribution]].\n- Uniformity: It helps in bringing all features to the same scale.\n\n### Use Case\n\n- Centred Data Assumption: Standardisation is beneficial when the model assumes that the data is centred around zero. This is common in algorithms such as linear regression, logistic regression, and [[Principal Component Analysis]] (PCA), and distance-based algorithms like [[K-nearest neighbours|KNN]] and [[Gradient Descent]] descent optimization.\n  \n- Improved Performance: It can improve the performance and convergence speed of machine learning algorithms by ensuring that each feature contributes equally to the result.\n### Formula\n\nThe formula for standardisation is:\n\n$$z = \\frac{x - \\mu}{\\sigma}$$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the feature.\n- $\\sigma$ is the standard deviation of the feature.\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_standardized = scaler.fit_transform(df)  # Rescales each feature to have mean 0 and std deviation 1\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing"
    ],
    "normalized_filename": "standardisation",
    "outlinks": [
      "gradient_descent",
      "data_transformation",
      "preprocessing",
      "principal_component_analysis",
      "variance",
      "gaussian_distribution",
      "k-nearest_neighbours"
    ],
    "inlinks": [
      "anomaly_detection",
      "correlated_time_series",
      "feature_scaling",
      "feature_selection",
      "normalisation",
      "normalisation_vs_standardisation",
      "statistical_tests",
      "why_standardise_features"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Statistical Assumptions",
    "sha": "da6899401e9716e567083146d38d5f5f7f14460d",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Statistical%20Assumptions.md",
    "text": "Statistical assumptions are essential conditions that must be met for various statistical methods and models to produce valid results. Necessary for robustness and reliability of statistical analyses. If any assumptions are violated, it may be necessary to employ alternative statistical methods or to transform the data accordingly.\n\nPurpose of Statistical Assumptions:\n- [[Data Analysis]]: Ensures that the chosen statistical methods are appropriate for the data.\n- [[Interpretability]]: Facilitates accurate interpretation of results and conclusions drawn from analyses.\n\n#### Key Assumptions:\n\n- [[Assumption of Normality]]: This assumption posits that the data follows a normal distribution. Many [[Statistical Tests]], such as t-tests and [[ANOVA]], rely on this assumption to validate their results. If the data is not normally distributed, alternative methods or transformations may be necessary. Heavy tailed distributions can violate this.\n\n- Homoscedasticity: This refers to the assumption that the variance of the residuals (errors) remains constant across all levels of the independent variable(s). Violations of this assumption can lead to inefficient estimates and impact [[Hypothesis testing]].\n\t- Huber Error?\n\n- ==Independence==: This assumption states that the observations in the dataset should be independent of one another. Dependence among observations can result in biased estimates and incorrect conclusions.\n\n- Normality of Residuals: In regression analysis, it is assumed that the residuals (the differences between observed and predicted values) are normally distributed. This is critical for conducting hypothesis tests on regression coefficients.\n\n#### Broader Categories of Assumptions:\n\nModel Assumptions: These are overarching assumptions that apply to specific statistical models. For example:\n  - Linear Regression: Assumes a ==linear relationship== between the independent and dependent variables.\n  - [[Logistic Regression]]: Assumes a binary outcome for the dependent variable.\n\nDistribution Assumptions: Different statistical tests make specific assumptions about the distribution of the data:\n  - Parametric Tests: Assume that the data follows a certain distribution (e.g., normal).\n  - [[Non-parametric tests]]: Do not require such distributional assumptions and can be applied to data that does not meet these criteria.\n#### Additional Considerations:\n\nTesting Assumptions: It is important to test these assumptions before conducting statistical analyses. Common methods include:\n  - Visual Inspection: Using plots (e.g., Q-Q plots, residual plots) to visually assess normality and homoscedasticity.\n  - [[Statistical Tests]]: Employing tests like the Shapiro-Wilk test for normality or Levene's test for homoscedasticity.\n\nConsequences of Violating Assumptions: Understanding the implications of assumption violations is crucial. For example, violating the [[Assumption of Normality]] can lead to:\n  - Increased Type I or Type II error rates.\n  - Misleading confidence intervals and p-values.\n\nTransformations and Alternatives: When assumptions are violated, consider:\n  - Data Transformations: Such as log, square root, or Box-Cox transformations to meet assumptions.\n  - Alternative Methods: Using robust statistical techniques that are less sensitive to assumption violations, such as bootstrapping or [[non-parametric]] tests.\n  - Contextual Relevance: The relevance of specific assumptions may vary depending on the context of the analysis and the nature of the data. Always consider the specific characteristics of the dataset when evaluating assumptions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "explainability",
      "selection"
    ],
    "normalized_filename": "statistical_assumptions",
    "outlinks": [
      "anova",
      "hypothesis_testing",
      "logistic_regression",
      "data_analysis",
      "assumption_of_normality",
      "non-parametric_tests",
      "non-parametric",
      "statistical_tests",
      "interpretability"
    ],
    "inlinks": [
      "assumption_of_normality",
      "eda",
      "em_algorithm",
      "parametric_vs_non-parametric_tests",
      "spearman_vs_pearson_correlation",
      "statistical_tests",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Statistical Tests",
    "sha": "0491b5956c9cb8c0a99c39b725fe7a71343e670b",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Statistical%20Tests.md",
    "text": "Statistical tests are methods used to determine if there is a significant difference between groups or if a relationship exists between variables. Each test has its specific [[Statistical Assumptions]] and applications.\n## Types of Statistical Tests\n\n[[Z-Test]]\n\n[[T-test]]\n\n[[Chi-Squared Test]]\n\n[[Proportion Test]]\n\n## Test Statistics\n\nFor each statistical test, a test statistic is calculated. This statistic measures the degree of deviation from the null hypothesis ([[Hypothesis testing]]). The [[Estimator]] is centered by the population mean, and then it is divided by the population standard deviation, a process known as [[Standardisation]].",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "statistical_tests",
    "outlinks": [
      "t-test",
      "z-test",
      "statistical_assumptions",
      "hypothesis_testing",
      "proportion_test",
      "estimator",
      "standardisation",
      "chi-squared_test"
    ],
    "inlinks": [
      "feature_importance",
      "stationary_time_series",
      "statistical_assumptions",
      "statistics",
      "time_series_python_packages"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Statistical theorems",
    "sha": "7794262decdfdcd7bf558c9d80f684485797aac9",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Statistical%20theorems.md",
    "text": "Statistical theorems\n- Asymptotic Theorem: Law of large numbers: Sample mean approaches the population mean.\n\t- finite mean assumption",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "statistical_theorems",
    "outlinks": [],
    "inlinks": [
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Statistics",
    "sha": "a2a382d818692ae3a381b5f85531510223b8997a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Statistics.md",
    "text": "Statisticians want to understand the world. The world is made of probabilities, we model probabilities with functions, and we model functions with parameters.\n\n\"Observe data and construct models, infer and refine hypotheses \"\n\nStatistical data is random (unknown values) and uncertain (unknown what kind).\n\nResources:\nhttps://github.com/unpingco/Python-for-Probability-Statistics-and-Machine-Learning-2E\n- [ ] Put links into relevant notes\n\nPortal for all statistics notes:\n\n[[Statistical theorems]]\n\n[[Statistical Assumptions]]\n\n[[Type I Error (False Positive)]]\n\n[[Distributions]]\n\n[[Statistical Tests]]\n\n[[Monte Carlo Simulation]]\n\n[[Logistic Regression]]: model how change and covariance influence the odds of an event\n\n[[Proportional Hazard Model]]: time to an event\n\n[[Hypothesis testing]]\n[[p values]]\n[[Confidence Interval]]\n\n[[Central Limit Theorem]]\n\n[[Correlation vs Causation]]\n\n[[Markov chain]]\n\n[[parametric vs non-parametric tests]]\n\n[[Multicollinearity]]\n\n[[univariate vs multivariate]]\n\n[[R]]\n[[tidyverse]]: visualisation in R\n\n[[Over parameterised models]]\n\n[[Casual Inference]]\n\n[[Bootstrap Sampling]]\n\n[[Adaptive decision analysis]]: interrupting the experiment in the middle\n\nEstimation Problems: using data to estimate model parameters\n- [[Maximum Likelihood Estimation]]\n- [[Expectation Maximisation Algorithm]]\n\n[[Likelihood ratio]]: [[Type I Error (False Positive)]] UMP test. used to maximise power. [[T-test]] is a consequence of this.\n\n[[Estimator]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "portal",
      "statistics"
    ],
    "normalized_filename": "statistics",
    "outlinks": [
      "statistical_assumptions",
      "hypothesis_testing",
      "proportional_hazard_model",
      "p_values",
      "correlation_vs_causation",
      "likelihood_ratio",
      "statistical_theorems",
      "t-test",
      "r",
      "estimator",
      "multicollinearity",
      "confidence_interval",
      "adaptive_decision_analysis",
      "maximum_likelihood_estimation",
      "central_limit_theorem",
      "logistic_regression",
      "casual_inference",
      "expectation_maximisation_algorithm",
      "parametric_vs_non-parametric_tests",
      "type_i_error_(false_positive)",
      "bootstrap_sampling",
      "univariate_vs_multivariate",
      "markov_chain",
      "monte_carlo_simulation",
      "over_parameterised_models",
      "statistical_tests",
      "distributions",
      "tidyverse"
    ],
    "inlinks": [
      "covariance_structures",
      "data_analysis",
      "data_analyst",
      "data_science",
      "ds_&_ml_portal",
      "hypothesis_testing",
      "r",
      "tf-idf"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Stochastic Gradient Descent",
    "sha": "7de342de61a7de9c40b07a589cc081396646c7b1",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Stochastic%20Gradient%20Descent.md",
    "text": "Stochastic Gradient Descent (SGD) is an optimization algorithm that updates [[Model Parameters]] using the gradient computed from a single randomly selected training example at each iteration, rather than the entire dataset, as in [[Gradient Descent]] .\n\n#### Why do we use SGD\n* It allows efficient optimization ([[Model Optimisation|Optimisation]]) when working with large datasets, as computing the gradient on the entire dataset is expensive.\n* Introduces randomness, which can help escape local minima.\n* Can run on a deployed system.\n\n#### Key Characteristics\n* Update Rule:\n  Parameters are updated for each sample using its gradient contribution.\n\n* Objective:\n  Minimize the [[Loss function]] efficiently without processing the full dataset at every step.\n#### Pros:\n* Fast parameter updates.\n* Handles large-scale and streaming data well.\n#### Cons:\n* Noisy updates can cause high [[Variance]] in the cost function.\n* Requires techniques like [[Learning Rate]] scheduling or [[Momentum]] for stable convergence.",
    "aliases": [
      "SGD"
    ],
    "date modified": "27-09-2025",
    "tags": [
      "math",
      "optimisation",
      "statistics"
    ],
    "normalized_filename": "stochastic_gradient_descent",
    "outlinks": [
      "gradient_descent",
      "momentum",
      "model_optimisation",
      "model_parameters",
      "loss_function",
      "variance",
      "learning_rate"
    ],
    "inlinks": [
      "batch_gradient_descent",
      "deep_learning",
      "fitting_weights_and_biases_of_a_neural_network",
      "gradient_descent",
      "mini-batch_gradient_descent",
      "optimisation_techniques",
      "pytorch",
      "use_cases_for_a_simple_neural_network_like"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Symbolic computation",
    "sha": "41c56350f6738cd47bc88e8ca561d208d0a09e02",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Symbolic%20computation.md",
    "text": "[[Mathematical Reasoning in Transformers]]\n### Summary of Wolfram Alpha’s Approach:\n1. **Uses symbolic computation** with precise algorithms.\n2. **Leverages predefined mathematical rules** for various domains.\n3. **Provides step-by-step solutions** to explain problem-solving processes.\n4. **Handles natural language inputs** and translates them into mathematical expressions.\n5. **Produces both exact and numerical solutions**, depending on the problem type.\n6. **Visualizes results** with graphs and interactive displays.\n7. **Accesses a curated knowledge base** for real-world [[Data Integration]].\n\nExample: Wolfram alpha\n\nWolfram Alpha is designed specifically for symbolic computation and uses rule-based algorithms to perform exact and precise mathematical operations. Here's an overview of how Wolfram Alpha processes math problems:\n\n### 1. **Symbolic Computation Engine**\n   - Wolfram Alpha is powered by **[[Mathematica]]**, a robust computational engine that specializes in **symbolic computation**. Unlike neural network models, which rely on statistical pattern recognition, symbolic computation manipulates symbols and formulas directly according to established mathematical rules. This allows Wolfram Alpha to handle a wide variety of mathematical problems with precision, from basic arithmetic to complex calculus and algebraic manipulations.\n### 2. **Predefined Mathematical Rules and Algorithms**\n   - Wolfram Alpha uses **predefined algorithms and mathematical rules** that are coded into the system.\n   - Each mathematical operation is treated according to formal rules, so Wolfram Alpha can handle exact symbolic results (like expressing results in terms of radicals or π) or provide numerical approximations when needed.\n\n### 3. **Step-by-Step Solutions**\n\n### 5. **[[NLP|Natural Language Processing]] (NLP)**\n   - Wolfram Alpha uses **NLP** to interpret queries that are entered in natural language. For example, a user might type “solve x^2 + 2x + 1 = 0” or simply “solve quadratic equation,” and Wolfram Alpha translates this into formal mathematical expressions to process through its symbolic engine.\n   - This NLP capability allows users to input problems in a variety of ways, making it more accessible to non-expert users.\n\n### 6. **Exact vs. Numerical Solutions**\n   - Wolfram Alpha can handle both ==**exact symbolic solutions**== (such as expressing a result in terms of fractions, square roots, or constants like π) and ==**numerical approximations**==. For example, for equations that have no simple symbolic solutions, it can provide highly accurate numerical answers using **numerical methods** such as Newton’s method or Monte Carlo simulations.\n\n### 7. **Knowledge-Based System**\n   - Wolfram Alpha is connected to a vast database of curated knowledge, not only in [[Mathematics]] but across many disciplines. This allows it to draw on data, formulas, and [[Algorithms]] to solve not only pure math problems but also applied math problems, such as in physics, engineering, and economics.\n\n### 8. **Graphing and Visualization**\n\n### 9. **Error Handling and Interpretation**\n   - Wolfram Alpha handles user input carefully, identifying potential errors or ambiguities. For example, if an equation is underdetermined (too few equations for the number of variables), it may provide a parametric solution. If input is ambiguous, it often offers multiple possible interpretations or asks the user to clarify.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "symbolic_computation",
    "outlinks": [
      "data_integration",
      "algorithms",
      "mathematical_reasoning_in_transformers",
      "nlp",
      "mathematics",
      "mathematica"
    ],
    "inlinks": [
      "reasoning_tokens"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Sympy",
    "sha": "fa51c316d4a4bee41e2bb2b219798164f48e5753",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Sympy.md",
    "text": "symbolic differentiation",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "sympy",
    "outlinks": [],
    "inlinks": [
      "backpropagation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "T-test",
    "sha": "43b5c3bc9daacf0f3a528503045260274fc6d68e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/T-test.md",
    "text": "The T-test is a statistical method ==used to determine if there is a significant difference between the means of two groups, especially when the population [[Standard deviation]] is unknown.== It is particularly useful when dealing with small sample sizes.\n\n## Types of T-tests\n\n1. **One-Sample T-test**: This test compares the mean of a single sample to a known value (often the population mean). It helps determine if the sample mean significantly differs from the population mean.\n\n2. **Two-Sample T-test**: This test compares the means of two independent samples. It can be further categorized into:\n   - **Two-Sample T-test with Known [[Variance]]**: Used when the variances of the two groups are known and assumed to be equal.\n   - **Two-Sample T-test with Unknown Variance**: Used when the variances are unknown and may differ between the two groups. This version is more common in practice.\n\n## Characteristics of the T-distribution\n\nThe T-distribution resembles the normal [[Distributions|distribution]] but has fatter tails. This characteristic accounts for the increased variability expected with smaller sample sizes. As the sample size increases, the T-distribution approaches the normal distribution.\n\n## Assumptions\n\nFor the T-test to be valid, certain assumptions must be met:\n- The data should be approximately normally distributed, especially for small sample sizes.\n- The samples should be independent of each other.\n- For the two-sample T-test, the variances of the two groups should be equal (for the equal variance version).\n\n## Estimation of Standard Deviation\n\nSince the population standard deviation is unknown, the sample standard deviation is used to estimate it. This estimation is crucial for calculating the test statistic.\n\n## Test Statistic\n\nThe test statistic for the T-test is calculated using the formula:\n\n$$ T = \\frac{\\bar{X} - \\mu}{s / \\sqrt{n}} $$\n\nwhere:\n- $\\bar{X}$ = sample mean\n- $\\mu$ = population mean (or mean of the second sample in the two-sample test)\n- $s$ = sample standard deviation\n- $n$ = sample size\n\nThis formulation condenses all the data into a single variable, allowing for [[Hypothesis testing]].\n## Importance of the T-test\n\nThe T-test is a Uniformly Most Powerful Unbiased (UMPU) test, meaning it is optimal for detecting differences in means under the specified conditions.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "t-test",
    "outlinks": [
      "distributions",
      "hypothesis_testing",
      "variance",
      "standard_deviation"
    ],
    "inlinks": [
      "general_linear_regression",
      "hypothesis_testing",
      "parametric_vs_non-parametric_tests",
      "statistical_tests",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Variance",
    "sha": "3aee82df2fe42e375d2023f2c92a166ce9308d7e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Variance.md",
    "text": "Variance in a dataset is a statistical measure that represents the degree of spread or dispersion of the data points around the mean (average) of the dataset. \n\nIt quantifies how much the individual data points differ from the mean value. \n\nA higher variance indicates that the data points are more spread out from the mean, while a lower variance indicates that they are closer to the mean. \n\nVariance is calculated as the average of the squared differences between each data point and the mean.\n\nSee also:\n[[Boxplot]]\n[[Distributions]]\n\n**Variance**:\n- Measures how much a single variable deviates from its mean.\n- For variable $X$, variance is: \n$$\n\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu_X)^2\n$$\n- Variance determines the **spread** of data along a particular dimension.\n\nRelated:\n- [[Variance in ML]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "variance",
    "outlinks": [
      "boxplot",
      "distributions",
      "variance_in_ml"
    ],
    "inlinks": [
      "covariance_structures",
      "data_reduction",
      "feature_selection",
      "missing_data",
      "multicollinearity",
      "principal_component_analysis",
      "standard_deviation",
      "standardisation",
      "stochastic_gradient_descent",
      "t-test"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Violin plot",
    "sha": "4f2261be837215c789cdce9fb080649950b04adf",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Violin%20plot.md",
    "text": "An extension of a [[Boxplot]] showing the data distribution. Useful when comparing distributions, skewness.\n\n```python\ndata = [...]  # Your data\nsns.violinplot(data=data, color=\"purple\", fill=\"lightblue\", scale=\"area\")\nplt.show()\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "violin_plot",
    "outlinks": [
      "boxplot"
    ],
    "inlinks": [
      "distributions"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Z-Normalisation",
    "sha": "b1d749a2a9d79771316e92c7265fce3c73d45a15",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Z-Normalisation.md",
    "text": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_z_score.py\n\nZ-normalisation, also known as z-score normalization, is a technique used to standardize the range of independent variables or features of data. \n\nThis process is used in preparing data for [[Machine Learning Algorithms]], especially those that rely on distance calculations, such as k-nearest neighbors and [[Gradient Descent]] optimization.\n\n### Why Normalize?\n\n- Consistency Across Features: By normalizing, the peak-to-peak range of each column is reduced from a factor of thousands to a factor of 2-3. This ensures that each feature contributes equally to the distance calculations, preventing features with larger ranges from dominating the results.\n  \n- Centered Data: The range of the normalized data (x-axis) is centered around zero and roughly +/- 2. This centering is beneficial for algorithms that assume data is normally distributed around zero.\n\n- Improved Learning Rates: Normalization allows for a larger [[Learning Rate]] in [[Gradient Descent]], which can speed up convergence and improve the efficiency of the learning process.\n\n### Z-Score Normalization\n\nZ-score normalization transforms the data so that each feature has:\n- A mean of 0\n- A standard deviation of 1\n\nTo implement z-score normalization, adjust your input values using the formula:\n\n$$x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$\nWhere:\n- $x^{(i)}_j$ is the value of the feature $j$ for the $i$-th example.\n- $\\mu_j$ is the mean of all the values for feature $j$.\n- $\\sigma_j$ is the standard deviation of feature $j$.\n\nThe mean and standard deviation are calculated as follows:\n\n$$\\mu_j = \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j$$\n\n$$\\sigma^{2}_j = \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^{2}$$\n\nWhere $m$ is the number of examples.\n\n### Examples\n\n\n![[Pasted image 20241224091151.png]]\n\nSee that they are centred around 0.\n\n![[Pasted image 20241224091157.png]]\n\nBelow we see that its centered around 0 and been brought together.\n\n![[Pasted image 20241224091007.png]]\n\n\n==Rescales the feature values== to a range of [0, 1]. This is useful when you want to ensure that all features contribute equally to the distance calculations.\n```python\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_normalized = scaler.fit_transform(df)  # Rescales each feature to [0, 1]\n```",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "preprocessing",
      "transformation"
    ],
    "normalized_filename": "z-normalisation",
    "outlinks": [
      "gradient_descent",
      "machine_learning_algorithms",
      "pasted_image_20241224091157.png",
      "pasted_image_20241224091007.png",
      "learning_rate",
      "pasted_image_20241224091151.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "correlated_time_series",
      "normalisation",
      "z-score"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Z-Score",
    "sha": "c92353e6b3a851e26dd53dba198b4fb001b33ca7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Z-Score.md",
    "text": "Z-scores standardize a value relative to a distribution by measuring how many standard deviations it is from the mean. This is useful for [[uncategorised/Outliers|Outliers]] and [[Normalisation]].\n\nDefinition:  \nThe Z-score of a value $x$ is given by:\n    $$Z = \\frac{x - \\bar{x}}{s}$$\n    \nwhere $\\bar{x}$ is the sample mean and $s$ is the sample standard deviation.\n    \nInterpretation:\n- $Z = 0$: The value equals the mean.\n- $|Z| > 2$: Indicates a possible outlier (if normality is assumed).\n- Z-scores allow comparisons across different distributions.\n\t\nAssumptions:\n- Data is approximately normally distributed.\n- Useful primarily when comparing existing values to a distribution.\n\nUse Cases:\n- Standardizing data for machine learning algorithms.\n- Detecting anomalies.\n- Ranking or scoring values.\n\nRelated terms:\n- [[Z-Test]]\n- [[Z-Normalisation]]\n- [[Z-Score]]\n\n### **2. Modified Z-Score**\n\n- **Formula:**  \n    $M = \\frac{0.6745 \\cdot (X - \\text{median})}{\\text{MAD}}$\n    - $MAD$: Median Absolute Deviation\n- **Procedure:**\n    - Use this method for datasets with extreme outliers.\n    - Points with $M > 3.5$ are typically anomalies.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "z-score",
    "outlinks": [
      "z-test",
      "normalisation",
      "z-normalisation",
      "uncategorised/outliers",
      "z-score"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "z-score",
      "z-scores_vs_prediction_intervals"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Z-Scores vs Prediction Intervals",
    "sha": "0196a10ada224d762ca54f07f32eaceb2f2f0f67",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Z-Scores%20vs%20Prediction%20Intervals.md",
    "text": "[[Z-Score]] and [[Prediction Intervals]] serve different purposes. Z-scores assess existing values within a dataset, while prediction intervals estimate the likely range for future observations.\n\nUse Z-scores to evaluate existing values or standardize. Use prediction intervals to express uncertainty about where a **new** observation is likely to fall.\n\n**Comparison Table**:\n\n|Feature|Z-Score|Prediction Interval|\n|---|---|---|\n|**Purpose**|Assess deviation from the mean|Forecast future values|\n|**Formula**|$Z = \\frac{x - \\bar{x}}{s}$|$\\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n}}$|\n|**Distribution**|Standard Normal (Z)|Student’s t-distribution|\n|**Use case**|Outlier detection, normalization|Prediction of new measurements|\n|**Width of range**|Based on fixed $\\sigma$|Wider—accounts for both sampling error and variability|\n|**Needs population $\\sigma$?**|Yes (or large $n$ to approximate)|No (uses sample $s$ and $t$ for small $n$)|",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "z-scores_vs_prediction_intervals",
    "outlinks": [
      "prediction_intervals",
      "z-score"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "information theory",
    "sha": "cf394b9d8e877fea8b0f26a1b92d00e3a68dadba",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/information%20theory.md",
    "text": "Information theory is a mathematical framework for quantifying the transmission, processing, and storage of information. \n\nInformation theory has profound implications and applications across various domains, providing the theoretical foundation for understanding and optimizing how information is communicated and processed.\n\n1. **Entropy**: Often referred to as Shannon entropy, it measures the average amount of uncertainty or surprise associated with random variables. In essence, it quantifies the amount of information contained in a message or dataset.\n\n2. **Information**: In information theory, ==information is defined as the reduction in uncertainty==. When you receive a message, the amount of information it provides is related to how much it reduces your uncertainty about the subject.\n\n3. **Mutual Information**: This measures the amount of information that two random variables share. It quantifies the reduction in uncertainty about one variable given knowledge of the other.\n\n4. **Channel Capacity**: This is the maximum rate at which information can be reliably transmitted over a communication channel. It is determined by the channel's bandwidth and noise characteristics.\n\n5. **Data Compression**: Information theory provides the basis for data compression techniques, which aim to reduce the size of data without losing essential information. Lossless compression (e.g., ZIP) and lossy compression (e.g., JPEG) are two types of compression.\n\n6. **Error Detection and Correction**: Information theory also deals with methods for detecting and correcting errors in data transmission, ensuring that information can be accurately received even in the presence of noise.\n\n7. **Rate-Distortion Theory**: This aspect of information theory deals with the trade-offs between the fidelity of data representation and the amount of compression, which is crucial in applications like audio and video compression.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "math"
    ],
    "normalized_filename": "information_theory",
    "outlinks": [],
    "inlinks": [
      "language_model_output_optimisation",
      "mathematics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "Z-Test",
    "sha": "e6d1feedce3a336ea12d3e94deb6a906da0a3129",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/Z-Test.md",
    "text": "The Z-test is a statistical method used to determine if there is a ==significant difference between the means of two groups or to compare a sample mean to a known population mean when the population [[Standard deviation]] is known==. \n\nIt is typically applied when the sample size is large (usually n > 30).\n\n## Types of Z-tests\n\n1. **One-Sample Z-test**: This test compares the mean of a single sample to a known population mean. It assesses whether the sample mean significantly differs from the population mean.\n\n2. **Two-Sample Z-test**: This test compares the means of two independent samples. It is used when both sample sizes are large and the population variances are known or can be assumed to be equal.\n\n## Characteristics of the Z-distribution\n\nThe Z-distribution is a normal distribution with a mean of 0 and a standard deviation of 1. It is symmetric and bell-shaped, which allows for the application of the [[Central Limit Theorem]]. As sample sizes increase, the distribution of sample means approaches a normal distribution, making the Z-test applicable.\n\n## Assumptions\n\nFor the Z-test to be valid, certain assumptions must be met:\n- The data should be normally distributed, especially for smaller sample sizes. However, with large samples, the Central Limit Theorem allows for the Z-test to be used even if the data is not perfectly normal.\n- The samples should be independent of each other.\n- The population standard deviation should be known.\n\n## Test Statistic\n\nThe test statistic for the Z-test is calculated using the formula:\n\n$$ Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} $$\n\nwhere:\n- $\\bar{X}$ = sample mean\n- $\\mu$ = population mean (or mean of the second sample in the two-sample test)\n- $\\sigma$ = population standard deviation\n- $n$ = sample size\n\nThis formula allows for the comparison of the sample mean to the population mean, standardizing the difference in terms of standard deviations.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "z-test",
    "outlinks": [
      "central_limit_theorem",
      "standard_deviation"
    ],
    "inlinks": [
      "statistical_tests",
      "z-score"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "mean vs median",
    "sha": "38df8ecb2759ef6ed8dbd0c3c097b4969e7e33fd",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/mean%20vs%20median.md",
    "text": "The mean represents the arithmetic average, sensitive to [[uncategorised/Outliers|outliers]], while the median is the middle value, robust to skewed data.  \n\nIn a box plot:  \n  - Median is typically shown as a line inside the box; it divides the data into two equal halves.  \n  - Mean (if shown) is often marked as a dot or a line; it reflects the center of mass of the data.  \n  \nKey difference: \n- Use the median to ==highlight the data's central tendency when skewed or with outliers.== \n- Use the mean if ==the data is symmetric and outliers are minimal.==",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "visualization"
    ],
    "normalized_filename": "mean_vs_median",
    "outlinks": [
      "uncategorised/outliers"
    ],
    "inlinks": []
  },
  {
    "category": "STATISTICS",
    "filename": "non-parametric",
    "sha": "924ddf30ed00933ff409f661bb75ce4df7cc0357",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/non-parametric.md",
    "text": "Not explained by parameters.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "non-parametric",
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "f-regression",
      "k-nearest_neighbours",
      "model-agnostic_feature_importance",
      "parametric_vs_non-parametric_models",
      "parametric_vs_non-parametric_tests",
      "statistical_assumptions"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "p values",
    "sha": "71ac650e3c237cbebcc8ccdd96e44bc00bc2d797",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/p%20values.md",
    "text": "A p-value is a measure of the evidence against a null hypothesis.\np-values indicate whether an effect exists\nUsed in [[Feature Selection]]",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "p_values",
    "outlinks": [
      "feature_selection"
    ],
    "inlinks": [
      "anova",
      "hypothesis_testing",
      "model_interpretability",
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "parametric vs non-parametric models",
    "sha": "9cb93566103731ebd38c975d92fa2cd4fb2c20b3",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/parametric%20vs%20non-parametric%20models.md",
    "text": "### Parametric Models [[parametric]]\n\nDefinition: Models that summarize data with a ==set of parameters of fixed size, regardless of the number of data points.==\n\nCharacteristics:\n  - Assumes a specific form for the function mapping inputs to outputs (e.g., linear regression assumes a linear relationship).\n  - Requires estimation of a finite number of parameters.\n  - Generally faster to train and predict due to their simplicity.\n  - Risk of underfitting if the model assumptions do not align well with the data.\n\n  Examples: \n  - Linear [[Regression]], [[Logistic Regression]], neural networks (with a fixed architecture).\n\n### [[non-parametric]] Models [[non-parametric]] Models\n\nDefinition: Models that do ==not assume a fixed form for the function== mapping inputs to outputs and can grow in complexity with more data.\n\nCharacteristics:\n  - Do not make strong assumptions about the underlying data distribution.\n  - Can adapt to the data's complexity, potentially capturing more intricate patterns.\n  - Generally require more data to make accurate predictions.\n  - Risk of overfitting, especially with small datasets, as they can model noise in the data.\n\nExamples: \n- K-nearest neighbors, decision trees, [[Support Vector Machines]] (with certain kernels).\n\n### Key Differences\n\n- Flexibility: Non-parametric models are more flexible and can model complex relationships, while parametric models are simpler and rely on assumptions about the data.\n- Data Requirements: Non-parametric models typically require more data to achieve good performance compared to parametric models.\n- Computation: Parametric models are usually computationally less intensive than non-parametric models.",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "parametric_vs_non-parametric_models",
    "outlinks": [
      "regression",
      "logistic_regression",
      "non-parametric",
      "parametric",
      "support_vector_machines"
    ],
    "inlinks": [
      "maximum_likelihood_estimation"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "parametric vs non-parametric tests",
    "sha": "429181c3f9ad40f8b2be2f2b6aab1ed981233cf8",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/parametric%20vs%20non-parametric%20tests.md",
    "text": "[[Parametric tests ]] are statistical tests that make ==assumptions about the distribution== of the data. For example, a [[T-test]] assumes that the data is normally distributed. [[non-parametric]] tests do not make assumptions about the distribution of the data. Parametric tests are generally more powerful than non-parametric tests, but they are only valid if the data meets the [[Statistical Assumptions]] of the test.\n\n[[Non-parametric tests ]] are less powerful than parametric tests, but they can be used on any type of data, regardless of the distribution.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "statistics",
      "test"
    ],
    "normalized_filename": "parametric_vs_non-parametric_tests",
    "outlinks": [
      "parametric_tests",
      "t-test",
      "statistical_assumptions",
      "non-parametric",
      "non-parametric_tests"
    ],
    "inlinks": [
      "statistics"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "statsmodels",
    "sha": "6fee179068dfe3fbb3b8be85ff31d5a672d801f0",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/statsmodels.md",
    "text": "from statsmodels.tsa.seasonal import seasonal_decompose",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [],
    "normalized_filename": "statsmodels",
    "outlinks": [],
    "inlinks": [
      "time_series_python_packages",
      "varmax"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "parsimonious",
    "sha": "e36d64e2b71486d7ff14acddfc488245f0923151",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/parsimonious.md",
    "text": "**Parsimonious** refers to a principle in [[Model Selection]] and statistical modeling that emphasizes ==simplicity==. In the context of regression and other statistical models, a parsimonious model is one that explains the data with the fewest possible parameters or predictors while still providing a good fit.\n\nA parsimonious model is one that achieves a good balance between simplicity and explanatory power.\n### Key Points about Parsimonious Models:\n\n1. **Simplicity**: A parsimonious model avoids unnecessary complexity. It uses only the essential variables that contribute meaningfully to the prediction or explanation of the outcome.\n\n2. **Avoiding [[Overfitting]]**: By keeping the model simple, a parsimonious approach helps prevent overfitting, where a model learns the noise in the training data rather than the underlying pattern. Overfitting can lead to poor generalization to new, unseen data.\n\n3. **Interpretability**: Simpler models are often easier to interpret and understand. This is particularly important in fields where explaining the model's decisions is crucial, such as healthcare or finance.\n\n4. **Balance**: The goal is to strike a balance between model accuracy and complexity. A parsimonious model should provide a good fit to the data without being overly complicated.",
    "aliases": [
      null
    ],
    "date modified": "27-09-2025",
    "tags": [
      "selection",
      "statistics"
    ],
    "normalized_filename": "parsimonious",
    "outlinks": [
      "model_selection",
      "overfitting"
    ],
    "inlinks": [
      "adjusted_r_squared"
    ]
  },
  {
    "category": "STATISTICS",
    "filename": "univariate vs multivariate",
    "sha": "1e3864910840bec1d3469b5302a2f5b435b03e2e",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/statistics/univariate%20vs%20multivariate.md",
    "text": "Single feature versus multiple features\n\n[[Univariate Analysis]] explores individual variables to understand their distribution, central tendency, and spread.\n\n[[Multivariate Analysis]] examines interactions between variables (e.g. correlations, group comparisons).",
    "aliases": [],
    "date modified": "27-09-2025",
    "tags": [
      "statistics"
    ],
    "normalized_filename": "univariate_vs_multivariate",
    "outlinks": [
      "multivariate_analysis",
      "univariate_analysis"
    ],
    "inlinks": [
      "eda",
      "statistics"
    ]
  },
  {
    "category": null,
    "filename": "Granger Causality Test",
    "sha": "fa46458bc5bd795d1faff63a451f1bfb8358e13a",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Granger%20Causality%20Test.md",
    "text": "The <a href='https://en.wikipedia.org/wiki/Granger_causality'>Granger causality test</a> is a a hypothesis test to determine if one time series is useful in forecasting another. \n\nWhile it is fairly easy to measure correlations between series - when one goes up the other goes up, and vice versa - it's another thing to observe changes in one series correlated to changes in another after a consistent amount of time. \n\nThis <em>may</em> indicate the presence of causality, that changes in the first series influenced the behavior of the second. However, it may also be that both series are affected by some third factor, just at different rates. Still, it can be useful if changes in one series can predict upcoming changes in another, whether there is causality or not. In this case we say that one series \"Granger-causes\" another.\n\nIn the case of two series, $y$ and $x$, the null hypothesis is that lagged values of $x$ do <em>not</em> explain variations in $y$.<br>\n\nIn other words, it assumes that $x_t$ doesn’t Granger-cause $y_t$.\n\n  \n\nThe stattools <tt><strong>grangercausalitytests</strong></tt> function offers four tests for granger non-causality of 2 timeseries",
    "tags": null,
    "normalized_filename": "granger_causality_test",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "uncategorised",
    "filename": "Mean reverting",
    "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Mean%20reverting.md",
    "text": "",
    "normalized_filename": "mean_reverting",
    "outlinks": [],
    "inlinks": [
      "adf_test",
      "autocorrelation"
    ]
  },
  {
    "category": "uncategorised",
    "filename": "Overview-Base",
    "sha": "302df2bd3a68b2be66e41406aadea2fae9806120",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Overview-Base.base",
    "text": "views:\n  - type: cards\n    name: Table\n    order: []\n  - type: table\n    name: View\n    order:\n      - file.name\n      - category\n      - file.tags\n      - file.folder\n    sort: []",
    "normalized_filename": "overview-base",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": null,
    "filename": "PGN",
    "sha": "69080771acd3e3a68b336a767b3b40c837fccdeb",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/PGN.md",
    "text": "A PGN file is a plain-text format used to record chess games. PGN stands for *Portable Game Notation*. It captures both the **metadata** of a game and the **move sequence**, making it a standard for storing, sharing, and analysing chess data.\n\n**Structure**\nA PGN file contains two main parts:\n\n1. **Tag Pairs** (metadata)\n   Written inside square brackets.\n   Examples:\n\n   * `[Event \"World Championship\"]`\n   * `[White \"Carlsen\"]`\n   * `[Black \"Nepo\"]`\n   * `[Result \"1-0\"]`\n\n2. **Movetext** (game record)\n   A sequence of moves in [[algebraic chess notation]].\n   Example:\n   `1. e4 e5 2. Nf3 Nc6 3. Bb5 a6`\n\n**Key Features**\n\n* Human-readable and easy to parse.\n* Supports annotations, comments, and variations.\n* Widely used in chess engines, databases, and analytical workflows.\n\n**Example**\n\n```\n[Event \"Casual Game\"]\n[White \"Player1\"]\n[Black \"Player2\"]\n[Result \"1-0\"]\n\n1. d4 d5 2. c4 e6 3. Nc3 Nf6 4. Bg5 Be7 1-0\n```",
    "aliases": [],
    "date modified": "14-11-2025",
    "tags": [
      "analysis",
      "data"
    ],
    "normalized_filename": "pgn",
    "outlinks": [
      "algebraic_chess_notation"
    ],
    "inlinks": []
  },
  {
    "category": null,
    "filename": "Time Series Shapelet",
    "sha": "fd66480567fe3af796c6e2bc928ccd92d7cf2abf",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Time%20Series%20Shapelet.md",
    "text": "Time series shapelets are ==**discriminative, short subsequences or patterns within a time series dataset that are highly predictive of a target variable, typically a class label**==. They are local features used in time series data mining, primarily for classification and clustering, offering advantages in terms of interpretability, accuracy, and efficiency compared to methods that consider only global features. \n\nKey Concepts\n\n- **Discriminative Patterns**: Shapelets represent the most characteristic local patterns of a specific class. For example, a particular spike pattern in an ECG time series might be a shapelet indicative of a specific heart condition.\n- **Distance Measure**: The \"distance\" between a shapelet and a longer time series is defined as the minimum (usually z-normalized Euclidean) distance between the shapelet and all possible subsequences of the same length within the time series. A small distance means the shapelet is present in the time series.\n- **Interpretability**: A major advantage of shapelets is their interpretability. Domain experts can visually inspect the discovered shapelets to understand which specific patterns in the data drive the classification decision.\n- **Feature Transformation**: Shapelets are often used to transform the original time series data into a new, typically lower-dimensional, feature space. Each time series is represented by a feature vector containing the distances to the set of discovered shapelets. This transformed data can then be used with any standard, off-the-shelf classifier (e.g., a support vector machine or random forest). \n\nUsage in Data Mining\n\nThe general process of using time series shapelets involves:\n\n1. **Shapelet Candidate Extraction**: Potential shapelets (all possible subsequences of varying lengths) are identified from the training data. This is often the most computationally intensive step, leading to the development of numerous optimization algorithms to speed up the process.\n2. **Candidate Evaluation**: Each candidate shapelet is evaluated for its \"quality\" or ability to discriminate between different classes, often using metrics like information gain or F-statistic. The best candidates are selected.\n3. **Data Transformation**: The time series dataset is transformed into a tabular format using the selected shapelets' distances as features.\n4. **Classification/Clustering**: A standard machine learning model is trained on the new feature space to perform the final task. \n\nApplications\n\nTime series shapelets have been successfully applied in a wide range of fields where identifying specific temporal patterns is crucial, including: \n\n- **Human Activity Recognition**: Classifying activities like walking, running, or standing based on accelerometer data from wearable devices.\n- **Healthcare**: Analyzing ECG or other sensor data to detect medical conditions or anomalies.\n- **Structural Health Monitoring**: Detecting earthquake events or structural anomalies from vibration data in buildings and bridges.\n- **Environmental Monitoring**: Identifying patterns in air pollution exposure time series associated with specific health outcomes.",
    "aliases": [],
    "date modified": "5-11-2025",
    "tags": [
      "time_series"
    ],
    "normalized_filename": "time_series_shapelet",
    "outlinks": [],
    "inlinks": []
  },
  {
    "category": "uncategorised",
    "filename": "Time sampling",
    "sha": "179225e090e81f3691aa56d25ad9dc8d315bcfff",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Time%20sampling.md",
    "text": "upsampling \n\ndown samplings",
    "normalized_filename": "time_sampling",
    "outlinks": [],
    "inlinks": [
      "resampling"
    ]
  },
  {
    "category": "DS",
    "filename": "Varmax",
    "sha": "9a14e58d6f75f6da79349589f96b3b79cf830bf7",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/Varmax.md",
    "text": "A **VARMAX** model (Vector Autoregressive Moving Average with [[Exogenous Regressors]]) is a multivariate time-series model extending two classical ideas:\n\n1. **VAR($p$)**: Models interactions among multiple endogenous series using autoregressive lags.\n2. **VMA($q$)**: Captures correlations in multivariate residuals using moving-average terms.\n3. **X**: Allows the inclusion of exogenous predictors.\n\nFormally, a $k$-dimensional VARMAX($p$, $q$) for endogenous vector $y_t$ and exogenous predictors $x_t$ can be written as:\n\n$y_t = c + \\sum_{i=1}^{p} A_i y_{t-i} + \\sum_{j=1}^{q} M_j \\varepsilon_{t-j} + B x_t + \\varepsilon_t$\n\nWhere:\n\n* $y_t$: vector of endogenous variables\n* $A_i$: autoregressive coefficient matrices\n* $M_j$: moving-average coefficient matrices\n* $B$: matrix of coefficients for exogenous variables\n* $\\varepsilon_t$: error terms (possibly correlated across series)\n\n[[Statsmodels]] implements this within the statespace framework, giving access to [[Kalman filtering]], smoothing, and forecast intervals.\n\n[[Multivariate Analysis]]\n\n## How VARMAX is used\n\n### 1. **Multivariate forecasting**\n\nUseful when several time-dependent variables influence each other.\nExamples:\n\n* Forecasting $[ \\text{demand}, \\text{price} ]$ together\n* Forecasting multiple KPIs: $[\\text{revenue}, \\text{users},\\text{conversion}]$\n\nBecause the model estimates cross-lag impacts, you obtain richer dynamics than running separate univariate models.\n\n### 2. **Understanding cross-relationships**\n\nYou can quantify how a change in one variable affects others over future periods.\nCommon analyses include:\n\n* Impulse-response functions\n* Forecast error variance decomposition\n\n### 3. **Including external drivers**\n\nExogenous regressors allow structured drivers to influence all series.\n\nExamples:\n\n* $x_t$: temperature affecting energy demand and cost\n* $x_t$: marketing spend affecting multiple product KPIs\n\n### 4. **Scenario analysis**\n\nOnce fitted, you can simulate future paths under different exogenous scenarios.\n\n## Example usage in Statsmodels\n\n```python\nimport pandas as pd\nfrom statsmodels.tsa.statespace.varmax import VARMAX\n\n# Assume df contains:\n#   'revenue', 'customers', plus exogenous column 'marketing_spend'\nendog = df[['revenue', 'customers']]\nexog = df[['marketing_spend']]\n\nmodel = VARMAX(endog, exog=exog, order=(2, 1))\nfit = model.fit(disp=False)\n\n# Forecast 12 periods ahead\nforecast = fit.get_forecast(steps=12, exog=future_exog)\nforecast_df = forecast.predicted_mean\n```\n\n## When to consider VARMAX\n\nChoose VARMAX when:\n\n* Your variables interact across time.\n* Residuals show multivariate [[autocorrelation]].\n* You need forecasts that account for cross-effects.\n* You need to incorporate exogenous drivers.\n\nAvoid it when:\n\n* You want a purely univariate model (consider ARIMA/ETS).\n* You have very short [[time series]] (many parameters).\n* You need nonlinear or regime-switching dynamics (consider deep learning or structural models).",
    "aliases": [],
    "date modified": "9-11-2025",
    "tags": [
      "analysis",
      "statistics",
      "time_series"
    ],
    "normalized_filename": "varmax",
    "outlinks": [
      "kalman_filtering",
      "'revenue',_'customers'",
      "'marketing_spend'",
      "time_series",
      "exogenous_regressors",
      "multivariate_analysis",
      "statsmodels",
      "autocorrelation"
    ],
    "inlinks": []
  },
  {
    "category": null,
    "filename": "algebraic chess notation",
    "sha": "35e85072c23b00181332149579abea052dcf3981",
    "url": "https://github.com/rhyslwells/Data-Archive/blob/main/content/categories/uncategorised/algebraic%20chess%20notation.md",
    "text": "Algebraic notation is compact, but it follows strict rules that make it possible to know **which piece moved**, even when it doesn’t explicitly say so. Below is a clear explanation in simple steps.\n## 1. Pawns have no letter\n\nA move like\n**1. d4**\nmeans: *the pawn on the d-file moves to square d4*.\nIf a pawn were to capture, it would show the file it came from, for example:\n**exd5** = pawn from the e-file captures something on d5.\n\nBecause pawns are the only pieces with **no letter**, the meaning is unambiguous.\n## 2. Pieces use letters\n\nK = King\nQ = Queen\nR = Rook\nB = Bishop\nN = Knight\n\nSo\n**Nc3**\nmeans: a knight moves to $c3$.\n## 3. If two pieces can move to the same square, notation adds a qualifier\n\nExample: if two knights could both move to $d2$, it might show:\n\n* **Nbd2** (knight from the b-file goes to d2)\n* **N1d2** (knight from rank 1 goes to d2)\n\nThis disambiguation is rarely needed in the opening (because piece positions are still simple), which makes the early moves readable.\n## 4. Geometry determines what pieces can reach the square\n\nYou infer “which knight moved” by knowing piece movement rules and the board layout.\n\nExample move:\n**Nc3**\n\nIn the starting position:\n\n* Knight on $g1$ cannot reach $c3$\n* Knight on $b1$ *can* reach $c3$\n  Therefore the knight on $b1$ moved.\n\nThe notation does not state the starting square because only one option exists.\n## 5. Captures use “x”\n\nExample:\n**Bxf7+**\nmeans: bishop captures something on $f7$ and gives check.\n## 6. Checks and checkmates\n\n`# means checkmate\n\nExample:\n**Qh5#** — queen on h5 delivers mate.\n\n## 7. Game endings\n\n1-0 = White wins\n0-1 = Black wins\n1/2-1/2 = Draw\n\n## Example\n\n1. d4 d5 2. c4 e6 3. Nc3 Nf6 4. Bg5 Be7 1-0\n\n**1. d4 d5**\nWhite plays $d4$, taking space in the centre.\nBlack responds with $d5$, contesting the centre symmetrically.\n\n**2. c4 e6**\nWhite plays $c4$, entering the *Queen’s Gambit* structure by attacking Black’s $d5$ pawn.\nBlack replies with $e6$, supporting the $d5$ pawn and preparing to develop the dark-squared bishop (a standard Queen’s Gambit Declined setup).\n\n**3. Nc3 Nf6**\nWhite develops the knight to $c3$, increasing pressure on $d5$ and preparing kingside development.\nBlack develops the knight to $f6$, targeting the $d5$ and $e4$ squares.\n\n**4. Bg5 Be7**\nWhite plays $Bg5$, pinning the knight on $f6$ to the queen.\nBlack responds with $Be7$ to break the pin and prepare to castle.\n\n**1-0**\nThe result indicates **White won the game**.\nThe notation does **not** show *how* the win occurred; this is simply the final recorded result.\n\nIf you'd like, I can annotate this in the style of an Obsidian note with arrows or highlight key strategic themes (e.g., centre control, development, pins).",
    "aliases": [],
    "date modified": "15-11-2025",
    "tags": [
      "analysis",
      "learning"
    ],
    "normalized_filename": "algebraic_chess_notation",
    "outlinks": [],
    "inlinks": [
      "pgn"
    ]
  }
]