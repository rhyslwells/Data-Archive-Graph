{
  "1-on-1_template": {
    "title": "1-on-1 Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Decisions [Your name] add decisions that need to be made [Other person's name] add decisions that need to be made Action items [Your name] add...",
    "TFIDF_Score": {
      "add": 0.4711112048057346,
      "name": 0.44147266637033267,
      "person": 0.3754032147055681,
      "discus": 0.28155241102917605,
      "action": 0.25508518127200086,
      "topic": 0.20691725714571982,
      "directional": 0.20349409769691357,
      "needed": 0.1926581374212508,
      "discussion": 0.17317115423681162,
      "update": 0.1647981937876021
    }
  },
  "ab_testing": {
    "title": "AB testing",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "A/B testing is a method of performance testing two versions of a product like an app.",
    "TFIDF_Score": {
      "testing": 0.6119533194290909,
      "app": 0.4389469760532023,
      "version": 0.35205070748227335,
      "product": 0.32999098715715436,
      "two": 0.2789740210887594,
      "method": 0.22774685306154613,
      "performance": 0.19693648833786076,
      "like": 0.17755512059365267
    }
  },
  "accessing_gen_ai_generated_content": {
    "title": "Accessing Gen AI generated content",
    "tags": [
      "GenAI",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "bertscore",
      "rag",
      "generative_ai",
      "interpretability",
      "knowledge_graph"
    ],
    "inlinks": [],
    "summary": "To assess whether the content generated by a [[Generative AI]] is truthful and faithful, several methods and frameworks can be employed. Truthfulness refers to whether...",
    "TFIDF_Score": {
      "faithfulness": 0.3776925025947736,
      "truthfulness": 0.3557767124110806,
      "content": 0.3543440798298396,
      "fact": 0.20961584758506357,
      "generated": 0.18363791042654418,
      "factually": 0.14231068496443222,
      "sme": 0.14231068496443222,
      "check": 0.1357008605355177,
      "whether": 0.1357008605355177,
      "knowledge": 0.13473271122205013
    }
  },
  "accuracy": {
    "title": "Accuracy",
    "tags": [
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "confusion_matrix",
      "classification",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "handling_different_distributions",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "evaluation_metrics",
      "precision",
      "model_observability",
      "class_separability",
      "confusion_matrix",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Definition Accuracy Score is the proportion of correct predictions out of all predictions made. In other words, it is the percentage of correct predictions. Accuracy...",
    "TFIDF_Score": {
      "accuracy": 0.3837289003410616,
      "negative": 0.32737557415297674,
      "positive": 0.32737557415297674,
      "pred": 0.2290842395703039,
      "predicted": 0.22541693073299723,
      "accuracy_score": 0.19904466427470519,
      "true": 0.1968845979154929,
      "class": 0.19159274104993396,
      "y_test": 0.17977473043835476,
      "prediction": 0.17085749718654775
    }
  },
  "acid_transaction": {
    "title": "ACID Transaction",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "transaction"
    ],
    "inlinks": [
      "transaction",
      "data_lakehouse",
      "schema_evolution"
    ],
    "summary": "An ACID [[Transaction]] ensures that either all changes are successfully committed or rolled back, preventing the database from ending up in an inconsistent state. This...",
    "TFIDF_Score": {
      "transaction": 0.7306006517626124,
      "rolled": 0.23687045097221152,
      "committed": 0.22651784190686688,
      "state": 0.18807174191565382,
      "property": 0.17044600552015435,
      "back": 0.16684437365092572,
      "acid": 0.1456584896402699,
      "ensures": 0.12563531611907935,
      "change": 0.1136830532545273,
      "constraint": 0.11240010027117113
    }
  },
  "activation_atlases": {
    "title": "Activation atlases",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "feature_extraction"
    ],
    "summary": "is a viewing method for high dimensional space that AI system use for predictions. Example AlexNet (cofounder of OpenAI)",
    "TFIDF_Score": {
      "alexnet": 0.4444505356708654,
      "cofounder": 0.4444505356708654,
      "openai": 0.4186610783216333,
      "viewing": 0.35627579859818725,
      "dimensional": 0.2701204256698071,
      "space": 0.22918765212225553,
      "prediction": 0.19889025877218391,
      "high": 0.19562776786616876,
      "method": 0.1774890673453037,
      "system": 0.17243477829070586
    }
  },
  "activation_function": {
    "title": "Activation Function",
    "tags": [
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "how_do_we_choose_the_right_activation_function",
      "interpretability",
      "binary_classification",
      "neural_network",
      "backpropagation",
      "data_transformation"
    ],
    "inlinks": [
      "neural_network",
      "forward_propagation",
      "typical_output_formats_in_neural_networks"
    ],
    "summary": "Activation functions play a role in [[Neural network]] by introducing non-linearity, allowing models to learn from complex patterns and relationships in the data. [[How do...",
    "TFIDF_Score": {
      "softmax": 0.3754360648539243,
      "function": 0.3578317340718655,
      "output": 0.26995070423420164,
      "activation": 0.26722958640102507,
      "probability": 0.24095670694314486,
      "mathbf": 0.1744387754687784,
      "tanh": 0.16681479180659697,
      "purpose": 0.16646565691043336,
      "linear": 0.16146003827806904,
      "linearity": 0.15606939035194284
    }
  },
  "active_learning": {
    "title": "Active Learning",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "supervised_learning"
    ],
    "inlinks": [],
    "summary": "Think captchas for training. To help the [[Supervised Learning]] models when they are less confident. Reducing labelling time or need for it.",
    "TFIDF_Score": {
      "captchas": 0.4438825710715691,
      "labelling": 0.399851541908445,
      "confident": 0.38567671657403063,
      "think": 0.32497943108884014,
      "supervised": 0.3117894835821969,
      "less": 0.2520150716764005,
      "reducing": 0.2406921473459767,
      "need": 0.18649962816568577,
      "training": 0.18379609899294716,
      "help": 0.17355180642560006
    }
  },
  "ada_boosting": {
    "title": "Ada boosting",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "boosting",
      "decision_tree",
      "random_forests"
    ],
    "inlinks": [
      "boosting"
    ],
    "summary": "Resources: LINK Overview: Ada Boosting short for ==Adaptive Boosting==, is a specific type of [[Boosting]] algorithm that focuses on improving the accuracy of predictions by...",
    "TFIDF_Score": {
      "stump": 0.6279495976911864,
      "adaboost": 0.5171349628045065,
      "tree": 0.17010635042274586,
      "final": 0.1598776278230128,
      "prediction": 0.1579316961828357,
      "forest": 0.1489177394308285,
      "learner": 0.1321936399546499,
      "random": 0.107485762689196,
      "accuracy": 0.10748449705459215,
      "boosting": 0.10631922381811301
    }
  },
  "adam_optimizer": {
    "title": "Adam Optimizer",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "deep_learning",
      "momentum",
      "learning_rate",
      "gradient_descent",
      "why_does_the_adam_optimizer_converge",
      "hyperparameter"
    ],
    "inlinks": [
      "orthogonalization",
      "learning_rate",
      "backpropagation",
      "adaptive_learning_rates",
      "optimisation_techniques"
    ],
    "summary": "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both [[Momentum]] and adaptive learning rates. It is widely used due...",
    "TFIDF_Score": {
      "moment": 0.3549288997419353,
      "adam": 0.3327240401988084,
      "rate": 0.29493378817873167,
      "learning": 0.22088965989646905,
      "second": 0.20838063075499227,
      "epsilon": 0.1892334996924959,
      "momentum": 0.1892334996924959,
      "first": 0.18544595004992454,
      "adaptive": 0.1663620200994042,
      "gradient": 0.16494013340088615
    }
  },
  "adaptive_learning_rates": {
    "title": "Adaptive Learning Rates",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "learning_rate",
      "momentum",
      "adam_optimizer"
    ],
    "inlinks": [
      "optimisation_techniques"
    ],
    "summary": "[[Adam Optimizer]] Adaptive [[learning rate]] adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients....",
    "TFIDF_Score": {
      "adaptive": 0.49981245559020204,
      "rate": 0.37975249266127503,
      "moment": 0.355445481354352,
      "adam": 0.3332083037268014,
      "learning": 0.24886254821271142,
      "momentum": 0.18950891977371637,
      "estimation": 0.17457325437883317,
      "optimizer": 0.1666041518634007,
      "second": 0.15651293891005846,
      "short": 0.15651293891005846
    }
  },
  "adding_a_database_to_postgresql": {
    "title": "Adding a database to PostgreSQL",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "postgresql"
    ],
    "summary": "How to Add a Database to PostgreSQL Using pgAdmin (GUI) Open pgAdmin and log in. In the Object Explorer, right-click Databases → Create → Database....",
    "TFIDF_Score": {
      "psycopg2": 0.4274804479700274,
      "database": 0.3763165487766933,
      "cursor": 0.2971403092933663,
      "conn": 0.28821733696888235,
      "mydatabase": 0.2564882687820164,
      "postgres": 0.22285523197002474,
      "create": 0.20180931878331876,
      "postgresql": 0.19072059255165338,
      "connect": 0.18778257382018673,
      "close": 0.16527818237664504
    }
  },
  "addressing_multicollinearity": {
    "title": "Addressing Multicollinearity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "addressing_multicollinearity.py",
      "principal_component_analysis",
      "dimensionality_reduction",
      "multicollinearity",
      "'var1',_'var2',_'var3'",
      "ml_tools"
    ],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "In [[ML_Tools]] see: [[Addressing_Multicollinearity.py]] Multicollinearity can impact the performance and [[interpretability]] of regression models by causing instability in coefficient estimates and complicating the analysis of...",
    "TFIDF_Score": {
      "variable": 0.3945170107874304,
      "vif": 0.3471845000420638,
      "principal": 0.30321242870300186,
      "correlated": 0.2698416323764524,
      "pca": 0.25888956395099155,
      "component": 0.2473859493882907,
      "multicollinearity": 0.15955965656447707,
      "stability": 0.15533373837059494,
      "data_cleaned": 0.1474284045117489,
      "variance_inflation_factor": 0.1474284045117489
    }
  },
  "addressing_multicollinearity.py": {
    "title": "Addressing_Multicollinearity.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "addressing_multicollinearity"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Regression/Addressing_Multicollinearity.py",
    "TFIDF_Score": {
      "addressing_multicollinearity": 0.5354801015766321,
      "build": 0.30341217133100346,
      "rhyslwells": 0.2852589456607943,
      "blob": 0.2815882975996676,
      "github": 0.2714764010745356,
      "exploration": 0.269390332143056,
      "ml_tools": 0.2596940803873211,
      "regression": 0.2561175822275916,
      "com": 0.2543866183999853,
      "main": 0.24246009874316413
    }
  },
  "adjusted_r_squared": {
    "title": "Adjusted R squared",
    "tags": [
      "statistics",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "r_squared",
      "regression_metrics",
      "parsimonious"
    ],
    "inlinks": [
      "r_squared",
      "regression_metrics",
      "linear_regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression"
    ],
    "summary": "Adjusted R-squared is a [[Regression Metrics]]or assessing the quality of a regression model, ==especially when multiple predictors== are involved. It helps ensure that the model...",
    "TFIDF_Score": {
      "adjusted": 0.506315786103658,
      "predictor": 0.44302631284070076,
      "model": 0.23233723768138823,
      "squared": 0.20719427461799697,
      "number": 0.1938420174303468,
      "variable": 0.15865659298547433,
      "fit": 0.15437747611859876,
      "may": 0.15361202623313733,
      "addition": 0.14597864046645603,
      "extra": 0.14257970067656736
    }
  },
  "agent-based_modelling": {
    "title": "Agent-Based Modelling",
    "tags": null,
    "aliases": [
      "ABM"
    ],
    "outlinks": [
      "agent-based_modelling",
      "policy",
      "prompt_engineering",
      "chain_of_thought",
      "llm",
      "emergent_behavior"
    ],
    "inlinks": [
      "agent-based_modelling",
      "energy_abm",
      "agentic_solutions",
      "what_can_abm_solve_within_the_energy_sector",
      "energy"
    ],
    "summary": "(ABM) is a computational approach that simulates the interactions of individual agents within a defined environment to observe complex phenomena and [[emergent behavior]] at a...",
    "TFIDF_Score": {
      "agent": 0.7965223120808257,
      "interaction": 0.22147507161442406,
      "behavior": 0.18704110394158105,
      "abm": 0.14068401564602293,
      "system": 0.1312827317373329,
      "energy": 0.1296664228968786,
      "environment": 0.10687845163438563,
      "dynamic": 0.0971927406340181,
      "based": 0.09246821183654395,
      "demand": 0.07871642669448449
    }
  },
  "agentic_solutions": {
    "title": "Agentic Solutions",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "graphrag",
      "agent-based_modelling",
      "small_language_models"
    ],
    "inlinks": [
      "scaling_agentic_systems",
      "langchain"
    ],
    "summary": "Agentic solutions leverage multiple autonomous agents (usually [[Small Language Models|SLM]]) to achieve goals collaboratively. These systems distribute tasks across agents that operate individually or collectively...",
    "TFIDF_Score": {
      "agent": 0.726055316734472,
      "agentic": 0.21624436321743237,
      "vertical": 0.1621832724130743,
      "task": 0.14577285069388782,
      "solution": 0.13160074266510438,
      "delegation": 0.12002842788420025,
      "silo": 0.10812218160871619,
      "workflow": 0.10704313298157592,
      "autonomous": 0.10428922642798986,
      "budget": 0.10428922642798986
    }
  },
  "aggregation": {
    "title": "Aggregation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pandas_pivot_table",
      "groupby",
      "de_tools"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ],
    "summary": "Aggregation Summarizing data for analysis ([[Pandas Pivot Table]] and [[Groupby]]). In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/group_by.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb",
    "TFIDF_Score": {
      "de_tools": 0.46444079673603667,
      "ipynb": 0.2856764992789343,
      "transformation": 0.24662007375801842,
      "rhyslwells": 0.24111607744164162,
      "blob": 0.23801344989698234,
      "github": 0.22946633555500187,
      "exploration": 0.22770307881693155,
      "group_by": 0.2263081729849401,
      "pivot_table": 0.2263081729849401,
      "com": 0.2150211396181229
    }
  },
  "ai_engineer": {
    "title": "AI Engineer",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "attention_mechanism",
      "neural_network",
      "prompting",
      "lstm"
    ],
    "inlinks": [],
    "summary": "They know what - [[LSTM]] means - [[Attention mechanism]] - [[Prompting]] optimisation - [[Neural network]]",
    "TFIDF_Score": {
      "prompting": 0.45063886757389243,
      "lstm": 0.3991913928690652,
      "know": 0.36430630568272876,
      "attention": 0.3311815306457472,
      "optimisation": 0.3311815306457472,
      "mechanism": 0.3254692500101835,
      "neural": 0.2649747453467967,
      "network": 0.24392693184623213,
      "mean": 0.22326475224573686
    }
  },
  "ai_governance": {
    "title": "AI governance",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_governance"
    ],
    "inlinks": [],
    "summary": "AI Governance [[Data Governance]] Used in regulated sectors. Constraints to using ai: - legal, - transparency, - security, - historical bias AI acts and standards:...",
    "TFIDF_Score": {
      "governance": 0.4693008524147256,
      "standard": 0.3615444766545105,
      "innovation": 0.30771425025497245,
      "act": 0.2985851441506437,
      "security": 0.2443206068611166,
      "beaurcracy": 0.20691288305975283,
      "nist": 0.20691288305975283,
      "owasp": 0.20691288305975283,
      "stifle": 0.20691288305975283,
      "legal": 0.1863881141636992
    }
  },
  "algorithms": {
    "title": "Algorithms",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "recursive_algorithm",
      "k-means",
      "memoization"
    ],
    "inlinks": [
      "computer_science",
      "checksum",
      "machine_learning_algorithms"
    ],
    "summary": "[[Recursive Algorithm]] Backtracking Backtracking is a method for solving problems incrementally, by trying partial solutions and then abandoning them if they are not valid. Example:...",
    "TFIDF_Score": {
      "subproblems": 0.3583940716568582,
      "problem": 0.3305528611589905,
      "bound": 0.28560848110123876,
      "algorithm": 0.2562537505612026,
      "solution": 0.2085767471492388,
      "backtracking": 0.19023554538162846,
      "piece": 0.1561299325456616,
      "node": 0.1497521185627763,
      "brute": 0.12682369692108564,
      "conquer": 0.12682369692108564
    }
  },
  "alternatives_to_batch_processing": {
    "title": "Alternatives to Batch Processing",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event-driven_architecture",
      "batch_processing",
      "apache_spark_streaming",
      "apache_kafka",
      "data_streaming",
      "apache_flink",
      "data_analysis"
    ],
    "inlinks": [
      "data_streaming"
    ],
    "summary": "If you’re working with a streaming dataset ([[Data Streaming]]), why might [[batch processing]] not be suitable, and what alternatives would you consider? Latency: Batch processing...",
    "TFIDF_Score": {
      "streaming": 0.44759801127117,
      "batch": 0.3406294376595085,
      "processing": 0.3380341638979324,
      "event": 0.29709239429411916,
      "apache": 0.19147341994434383,
      "stream": 0.19092519902747637,
      "data": 0.17028910504786385,
      "real": 0.1563358632538284,
      "batching": 0.1550877476295397,
      "cep": 0.1550877476295397
    }
  },
  "amazon_s3": {
    "title": "Amazon S3",
    "tags": [],
    "aliases": [
      "S3"
    ],
    "outlinks": [],
    "inlinks": [
      "data_storage",
      "data_engineering_tools",
      "distributed_computing"
    ],
    "summary": "Amazon S3 Amazon S3 buckets (onedrive essentially) Amazon Simple Storage Service (S3) is a versatile ==object storage solution== known for its scalability, data availability, security,...",
    "TFIDF_Score": {
      "bucket": 0.3605235817839861,
      "storage": 0.28528889512207367,
      "amazon": 0.27135044441766015,
      "requester": 0.20746667809733996,
      "aws": 0.19685831174717366,
      "service": 0.19512348550021677,
      "object": 0.19395910883521672,
      "event": 0.17663626796021925,
      "cost": 0.1758513180508127,
      "console": 0.16281026665059609
    }
  },
  "anomaly_detection_in_time_series": {
    "title": "Anomaly Detection in Time Series",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ts_anomaly_detection.py",
      "isolated_forest",
      "time_series",
      "lstm",
      "ml_tools"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "In [[Time Series]] In [[ML_Tools]] see: - [[TS_Anomaly_Detection.py]] To perform anomaly detection specifically for time series data, you can utilize various techniques that account for...",
    "TFIDF_Score": {
      "series": 0.522247783661738,
      "time": 0.3889906552192426,
      "anomaly": 0.3376971838381107,
      "residual": 0.23442946261613967,
      "average": 0.18845324813098452,
      "moving": 0.16779598597417092,
      "identify": 0.15425346684044489,
      "arima": 0.13965944835793137,
      "point": 0.13648241589867502,
      "chart": 0.12724307163086296
    }
  },
  "anomaly_detection_with_clustering": {
    "title": "Anomaly Detection with Clustering",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "dbscan",
      "isolated_forest"
    ],
    "inlinks": [
      "anomaly_detection",
      "isolated_forest"
    ],
    "summary": "[[Clustering]] - Description: Outliers often form small clusters or are isolated from main clusters. 8. [[DBSCAN]] (Density-Based Spatial Clustering of Applications with Noise) Purpose: Finds...",
    "TFIDF_Score": {
      "density": 0.5141100022230184,
      "lof": 0.42756476261895776,
      "anomaly": 0.3326072208817534,
      "neighbor": 0.2468146984544745,
      "point": 0.23524423253062693,
      "local": 0.20663847653600123,
      "isolated": 0.16162720641358888,
      "based": 0.15189082521956243,
      "clustering": 0.12535415825789853,
      "outlier": 0.12535415825789853
    }
  },
  "anomaly_detection_with_statistical_methods": {
    "title": "Anomaly Detection with Statistical Methods",
    "tags": [
      "anomaly_detection",
      "statistics",
      "ml"
    ],
    "aliases": [],
    "outlinks": [
      "z-normalisation",
      "interquartile_range_(iqr)_detection",
      "isolated_forest",
      "multivariate_data",
      "percentile_detection",
      "gaussian_model"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "Basic: - [[Z-Normalisation|Z-Score]] - [[Interquartile Range (IQR) Detection]] - [[Percentile Detection]] Advanced: - [[Gaussian Model]] - [[Isolated Forest]] Grubbs' Test Context: Grubbs' test is a...",
    "TFIDF_Score": {
      "outlier": 0.33502312840147963,
      "point": 0.31435837397061306,
      "bin": 0.3102725111818576,
      "test": 0.2046431057656408,
      "histogram": 0.1930055977087595,
      "normal": 0.1895473827566035,
      "data": 0.18820821862418616,
      "density": 0.1717521218291881,
      "detection": 0.1698483742442298,
      "deviation": 0.15247754443001224
    }
  },
  "anomaly_detection": {
    "title": "Anomaly Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "anomaly_detection_with_statistical_methods",
      "data_cleansing",
      "pycaret_anomaly.ipynb",
      "anomaly_detection_with_clustering",
      "pca-based_anomaly_detection",
      "standardised/outliers",
      "anomaly_detection_in_time_series",
      "standardisation",
      "data_integrity",
      "normalisation",
      "boxplot",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "outliers",
      "gaussian_mixture_models",
      "time_series",
      "clustering",
      "imbalanced_datasets"
    ],
    "summary": "Anomaly detection involves identifying [[standardised/Outliers|Outliers]]. Detecting these anomalies is crucial for maintaining [[data integrity]] and improving model performance. Methods for Detecting Anomalies In [[ML_Tools]] see:...",
    "TFIDF_Score": {
      "anomaly": 0.6717414487175617,
      "detection": 0.37435412321896727,
      "plot": 0.21545378339959703,
      "outlier": 0.2109735312012051,
      "method": 0.17960537259755585,
      "scatter": 0.13171005249910728,
      "detecting": 0.12636433362471633,
      "using": 0.12041139498938765,
      "data": 0.11852003374719129,
      "validation": 0.10342604935046011
    }
  },
  "apache_airflow": {
    "title": "Apache Airflow",
    "tags": [
      "data_orchestration",
      "software",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "directed_acyclic_graph_(dag)"
    ],
    "inlinks": [
      "directed_acyclic_graph_(dag)",
      "data_engineering",
      "etl",
      "data_management"
    ],
    "summary": "Schedular think CROM jobs with python. Apache Nifi may be better. Airflow is a data orchestrator and the first that made task scheduling popular with...",
    "TFIDF_Score": {
      "airflow": 0.6433386501483421,
      "extensible": 0.20655675063843384,
      "workflow": 0.20449533479492488,
      "dag": 0.1992342682791826,
      "schedule": 0.17994597847918606,
      "job": 0.1764885605402751,
      "dynamic": 0.14270147606192807,
      "python": 0.1384791603089887,
      "pipeline": 0.13445456300137096,
      "crom": 0.11465122918867068
    }
  },
  "apache_kafka": {
    "title": "Apache Kafka",
    "tags": [
      "software",
      "data_orchestration"
    ],
    "aliases": [
      "Kafka"
    ],
    "outlinks": [
      "publish_and_subscribe",
      "scalability",
      "data_integrity",
      "data_integration",
      "data_storage"
    ],
    "inlinks": [
      "data_streaming",
      "data_engineering_tools",
      "publish_and_subscribe",
      "alternatives_to_batch_processing"
    ],
    "summary": "Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and data streaming applications. It is designed to handle high-throughput,...",
    "TFIDF_Score": {
      "kafka": 0.598991628117628,
      "data": 0.251430493134911,
      "apache": 0.18847264239366235,
      "stream": 0.18793301321250752,
      "record": 0.14621817640262638,
      "topic": 0.14621817640262638,
      "tolerant": 0.1437991991793425,
      "log": 0.14250441749829368,
      "durability": 0.13751434225901688,
      "partition": 0.1326394285017885
    }
  },
  "apache_spark": {
    "title": "Apache Spark",
    "tags": [
      "software"
    ],
    "aliases": [
      "Spark"
    ],
    "outlinks": [
      "data_engineer"
    ],
    "inlinks": [
      "map_reduce",
      "ds_&_ml_portal",
      "batch_processing",
      "distributed_computing",
      "parquet",
      "pyspark",
      "databricks",
      "scala",
      "big_data",
      "hadoop"
    ],
    "summary": "Apache Spark is an open-source multi-language engine for executing Data Engineer and Machine Learning on single-node machines or clusters. It's optimized for large-scale data processing....",
    "TFIDF_Score": {
      "spark": 0.5064383279186433,
      "processing": 0.3182320421075815,
      "engineer": 0.2502334633337231,
      "machine": 0.2192366959669752,
      "data": 0.20039191092093647,
      "scale": 0.1882212223829331,
      "kubernetes": 0.15857186737492657,
      "large": 0.15474025595689345,
      "disk": 0.15381003486760675,
      "massive": 0.1462964206331574
    }
  },
  "api_driven_microservices": {
    "title": "API Driven Microservices",
    "tags": [
      "software",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "microservices",
      "software_architecture",
      "api"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "API-driven microservices refer to a [[software architecture]] approach where [[microservices]] communicate with each other and with external systems primarily through well-defined [[API]] (Application Programming Interfaces)....",
    "TFIDF_Score": {
      "microservices": 0.4714780452312801,
      "api": 0.3604986266139527,
      "service": 0.24313119002376293,
      "microservice": 0.21542605958295055,
      "scaled": 0.16905642161837398,
      "independently": 0.15131835364783003,
      "resilience": 0.1352839055043438,
      "driven": 0.13092790815761043,
      "gateway": 0.12937121618082914,
      "architecture": 0.11876099969688336
    }
  },
  "api": {
    "title": "API",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "rest_api",
      "wikipedia_api.py",
      "fastapi",
      "ml_tools"
    ],
    "inlinks": [
      "normalisation_of_data",
      "model_deployment",
      "data_ingestion",
      "api_driven_microservices",
      "data_contract"
    ],
    "summary": "An API (Application Programming Interfaces) allows one system (client) to ==request specific actions from another system== (server). Using a predefined set of rules and ==protocols==....",
    "TFIDF_Score": {
      "apis": 0.5708545618036271,
      "api": 0.33099405478598665,
      "apps": 0.21140557757572195,
      "weather": 0.21140557757572195,
      "request": 0.18524518260673878,
      "system": 0.1705311542449224,
      "server": 0.15463948248491968,
      "access": 0.13864410891909904,
      "client": 0.13532254097457083,
      "device": 0.13532254097457083
    }
  },
  "attention_is_all_you_need": {
    "title": "Attention Is All You Need",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "attention_mechanism": {
    "title": "Attention mechanism",
    "tags": [
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "multi-head_attention",
      "recurrent_neural_networks",
      "ml",
      "transformer",
      "bert",
      "standardised/attention_is_all_you_need",
      "lstm",
      "llm",
      "nlp"
    ],
    "inlinks": [
      "ai_engineer",
      "ds_&_ml_portal",
      "multi-head_attention",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding",
      "language_model_output_optimisation",
      "lstm",
      "feature_extraction",
      "llm"
    ],
    "summary": "[!intuition] Think of attention like human reading behavior: when reading a complex sentence, we don't process all the words equally at every moment. Instead, we...",
    "TFIDF_Score": {
      "attention": 0.6648570743093751,
      "word": 0.3156994317956848,
      "sequence": 0.20566143236557025,
      "vector": 0.17288210011117874,
      "mechanism": 0.14701263656351965,
      "transformer": 0.138167319489308,
      "token": 0.1296702114169295,
      "dot": 0.12256322985404428,
      "model": 0.12089462501972627,
      "input": 0.11727642602239521
    }
  },
  "auc": {
    "title": "AUC",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "inlinks": [
      "precision-recall_curve"
    ],
    "summary": "AUC (Area Under the Curve) is a metric for binary classification problems, representing the area under the [[ROC (Receiver Operating Characteristic)]] Key Concepts Represents the...",
    "TFIDF_Score": {
      "auc": 0.573800243253003,
      "roc_auc_score": 0.2850384878191459,
      "roc": 0.23290145499953335,
      "area": 0.1867950492414342,
      "guessing": 0.1819233745457628,
      "curve": 0.16150734634152983,
      "score": 0.16068280942505286,
      "equivalent": 0.15129840718165308,
      "classification": 0.14285298254609968,
      "discriminative": 0.14251924390957296
    }
  },
  "automated_feature_creation": {
    "title": "Automated Feature Creation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "automated_feature_creation",
      "feature_engineering"
    ],
    "inlinks": [
      "automated_feature_creation"
    ],
    "summary": "Question: Can autodetect meaningful features. LINK Rough notes [[Feature Engineering]] was an ad-hoc manual process that depended on domain knowledge, intuition, data exploration and creativity....",
    "TFIDF_Score": {
      "feature": 0.44203085858952884,
      "featuretools": 0.39055879121534853,
      "explorekit": 0.26037252747689904,
      "engineering": 0.23058390437951132,
      "pycaret": 0.22623015190921048,
      "automated": 0.17648599733240694,
      "library": 0.148747021700231,
      "framework": 0.1355500453924979,
      "autodetect": 0.13018626373844952,
      "depended": 0.13018626373844952
    }
  },
  "aws_lambda": {
    "title": "AWS Lambda",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "s3_bucket",
      "event_driven_events"
    ],
    "inlinks": [],
    "summary": "AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that allows you to run code without provisioning or managing servers. AWS...",
    "TFIDF_Score": {
      "aws": 0.5744316781037933,
      "lambda": 0.5230792375186526,
      "service": 0.19521238725642098,
      "event": 0.1840799443842286,
      "amazon": 0.18098271769478466,
      "server": 0.13522920836637323,
      "code": 0.1345751265139391,
      "scaling": 0.1085429395603656,
      "triggered": 0.10387340246415047,
      "bucket": 0.10019106744102381
    }
  },
  "azure": {
    "title": "Azure",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_engineering_tools"
    ],
    "summary": "Public cloud computing platform from Microsoft offering various services like infrastructure, data storage, and machine learning.",
    "TFIDF_Score": {
      "public": 0.39376373176905766,
      "offering": 0.3532355542681833,
      "microsoft": 0.3420657841348315,
      "computing": 0.3039173309994374,
      "infrastructure": 0.3039173309994374,
      "cloud": 0.2839591355724278,
      "platform": 0.2703458585668363,
      "service": 0.2636539585843462,
      "storage": 0.24092930928541897,
      "various": 0.20565009743152504
    }
  },
  "b-tree": {
    "title": "B-tree",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_index"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "backpropagation": {
    "title": "Backpropagation",
    "tags": [
      "deep_learning",
      "ml_optimisation",
      "statistics"
    ],
    "aliases": [
      "Backprop",
      "BP",
      "backward propagation"
    ],
    "outlinks": [
      "supervised_learning",
      "regularisation",
      "gradient_descent",
      "vanishing_and_exploding_gradients_problem",
      "sympy",
      "adam_optimizer"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "deep_learning",
      "recurrent_neural_networks",
      "activation_function",
      "named_entity_recognition",
      "forward_propagation"
    ],
    "summary": "[!Summary] Backpropagation is an essential algorithm in the training of neural networks and iteratively correcting its mistakes. It involves a process of calculating the gradient...",
    "TFIDF_Score": {
      "derivative": 0.38318916083455157,
      "gradient": 0.320194630661413,
      "loss": 0.25670105247449826,
      "backpropagation": 0.2554594405563677,
      "sympy": 0.21701875281438401,
      "network": 0.20413322345475168,
      "weight": 0.19636479202119375,
      "backpropgation": 0.162764064610788,
      "node": 0.16015844628600615,
      "layer": 0.15308044594100356
    }
  },
  "bag_of_words": {
    "title": "Bag of words",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "tf-idf",
      "bag_of_words.py",
      "ml_tools"
    ],
    "inlinks": [
      "tf-idf",
      "naive_bayes",
      "nlp",
      "word2vec"
    ],
    "summary": "In [[ML_Tools]] see: [[Bag_of_Words.py]] In the context of natural language processing (NLP), the Bag of Words (BoW) model is a simple and commonly used ==method...",
    "TFIDF_Score": {
      "bow": 0.4417377671633842,
      "word": 0.344698760311937,
      "vocabulary": 0.28162084744611887,
      "countvectorizer": 0.2716373263021825,
      "text": 0.24906188124542814,
      "document": 0.2357323748348464,
      "vector": 0.15972220006478532,
      "get_feature_names_out": 0.14724592238779471,
      "normalize_document": 0.14081042372305944,
      "tokenizer": 0.14081042372305944
    }
  },
  "bagging": {
    "title": "Bagging",
    "tags": [
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "overfitting",
      "model_ensemble",
      "random_forests"
    ],
    "inlinks": [
      "bias_and_variance",
      "random_forests",
      "imbalanced_datasets",
      "model_ensemble"
    ],
    "summary": "Overview: Bagging, short for Bootstrap Aggregating, is an [[Model Ensemble]] technique designed to improve the stability and accuracy of machine learning algorithms. It works by...",
    "TFIDF_Score": {
      "bagging": 0.6400042748848949,
      "model": 0.24252867349682225,
      "bootstrap": 0.2347168118554311,
      "subset": 0.17477003925411153,
      "prediction": 0.17448112539669236,
      "multiple": 0.1408352823951239,
      "averaging": 0.14083008711325865,
      "trained": 0.13064330568281432,
      "algorithm": 0.1250511376893553,
      "tree": 0.12081313521977878
    }
  },
  "bag_of_words.py": {
    "title": "Bag_of_Words.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bag_of_words"
    ],
    "summary": "Summary of What the Script Does: It takes a dataset of text (movie reviews in this case) and processes it to remove HTML tags, non-alphabetic...",
    "TFIDF_Score": {
      "review": 0.44442068969646153,
      "word": 0.32406141183602666,
      "text": 0.2676004314952406,
      "feature": 0.19959016558746154,
      "alphabetic": 0.19594331807272308,
      "counted": 0.19594331807272308,
      "textual": 0.17650667758568692,
      "cleaned": 0.1702494769426848,
      "movie": 0.1702494769426848,
      "stopwords": 0.16513697176077216
    }
  },
  "bandit_example_output": {
    "title": "Bandit example output",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bandit_example_fixed.py",
      "bandit_example_nonfixed.py",
      "ml_tools"
    ],
    "inlinks": [
      "tool.bandit"
    ],
    "summary": "Complicated example output of bandit Running bandit on [[ML_Tools]] file [[Bandit_Example_Nonfixed.py]] gives. Fixing this gives [[Bandit_Example_Fixed.py]] ```bandit [main] INFO profile include tests: None [main] INFO...",
    "TFIDF_Score": {
      "cwe": 0.4972960942984901,
      "info": 0.29897797772116874,
      "subprocess": 0.242648628605152,
      "html": 0.22999561281328035,
      "bandit": 0.2210760331202653,
      "blacklist": 0.1864860353619338,
      "severity": 0.1864860353619338,
      "mitre": 0.16576536476616338,
      "readthedocs": 0.16576536476616338,
      "issue": 0.14686576640081755
    }
  },
  "bandit_example_fixed.py": {
    "title": "Bandit_Example_Fixed.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bandit_example_output"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Fixed.py",
    "TFIDF_Score": {
      "bandit_example_fixed": 0.5455115588265111,
      "deployment": 0.35468194348222304,
      "rhyslwells": 0.29060286583657013,
      "blob": 0.2868634534105377,
      "github": 0.2765621249020166,
      "exploration": 0.27443697643939324,
      "ml_tools": 0.2645590792132005,
      "com": 0.25915218948265895,
      "main": 0.24700224346186092,
      "http": 0.2447722546956577
    }
  },
  "bandit_example_nonfixed.py": {
    "title": "Bandit_Example_Nonfixed.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tool.bandit",
      "bandit_example_output"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Nonfixed.py",
    "TFIDF_Score": {
      "bandit_example_nonfixed": 0.5284355525931981,
      "deployment": 0.35928213482500865,
      "rhyslwells": 0.2943719575881413,
      "blob": 0.29058404533575616,
      "github": 0.28014910956840805,
      "exploration": 0.27799639813073185,
      "ml_tools": 0.2679903855094932,
      "com": 0.2625133689292857,
      "main": 0.2502058392549431,
      "http": 0.2479469277448413
    }
  },
  "bash": {
    "title": "Bash",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bash_folder",
      "ml_tools"
    ],
    "inlinks": [
      "command_prompt",
      "command_line",
      "powershell_vs_bash"
    ],
    "summary": "Automation Scripts In [[ML_Tools]], see: [[Bash_folder]] Basic Commands Show Current Directory: bash pwd Display Contents of a Text File: bash cat filename.txt Search for a...",
    "TFIDF_Score": {
      "bash": 0.7705530643592946,
      "history": 0.3201582375385385,
      "txt": 0.18771350919295351,
      "clear": 0.1801274848119448,
      "filename": 0.16626751635686735,
      "hello": 0.1472854789483836,
      "ctrl": 0.14352030367900867,
      "file": 0.14341129054282098,
      "script": 0.1304105940002009,
      "echo": 0.1126281055157721
    }
  },
  "batch_normalisation": {
    "title": "Batch Normalisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "overfitting",
      "neural_network",
      "pasted_image_20241219071904.png",
      "vanishing_and_exploding_gradients_problem",
      "normalisation_vs_standardisation"
    ],
    "inlinks": [
      "normalisation"
    ],
    "summary": "Links: - Batch normalization | What it is and how to implement it Can be used to handle [[vanishing and exploding gradients problem]] and [[Overfitting]]...",
    "TFIDF_Score": {
      "kera": 0.5269968260996049,
      "layer": 0.3764102335148055,
      "5000": 0.31416524422498776,
      "255": 0.2501383308272813,
      "x_train_full": 0.2501383308272813,
      "activation": 0.2105608888962048,
      "y_train_full": 0.20844860902273446,
      "dense": 0.19252542801353745,
      "normalisation": 0.1804807619110327,
      "relu": 0.1308646725505068
    }
  },
  "batch_processing": {
    "title": "Batch Processing",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "batch_processing",
      "distributed_computing",
      "databricks",
      "apache_spark"
    ],
    "inlinks": [
      "map_reduce",
      "publish_and_subscribe",
      "ds_&_ml_portal",
      "batch_processing",
      "lambda_architecture",
      "data_streaming",
      "hadoop",
      "alternatives_to_batch_processing"
    ],
    "summary": "Batch Processing is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them...",
    "TFIDF_Score": {
      "batch": 0.5463345776307388,
      "processing": 0.43373762039331853,
      "data": 0.16387572416645624,
      "map": 0.16180338824334733,
      "scalable": 0.16180338824334733,
      "distributed": 0.1448795016543272,
      "smaller": 0.13952983360385274,
      "order": 0.13488008398317208,
      "reduce": 0.1282690211884702,
      "unmanageable": 0.12437239670046173
    }
  },
  "bellman_equations": {
    "title": "Bellman Equations",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "what_are_the_bellman_equations_that_are_used_in_rl?"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "[[What are the Bellman equations that are used in RL?]] Equations here may not be accurate. In reinforcement learning, Bellman's equations are fundamental to understanding...",
    "TFIDF_Score": {
      "state": 0.46203989121805555,
      "equation": 0.4522883904878726,
      "bellman": 0.37099416767790455,
      "value": 0.29794334008623025,
      "policy": 0.2442428283468026,
      "taking": 0.20416214604705676,
      "action": 0.20262719137541296,
      "sum_": 0.18610594032285874,
      "gamma": 0.17892117131174126,
      "function": 0.14210957822596731
    }
  },
  "benefits_of_data_transformation": {
    "title": "Benefits of Data Transformation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_transformation",
      "interoperability",
      "data_quality"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Benefits of [[Data Transformation]] Efficiency: Faster query performance. [[Interoperability]]: Converting data into the required format for target systems. Enrichment: Adding contextual data for better insights....",
    "TFIDF_Score": {
      "data": 0.35269553878194104,
      "deduplicating": 0.32121113498895776,
      "enrichment": 0.32121113498895776,
      "interoperability": 0.24722844772304892,
      "cleansing": 0.24284722669473566,
      "validating": 0.24284722669473566,
      "contextual": 0.2351680798282151,
      "converting": 0.22562330766285216,
      "adding": 0.19989887336934428,
      "required": 0.19672740711062012
    }
  },
  "bernoulli": {
    "title": "Bernoulli",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "distributions"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding": {
    "title": "BERT Pretraining of Deep Bidirectional Transformers for Language Understanding",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "bert": {
    "title": "BERT",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": null,
    "outlinks": [
      "google",
      "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding",
      "summarisation",
      "vector_embedding",
      "transformer",
      "sentence_similarity",
      "positional_encoding",
      "named_entity_recognition",
      "nlp",
      "transfer_learning"
    ],
    "inlinks": [
      "word2vec.py",
      "ds_&_ml_portal",
      "rag",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding",
      "lstm",
      "small_language_models"
    ],
    "summary": "BERT (==Bidirectional Encoder Representations from [[Transformer]]==) is used in [[NLP]]processing, developed by [[Google]]. Introduced in the paper \"[[BERT Pretraining of Deep Bidirectional Transformers for Language...",
    "TFIDF_Score": {
      "bert": 0.51435204507069,
      "sentence": 0.3906695333324661,
      "masked": 0.2075160272201985,
      "text": 0.1889368788637576,
      "bidirectional": 0.18693142930663234,
      "language": 0.1602764830708577,
      "word": 0.15253376429021337,
      "transformer": 0.13768653707711268,
      "context": 0.13317719588453497,
      "entity": 0.1302231777774887
    }
  },
  "bertscore": {
    "title": "BERTScore",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "accessing_gen_ai_generated_content"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "bias_and_variance": {
    "title": "Bias and variance",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": null,
    "outlinks": [
      "boosting",
      "regularisation",
      "overfitting",
      "bagging"
    ],
    "inlinks": [
      "cross_validation",
      "overfitting",
      "machine_learning_algorithms"
    ],
    "summary": "Related to [[Overfitting]] Ways to Reduce Bias and Variance: - [[Regularisation]] - [[Boosting]] - [[Bagging]] What is Bias in Machine Learning? Bias occurs when a...",
    "TFIDF_Score": {
      "bias": 0.5443462680702073,
      "variance": 0.39726260781891987,
      "model": 0.311985206814089,
      "training": 0.22251572377776063,
      "data": 0.1770203266690454,
      "occurs": 0.16861614563581134,
      "high": 0.157691511265461,
      "machine": 0.14345722676004644,
      "underfitting": 0.13787296144354985,
      "learning": 0.12870765772100268
    }
  },
  "big_data": {
    "title": "Big Data",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "hadoop",
      "apache_spark",
      "databricks",
      "scala",
      "big_data",
      "data_storage"
    ],
    "inlinks": [
      "parquet",
      "databricks",
      "scala",
      "big_data",
      "hadoop"
    ],
    "summary": "The concept of Big Data revolves around datasets that are too large or complex to be managed using traditional data processing techniques. It’s characterized by...",
    "TFIDF_Score": {
      "data": 0.35993510039513144,
      "big": 0.31741824110422295,
      "spark": 0.24365414960794385,
      "cloud": 0.21345780586843227,
      "four": 0.21091985915827235,
      "hadoop": 0.1770228574540177,
      "apache": 0.1445399168400273,
      "generated": 0.14230520391228818,
      "speed": 0.13637656105840607,
      "structured": 0.13293679741747585
    }
  },
  "big_o_notation": {
    "title": "Big O Notation",
    "tags": [
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "mathematics"
    ],
    "summary": "Big-O Notation is an analysis of the algorithm using Big – O asymptotic notation. Mostly related to computing rather than storage Doing things not exponentially,...",
    "TFIDF_Score": {
      "notation": 0.4442483289107483,
      "big": 0.36689888606154647,
      "order": 0.29351150356812405,
      "time": 0.27858402368232993,
      "function": 0.22412974399132365,
      "method": 0.21616206071892222,
      "algorithmic": 0.16996096865081065,
      "quadratic": 0.16996096865081065,
      "asymptotic": 0.16253269105186458,
      "copying": 0.16253269105186458
    }
  },
  "bigquery": {
    "title": "BigQuery",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "google",
      "data_warehouse"
    ],
    "inlinks": [
      "elt",
      "google_cloud_platform"
    ],
    "summary": "cloud-based [[Data Warehouse]] BigQuery is a fully managed, serverless data warehouse offered by [[Google]] Cloud Platform (GCP). It is designed to handle large-scale data analytics...",
    "TFIDF_Score": {
      "bigquery": 0.6094155244974211,
      "google": 0.28448740901962805,
      "user": 0.22804304170874043,
      "data": 0.21198334044455913,
      "serverless": 0.20066468424853448,
      "sql": 0.19919255457645385,
      "cloud": 0.18051487374677688,
      "analytics": 0.17535637988925173,
      "compliance": 0.11430279625278285,
      "query": 0.10722097426473512
    }
  },
  "binary_classification": {
    "title": "Binary Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "classification"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "logistic_regression",
      "precision-recall_curve",
      "use_cases_for_a_simple_neural_network_like",
      "determining_threshold_values",
      "typical_output_formats_in_neural_networks",
      "cosine_similarity",
      "activation_function"
    ],
    "summary": "Binary classification is a type of [[Classification]] task that involves predicting one of two possible classes or outcomes. It is used in scenarios where the...",
    "TFIDF_Score": {
      "disease": 0.45001322206213684,
      "spam": 0.4036955592861975,
      "two": 0.2611986733732891,
      "classification": 0.2520788311846801,
      "categorize": 0.2514899149162384,
      "diagnosis": 0.21401504681392577,
      "filtering": 0.19852330714589844,
      "distinct": 0.19546508890515243,
      "medical": 0.19546508890515243,
      "predicting": 0.18096580324873482
    }
  },
  "binder": {
    "title": "Binder",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://mybinder.org/",
    "TFIDF_Score": {
      "mybinder": 0.7558777097701961,
      "org": 0.5714708661776957,
      "http": 0.319483860285113
    }
  },
  "boosting": {
    "title": "Boosting",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "gradient_boosting",
      "ada_boosting",
      "model_ensemble",
      "weak_learners",
      "interpretability",
      "xgboost"
    ],
    "inlinks": [
      "gradient_boosting",
      "ada_boosting",
      "model_ensemble",
      "bias_and_variance",
      "gradient_boosting_regressor"
    ],
    "summary": "Boosting is a type of [[Model Ensemble]] in machine learning that focuses on improving the accuracy of predictions by building a ==sequence of models==. Each...",
    "TFIDF_Score": {
      "boosting": 0.6707554750199478,
      "model": 0.35906298923017255,
      "learner": 0.31274779589986174,
      "weak": 0.2030409536562079,
      "previous": 0.1624984943660288,
      "accuracy": 0.11301784112982864,
      "misclassified": 0.11142684319959618,
      "algorithm": 0.11108270537577188,
      "guessing": 0.09915669273560307,
      "focus": 0.09613325385323246
    }
  },
  "bootstrap": {
    "title": "Bootstrap",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "sampling with replacement from an original dataset.",
    "TFIDF_Score": {
      "replacement": 0.6512104105304418,
      "sampling": 0.5080203403737009,
      "original": 0.4488063509614181,
      "dataset": 0.34119377825520086
    }
  },
  "boxplot": {
    "title": "Boxplot",
    "tags": [
      "#statistics",
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "standardised/outliers",
      "distributions"
    ],
    "inlinks": [
      "variance",
      "distributions",
      "violin_plot",
      "anomaly_detection"
    ],
    "summary": "A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first...",
    "TFIDF_Score": {
      "boxplot": 0.49659748869463516,
      "plt": 0.4535221582922385,
      "quartile": 0.23590591196922034,
      "median": 0.21493277546043552,
      "title": 0.19473240329200683,
      "outlier": 0.19194861826849166,
      "data": 0.18690928390238398,
      "show": 0.15355889461479336,
      "matplotlib": 0.15117405276407953,
      "excluding": 0.13094175182630347
    }
  },
  "business_intelligence": {
    "title": "business intelligence",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "rollup",
      "single_source_of_truth"
    ],
    "inlinks": [
      "data_scientist",
      "granularity",
      "data_ingestion"
    ],
    "summary": "Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization’s business decisions. The new term is Data Engineer....",
    "TFIDF_Score": {
      "business": 0.2800448005435575,
      "drill": 0.23753255236450205,
      "instead": 0.2168342783590276,
      "engineer": 0.1807744429843933,
      "detail": 0.17320190096838067,
      "overview": 0.1683779600607271,
      "organization": 0.152577438734232,
      "service": 0.14880067305708528,
      "data": 0.14476775245282889,
      "airplane": 0.13184463357764087
    }
  },
  "business_observability": {
    "title": "Business observability",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "data_visualisation",
      "model_observability",
      "data_collection"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "Business [[Model Observability|observability]] refers to the ability to gain insights into the internal state and performance of a business through the continuous monitoring and analysis...",
    "TFIDF_Score": {
      "business": 0.4553761922640435,
      "observability": 0.3814274463192251,
      "customer": 0.2053474502065837,
      "data": 0.1883235475085044,
      "insight": 0.15730990730324945,
      "improvement": 0.1437445837848892,
      "monitoring": 0.13689830013772247,
      "interaction": 0.13029845213890118,
      "operation": 0.11502423484643831,
      "gathering": 0.10719518286449287
    }
  },
  "career_interest": {
    "title": "Career Interest",
    "tags": [],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "This is a portal to notes that I find relevant to my career:",
    "TFIDF_Score": {
      "career": 0.6340462834315341,
      "portal": 0.48801010102820763,
      "note": 0.3599808372277585,
      "relevant": 0.34017708007053366,
      "find": 0.338415133799576
    }
  },
  "casual_inference": {
    "title": "Casual Inference",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "missing data problem",
    "TFIDF_Score": {
      "missing": 0.7857540163785742,
      "problem": 0.5612089639736828,
      "data": 0.2600675383444178
    }
  },
  "catboost": {
    "title": "CatBoost",
    "tags": null,
    "aliases": [
      "CAT"
    ],
    "outlinks": [
      "gradient_boosting",
      "categorical",
      "hyperparameter"
    ],
    "inlinks": [
      "lightgbm_vs_xgboost_vs_catboost",
      "gradient_boosting",
      "optuna"
    ],
    "summary": "CatBoost is a [[Gradient Boosting]] library developed by Yandex, designed to handle [[categorical]] features efficiently and provide robust performance with minimal [[Hyperparameter|Hyperparameter tuning]] It is...",
    "TFIDF_Score": {
      "catboost": 0.573232836919079,
      "categorical": 0.27838315103965505,
      "pool": 0.2748935124285724,
      "step": 0.19234739857979669,
      "python": 0.1770797599868913,
      "feature": 0.16800585660443246,
      "catboostclassifier": 0.16493610745714343,
      "categorical_features_indices": 0.16493610745714343,
      "install": 0.16352515092544795,
      "import": 0.13794297235416159
    }
  },
  "central_limit_theorem": {
    "title": "Central Limit Theorem",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "central_limit_theorem",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes"
    ],
    "inlinks": [
      "statistics",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "central_limit_theorem",
      "z-test"
    ],
    "summary": "The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a...",
    "TFIDF_Score": {
      "distribution": 0.3650922810789707,
      "theorem": 0.3542440219278628,
      "central": 0.32353958411368544,
      "limit": 0.3065740593035078,
      "sampling": 0.2995399385631739,
      "clt": 0.20813676482215307,
      "sum": 0.1956786645838131,
      "variance": 0.18375846270838203,
      "population": 0.1552040378543651,
      "variable": 0.14782034891148224
    }
  },
  "chain_of_thought": {
    "title": "Chain of thought",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling",
      "deepseek",
      "llm"
    ],
    "summary": "Chain of Thought (CoT) reasoning in AI systems is a cognitive-inspired framework that improves the performance of large language models (LLMs) by explicitly guiding the...",
    "TFIDF_Score": {
      "reasoning": 0.43035706690638437,
      "chain": 0.3364221745746314,
      "cot": 0.288565712795102,
      "step": 0.2679399136259977,
      "intermediate": 0.2581781186629927,
      "thought": 0.24040251560857448,
      "debugging": 0.21800790424372296,
      "model": 0.17784721412515764,
      "easier": 0.15948141561395787,
      "spotted": 0.15317065792476012
    }
  },
  "change_management": {
    "title": "Change Management",
    "tags": [
      "business"
    ],
    "aliases": [
      "Change Program"
    ],
    "outlinks": [],
    "inlinks": [
      "prevention_is_better_than_the_cure",
      "digital_transformation",
      "data_quality"
    ],
    "summary": "Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a desired future state. It involves - preparing,...",
    "TFIDF_Score": {
      "change": 0.6209310655360636,
      "resistance": 0.23867378273402762,
      "successful": 0.17250325040312714,
      "organization": 0.15894514067983376,
      "help": 0.143202028572448,
      "people": 0.1409502917252655,
      "fails": 0.1340744148237726,
      "team": 0.11590885139301971,
      "training": 0.11374114203231707,
      "outcome": 0.11129921992060722
    }
  },
  "checksum": {
    "title": "Checksum",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "security",
      "data_storage",
      "algorithms"
    ],
    "inlinks": [
      "data_integrity"
    ],
    "summary": "A checksum is a value calculated from a data set that is used to verify the integrity of that data. It acts as a fingerprint...",
    "TFIDF_Score": {
      "checksum": 0.7337612623714209,
      "108": 0.2921103157799715,
      "data": 0.19244539304900812,
      "111": 0.18344031559285523,
      "sent": 0.16920413161744072,
      "ascii": 0.14605515788998574,
      "corrupted": 0.13758023669464145,
      "match": 0.13199902696105692,
      "114": 0.0973701052599905,
      "256": 0.09172015779642761
    }
  },
  "chi-squared_test": {
    "title": "Chi-Squared Test",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Chi-Squared Test The Chi-squared test is used to determine if there is a significant association between categorical variables. It assesses whether the observed frequencies in...",
    "TFIDF_Score": {
      "chi": 0.4416490625340627,
      "frequency": 0.3882696791930037,
      "squared": 0.3134265288917067,
      "test": 0.25698673602064226,
      "contingency": 0.25344980809289314,
      "association": 0.2337804236128085,
      "assesses": 0.22082453126703136,
      "assuming": 0.22082453126703136,
      "differ": 0.20709073194227903,
      "observed": 0.19413483959650185
    }
  },
  "choosing_a_threshold": {
    "title": "Choosing a Threshold",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cost-sensitive_analysis",
      "precision-recall_curve",
      "roc_(receiver_operating_characteristic)"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal threshold depends on the specific problem and the desired trade-off between different types of errors: Manual Selection: Based on domain expertise or prior...",
    "TFIDF_Score": {
      "threshold": 0.4468571282500446,
      "curve": 0.27935966121374406,
      "precision": 0.25295603822304813,
      "recall": 0.25295603822304813,
      "different": 0.19438813647040545,
      "positive": 0.18031529562456372,
      "false": 0.17759608665287743,
      "fpr": 0.17680629167340697,
      "tpr": 0.17680629167340697,
      "optimal": 0.17382044721171713
    }
  },
  "choosing_the_number_of_clusters": {
    "title": "Choosing the Number of Clusters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "silhouette_analysis",
      "granularity",
      "wcss_and_elbow_method"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal number of clusters ([[clustering]]) depends on the data and the desired level of [[granularity]]. Here are some common approaches: Elbow Method: [[WCSS and...",
    "TFIDF_Score": {
      "cluster": 0.5532772033898623,
      "silhouette": 0.3860177524838082,
      "wcss": 0.35606021711665053,
      "optimal": 0.24194168155481605,
      "number": 0.23818804659801657,
      "elbow": 0.23024527328962685,
      "point": 0.18037452054978984,
      "coefficient": 0.16359493458393148,
      "maximizes": 0.11868673903888352,
      "slowly": 0.11868673903888352
    }
  },
  "ci-cd": {
    "title": "CI-CD",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "continuous_delivery/deployment",
      "docker",
      "gitlab",
      "software_development_life_cycle",
      "continuous_integration"
    ],
    "inlinks": [
      "devops",
      "gitlab"
    ],
    "summary": "CI/CD stands for [[Continuous Integration]] and [[Continuous Delivery/Deployment]]. It is a set of practices aimed at streamlining and accelerating the [[Software Development Life Cycle]]. The...",
    "TFIDF_Score": {
      "software": 0.28156042859860947,
      "continuous": 0.27527326757346665,
      "integration": 0.26676264589591203,
      "accelerating": 0.25245458558371975,
      "aimed": 0.22741229749980915,
      "docker": 0.22741229749980915,
      "streamlining": 0.22741229749980915,
      "deliver": 0.2193504814053353,
      "gitlab": 0.2193504814053353,
      "reliably": 0.2193504814053353
    }
  },
  "class_separability": {
    "title": "Class Separability",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "classification",
      "feature_engineering",
      "imbalanced_datasets",
      "accuracy"
    ],
    "inlinks": [],
    "summary": "If you have a perfectly balanced dataset (unlike [[Imbalanced Datasets]]) but still experience poor [[classification]] [[accuracy]], class separability might be an issue due to the...",
    "TFIDF_Score": {
      "class": 0.3534958091260604,
      "may": 0.347649443709062,
      "separability": 0.2843881843253254,
      "balanced": 0.22825218888764587,
      "feature": 0.20501697257569648,
      "model": 0.2044846335330141,
      "poor": 0.19454515372887643,
      "underfitting": 0.15491339998984946,
      "classification": 0.14252706938832516,
      "boundary": 0.14137561445259897
    }
  },
  "classification_report": {
    "title": "Classification Report",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "evaluation_metrics.py",
      "pasted_image_20240404163858.png",
      "f1_score",
      "precision",
      "recall",
      "ml_tools"
    ],
    "inlinks": [
      "precision"
    ],
    "summary": "The classification_report function in sklearn.metrics is used to evaluate the performance of a classification model. It provides a summary of key metrics for each class,...",
    "TFIDF_Score": {
      "label": 0.2916093779313349,
      "positive": 0.27148778216467323,
      "predicted": 0.2093671645322237,
      "none": 0.20871838155651837,
      "optional": 0.1947220106223023,
      "array": 0.18233575635563629,
      "class": 0.1779512692686546,
      "classification_report": 0.17731113305572954,
      "output_dict": 0.17731113305572954,
      "sample_weight": 0.17731113305572954
    }
  },
  "classification": {
    "title": "Classification",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "support_vector_machines",
      "model_ensemble",
      "interpretability",
      "neural_network",
      "naive_bayes",
      "decision_tree",
      "k-nearest_neighbours",
      "supervised_learning",
      "regression"
    ],
    "inlinks": [
      "precision",
      "recall",
      "neural_network_classification",
      "k-nearest_neighbours",
      "supervised_learning",
      "class_separability",
      "precision_or_recall",
      "confusion_matrix",
      "gini_impurity",
      "learning_styles",
      "support_vector_machines",
      "machine_learning_algorithms",
      "decision_tree",
      "typical_output_formats_in_neural_networks",
      "cross_entropy",
      "imbalanced_datasets",
      "ds_&_ml_portal",
      "accuracy",
      "loss_function",
      "binary_classification"
    ],
    "summary": "Classification is a type of [[Supervised Learning]] in machine learning, where the algorithm learns from labeled data to predict which category or class a new,...",
    "TFIDF_Score": {
      "classifier": 0.30243888654436774,
      "model": 0.27756517728944463,
      "spam": 0.24097655069329965,
      "algorithm": 0.19082192469610504,
      "bayes": 0.17439557957174498,
      "naive": 0.17033498500329983,
      "data": 0.16332338888156528,
      "classifying": 0.16065103379553308,
      "classification": 0.1504725178277672,
      "email": 0.14098756214218972
    }
  },
  "claude": {
    "title": "Claude",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Claude is better for code and uses Artifact for tracking code changes. Claude is crazy see: https://youtu.be/RudrWy9uPZE?t=473",
    "TFIDF_Score": {
      "claude": 0.5848976807981029,
      "473": 0.31046364351029165,
      "crazy": 0.31046364351029165,
      "rudrwy9upze": 0.31046364351029165,
      "code": 0.28986238302590844,
      "artifact": 0.2697528726023209,
      "youtu": 0.26165233106891306,
      "tracking": 0.21807411551987643,
      "us": 0.14826500266993373,
      "better": 0.14283998335697012
    }
  },
  "cleaning_terminal_path": {
    "title": "cleaning terminal path",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.youtube.com/watch?v=18hUejOK0qk cmd prompt $g powershell ```powershell $profile microsfot_Powershell_profile have function prompt{ $p = -path \"$p> \" } ``` getting the script working https://stackoverflow.com/questions/41117421/ps1-cannot-be-loaded-because-running-scripts-is-disabled-on-this-system",
    "TFIDF_Score": {
      "powershell": 0.37812971660641564,
      "prompt": 0.2968891503533528,
      "script": 0.2511869564449866,
      "18huejok0qk": 0.2303649008577661,
      "41117421": 0.2303649008577661,
      "disabled": 0.2303649008577661,
      "microsfot_powershell_profile": 0.2303649008577661,
      "ps1": 0.21699786603922558,
      "stackoverflow": 0.20751380390358304,
      "com": 0.20617518066199061
    }
  },
  "click_implementation.py": {
    "title": "Click_Implementation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "python_click"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Click_Implementation.py This script implements a command-line interface (CLI) tool using Python's click library. The CLI allows users to interact with a JSON file, enabling them...",
    "TFIDF_Score": {
      "json": 0.520356154328136,
      "key": 0.2950990625979141,
      "value": 0.26796773551963454,
      "cli": 0.25747231590571135,
      "command": 0.22512266206406015,
      "name": 0.1892415031471377,
      "age": 0.16271375056180593,
      "script": 0.16155707809481262,
      "accepts": 0.1481648601699306,
      "get_value": 0.1481648601699306
    }
  },
  "cloud_providers": {
    "title": "Cloud Providers",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "load_balancing",
      "memory_caching",
      "databricks",
      "scaling_server",
      "data_lakehouse",
      "data_warehouse"
    ],
    "inlinks": [
      "storage_layer_object_store",
      "parquet",
      "data_storage"
    ],
    "summary": "Among the biggest cloud providers are AWS, Microsoft Azure, Google Cloud. Whereas [[Databricks]] ( Databrick) and Snowflake provide dedicated [[Data Warehouse]]and [[Data Lakehouse|Lakehouse]] solutions Features...",
    "TFIDF_Score": {
      "lakehouse": 0.43634167843827587,
      "cloud": 0.3052146398698573,
      "databrick": 0.25109691325336725,
      "dedicated": 0.25109691325336725,
      "biggest": 0.22618929977456084,
      "databricks": 0.2060800154357382,
      "whereas": 0.20128168629575446,
      "caching": 0.19704926003944376,
      "provider": 0.1898383411187943,
      "azure": 0.1867116664281959
    }
  },
  "clustering": {
    "title": "Clustering",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "hierarchical_clustering",
      "anomaly_detection",
      "unsupervised_learning",
      "gaussian_mixture_models",
      "interpretability",
      "standardised/outliers",
      "correlation",
      "feature_scaling",
      "k-means",
      "dbscan",
      "dendrograms"
    ],
    "inlinks": [
      "feature_selection",
      "gaussian_mixture_models",
      "tf-idf",
      "neural_network_classification",
      "problem_definition",
      "covariance_structures",
      "model_parameters",
      "anomaly_detection_with_clustering",
      "unsupervised_learning",
      "choosing_the_number_of_clusters",
      "wcss_and_elbow_method",
      "multicollinearity",
      "learning_styles",
      "machine_learning_algorithms",
      "dbscan",
      "ds_&_ml_portal",
      "correlation",
      "silhouette_analysis",
      "data_mining_-_crisp"
    ],
    "summary": "Clustering involves grouping a set of data points into subsets or clusters based on inherent patterns or similarities. It is an [[Unsupervised Learning]]technique used for...",
    "TFIDF_Score": {
      "segmentation": 0.332735440272927,
      "clustering": 0.2887845468049357,
      "cluster": 0.284973831993885,
      "customer": 0.23586373368771174,
      "anomaly": 0.22987308968747536,
      "grouping": 0.18028720287349356,
      "feature": 0.16722243418593907,
      "preprocessing": 0.1508173812638045,
      "behavior": 0.14745860023815632,
      "detection": 0.1464065660651815
    }
  },
  "clustering_dashboard.py": {
    "title": "Clustering_Dashboard.py",
    "tags": [
      "code_snippet"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "dash"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Clustering_Dashboard.py",
    "TFIDF_Score": {
      "clustering_dashboard": 0.5338796551580753,
      "preprocess": 0.40333961929155804,
      "rhyslwells": 0.28440636186430984,
      "blob": 0.2807466846600383,
      "github": 0.2706650106371633,
      "exploration": 0.26858517656210584,
      "ml_tools": 0.2589179050268315,
      "com": 0.2536263060164844,
      "main": 0.24173543241936632,
      "http": 0.23955299354296924
    }
  },
  "clustermap": {
    "title": "Clustermap",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "dendrograms"
    ],
    "inlinks": [],
    "summary": "Clustermap Related to: [[Preprocessing|Preprocess]] Purpose: Identify which features are most similar using [[Dendrograms]]. Visualization: Regions of color show clustering, similar to a heatmap. Functionality: Performs...",
    "TFIDF_Score": {
      "clustermap": 0.5282706888033026,
      "seaborn": 0.2968103996502948,
      "sn": 0.28904120535851247,
      "clustering": 0.2065057220700663,
      "row": 0.20512481376592176,
      "column": 0.17750736906059394,
      "mako": 0.1760902296011009,
      "standard_scale": 0.1760902296011009,
      "similar": 0.17445188810984158,
      "x_scaled": 0.1658725088393082
    }
  },
  "code_diagrams": {
    "title": "Code Diagrams",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "architecture_diagram",
      "documentation_&_meetings",
      "mermaid",
      "classes",
      "sequence_diagram"
    ],
    "inlinks": [],
    "summary": "[[Documentation & Meetings]] There are class diagrams showing the hierarchy of classes [[Classes]] (Object orientated). Done in [[Mermaid]]. Overall [[Architecture Diagram]]: showing how software components...",
    "TFIDF_Score": {
      "diagram": 0.5129201263544186,
      "componets": 0.3603736914452701,
      "interact": 0.34517324697961116,
      "class": 0.27125604593407143,
      "showing": 0.2600181994883613,
      "sequence": 0.21902166185144092,
      "architecture": 0.1986683502606462,
      "componts": 0.18018684572263505,
      "diagraph": 0.18018684572263505,
      "orientated": 0.16973141682852338
    }
  },
  "columnar_storage": {
    "title": "Columnar Storage",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying"
    ],
    "inlinks": [
      "vectorized_engine",
      "row-based_storage",
      "duckdb_vs_sqlite",
      "database_storage",
      "duckdb",
      "types_of_database_schema"
    ],
    "summary": "A database storage technique that stores ==data by columns== rather than rows, Useful for read-heavy operations and ==large-scale data analytics==, as it enables the retrieval...",
    "TFIDF_Score": {
      "order_amount": 0.41727729660369395,
      "2024": 0.28867036011405794,
      "order_date": 0.28867036011405794,
      "column": 0.2790915881990547,
      "row": 0.19350839468559988,
      "102": 0.19244690674270531,
      "103": 0.19244690674270531,
      "customer_id": 0.19244690674270531,
      "101": 0.18666782403635218,
      "columnar": 0.1738157157311033
    }
  },
  "command_line": {
    "title": "Command Line",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "command_prompt",
      "bash",
      "powershell_vs_bash",
      "powershell"
    ],
    "inlinks": [],
    "summary": "The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts,...",
    "TFIDF_Score": {
      "command": 0.493884300750946,
      "powershell": 0.44462561108087656,
      "bash": 0.3966320019197846,
      "computer": 0.20848644557446602,
      "operating": 0.19026694495156507,
      "prompt": 0.17454925400182372,
      "interact": 0.17296680589225605,
      "execute": 0.1672133644134254,
      "interface": 0.1672133644134254,
      "line": 0.15673566908378855
    }
  },
  "command_prompt": {
    "title": "Command Prompt",
    "tags": [
      "software"
    ],
    "aliases": [
      "cmd"
    ],
    "outlinks": [
      "bash",
      "powershell"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_line",
      "powershell"
    ],
    "summary": "Command Prompt (cmd) is a command-line interpreter on Windows systems that allows users to execute commands to perform various basic tasks. Below are some common...",
    "TFIDF_Score": {
      "cmd": 0.5753465813745741,
      "directory": 0.3904842951457713,
      "file": 0.3643708105109133,
      "txt": 0.24883368628701516,
      "command": 0.15416994498203704,
      "hello": 0.14643161067966426,
      "viewing": 0.1270891958006171,
      "newfolder": 0.12683401201835362,
      "system": 0.11071836845794394,
      "echo": 0.09953347451480606
    }
  },
  "common_security_vulnerabilities_in_software_development": {
    "title": "Common Security Vulnerabilities in Software Development",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "security",
      "input_is_not_properly_sanitized",
      "sql_injection",
      "software_development_portal",
      "tool.bandit",
      "why_json_is_better_than_pickle_for_untrusted_data"
    ],
    "inlinks": [
      "testing",
      "security",
      "tool.bandit"
    ],
    "summary": "[[Security]] vulnerabilities can be encountered and mitigated in [[Software Development Portal]]. In this not describe potential security risks in their applications. Useful Tools - [[tool.bandit]]...",
    "TFIDF_Score": {
      "subprocess": 0.24941310492003302,
      "password": 0.209329840706426,
      "eval": 0.20784425410002752,
      "untrusted": 0.20047612945914736,
      "mitigation": 0.1944559332793099,
      "user_input": 0.1944559332793099,
      "security": 0.19071196768910378,
      "pickle": 0.1893659299350308,
      "input": 0.1871177797898022,
      "sanitized": 0.16038090356731788
    }
  },
  "common_table_expression": {
    "title": "Common Table Expression",
    "tags": [
      "database",
      "querying"
    ],
    "aliases": [
      "CTE"
    ],
    "outlinks": [
      "querying",
      "views",
      "de_tools",
      "recursive_algorithm"
    ],
    "inlinks": [
      "views"
    ],
    "summary": "A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE...",
    "TFIDF_Score": {
      "cte": 0.4244660683430868,
      "employee": 0.35515890371931463,
      "recursive": 0.34025605852516294,
      "select": 0.2771328739333416,
      "level": 0.24450405005517384,
      "superior_id": 0.24255203905319248,
      "avg": 0.20441783683982967,
      "store": 0.16621077313206897,
      "sql": 0.16266684027329914,
      "average_order": 0.12127601952659624
    }
  },
  "communication_principles": {
    "title": "Communication principles",
    "tags": [
      "communication"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240916075439.png",
      "pasted_image_20240916075433.png"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20240916075433.png]] ![[Pasted image 20240916075439.png]]",
    "TFIDF_Score": {
      "pasted": 0.4623978599966367,
      "20240916075433": 0.4566643257436677,
      "20240916075439": 0.4566643257436677,
      "png": 0.45435193257110934,
      "image": 0.4033211222160223
    }
  },
  "communication_techniques": {
    "title": "Communication Techniques",
    "tags": [
      "communication"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Overview Using these structured communication bridges can enhance clarity and engagement, especially in spontaneous or high-stakes discussions. Tips for Using Communication Bridges 1. Start Small:...",
    "TFIDF_Score": {
      "bridge": 0.5503417895923564,
      "effect": 0.27554075722562393,
      "discussion": 0.2617288072638116,
      "purpose": 0.23480070582236282,
      "phrase": 0.1786454185459306,
      "use": 0.15362761028105107,
      "point": 0.14371288353784542,
      "clear": 0.13659475491608752,
      "listener": 0.10883469964055502,
      "communication": 0.10619211111630512
    }
  },
  "comparing_llm": {
    "title": "Comparing LLM",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "llm"
    ],
    "inlinks": [],
    "summary": "Use lmarena.ai as a bench marking tool. [[LLM]] web dev arena text to image leader board",
    "TFIDF_Score": {
      "bench": 0.365141893257266,
      "lmarena": 0.365141893257266,
      "arena": 0.343954358252134,
      "marking": 0.343954358252134,
      "board": 0.3289215629300886,
      "leader": 0.3289215629300886,
      "dev": 0.30773402792495663,
      "llm": 0.22026057194855655,
      "web": 0.2170926389471158,
      "text": 0.16622515999987345
    }
  },
  "comparing_ensembles.py": {
    "title": "Comparing_Ensembles.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": [
      "model_ensemble"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Comparing_Ensembles.py This script explores various [[Model Ensemble]] techniques in machine learning, demonstrating their effectiveness in improving prediction accuracy through programmatic examples [!Important Notes] - {{Note...",
    "TFIDF_Score": {
      "ensemble": 0.5673005714316368,
      "note": 0.297090852695604,
      "technique": 0.20620403111432759,
      "explores": 0.17442535038235243,
      "comparing_ensembles": 0.16430423504266534,
      "programmatic": 0.16430423504266534,
      "stacking": 0.1515531377146457,
      "demonstrating": 0.1470020740883998,
      "estimator": 0.13982102847382138,
      "observe": 0.13982102847382138
    }
  },
  "components_of_the_database": {
    "title": "Components of the database",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "dimension_table",
      "fact_table",
      "obsidian_csp0fnavd1.png"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "[[Fact Table]] in main table that [[Dimension Table]] connect to them. ![[Obsidian_CSP0FnAVD1.png]]",
    "TFIDF_Score": {
      "table": 0.6397149871385974,
      "obsidian_csp0fnavd1": 0.4584706781357772,
      "connect": 0.3356598115393378,
      "fact": 0.3180587353802944,
      "dimension": 0.27655827954170087,
      "png": 0.2280745691038533,
      "main": 0.19554542498016902
    }
  },
  "computer_science": {
    "title": "Computer Science",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "algorithms"
    ],
    "inlinks": [],
    "summary": "[[Algorithms]]",
    "TFIDF_Score": {
      "algorithm": 1.0
    }
  },
  "concatenate": {
    "title": "Concatenate",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "conceptual_data_model": {
    "title": "conceptual data model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_schema"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "conceptual_model": {
    "title": "Conceptual Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "er_diagrams"
    ],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Conceptual Model - Entities: Customer, Order, Book - Relationships: Customers place Orders, Orders include Books Conceptual Model - Focuses on high-level business requirements. - Defines...",
    "TFIDF_Score": {
      "conceptual": 0.41105919317207684,
      "book": 0.388278677283801,
      "order": 0.3847991684710653,
      "customer": 0.30209381075726194,
      "entity": 0.29688324052423104,
      "dbschema": 0.2365478698696641,
      "relationship": 0.21314946515925434,
      "studio": 0.18563184277255898,
      "place": 0.17067492071811874,
      "diagram": 0.16833937407559496
    }
  },
  "concurrency": {
    "title": "Concurrency",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "transaction",
      "database_techniques",
      "sqlite"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb",
    "TFIDF_Score": {
      "de_tools": 0.5630498068433919,
      "concurrency": 0.3502125333825401,
      "sqlite": 0.2929240437336349,
      "transaction": 0.2929240437336349,
      "ipynb": 0.2597480112682461,
      "rhyslwells": 0.21923196958219343,
      "blob": 0.21641094182364323,
      "github": 0.20863957820775114,
      "exploration": 0.20703635766904693,
      "com": 0.19550545297709676
    }
  },
  "confidence_interval": {
    "title": "Confidence Interval",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "statistics",
      "data_analysis",
      "model_interpretability"
    ],
    "summary": "A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with...",
    "TFIDF_Score": {
      "interval": 0.5435475853705145,
      "confidence": 0.4405557303201484,
      "true": 0.31922872960566745,
      "contain": 0.22333400833051206,
      "parameter": 0.21918983614231285,
      "range": 0.15741739358434534,
      "level": 0.156011037167614,
      "repeated": 0.1344712756946961,
      "mean": 0.1332450359365489,
      "derived": 0.12406151631925084
    }
  },
  "confusion_matrix": {
    "title": "Confusion Matrix",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "accuracy",
      "f1_score",
      "pasted_image_20240120215414.png",
      "pasted_image_20240116210541.png",
      "classification",
      "precision",
      "recall",
      "pasted_image_20240116205937.png",
      "specificity"
    ],
    "inlinks": [
      "logistic_regression",
      "evaluation_metrics",
      "accuracy"
    ],
    "summary": "A Confusion Matrix is a table used to evaluate the performance of a [[Classification]] model. It provides a detailed breakdown of the model's predictions across...",
    "TFIDF_Score": {
      "positive": 0.4465358398967983,
      "negative": 0.26792150393807895,
      "true": 0.2631769609873806,
      "class": 0.25610330046439855,
      "recall": 0.23490922510272907,
      "predicted": 0.2152259438614915,
      "precision": 0.18792738008218327,
      "false": 0.1759207779711485,
      "case": 0.16537731393466837,
      "instance": 0.15987710073986444
    }
  },
  "continuous_delivery_-_deployment": {
    "title": "Continuous Delivery - Deployment",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_deployment"
    ],
    "inlinks": [],
    "summary": "Continuous Delivery - Ensures that code changes are automatically prepared for a release to production. - Builds, tests, and releases are automated, but the deployment...",
    "TFIDF_Score": {
      "deployment": 0.49793357857559795,
      "continuous": 0.4432491557806003,
      "release": 0.29294616152006814,
      "delivery": 0.23809205900786481,
      "automated": 0.22043077332551564,
      "production": 0.19917343143023916,
      "automatically": 0.19068826367750247,
      "test": 0.15530485391944934,
      "change": 0.14702159353296257,
      "deliver": 0.1412805777473973
    }
  },
  "continuous_integration": {
    "title": "Continuous Integration",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "ci-cd",
      "data_engineer"
    ],
    "summary": "Developers frequently integrate code into a shared repository. Automated builds and tests are run to detect issues early. Encourages smaller, more manageable code changes.",
    "TFIDF_Score": {
      "code": 0.3405834333901144,
      "manageable": 0.29241880576751156,
      "encourages": 0.2862700047804665,
      "frequently": 0.2712517145726369,
      "repository": 0.2712517145726369,
      "integrate": 0.2530685480955997,
      "shared": 0.2530685480955997,
      "automated": 0.2472620727306272,
      "early": 0.2445843334670622,
      "developer": 0.24203726879063198
    }
  },
  "converting_categorical_variables_to_a_dummy_indicators": {
    "title": "Converting categorical variables to a dummy indicators",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "convolutional_neural_networks": {
    "title": "Convolutional Neural Networks",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "feature_extraction",
      "pasted_image_20241006124735.png",
      "deep_learning",
      "pasted_image_20241006124829.png"
    ],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Convolutional networks, or CNNs, are specialized [[Deep Learning]] architectures designed for processing data with grid-like structures, such as images. They use convolutional layers with learnable...",
    "TFIDF_Score": {
      "convolutional": 0.49667521646416607,
      "image": 0.32597572261740504,
      "cnns": 0.29800512987849964,
      "pooling": 0.2849805816647535,
      "spatial": 0.16551096153713998,
      "filter": 0.15682792939464255,
      "network": 0.14878906716013063,
      "feature": 0.14322233460498116,
      "500": 0.13349094285749713,
      "detection": 0.12539400168064807
    }
  },
  "correlation_vs_causation": {
    "title": "Correlation vs Causation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "correlation"
    ],
    "inlinks": [
      "correlation",
      "statistics"
    ],
    "summary": "What is the meaning of [[Correlation]] does not imply causation? Correlation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. Correlation:...",
    "TFIDF_Score": {
      "causation": 0.45509571732198567,
      "cause": 0.37274693995906527,
      "correlation": 0.3440389562253339,
      "variable": 0.3114723322889982,
      "association": 0.3033971448813238,
      "imply": 0.2942862829671609,
      "effect": 0.22101137906485802,
      "implies": 0.17459273393788727,
      "change": 0.15786292930354712,
      "relationship": 0.15732269278120092
    }
  },
  "correlation": {
    "title": "Correlation",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "standardised/outliers",
      "heatmap",
      "multicollinearity",
      "clustering",
      "covariance",
      "correlation_vs_causation",
      "'var1',_'target'"
    ],
    "inlinks": [
      "eda",
      "feature_selection",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "pca_principal_components",
      "filter_methods",
      "heatmap",
      "multicollinearity",
      "clustering",
      "covariance",
      "correlation_vs_causation"
    ],
    "summary": "Use in understanding relationships between variables in data analysis. While it helps identify associations, it's important to remember that ==correlation does not imply causation.== Visualization...",
    "TFIDF_Score": {
      "correlation": 0.6313329469159792,
      "correlated": 0.3283921864069364,
      "causation": 0.2227010750937928,
      "variable": 0.20004990613418844,
      "two": 0.156724375978076,
      "feature": 0.1522973896742515,
      "target": 0.14210633181957041,
      "relationship": 0.11547878221704592,
      "model": 0.11160142547956346,
      "association": 0.1113505375468964
    }
  },
  "cosine_similarity": {
    "title": "Cosine Similarity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "tf-idf",
      "metric",
      "binary_classification"
    ],
    "inlinks": [
      "word2vec.py",
      "vector_database"
    ],
    "summary": "Cosine similarity is a [[Metric]] used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from...",
    "TFIDF_Score": {
      "cosine": 0.4774749347821568,
      "similarity": 0.4742515605068438,
      "document": 0.28744368799811215,
      "indicates": 0.20815741432550922,
      "orientation": 0.19554296019858103,
      "class": 0.17933870504414162,
      "two": 0.17482296883900492,
      "classification": 0.16871896430419908,
      "calculating": 0.1633398904340699,
      "used": 0.14328305054524829
    }
  },
  "cost_function": {
    "title": "Cost Function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241216202917.png",
      "model_optimisation",
      "model_parameters",
      "loss_versus_cost_function",
      "loss_function",
      "reward_function",
      "gradient_descent",
      "reinforcement_learning",
      "pasted_image_20241216202825.png"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "momentum",
      "loss_versus_cost_function",
      "loss_function",
      "optimising_a_logistic_regression_model",
      "gradient_descent",
      "model_parameters_tuning",
      "lbfgs",
      "optimisation_techniques"
    ],
    "summary": "The concept of a Cost Function is central to [[Model Optimisation]], particularly in training models. A cost function, also known as a loss function or...",
    "TFIDF_Score": {
      "function": 0.5411978678056916,
      "cost": 0.5247761191886481,
      "loss": 0.21698684125910347,
      "parameter": 0.194854522671244,
      "error": 0.17992795001528838,
      "surface": 0.1739279557283097,
      "model": 0.1198110922217264,
      "plotting": 0.11028772122701443,
      "finding": 0.09325655535646447,
      "minimum": 0.09128598537589308
    }
  },
  "cost-sensitive_analysis": {
    "title": "Cost-Sensitive Analysis",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "model_parameters"
    ],
    "inlinks": [
      "determining_threshold_values",
      "choosing_a_threshold",
      "imbalanced_datasets"
    ],
    "summary": "Cost-sensitive analysis in machine learning refers to the practice of incorporating the costs associated with different types of errors into the model training and evaluation...",
    "TFIDF_Score": {
      "cost": 0.7454395397274284,
      "positive": 0.211226909999401,
      "instance": 0.1890685077862361,
      "negative": 0.18482354624947586,
      "minimize": 0.15720640079809722,
      "false": 0.15603115842892568,
      "sensitive": 0.15603115842892568,
      "classified": 0.1415270578846098,
      "threshold": 0.1402131027594026,
      "matrix": 0.1341450778081772
    }
  },
  "covariance_structures": {
    "title": "Covariance Structures",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gaussian_mixture_models",
      "variance",
      "principal_component_analysis",
      "distributions",
      "clustering",
      "statistics",
      "covariance",
      "data_analysis"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "kmeans_vs_gmm"
    ],
    "summary": "A covariance structure in general refers to the way variability and relationships between variables (or dimensions) are modeled and described in a dataset. It specifies...",
    "TFIDF_Score": {
      "covariance": 0.5216093835026905,
      "cov": 0.26163235014619424,
      "x_p": 0.24203574761792718,
      "structure": 0.23908894898273478,
      "height": 0.21802695845516187,
      "x_1": 0.20398271710722368,
      "x_2": 0.20398271710722368,
      "text": 0.19832940812748281,
      "variable": 0.1943056088210925,
      "weight": 0.146000590966866
    }
  },
  "covariance_vs_correlation": {
    "title": "Covariance vs Correlation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "covariance"
    ],
    "inlinks": [],
    "summary": "[[Covariance]] provides a basic measure of how two variables move together, correlation offers a more [[interpretability|interpretable]] and standardized way to understand their relationship. Definition: -...",
    "TFIDF_Score": {
      "correlation": 0.39681073017543456,
      "covariance": 0.387480463360947,
      "two": 0.29551719598324494,
      "relationship": 0.27218138294832855,
      "variable": 0.2694365595129553,
      "sigma_x": 0.20137332991977275,
      "sigma_y": 0.20137332991977275,
      "strength": 0.19840536508771728,
      "formula": 0.17365688434767299,
      "standardized": 0.15224551509704662
    }
  },
  "covariance": {
    "title": "Covariance",
    "tags": [
      "statistics",
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "gaussian_mixture_models",
      "correlation"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "correlation",
      "covariance_structures",
      "covariance_vs_correlation"
    ],
    "summary": "In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between...",
    "TFIDF_Score": {
      "covariance": 0.653323713248864,
      "variable": 0.3785767072812545,
      "bar": 0.24162798348404194,
      "tends": 0.229388957976558,
      "decrease": 0.16895912787202447,
      "x_i": 0.14307522107662018,
      "increase": 0.1428416649424652,
      "y_i": 0.13322447016886838,
      "two": 0.12456653306725439,
      "zero": 0.11263941858134964
    }
  },
  "covering_index": {
    "title": "Covering Index",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "database_index"
    ],
    "summary": "Like an [[Database Index|Index]] but for partial indexes?",
    "TFIDF_Score": {
      "index": 0.8843494783898868,
      "partial": 0.3800829523644611,
      "database": 0.2212803757757335,
      "like": 0.15651819284032187
    }
  },
  "cron_jobs": {
    "title": "Cron jobs",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "cross_entropy": {
    "title": "Cross Entropy",
    "tags": [
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "categorical_data",
      "loss_function",
      "cross_entropy_net.py",
      "cross_entropy_single.py",
      "classification",
      "cross_entropy.py",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "gini_impurity_vs_cross_entropy",
      "loss_function",
      "decision_tree",
      "language_model_output_optimisation",
      "cross_entropy.py",
      "neural_scaling_laws"
    ],
    "summary": "Cross entropy is a [[Loss function]] used in [[Classification]] tasks, particularly for [[categorical data]]. The cross entropy loss function is particularly effective for multi-class classification...",
    "TFIDF_Score": {
      "class": 0.40243843603403795,
      "probability": 0.3975302658494989,
      "entropy": 0.3584753758361982,
      "cross": 0.2964282212708906,
      "loss": 0.25765134336566226,
      "true": 0.25272739734033633,
      "distribution": 0.2103375537588132,
      "label": 0.19540068048025822,
      "predicted": 0.1578285909348682,
      "model": 0.14226433585515846
    }
  },
  "cross_validation": {
    "title": "Cross Validation",
    "tags": [
      "#evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "kfold_cross_validation.py",
      "ml_tools",
      "model_optimisation",
      "overfitting",
      "bias_and_variance",
      "time_series",
      "data_leakage",
      "model_validation",
      "model_evaluation",
      "hyperparameter"
    ],
    "inlinks": [
      "model_optimisation",
      "overfitting",
      "decision_tree",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "model_selection",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "kaggle_abalone_regression_example",
      "train-dev-test_sets",
      "hyperparameter_tuning",
      "model_evaluation"
    ],
    "summary": "Cross-validation is a statistical technique used in machine learning to ==assess how well a model will generalize== to an independent dataset. It is a crucial...",
    "TFIDF_Score": {
      "fold": 0.5746438926802074,
      "validation": 0.4119119273298051,
      "cross": 0.24445484650043714,
      "model": 0.22397613920343368,
      "training": 0.18256621196969194,
      "performance": 0.1522559194424353,
      "set": 0.1410423598619996,
      "data": 0.13313562343003657,
      "time": 0.12764377596630513,
      "estimate": 0.10755165270843431
    }
  },
  "crosstab": {
    "title": "Crosstab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "groupby_vs_crosstab"
    ],
    "summary": "Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables. Here's an example: ```python Sample DataFrame...",
    "TFIDF_Score": {
      "subcategory": 0.6658640201469159,
      "category": 0.3987296540298782,
      "crosstab": 0.3471299830787549,
      "tabulation": 0.26634560805876634,
      "de_tools": 0.17163014196710788,
      "dataframe": 0.16714068755607936,
      "cross": 0.14766990097716065,
      "reshaping": 0.10929745473335799,
      "frequency": 0.09608734917819452,
      "computing": 0.08662617865971178
    }
  },
  "cross_entropy.py": {
    "title": "Cross_Entropy.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "mean_squared_error",
      "cross_entropy"
    ],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py Generalized Script Description: Dataset: Uses the Iris dataset from sklearn to classify flower species. Preprocessing: One-hot encodes the target labels and splits the data...",
    "TFIDF_Score": {
      "entropy": 0.37900646140174016,
      "loss": 0.35660670396158584,
      "cross": 0.31340565845400964,
      "model": 0.24612920331542765,
      "mse": 0.16091148664332336,
      "performs": 0.1532623482396758,
      "histogram": 0.15276094149090372,
      "value": 0.14312836685626162,
      "show": 0.13258280879776588,
      "dataset": 0.12866650088678805
    }
  },
  "cross_entropy_single.py": {
    "title": "Cross_Entropy_Single.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py Example Let's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class...",
    "TFIDF_Score": {
      "probability": 0.38083784375043267,
      "class": 0.37697236457010014,
      "loss": 0.3231261577837351,
      "true": 0.28173415990950823,
      "entropy": 0.274738304776561,
      "predicted": 0.24192210925267366,
      "cross": 0.22718488490297054,
      "log": 0.21250614748370264,
      "3567": 0.20488161741437497,
      "label": 0.18719574385725382
    }
  },
  "crud": {
    "title": "CRUD",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "rest_api",
      "row-based_storage",
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": "Create,Read,Update,Delete.",
    "TFIDF_Score": {
      "delete": 0.6299440604547798,
      "read": 0.5185363229571126,
      "update": 0.42374828567172446,
      "create": 0.393354741756263
    }
  },
  "cryptography": {
    "title": "Cryptography",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "javascript",
      "node.js",
      "security",
      "hash"
    ],
    "inlinks": [],
    "summary": "Cryptography is the foundation of digital [[Security]], enabling privacy and secure communication over the internet. Examples are implemented in [[Node.JS]] (using crypto module) and are...",
    "TFIDF_Score": {
      "const": 0.6941039434590192,
      "password": 0.24090519913778863,
      "hex": 0.20635522643376245,
      "hash": 0.1903406935416604,
      "crypto": 0.17923637152379893,
      "console": 0.17191340947314213,
      "log": 0.13633145621808726,
      "javascript": 0.12045259956889431,
      "privatekey": 0.11949091434919927,
      "publickey": 0.11949091434919927
    }
  },
  "current_challenges_within_the_energy_sector": {
    "title": "Current challenges within the energy sector",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "current_challenges_within_the_energy_sector"
    ],
    "inlinks": [
      "current_challenges_within_the_energy_sector"
    ],
    "summary": "[[Current challenges within the energy sector]] related to reinforcement learning and that can be progressed with recent technological advances",
    "TFIDF_Score": {
      "progressed": 0.40306829251100046,
      "advance": 0.3630858446770192,
      "technological": 0.3630858446770192,
      "sector": 0.33969761200710036,
      "recent": 0.2997151641731191,
      "energy": 0.2868436909129922,
      "reinforcement": 0.2868436909129922,
      "challenge": 0.24496984480563616,
      "current": 0.2247693004496282,
      "within": 0.20226173342352385
    }
  },
  "dagster": {
    "title": "dagster",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "declarative",
      "data_lifecycle_management"
    ],
    "inlinks": [
      "directed_acyclic_graph_(dag)",
      "etl",
      "data_management"
    ],
    "summary": "Dagster is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [[Data Lifecycle Management]] lifecycle, with integrated lineage and observability, a [[declarative]]...",
    "TFIDF_Score": {
      "lifecycle": 0.37807687618404046,
      "data": 0.26968180396477004,
      "aware": 0.24560786517617506,
      "glass": 0.24560786517617506,
      "pane": 0.24560786517617506,
      "testability": 0.24560786517617506,
      "orchestrator": 0.22124473902738748,
      "dagster": 0.2069932238300077,
      "asset": 0.1968816128785999,
      "declarative": 0.1968816128785999
    }
  },
  "dash": {
    "title": "Dash",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "plotly",
      "clustering_dashboard.py",
      "ml_tools"
    ],
    "inlinks": [
      "dashboarding"
    ],
    "summary": "Dash is an open-source framework for building interactive web applications using Python. It is particularly well-suited for data visualization and dashboard creation. Dash integrates with...",
    "TFIDF_Score": {
      "dash": 0.7080279190943085,
      "interactive": 0.33473537608192017,
      "plotly": 0.23600930636476952,
      "html": 0.23175252466802232,
      "component": 0.20020021710854669,
      "dcc": 0.16703162863485613,
      "visualization": 0.1381228661663087,
      "graph": 0.10002111764940924,
      "creating": 0.09156783423888568,
      "__name__": 0.08351581431742806
    }
  },
  "dashboarding": {
    "title": "Dashboarding",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "streamlit.io",
      "dash"
    ],
    "inlinks": [
      "react"
    ],
    "summary": "[[Dash]] [[Streamlit.io]]",
    "TFIDF_Score": {
      "dash": 0.7071067811865476,
      "streamlit": 0.7071067811865476
    }
  },
  "data_ai_education_at_work": {
    "title": "Data AI Education at Work",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Introduction Organizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are...",
    "TFIDF_Score": {
      "staff": 0.3213367061872319,
      "partner": 0.20467859840447017,
      "employee": 0.199801595588581,
      "training": 0.1977504207196737,
      "productivity": 0.1843754598656932,
      "culture": 0.17249888517356213,
      "skill": 0.17249888517356213,
      "learning": 0.17157434786221504,
      "encourage": 0.16407232132691627,
      "strategy": 0.15895131139944615
    }
  },
  "data_analysis_portal": {
    "title": "Data Analysis Portal",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "data_analysis": {
    "title": "Data Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "eda",
      "confidence_interval",
      "data_visualisation",
      "hypothesis_testing",
      "statistics",
      "data_analyst"
    ],
    "inlinks": [
      "eda",
      "covariance_structures",
      "ds_&_ml_portal",
      "statistical_assumptions",
      "data_lifecycle_management",
      "alternatives_to_batch_processing",
      "duckdb",
      "data_roles",
      "fact_table",
      "data_analyst"
    ],
    "summary": "What is it? Usually done with a [[Data Analyst]].After processing, data is analyzed to extract meaningful insights and derive value from the data. Types of...",
    "TFIDF_Score": {
      "data": 0.3404961966102651,
      "happened": 0.22469773372024676,
      "involves": 0.21949699305460507,
      "often": 0.2047345945978162,
      "past": 0.18034416472475234,
      "historical": 0.16168682503391013,
      "prediction": 0.16011826529576598,
      "hypothesis": 0.15371192461051458,
      "future": 0.14844968928770114,
      "trend": 0.14844968928770114
    }
  },
  "data_analyst": {
    "title": "Data Analyst",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "eda",
      "data_collection",
      "documentation_&_meetings",
      "data_visualisation",
      "distributions",
      "hypothesis_testing",
      "statistics",
      "data_analysis"
    ],
    "inlinks": [
      "data_roles",
      "data_analysis"
    ],
    "summary": "Summary: - Gathers and processes data to generate reports. - Communicates insights and findings to management - Conducts [[Data Analysis]]. Key responsibilities of a data...",
    "TFIDF_Score": {
      "data": 0.34257900567895594,
      "analysis": 0.34005356833585826,
      "finding": 0.3253517208293795,
      "gather": 0.175709724974803,
      "objective": 0.15611392054324336,
      "documentation": 0.15060682821513052,
      "tool": 0.15036652125228617,
      "technique": 0.14186183370061117,
      "process": 0.12620763474831756,
      "insight": 0.1173998572309611
    }
  },
  "data_architect": {
    "title": "Data Architect",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Architect - Designs and manages the data infrastructure. - Ensures data is stored, organized, and accessible for analysis.",
    "TFIDF_Score": {
      "architect": 0.43569724886887173,
      "manages": 0.3795662331348196,
      "organized": 0.348983427519671,
      "data": 0.3186506144480108,
      "infrastructure": 0.3146210297209246,
      "accessible": 0.3116751358094572,
      "stored": 0.27460760280884683,
      "design": 0.27293961705645065,
      "ensures": 0.2416540839920234,
      "analysis": 0.19580573588790062
    }
  },
  "data_archive_graph_analysis": {
    "title": "Data Archive Graph Analysis",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "graph_analysis_plugin",
      "dataview",
      "graph_view"
    ],
    "inlinks": [],
    "summary": "Use the following to [[Dataview]] [[Graph View]] Check out [[Graph Analysis Plugin]] Convert Dataview to CSV",
    "TFIDF_Score": {
      "dataview": 0.6611940948678422,
      "graph": 0.4203226561518966,
      "plugin": 0.3305970474339211,
      "csv": 0.2532272770596785,
      "convert": 0.2221356638349743,
      "view": 0.21494833866689697,
      "check": 0.2101613280759483,
      "following": 0.20177093710002517,
      "analysis": 0.14207939237677641,
      "use": 0.11009033545292454
    }
  },
  "data_asset": {
    "title": "data asset",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_product"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "data_cleansing": {
    "title": "Data Cleansing",
    "tags": [
      "data_transformation",
      "data_cleaning",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "deleting_rows_or_filling_them_with_the_mean_is_not_always_best_",
      "handling_different_distributions",
      "data_selection",
      "de_tools",
      "data_quality",
      "handling_missing_data",
      "standardised/outliers"
    ],
    "inlinks": [
      "anomaly_detection",
      "preprocessing",
      "why_use_er_diagrams",
      "pandas_stack",
      "fuzzywuzzy",
      "boxplot",
      "data_transformation",
      "data_analyst"
    ],
    "summary": "Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [[Data Quality]] for analysis. Involves: [[standardised/Outliers|Handling Outliers]] [[Handling...",
    "TFIDF_Score": {
      "de_tools": 0.36378451712827936,
      "handling": 0.29249566100470265,
      "cleaning": 0.24971507621324762,
      "outlier": 0.22068446639910558,
      "data": 0.20662568629804726,
      "rhyslwells": 0.18886001492634708,
      "dataframe_cleaing": 0.1881806371662177,
      "blob": 0.18642980666063294,
      "github": 0.17973507207747633,
      "exploration": 0.1783539584769055
    }
  },
  "data_collection": {
    "title": "Data Collection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "imbalanced_datasets",
      "data_quality"
    ],
    "inlinks": [
      "preprocessing",
      "business_observability",
      "data_analyst"
    ],
    "summary": "Determine the [[Data Quality]] and quantity of data required and get it. [[Imbalanced Datasets]]",
    "TFIDF_Score": {
      "quantity": 0.46781079869349274,
      "imbalanced": 0.4216558743300317,
      "required": 0.3722516817660669,
      "get": 0.3429847691948455,
      "determine": 0.3350718932145523,
      "quality": 0.29855231016040423,
      "datasets": 0.27291015776080885,
      "data": 0.2669511267215374
    }
  },
  "data_contract": {
    "title": "Data Contract",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_quality",
      "api",
      "dbt",
      "data_contract",
      "pasted_image_20250312163351.png"
    ],
    "inlinks": [
      "data_contract",
      "data_quality"
    ],
    "summary": "[[Data Contract]] pattern to handle schema changes Pattern to apply to organisation using tools they have. Tooling: - [[dbt]] Data contracts help prevent preventable data...",
    "TFIDF_Score": {
      "contract": 0.6620232279413873,
      "data": 0.3429579304083609,
      "quality": 0.18049725152266582,
      "agreed": 0.17306996574361433,
      "interface": 0.17012761866287307,
      "rule": 0.16385407547980269,
      "product": 0.1594673143160328,
      "business": 0.14634533323317223,
      "schema": 0.11166486506158782,
      "20250312163351": 0.09186552436598094
    }
  },
  "data_distribution": {
    "title": "Data Distribution",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_management",
      "data_lifecycle_management"
    ],
    "summary": "Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. This can involve supplying data to -...",
    "TFIDF_Score": {
      "supplying": 0.3163892906938782,
      "data": 0.2779208543936676,
      "downstream": 0.25966671220727294,
      "application": 0.25904195288307497,
      "making": 0.24740747446370676,
      "system": 0.24550096272019797,
      "analyzed": 0.243517190701166,
      "process": 0.22183911364699585,
      "involve": 0.20992376534704543,
      "processed": 0.20781743294067048
    }
  },
  "data_drift": {
    "title": "Data Drift",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "performance_drift"
    ],
    "inlinks": [
      "model_observability",
      "performance_drift"
    ],
    "summary": "Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead...",
    "TFIDF_Score": {
      "drift": 0.7480565955817732,
      "data": 0.265156778524178,
      "model": 0.20291490338598492,
      "production": 0.15568401751958405,
      "input": 0.11452622989362682,
      "divergence": 0.10711582004419308,
      "training": 0.10525384037696049,
      "statistical": 0.10393978078083808,
      "distribution": 0.09000279123265537,
      "change": 0.08618962948137912
    }
  },
  "data_engineer": {
    "title": "Data Engineer",
    "tags": [
      "career",
      "field"
    ],
    "aliases": [
      "Data Engineering"
    ],
    "outlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "data_management",
      "documentation_&_meetings",
      "data_engineering_portal",
      "data_roles",
      "continuous_integration"
    ],
    "inlinks": [
      "data_engineering",
      "data_roles",
      "databricks",
      "apache_spark"
    ],
    "summary": "The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on - automating...",
    "TFIDF_Score": {
      "data": 0.3931379342965132,
      "engineer": 0.28877608885853795,
      "infrastructure": 0.20549985856211175,
      "flow": 0.19995689666608163,
      "pipeline": 0.1852442555436292,
      "responsibility": 0.15923158270816126,
      "interact": 0.1344867337262759,
      "role": 0.13330459777738776,
      "documentation": 0.13216708153037324,
      "analysis": 0.1278937109394937
    }
  },
  "data_engineering_portal": {
    "title": "Data Engineering Portal",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "spreadsheets_vs_databases",
      "structured_data",
      "mysql",
      "components_of_the_database",
      "database_management_system_(dbms)",
      "database_techniques",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "postgresql"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] They facilitate efficient CRUD.md operations and transactional processing (OLTP.md), structured...",
    "TFIDF_Score": {
      "database": 0.5012107182269826,
      "spreadsheet": 0.29786910719363807,
      "table": 0.23538808264707664,
      "crud": 0.21326256510086716,
      "efficient": 0.18801930721848947,
      "organized": 0.18257925829725957,
      "schema": 0.15379236011385128,
      "flexibility": 0.1464194048632549,
      "structured": 0.14366771740414988,
      "data": 0.13892491795235867
    }
  },
  "data_engineering_tools": {
    "title": "Data Engineering Tools",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "amazon_s3",
      "dbt",
      "sql",
      "data_ingestion",
      "apache_kafka",
      "azure",
      "data_storage",
      "cloud"
    ],
    "inlinks": [
      "data_storage",
      "data_ingestion",
      "data_engineer"
    ],
    "summary": "Snowflake: [[Cloud]]-based data warehousing for scalable storage and processing. Microsoft SQL Server: [[SQL]]-based relational database management. [[Azure]] SQL Database: Managed relational database service on Azure....",
    "TFIDF_Score": {
      "sql": 0.32407275455708157,
      "storage": 0.3114771658217508,
      "azure": 0.269488206616246,
      "relational": 0.2456545303294255,
      "data": 0.2387648037105765,
      "database": 0.21269412074080085,
      "service": 0.2045135679534547,
      "amazon": 0.18960590490117987,
      "ingestion": 0.1826673718316058,
      "snowflake": 0.17432878057643134
    }
  },
  "data_engineering": {
    "title": "Data Engineering",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "data_management",
      "apache_airflow",
      "data_engineer",
      "prefect"
    ],
    "inlinks": [
      "data_storage",
      "data_transformation_in_data_engineering",
      "normalisation"
    ],
    "summary": "The definition from the Fundamentals of Data Engineering, as it’s one of the most recent and complete: Data engineering is the development, implementation, and maintenance...",
    "TFIDF_Score": {
      "engineering": 0.5120275651790382,
      "data": 0.36186263582645856,
      "tool": 0.18112260305367955,
      "engineer": 0.17836786122684603,
      "software": 0.14508762925335864,
      "downstream": 0.14235583439292795,
      "business": 0.1381583350390006,
      "intelligence": 0.11629659289158534,
      "role": 0.10978404798561345,
      "process": 0.09121334379682904
    }
  },
  "data_governance": {
    "title": "Data Governance",
    "tags": [
      "business",
      "#data_governance"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability"
    ],
    "inlinks": [
      "data_roles",
      "master_data_management",
      "data_principles",
      "data_steward",
      "ai_governance",
      "data_storage"
    ],
    "summary": "Data governance is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization...",
    "TFIDF_Score": {
      "governance": 0.4273104630419786,
      "data": 0.33098557978965715,
      "policy": 0.268149121313586,
      "standard": 0.2194636794067769,
      "organization": 0.21802564437963268,
      "tandem": 0.18839948705543724,
      "quality": 0.1850835220093688,
      "ensure": 0.1765515903479784,
      "adheres": 0.16971115854361993,
      "establishes": 0.16369486054918142
    }
  },
  "data_hierarchy_of_needs": {
    "title": "Data Hierarchy of Needs",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241005170237.png"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20241005170237.png|500]] The Data Hierarchy of Needs is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow’s...",
    "TFIDF_Score": {
      "data": 0.44228884174014166,
      "advanced": 0.2361910534513831,
      "hierarchy": 0.23332467939619522,
      "insight": 0.19704079881923783,
      "need": 0.1692413519377526,
      "capability": 0.16198710694260673,
      "analytics": 0.15854330972016356,
      "organization": 0.15538289780253292,
      "20241005170237": 0.13426887615207217,
      "maslow": 0.13426887615207217
    }
  },
  "data_ingestion": {
    "title": "Data Ingestion",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "scalability",
      "data_engineering_tools",
      "data_quality",
      "api",
      "data_ingestion",
      "database",
      "data_streaming",
      "latency",
      "business_intelligence",
      "machine_learning"
    ],
    "inlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "data_lifecycle_management",
      "data_ingestion",
      "data_storage",
      "data_warehouse"
    ],
    "summary": "Data ingestion is the process of collecting and importing raw data from various sources ([[Database]], [[API]], [[Data Streaming]] services) into a system for processing and...",
    "TFIDF_Score": {
      "ingestion": 0.46873004606414737,
      "data": 0.4424894982473787,
      "raw": 0.1929167761103674,
      "data_collection": 0.1549959126040897,
      "realtime": 0.1460021997910881,
      "source": 0.14161462544597073,
      "importing": 0.13962105899909122,
      "ingested": 0.13962105899909122,
      "various": 0.13644489983727776,
      "processing": 0.13513359894780225
    }
  },
  "data_integration": {
    "title": "Data Integration",
    "tags": [
      "data_storage",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "data_virtualization",
      "single_source_of_truth"
    ],
    "inlinks": [
      "apache_kafka",
      "data_virtualization"
    ],
    "summary": "Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [[Single Source of Truth]]....",
    "TFIDF_Score": {
      "integration": 0.4223399120190046,
      "application": 0.29088155326423537,
      "system": 0.24121667551522458,
      "data": 0.23406068663048807,
      "multiple": 0.18714479237103201,
      "manual": 0.1786550669059881,
      "report": 0.1733256054733394,
      "source": 0.16230268959345315,
      "outdated": 0.16001788888736188,
      "point": 0.15637773397457788
    }
  },
  "data_integrity": {
    "title": "Data Integrity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "checksum",
      "hash",
      "data_integrity",
      "database"
    ],
    "inlinks": [
      "data_pipeline",
      "performance_dimensions",
      "anomaly_detection",
      "transaction",
      "hash",
      "soft_deletion",
      "data_lifecycle_management",
      "data_integrity",
      "apache_kafka",
      "data_principles",
      "relating_tables_together"
    ],
    "summary": "Data integrity refers to the - accuracy, - consistency, and - reliability of data throughout its lifecycle. It ensures that data remains ==unaltered== and ==trustworthy==,...",
    "TFIDF_Score": {
      "integrity": 0.45500083159383087,
      "data": 0.3670973537561125,
      "prevent": 0.18070186263226828,
      "access": 0.1597817090772938,
      "loss": 0.1597817090772938,
      "unaltered": 0.15196696801114049,
      "checksum": 0.14314901117349202,
      "data_quality": 0.14314901117349202,
      "trustworthy": 0.14314901117349202,
      "backup": 0.13689257123053067
    }
  },
  "data_lake": {
    "title": "Data Lake",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "structured_data",
      "unstructured_data"
    ],
    "inlinks": [
      "fabric",
      "data_storage",
      "databricks",
      "event_driven_events",
      "hadoop"
    ],
    "summary": "A Data Lake is a storage system with vast amounts of [[unstructured data]] and [[structured data]], stored as-is, without a specific purpose in mind, that...",
    "TFIDF_Score": {
      "data": 0.4532199527821091,
      "lake": 0.34972345035693886,
      "raw": 0.3210919475425538,
      "store": 0.18856566952229736,
      "unstructured": 0.1812061920294145,
      "storage": 0.17737245715355712,
      "format": 0.1749310592016855,
      "apache": 0.15925045219755282,
      "analytics": 0.15230783150932292,
      "structured": 0.1464664264741354
    }
  },
  "data_lakehouse": {
    "title": "Data Lakehouse",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Lakehouse"
    ],
    "outlinks": [
      "database_schema",
      "data_warehouse",
      "acid_transaction",
      "data_management"
    ],
    "inlinks": [
      "cloud_providers",
      "single_source_of_truth"
    ],
    "summary": "A Data Lakehouse open [[Data Management]] architecture that combines the flexibility, cost-efficiency, and scale of Data Lake with the data management and ACID transactions of...",
    "TFIDF_Score": {
      "data": 0.5659084675343836,
      "lakehouses": 0.2973409529151236,
      "lake": 0.2687253023274493,
      "lakehouse": 0.21529245551448706,
      "acid": 0.17223396441158964,
      "warehouse": 0.1505939325157698,
      "unified": 0.1451280136012826,
      "organization": 0.14337431299352474,
      "platform": 0.14337431299352474,
      "transaction": 0.13290756287377994
    }
  },
  "data_leakage": {
    "title": "Data Leakage",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cross_validation",
      "data_selection_in_ml",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data"
    ],
    "summary": "Data Leakage refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly...",
    "TFIDF_Score": {
      "data": 0.3061191040180909,
      "unintentional": 0.27879248255602973,
      "optimistic": 0.26261541388108117,
      "misleading": 0.24223471766003568,
      "leakage": 0.23496053232028866,
      "training": 0.23087624548383423,
      "information": 0.23021404150934394,
      "overly": 0.22348271943444475,
      "inclusion": 0.21878346364534013,
      "generalization": 0.18312231087495462
    }
  },
  "data_lifecycle_management": {
    "title": "Data Lifecycle Management",
    "tags": [
      "data_management",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "performance_dimensions",
      "preprocessing",
      "data_distribution",
      "data_ingestion",
      "data_visualisation",
      "data_integrity",
      "data_storage",
      "data_analysis",
      "software_development_life_cycle"
    ],
    "inlinks": [
      "data_principles",
      "data_lineage",
      "dagster"
    ],
    "summary": "This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. Used for maintaining [[data integrity]], optimizing...",
    "TFIDF_Score": {
      "data": 0.4289156535794127,
      "ingestion": 0.3281425652323488,
      "downstream": 0.17810844877191756,
      "timely": 0.17810844877191756,
      "cycle": 0.16703130222736765,
      "life": 0.16703130222736765,
      "lifecycle": 0.16703130222736765,
      "visualisation": 0.16136899643000827,
      "based": 0.154187670902586,
      "process": 0.15216205447644934
    }
  },
  "data_lineage": {
    "title": "data lineage",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "data_lifecycle_management"
    ],
    "inlinks": [
      "declarative",
      "model_observability",
      "dbt"
    ],
    "summary": "Data lineage uncovers the [[Data Lifecycle Management]] life cycle of data. It aims to show the complete data flow from start to finish. Data lineage...",
    "TFIDF_Score": {
      "data": 0.43366401400746,
      "lineage": 0.3799809936166272,
      "flow": 0.31247300010651075,
      "uncovers": 0.24684482582392098,
      "recording": 0.2325215353865371,
      "finish": 0.22235899909190693,
      "changed": 0.19371241821713922,
      "consumption": 0.19371241821713922,
      "complete": 0.1899904968083136,
      "cycle": 0.1899904968083136
    }
  },
  "data_literacy": {
    "title": "data literacy",
    "tags": [
      "#business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Data literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This...",
    "TFIDF_Score": {
      "literacy": 0.3848888199243765,
      "data": 0.3140534413894997,
      "employee": 0.2991464983006355,
      "decision": 0.22554335711320933,
      "literate": 0.2042989557028964,
      "skill": 0.1721789301646054,
      "informed": 0.14740656124529322,
      "program": 0.14538942306380837,
      "read": 0.12714108144771036,
      "driven": 0.1241652702590745
    }
  },
  "data_management": {
    "title": "Data Management",
    "tags": [
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "dagster",
      "data_quality",
      "data_management",
      "apache_airflow",
      "data_distribution",
      "master_data_management",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "digital_twin",
      "data_pipeline",
      "data_management",
      "difference_between_snowflake_to_hadoop",
      "spreadsheets_vs_databases",
      "data_engineering",
      "master_data_management",
      "data_storage",
      "data_principles",
      "data_steward",
      "data_engineer",
      "data_lakehouse",
      "data_roles",
      "database_schema"
    ],
    "summary": "Data management involves overseeing processes to maintain data integrity and quality. It includes: Responsibility: Identifying accountable individuals or teams. Issue Resolution: Mechanisms for detecting and...",
    "TFIDF_Score": {
      "management": 0.44368409682768223,
      "data": 0.3672466456422174,
      "issue": 0.1881458691263344,
      "accountable": 0.18581294140647361,
      "overseeing": 0.18581294140647361,
      "quality": 0.1825425014045339,
      "data_quality": 0.17503105558850646,
      "prefect": 0.16738118585859663,
      "related": 0.16410803258668627,
      "dagster": 0.15659930004062944
    }
  },
  "data_mining_-_crisp": {
    "title": "Data Mining - CRISP",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [],
    "summary": "CRISP-DM stands for Cross-Industry Standard Process for Data Mining, a widely adopted framework that provides a structured approach to planning, organizing, and conducting data mining...",
    "TFIDF_Score": {
      "mining": 0.3365280635874493,
      "understanding": 0.2948790108564015,
      "business": 0.2721710070016554,
      "data": 0.24762748462516418,
      "crisp": 0.205020263151842,
      "modeling": 0.18690548095499246,
      "project": 0.1803245596465284,
      "preparation": 0.155002728421483,
      "industry": 0.1479270017452648,
      "problem": 0.14573567205858196
    }
  },
  "data_modelling": {
    "title": "Data Modelling",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "physical_model",
      "data_modelling",
      "er_diagrams",
      "logical_model",
      "conceptual_model"
    ],
    "inlinks": [
      "data_modelling",
      "database_schema"
    ],
    "summary": "Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. This helps in organizing...",
    "TFIDF_Score": {
      "data": 0.3720994074750368,
      "modelling": 0.33342552695003735,
      "organizes": 0.23711813420623318,
      "hierarchical": 0.21372369872065075,
      "entity": 0.19332746818323823,
      "modeling": 0.18723667076634348,
      "model": 0.17885389894472178,
      "object": 0.17281044802755827,
      "database_design": 0.15403766378084846,
      "logically": 0.15403766378084846
    }
  },
  "data_observability": {
    "title": "Data Observability",
    "tags": [
      "#data_orchestration",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability",
      "standardised/outliers",
      "dbt",
      "data_quality"
    ],
    "inlinks": [
      "data_governance",
      "declarative",
      "data_quality",
      "data_observability",
      "performance_drift",
      "prevention_is_better_than_the_cure",
      "guardrails",
      "model_observability"
    ],
    "summary": "Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [[Data Quality]], reliability, and availability. It covers various...",
    "TFIDF_Score": {
      "metadata": 0.4686807665460052,
      "data": 0.3416519667189151,
      "pipeline": 0.2365079074122915,
      "dbt": 0.23094772233307562,
      "observability": 0.20503002360619013,
      "monitoring": 0.18396855011184476,
      "quality": 0.1415170734134904,
      "schema": 0.1400795615296676,
      "test": 0.13758740987999143,
      "infrastructure": 0.11244383733731639
    }
  },
  "data_orchestration": {
    "title": "Data Orchestration",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [
      "#data_orchestration"
    ],
    "outlinks": [
      "data_pipeline_to_data_products"
    ],
    "inlinks": [],
    "summary": "Data orchestration refers to the process of managing and coordinating the flow of data across various systems and environments, particularly in complex and heterogeneous cloud...",
    "TFIDF_Score": {
      "data": 0.3657241166492982,
      "orchestration": 0.32155631573597204,
      "logic": 0.25995315070223607,
      "orchestrator": 0.25003088687766417,
      "cloud": 0.22492395284471106,
      "time": 0.1785656423446689,
      "heterogeneous": 0.1666872579184428,
      "based": 0.1643392741337093,
      "legacy": 0.15186798278703084,
      "system": 0.14358304021206095
    }
  },
  "data_pipeline_to_data_products": {
    "title": "Data Pipeline to Data Products",
    "tags": [
      "#question",
      "data_orchestration",
      "anomaly_detection",
      "data_pipeline",
      "data_products"
    ],
    "aliases": [],
    "outlinks": [
      "data_product",
      "data_pipeline"
    ],
    "inlinks": [
      "data_pipeline",
      "data_orchestration"
    ],
    "summary": "The journey from [[Data Pipeline]] to [[Data Product]] involves transforming raw data into valuable insights or applications that can be used to drive business decisions....",
    "TFIDF_Score": {
      "pipeline": 0.38296170670789503,
      "data": 0.36880971326030976,
      "product": 0.3239213641397812,
      "recommendation": 0.2655933110251887,
      "customer": 0.2383105434478086,
      "deploy": 0.1991949832688915,
      "business": 0.14863353088907566,
      "ingestion": 0.14107921263992537,
      "website": 0.13463908117577608,
      "monitor": 0.1279278348946272
    }
  },
  "data_pipeline": {
    "title": "Data Pipeline",
    "tags": [
      "data_pipeline"
    ],
    "aliases": [
      "ETL Pipeline"
    ],
    "outlinks": [
      "data_pipeline",
      "data_management",
      "data_pipeline_to_data_products",
      "preprocessing",
      "data_ingestion",
      "data_integrity",
      "data_transformation",
      "data_storage"
    ],
    "inlinks": [
      "data_pipeline",
      "data_management",
      "data_pipeline_to_data_products",
      "data_engineering",
      "data_ingestion",
      "data_engineer"
    ],
    "summary": "A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can...",
    "TFIDF_Score": {
      "pipeline": 0.601499033569069,
      "data": 0.45054426402283393,
      "automate": 0.16447740814850922,
      "preprocessing": 0.13961428658188046,
      "workflow": 0.13553111783806632,
      "transformation": 0.11700190376911178,
      "cohesive": 0.10736549816998245,
      "ensure": 0.10681137546964577,
      "data_workflow": 0.10267300476230874,
      "movement": 0.10267300476230874
    }
  },
  "data_principles": {
    "title": "Data Principles",
    "tags": [
      "data_quality",
      "data_governance"
    ],
    "aliases": null,
    "outlinks": [
      "security",
      "data_governance",
      "performance_dimensions",
      "data_management",
      "data_quality",
      "documentation_&_meetings",
      "data_lifecycle_management",
      "data_integrity"
    ],
    "inlinks": [],
    "summary": "Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically. [[Data Quality]] Ensure data is accurate, complete, reliable, and...",
    "TFIDF_Score": {
      "data": 0.5443716248659822,
      "security": 0.26609455686890404,
      "privacy": 0.24086020373973496,
      "ensure": 0.1759843694362033,
      "sharing": 0.16756893353328361,
      "ethically": 0.15023536119366326,
      "regulation": 0.13053515635589868,
      "access": 0.11847079215403063,
      "transparency": 0.11789777250307525,
      "compliance": 0.1156325266940008
    }
  },
  "data_product": {
    "title": "Data Product",
    "tags": [
      "data_management",
      "business",
      "business_intelligence"
    ],
    "aliases": [],
    "outlinks": [
      "data_asset"
    ],
    "inlinks": [
      "data_pipeline_to_data_products"
    ],
    "summary": "A data product is \"a product that facilitates an end goal through data\". Delivering the final output, which could be dashboards, reports, or machine learning...",
    "TFIDF_Score": {
      "product": 0.555860174317628,
      "dashboard": 0.390691567085415,
      "data": 0.2531560485951079,
      "report": 0.24995469356632358,
      "delivering": 0.19213110827208732,
      "thinking": 0.1669371582020522,
      "essentially": 0.1576856570850987,
      "asset": 0.1540141332755285,
      "whereas": 0.1540141332755285,
      "machine": 0.15386819702724125
    }
  },
  "data_quality": {
    "title": "Data Quality",
    "tags": [
      "#data_quality"
    ],
    "aliases": [],
    "outlinks": [
      "change_management",
      "data_observability",
      "data_contract",
      "prevention_is_better_than_the_cure"
    ],
    "inlinks": [
      "eda",
      "declarative",
      "determining_threshold_values",
      "data_principles",
      "neural_network_classification",
      "data_management",
      "data_collection",
      "data_selection_in_ml",
      "data_observability",
      "master_data_management",
      "prevention_is_better_than_the_cure",
      "data_cleansing",
      "performance_dimensions",
      "benefits_of_data_transformation",
      "machine_learning_algorithms",
      "why_use_er_diagrams",
      "data_ingestion",
      "data_contract",
      "ds_&_ml_portal",
      "data_validation",
      "data_roles"
    ],
    "summary": "Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis. Definition: Data quality refers...",
    "TFIDF_Score": {
      "data": 0.39224870460917316,
      "quality": 0.3509459079096565,
      "garbage": 0.33650482623641903,
      "observability": 0.2542253243641254,
      "poor": 0.23019726823736095,
      "cure": 0.17861673561365757,
      "management": 0.17060039934612123,
      "term": 0.16676436586226934,
      "change": 0.16150134355900791,
      "contract": 0.16089880928048123
    }
  },
  "data_reduction": {
    "title": "Data Reduction",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "dimensionality_reduction",
      "variance",
      "sampling"
    ],
    "inlinks": [
      "preprocessing"
    ],
    "summary": "Reducing the volume of data through techniques: [[Dimensionality Reduction]] [[Sampling]]: Use subsets of data for training to speed up the process and address issues like...",
    "TFIDF_Score": {
      "redundant": 0.27019521762606685,
      "imbalanced": 0.2435373468207569,
      "feature": 0.23838955840599704,
      "sampling": 0.23794955799671347,
      "zero": 0.23292153338132587,
      "data": 0.2312759281577895,
      "volume": 0.23058444604348893,
      "remove": 0.22029591922073008,
      "subset": 0.22029591922073008,
      "address": 0.21846886994873968
    }
  },
  "data_roles": {
    "title": "Data Roles",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_scientist",
      "data_architect",
      "data_governance",
      "data_management",
      "data_quality",
      "ml_engineer",
      "data_engineer",
      "data_steward",
      "data_analysis",
      "data_analyst"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. The team...",
    "TFIDF_Score": {
      "data": 0.5424815632928578,
      "team": 0.2405418957595603,
      "business": 0.20180750868056083,
      "engineer": 0.19540609163776762,
      "accessibility": 0.16014600854597125,
      "manages": 0.1491199313036217,
      "analyst": 0.14625479405578956,
      "governance": 0.1436629621484572,
      "responsibility": 0.1436629621484572,
      "ensures": 0.14240771672383165
    }
  },
  "data_science": {
    "title": "Data Science",
    "tags": [
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "statistics",
      "unstructured_data",
      "scientific_method"
    ],
    "inlinks": [],
    "summary": "A field that uses the [[Scientific Method]], algorithms, and systems to ==extract knowledge== and insights from structured and [[unstructured data]]. It combines techniques from [[statistics]],...",
    "TFIDF_Score": {
      "auto_examples": 0.26684818604365984,
      "scientific": 0.21900738478368656,
      "expertise": 0.2094101314169216,
      "computer": 0.20538659973774984,
      "org": 0.20174687260281746,
      "science": 0.19842406206435054,
      "scikit": 0.19842406206435054,
      "informed": 0.19253732033972643,
      "stable": 0.19253732033972643,
      "unstructured": 0.1874379927117795
    }
  },
  "data_scientist": {
    "title": "Data Scientist",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "business_intelligence"
    ],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Scientist - Utilizes [[Business Intelligence]] (BI) tools to analyze data. - Works with data lakes to extract insights. - Develops and deploys production Machine...",
    "TFIDF_Score": {
      "deploys": 0.35909164075671285,
      "develops": 0.3433972586160568,
      "scientist": 0.321277269361535,
      "utilizes": 0.27909619957915804,
      "lake": 0.2583932927532625,
      "intelligence": 0.25559500728308837,
      "data": 0.2511462896354008,
      "extract": 0.2434214987404756,
      "production": 0.23347501802856657,
      "analyze": 0.2191618389294584
    }
  },
  "data_selection_in_ml": {
    "title": "Data Selection in ML",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "model_optimisation",
      "data_quality",
      "correlation",
      "dimensionality_reduction",
      "distributions",
      "data_leakage",
      "multicollinearity",
      "data_transformation",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "data_selection"
    ],
    "summary": "When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[[Model Optimisation]] and the insights you can derive from it....",
    "TFIDF_Score": {
      "feature": 0.3176113357187157,
      "data": 0.261446775005777,
      "model": 0.2468467313468612,
      "leakage": 0.21500620634650713,
      "lead": 0.21243491212423318,
      "consider": 0.19555758657846994,
      "imbalanced": 0.17698354942832822,
      "quality": 0.16708363623510455,
      "distribution": 0.16058346891800457,
      "ensure": 0.1593814585878564
    }
  },
  "data_selection": {
    "title": "Data Selection",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "data_selection_in_ml"
    ],
    "inlinks": [
      "data_cleansing",
      "data_transformation",
      "pandas",
      "how_do_you_do_the_data_selection"
    ],
    "summary": "Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame. In [[DE_Tools]] we explore...",
    "TFIDF_Score": {
      "row": 0.4252896182844911,
      "dataframe": 0.2915908127074496,
      "2013": 0.26552116389581787,
      "selection": 0.25333078600334635,
      "select": 0.2275325437207615,
      "selects": 0.20836874572683006,
      "rain": 0.18758562742990953,
      "999": 0.17938705027903026,
      "date": 0.17802679267095184,
      "python": 0.1603521745067849
    }
  },
  "data_steward": {
    "title": "Data Steward",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_governance",
      "data_steward",
      "data_management"
    ],
    "inlinks": [
      "data_roles",
      "data_steward"
    ],
    "summary": "A Data Steward is responsible for ensuring the quality, integrity, and governance of an organization's data assets. They act as a bridge between business users,...",
    "TFIDF_Score": {
      "data": 0.47486262492379594,
      "governance": 0.3269649282267331,
      "steward": 0.3116587207013791,
      "management": 0.20653160225797318,
      "compliance": 0.19971823297859134,
      "policy": 0.18466157327902952,
      "liaison": 0.17298904686164776,
      "ensuring": 0.16458640340329742,
      "responsible": 0.13866967383973136,
      "business": 0.13778912101889443
    }
  },
  "data_storage": {
    "title": "Data Storage",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "data_management"
    ],
    "outlinks": [
      "data_governance",
      "cloud_providers",
      "storage_layer_object_store\\",
      "data_management",
      "sqlite",
      "querying",
      "amazon_s3",
      "data_engineering_tools",
      "data_engineering",
      "data_ingestion",
      "nosql",
      "data_lake",
      "database",
      "data_transformation",
      "data_warehouse"
    ],
    "inlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "checksum",
      "data_lifecycle_management",
      "parquet",
      "apache_kafka",
      "big_data",
      "data_warehouse"
    ],
    "summary": "Data storage is a fundamental aspect of [[Data Engineering]], influencing processes such as - (occurring after [[Data Ingestion]]) - [[Data Transformation]] - [[Querying]] - [[data...",
    "TFIDF_Score": {
      "data": 0.4803980580104369,
      "storage": 0.39481868620280003,
      "storing": 0.2114468180284775,
      "database": 0.19257456892636807,
      "nosql": 0.18436383413217905,
      "lake": 0.14827808472320633,
      "retrieval": 0.132952302985833,
      "warehouse": 0.132952302985833,
      "method": 0.13103922185735345,
      "analytics": 0.12915298371611833
    }
  },
  "data_streaming": {
    "title": "Data Streaming",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "publish_and_subscribe",
      "batch_processing",
      "apache_kafka",
      "data_streaming",
      "alternatives_to_batch_processing"
    ],
    "inlinks": [
      "publish_and_subscribe",
      "distributed_computing",
      "data_ingestion",
      "lambda_architecture",
      "data_streaming",
      "alternatives_to_batch_processing"
    ],
    "summary": "Data Streaming is used for real-time data processing, allowing continuous flow and processing of data as it arrives. This is different from [[batch processing]], which...",
    "TFIDF_Score": {
      "streaming": 0.4029343843968704,
      "processing": 0.32459027205774005,
      "kafka": 0.2921634179460605,
      "data": 0.24527485461193624,
      "batch": 0.23363023548911246,
      "real": 0.18764782456006435,
      "netflix": 0.18614972823832054,
      "handle": 0.17779531053752598,
      "billion": 0.17534830020150421,
      "powering": 0.17534830020150421
    }
  },
  "data_terms": {
    "title": "Data Terms",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "data_transformation_in_data_engineering": {
    "title": "Data transformation in Data Engineering",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "elt",
      "data_engineering",
      "etl"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Data transformation in [[Data Engineering]] is a key step in data pipelines, often part of: ETL (Extract, Transform, Load) [[ETL]]: Data is transformed before loading...",
    "TFIDF_Score": {
      "elt": 0.48449202133905755,
      "etl": 0.4636851792341033,
      "extract": 0.3120402391501509,
      "transform": 0.29699682264435767,
      "load": 0.2757593546666691,
      "transformed": 0.21615554722024868,
      "data": 0.17885699561596763,
      "etlt": 0.15343898277723367,
      "tweak": 0.15343898277723367,
      "hybrid": 0.13728099405599903
    }
  },
  "data_transformation_in_machine_learning": {
    "title": "Data transformation in Machine Learning",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "encoding_categorical_variables",
      "supervised_learning"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Transforming raw data into a meaningful format is necessary for building effective models. [[Supervised Learning]]: Annotating datasets with correct labels (e.g., labeling images of apples...",
    "TFIDF_Score": {
      "encoding": 0.36776405684480074,
      "categorical": 0.35437865350568276,
      "labeling": 0.3516065648051408,
      "format": 0.18983065507237565,
      "annotating": 0.18663273727225138,
      "annotator": 0.18663273727225138,
      "recaptcha": 0.18663273727225138,
      "fruit": 0.16811966189335076,
      "datasets": 0.1676004303246845,
      "variable": 0.1664758110161346
    }
  },
  "data_transformation_with_pandas": {
    "title": "Data Transformation with Pandas",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "pandas_join_vs_merge",
      "de_tools",
      "aggregation",
      "pasted_image_20250323081817.png",
      "joining_datasets",
      "merge",
      "pandas",
      "pandas_stack",
      "concatenate",
      "multi-level_index",
      "crosstab"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Using [[pandas]] we can do the following: [[Merge]] [[Concatenate]] [[Joining Datasets]] [[Pandas join vs merge]] [[Multi-level index]] [[Aggregation]] [[Pandas Stack]] [[Crosstab]] A summary of transformations...",
    "TFIDF_Score": {
      "panda": 0.29088843070616804,
      "merge": 0.2577050792292436,
      "stack": 0.25228621691848263,
      "de_tools": 0.20716105400501444,
      "wide": 0.20716105400501444,
      "index": 0.18850675816001763,
      "row": 0.18724620932330205,
      "transformation": 0.1650051248002521,
      "format": 0.16349655835787916,
      "column": 0.16203589109154623
    }
  },
  "data_transformation": {
    "title": "Data Transformation",
    "tags": [
      "data_cleaning",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "data_selection",
      "data_transformation_in_machine_learning",
      "normalised_schema",
      "aggregation",
      "normalisation_of_data",
      "benefits_of_data_transformation",
      "data_transformation_with_pandas",
      "joining_datasets",
      "structuring_and_organizing_data",
      "data_transformation_in_data_engineering"
    ],
    "inlinks": [
      "eda",
      "data_pipeline",
      "data_selection_in_ml",
      "preprocessing",
      "handling_missing_data",
      "benefits_of_data_transformation",
      "melt",
      "etl",
      "dbt",
      "pandas_stack",
      "standardisation",
      "pandas",
      "duckdb",
      "activation_function",
      "data_storage"
    ],
    "summary": "Data transformation is the process of converting data from one format to another. Data transformation may involve: - [[Data Cleansing]] - [[Structuring and organizing data]]...",
    "TFIDF_Score": {
      "data": 0.5637849889410004,
      "transformation": 0.4650651645989824,
      "converting": 0.21215270934585295,
      "schema": 0.18356471488758092,
      "arranging": 0.15101678382845504,
      "ensuring": 0.14368140500674187,
      "sorting": 0.14225396188768838,
      "conversion": 0.13603664077661218,
      "joining": 0.12727381883584554,
      "structuring": 0.12394234855057984
    }
  },
  "data_validation": {
    "title": "Data Validation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "typescript",
      "type_checking",
      "pydantic",
      "data_quality"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "excel_&_sheets",
      "pydantic"
    ],
    "summary": "Data Validation: Error Prevention: It ensures data accuracy by preventing incorrect or inappropriate data entries. Consistent Data Entry: Helps maintain consistency across large datasets by...",
    "TFIDF_Score": {
      "entry": 0.3395571731833948,
      "data": 0.2790683893972586,
      "ensures": 0.2116362340231,
      "typescript": 0.19950747824599632,
      "input": 0.19084695328263626,
      "ready": 0.1907878472434115,
      "error": 0.18465587224684735,
      "chance": 0.18402437598681315,
      "prevention": 0.18402437598681315,
      "inappropriate": 0.17382591724745897
    }
  },
  "data_virtualization": {
    "title": "Data Virtualization",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_integration"
    ],
    "inlinks": [
      "semantic_layer",
      "data_integration"
    ],
    "summary": "Organizations may also consider adopting a data virtualization solution to integrate their data. In this type of [[data integration]], data from multiple sources is left...",
    "TFIDF_Score": {
      "virtualization": 0.679515632943411,
      "layer": 0.31523071788863105,
      "data": 0.22081469087865968,
      "source": 0.20415684352227964,
      "disadvantage": 0.17911789148188245,
      "executed": 0.1612229072124888,
      "analytics": 0.13192243251792685,
      "system": 0.1300375468686168,
      "integration": 0.11805593822157022,
      "single": 0.10548750987652357
    }
  },
  "data_visualisation": {
    "title": "Data Visualisation",
    "tags": [
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "looker_studio",
      "powerbi",
      "tableau"
    ],
    "inlinks": [
      "eda",
      "melt",
      "data_lifecycle_management",
      "grouped_plots",
      "dimensionality_reduction",
      "tableau",
      "business_observability",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Data visualization involves presenting data in a visual format, enabling stakeholders to quickly grasp insights and make informed decisions. Effective visualization tools include dashboards and...",
    "TFIDF_Score": {
      "report": 0.3652826356146191,
      "visualization": 0.3095786642678876,
      "grasp": 0.2807795149266261,
      "presenting": 0.26448714773708426,
      "looker": 0.25292752925326034,
      "powerbi": 0.2439611925666036,
      "tableau": 0.22507554357989457,
      "studio": 0.22034279487417666,
      "informed": 0.20258910585742487,
      "visual": 0.19722355790652882
    }
  },
  "data_warehouse": {
    "title": "Data Warehouse",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "Warehouse",
      "DWH"
    ],
    "outlinks": [
      "querying",
      "documentation_&_meetings",
      "etl",
      "data_ingestion",
      "data_storage"
    ],
    "inlinks": [
      "cloud_providers",
      "single_source_of_truth",
      "fabric",
      "fact_table",
      "databricks",
      "snowflake",
      "bigquery",
      "dimensional_modelling",
      "data_lakehouse",
      "data_storage",
      "databricks_vs_snowflake"
    ],
    "summary": "A Data Warehouse (DWH) is a centralized repository designed for [[Querying]] and analysis, storing large volumes of structured data from various sources within an organization....",
    "TFIDF_Score": {
      "data": 0.4862521061689249,
      "warehouse": 0.22428739482770702,
      "repository": 0.18294043066425286,
      "source": 0.16858880871414555,
      "historical": 0.16676108441763876,
      "tool": 0.15414262497347667,
      "analysis": 0.14939709380504268,
      "querying": 0.1484071125955347,
      "business": 0.1306425596427205,
      "dwh": 0.12301260635660698
    }
  },
  "database_index": {
    "title": "Database Index",
    "tags": [
      "database_optimisation"
    ],
    "aliases": [
      "Indexing",
      "Index"
    ],
    "outlinks": [
      "covering_index",
      "de_tools",
      "querying",
      "b-tree",
      "database"
    ],
    "inlinks": [
      "query_plan",
      "covering_index",
      "query_optimisation",
      "database_techniques"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Indexing/Indexing.ipynb Related terms: - [[Covering Index]] - Partial Index (Index with where clause) Indexing is a technique used to ==speed up...",
    "TFIDF_Score": {
      "index": 0.7457939403480793,
      "search": 0.1798950004001576,
      "movie": 0.1506974549706325,
      "data": 0.1396565759935801,
      "query": 0.13913577918246972,
      "indexing": 0.13610811645622392,
      "structure": 0.12979460682158897,
      "title": 0.1289676156756821,
      "creating": 0.12677498063886777,
      "tree": 0.1253958474467318
    }
  },
  "database_management_system_(dbms)": {
    "title": "Database Management System (DBMS)",
    "tags": [
      "database",
      "data_management"
    ],
    "aliases": [
      "DBMS"
    ],
    "outlinks": [
      "mongodb",
      "sqlite",
      "mysql",
      "crud",
      "oracle",
      "postgresql"
    ],
    "inlinks": [
      "sqlite",
      "data_management",
      "transaction",
      "data_engineering_portal",
      "database"
    ],
    "summary": "A Database Management System (DBMS) is software that allows you to interact with and manage databases. Easiest to use: - [[SQLite]] - [[PostgreSQL]] Others: -...",
    "TFIDF_Score": {
      "easiest": 0.23879461864604673,
      "oracle": 0.23879461864604673,
      "paid": 0.23879461864604673,
      "mongodb": 0.2249384453747,
      "proprietary": 0.2249384453747,
      "backup": 0.2151073339837573,
      "database": 0.2102139809716574,
      "crud": 0.2012511607124106,
      "supported": 0.2012511607124106,
      "dbms": 0.19598328812147833
    }
  },
  "database_schema": {
    "title": "Database Schema",
    "tags": [
      "data_modeling",
      "database_structure"
    ],
    "aliases": [
      "schema",
      "Schema"
    ],
    "outlinks": [
      "conceptual_data_model",
      "data_modelling",
      "data_management",
      "database",
      "implementing_database_schema",
      "types_of_database_schema",
      "database_schema"
    ],
    "inlinks": [
      "soft_deletion",
      "structured_data",
      "database",
      "views",
      "data_lakehouse",
      "schema_evolution",
      "database_schema",
      "fact_table"
    ],
    "summary": "A [[Database Schema|schema]] is the structure that defines how data is organized in a [[Database]], used in [[Data Management]]. It specifies the tables, columns, relationships,...",
    "TFIDF_Score": {
      "schema": 0.7443321263000408,
      "database": 0.2553458164023245,
      "data": 0.25479508536284223,
      "read": 0.1604570921297529,
      "structure": 0.14471264583778357,
      "design": 0.10912213859569721,
      "strategic": 0.10334085623681936,
      "table": 0.08994025858153504,
      "iterative": 0.0883799489013294,
      "write": 0.08385766781333415
    }
  },
  "database_storage": {
    "title": "Database Storage",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "row-based_storage",
      "columnar_storage",
      "database",
      "vectorized_engine"
    ],
    "inlinks": [],
    "summary": "Methods and optimizations for storing, retrieving, and processing data in [[database]] systems. [[Columnar Storage]] [[Row-based Storage]] [[Vectorized Engine]]",
    "TFIDF_Score": {
      "storage": 0.4374888441008222,
      "vectorized": 0.3995842479489959,
      "columnar": 0.3328914494866123,
      "retrieving": 0.3328914494866123,
      "engine": 0.2908129785877497,
      "storing": 0.2733488352524082,
      "row": 0.24707121456678008,
      "optimization": 0.22525491225304228,
      "database": 0.1867137370737158,
      "processing": 0.18491932856307122
    }
  },
  "database_techniques": {
    "title": "Database Techniques",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "race_conditions",
      "database_index",
      "querying",
      "concurrency",
      "vacuum",
      "soft_deletion",
      "stored_procedures",
      "query_plan",
      "sql_joins"
    ],
    "inlinks": [
      "melt",
      "data_engineering_portal",
      "database",
      "sql"
    ],
    "summary": "Techniques: - [[Soft Deletion]] - [[Concurrency]] - [[Race Conditions]] - [[Querying]] - [[SQL Joins]] - [[Stored Procedures]] - Cleaning: Use Levenshtein Distance (if SQLite extension...",
    "TFIDF_Score": {
      "levenshtein": 0.2808077582993292,
      "vacuum": 0.2645137522744226,
      "race": 0.2439857324155614,
      "procedure": 0.23046427140427928,
      "concurrency": 0.22509818373747636,
      "entry": 0.22509818373747636,
      "extension": 0.22509818373747636,
      "deletion": 0.22036495896858965,
      "indexing": 0.22036495896858965,
      "soft": 0.21230081373020274
    }
  },
  "database": {
    "title": "Database",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "mongodb",
      "spreadsheets_vs_databases",
      "oltp",
      "mysql",
      "structured_data",
      "components_of_the_database",
      "database_management_system_(dbms)",
      "database_techniques",
      "crud",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "postgresql",
      "database_schema"
    ],
    "inlinks": [
      "database_index",
      "rollup",
      "single_source_of_truth",
      "structured_data",
      "sql",
      "data_ingestion",
      "data_integrity",
      "database_storage",
      "slowly_changing_dimension",
      "microsoft_access",
      "software_design_patterns",
      "data_storage",
      "database_schema"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] [[MongoDB]] They facilitate efficient [[CRUD]] operations and transactional processing ([[OLTP]])...",
    "TFIDF_Score": {
      "database": 0.4905892941721544,
      "spreadsheet": 0.29155680383442717,
      "table": 0.23039984805366698,
      "schema": 0.22579989609227288,
      "crud": 0.20874320416826458,
      "efficient": 0.18403488964751846,
      "organized": 0.17871012370881495,
      "flexibility": 0.14331655304394503,
      "structured": 0.14062317806361718,
      "data": 0.13598088580840528
    }
  },
  "databricks_vs_snowflake": {
    "title": "Databricks vs Snowflake",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "data_warehouse",
      "databricks",
      "snowflake"
    ],
    "inlinks": [
      "databricks"
    ],
    "summary": "Comparison between [[Databricks]] and [[Snowflake]]: Databricks is a versatile platform that emphasizes collaborative data science and engineering through interactive notebooks, making it suitable for advanced...",
    "TFIDF_Score": {
      "data": 0.3779079885413996,
      "analytics": 0.2539974062989345,
      "notebook": 0.22075131644679463,
      "databricks": 0.17654329164232177,
      "cloud": 0.17431286601992363,
      "machine": 0.17226930137088237,
      "platform": 0.16595613776751447,
      "warehousing": 0.165563487335096,
      "structured": 0.16283729280647355,
      "support": 0.156786010341015
    }
  },
  "databricks": {
    "title": "Databricks",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "apache_spark",
      "databricks_vs_snowflake",
      "data_lake",
      "data_engineer",
      "big_data",
      "hadoop",
      "data_warehouse"
    ],
    "inlinks": [
      "batch_processing",
      "big_data",
      "cloud_providers",
      "databricks_vs_snowflake"
    ],
    "summary": "Databricks Overview [!Summary] Databricks is a cloud-based platform for [[big data]] processing built on [[Apache Spark]]. It provides an integrated workspace for collaboration among [[data...",
    "TFIDF_Score": {
      "databricks": 0.35347359919433485,
      "azure": 0.32025252232440016,
      "data": 0.2837416582740833,
      "spark": 0.2240884660663226,
      "big": 0.2189469177878813,
      "lake": 0.2189469177878813,
      "workspace": 0.20284842306169013,
      "cloud": 0.1963169203722043,
      "notebook": 0.16574496039778383,
      "processing": 0.1408110844653473
    }
  },
  "datasets": {
    "title": "Datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "time_series"
    ],
    "inlinks": [
      "handling_different_distributions",
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "mnist",
      "train-dev-test_sets"
    ],
    "summary": "This note collects notes on datasets that are good examples for exploring various concepts. Heart Failure Prediction Dataset Link: Heart Failure Prediction Dataset Useful for:...",
    "TFIDF_Score": {
      "series": 0.3162696088342264,
      "nab": 0.2581981878077408,
      "traffic": 0.2520459408272947,
      "wikipedia": 0.23258616183928846,
      "time": 0.23254989441959747,
      "dataset": 0.195901172816033,
      "link": 0.19197747918221986,
      "idb": 0.17213212520516052,
      "international": 0.17213212520516052,
      "numenta": 0.17213212520516052
    }
  },
  "dbscan": {
    "title": "DBScan",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "clustering",
      "standardised/outliers",
      "k-means"
    ],
    "inlinks": [
      "clustering",
      "unsupervised_learning",
      "isolated_forest",
      "anomaly_detection_with_clustering"
    ],
    "summary": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a [[Clustering]] algorithm that groups together data points ==based on density==. It is particularly useful when...",
    "TFIDF_Score": {
      "dbscan": 0.5511589810928225,
      "cluster": 0.4624992928543691,
      "outlier": 0.2130381390875123,
      "point": 0.19190170656021102,
      "density": 0.1747449620850013,
      "plt": 0.16778364527807324,
      "shape": 0.15728686048612675,
      "mean": 0.15640022414921553,
      "make_blobs": 0.14532840814157402,
      "core": 0.13679758173178028
    }
  },
  "dbt": {
    "title": "dbt",
    "tags": [
      "software"
    ],
    "aliases": [
      "data build tool"
    ],
    "outlinks": [
      "documentation_&_meetings",
      "dbt",
      "sql",
      "elt",
      "data_transformation",
      "data_lineage"
    ],
    "inlinks": [
      "data_engineering_tools",
      "data_observability",
      "dbt",
      "data_contract",
      "jinja_template",
      "elt"
    ],
    "summary": "Data build tool is an open-source framework designed for [[Data Transformation]] within a modern data stack. It enables analysts and engineers to transform, model, and...",
    "TFIDF_Score": {
      "dbt": 0.7242964156342578,
      "model": 0.2154441717073612,
      "data": 0.21259646609549054,
      "sql": 0.19044614642499694,
      "version": 0.1792829277889376,
      "transformation": 0.1490648572978889,
      "documentation": 0.1417522956506843,
      "lineage": 0.12418618072524448,
      "control": 0.1038797162318165,
      "analyst": 0.09934894458019558
    }
  },
  "debugging_ipynb": {
    "title": "Debugging ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "debugging jupyter cells https://www.youtube.com/watch?v=CY6uZIoF_kQ Sometimes dissapears: https://stackoverflow.com/questions/72671709/vs-code-debug-cell-disappears-arbitrarily-in-jupyter-notebook-view",
    "TFIDF_Score": {
      "cell": 0.4162845760090583,
      "jupyter": 0.36875918098590493,
      "72671709": 0.23955486545539242,
      "arbitrarily": 0.23955486545539242,
      "cy6uziof_kq": 0.23955486545539242,
      "disappears": 0.23955486545539242,
      "dissapears": 0.23955486545539242,
      "stackoverflow": 0.21579216794381573,
      "com": 0.21440014290293016,
      "http": 0.20250342661655593
    }
  },
  "debugging": {
    "title": "Debugging",
    "tags": [
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "types_of_computational_bugs",
      "stackbiz",
      "testing_unittest.py",
      "typescript",
      "git",
      "testing_pytest.py",
      "debugging.py",
      "software_development_life_cycle",
      "ml_tools"
    ],
    "inlinks": [
      "pyright",
      "types_of_computational_bugs",
      "pydantic"
    ],
    "summary": "Debugging is the process of identifying, analyzing, and resolving bugs or defects in software while [[Testing]]. It is a critical part of the [[Software Development...",
    "TFIDF_Score": {
      "debugging": 0.4711786054194319,
      "bug": 0.3896849145334352,
      "code": 0.2833227495165731,
      "tool": 0.18436543923243018,
      "program": 0.15705953513981064,
      "execution": 0.13968703992719744,
      "console": 0.12989497151114504,
      "help": 0.12943446882474152,
      "logging": 0.12308042459349593,
      "development": 0.12166712568174647
    }
  },
  "debugging.py": {
    "title": "Debugging.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Debugging.py This script includes examples of logging, using breakpoints, and reproducing a simple bug for practice Key Concepts Demonstrated in the Script Logging in Python:...",
    "TFIDF_Score": {
      "script": 0.3243908502694634,
      "bug": 0.2801576604462354,
      "logging": 0.26546003590204065,
      "divide_numbers": 0.2380004666455932,
      "num1": 0.2380004666455932,
      "num2": 0.2380004666455932,
      "breakpoints": 0.22419037442826883,
      "includes": 0.20793232611828524,
      "warning": 0.20679171587514208,
      "static": 0.15791291166342733
    }
  },
  "decision_tree": {
    "title": "Decision Tree",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "gini_impurity",
      "supervised_learning",
      "gridseachcv",
      "overfitting",
      "gini_impurity_vs_cross_entropy",
      "interpretability",
      "pasted_image_20240404154526.png",
      "decision_tree",
      "cross_validation",
      "pruning",
      "classification",
      "cross_entropy",
      "hyperparameter"
    ],
    "inlinks": [
      "gini_impurity",
      "gradient_boosting",
      "ds_&_ml_portal",
      "ada_boosting",
      "model_parameters",
      "weak_learners",
      "regularisation_of_tree_based_models",
      "machine_learning_algorithms",
      "decision_tree",
      "random_forests",
      "why_and_when_is_feature_scaling_necessary",
      "gradient_boosting_regressor",
      "classification",
      "feature_scaling",
      "xgboost",
      "supervised_learning",
      "embedded_methods"
    ],
    "summary": "A Decision Tree is a type of [[Supervised Learning]] algorithm used to predict a target variable based on input features. It involves splitting data into...",
    "TFIDF_Score": {
      "node": 0.41800801515460184,
      "tree": 0.3490142683167404,
      "impurity": 0.30758657143317897,
      "leaf": 0.2837755219789654,
      "split": 0.27752966472618007,
      "splitting": 0.21813956984360008,
      "criterion": 0.1603186563281911,
      "decision": 0.15632769628953866,
      "gini": 0.13981207792417225,
      "pruning": 0.13206382337731148
    }
  },
  "declarative": {
    "title": "declarative",
    "tags": [
      "data_orchestration",
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability",
      "data_lineage",
      "sql",
      "data_quality"
    ],
    "inlinks": [
      "dagster",
      "pydantic"
    ],
    "summary": "In a declarative data pipeline, the focus is on what needs to be achieved, not how it should be executed. You define the desired outcome...",
    "TFIDF_Score": {
      "declarative": 0.4933968814923273,
      "pipeline": 0.3281010811222151,
      "desired": 0.23582261286426748,
      "data": 0.20889551618370247,
      "system": 0.17367297460758513,
      "step": 0.1712934319218762,
      "executed": 0.1614921759795582,
      "imperative": 0.14147377424321925,
      "explicitly": 0.12920220681177774,
      "order": 0.1213653315711779
    }
  },
  "deep_learning_frameworks": {
    "title": "Deep Learning Frameworks",
    "tags": "deep_learning drafting",
    "aliases": [],
    "outlinks": [
      "sci-kit_learn"
    ],
    "inlinks": [],
    "summary": "Watch Overview Video TensorFlow Focus: TensorFlow is a comprehensive open-source platform for machine learning. It provides a flexible and comprehensive ecosystem of tools, libraries, and...",
    "TFIDF_Score": {
      "tensorflow": 0.4583423837322167,
      "sci": 0.29001077320170965,
      "kit": 0.27318275778484447,
      "kera": 0.269559241315637,
      "learning": 0.18232881822885308,
      "learn": 0.16172326086310804,
      "machine": 0.1451594538500943,
      "modular": 0.1162377042346874,
      "researcher": 0.1162377042346874,
      "level": 0.10962927102815442
    }
  },
  "deep_learning": {
    "title": "Deep Learning",
    "tags": [
      "deep_learning"
    ],
    "aliases": [
      "DL"
    ],
    "outlinks": [
      "stochastic_gradient_descent",
      "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.",
      "neural_network",
      "pytorch",
      "gradient_descent",
      "backpropagation",
      "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models._",
      "llm",
      "tensorflow",
      "optimisation_techniques",
      "transfer_learning"
    ],
    "inlinks": [
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "optimising_neural_networks",
      "model_parameters",
      "ds_&_ml_portal",
      "convolutional_neural_networks",
      "neural_network",
      "pytorch",
      "neural_network_classification",
      "adam_optimizer"
    ],
    "summary": "[!Summary] Deep learning is a subset of machine learning that uses neural networks to process large-scale data for tasks like image and speech recognition, natural...",
    "TFIDF_Score": {
      "neural": 0.3342339114922642,
      "deep": 0.30952556290208283,
      "network": 0.30768461516047163,
      "gradient": 0.2924975201627192,
      "learning": 0.2742011509993696,
      "descent": 0.27142374726807633,
      "activation": 0.157343483198047,
      "algorithm": 0.14687458717726304,
      "training": 0.1354433065171147,
      "task": 0.1324221470933138
    }
  },
  "deep_q-learning": {
    "title": "Deep Q-Learning",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "reinforcement_learning",
      "q-learning",
      "pasted_image_20250220133838.png",
      "neural_network"
    ],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": "Deep [[Q-Learning]] is a type of [[reinforcement learning]] algorithm that combines Q-Learning with [[Neural network]]. Necessary when Q-Table grows too large. Updates the weights in...",
    "TFIDF_Score": {
      "experience": 0.5081295084385734,
      "network": 0.37778673681348746,
      "target": 0.2474196629683414,
      "learning": 0.19238541761347314,
      "update": 0.18156255228723933,
      "replay": 0.17850392027875797,
      "oscillation": 0.16814614394769542,
      "buffer": 0.1607971846875137,
      "instability": 0.1607971846875137,
      "agent": 0.12703237710964335
    }
  },
  "deepseek": {
    "title": "DeepSeek",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "security",
      "distillation",
      "edge_machine_learning_models",
      "chain_of_thought",
      "llm",
      "jevon_paradox"
    ],
    "inlinks": [],
    "summary": "[[LLM]] example open source optimising for preformance vs efficency. Deepseek leading in efficency o3 mini [[Chain of thought]] can see it - ui choice [[Distillation]]...",
    "TFIDF_Score": {
      "deepseek": 0.62689020643806,
      "attention": 0.2360872952704716,
      "privacy": 0.1778254698904035,
      "access": 0.15549544844116361,
      "efficency": 0.1478903434917275,
      "concerned": 0.13930893476401335,
      "latent": 0.13322032837599823,
      "mini": 0.13322032837599823,
      "model": 0.128787161044919,
      "chatgpt": 0.1284976383578227
    }
  },
  "deleting_rows_or_filling_them_with_the_mean_is_not_always_best": {
    "title": "Deleting rows or filling them with the mean is not always best",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "demand_forecasting": {
    "title": "Demand forecasting",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "use_of_rnns_in_energy_sector",
      "energy"
    ],
    "summary": "Overview: Demand response programs encourage consumers to adjust their energy usage during peak periods in response to time-based rates or other incentives. RL can optimize...",
    "TFIDF_Score": {
      "consumer": 0.4270689181620479,
      "incentive": 0.4054669861552803,
      "demand": 0.2646086215244874,
      "response": 0.26333744229121814,
      "energy": 0.24907383094017527,
      "peak": 0.19149819257383288,
      "program": 0.1660492206267835,
      "adjust": 0.1418091214681047,
      "programming": 0.13872464109424637,
      "time": 0.13509759122596213
    }
  },
  "dendrograms": {
    "title": "Dendrograms",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240405173403.png",
      "dendrograms"
    ],
    "inlinks": [
      "clustering",
      "clustermap",
      "heatmaps_dendrograms.py",
      "dendrograms"
    ],
    "summary": "Dendrograms show close vectors is the data where taken as a vector. Can tell which ==features are the most similar== with [[Dendrograms]] ![[Pasted image 20240405173403.png]]",
    "TFIDF_Score": {
      "dendrograms": 0.6081258981475519,
      "vector": 0.3575768017258684,
      "20240405173403": 0.34995175441596593,
      "taken": 0.28052467208120396,
      "tell": 0.269349407906395,
      "close": 0.22550501106366505,
      "show": 0.205198891213507,
      "pasted": 0.17717274288997273,
      "png": 0.1740898587453863,
      "similar": 0.17334790363870128
    }
  },
  "dependency_manager": {
    "title": "dependency manager",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "virtual_environments",
      "toml",
      "poetry",
      "requirements.txt"
    ],
    "inlinks": [
      "poetry"
    ],
    "summary": "[[Virtual environments]] [[requirements.txt]] [[TOML]] [[Poetry]]",
    "TFIDF_Score": {
      "poetry": 0.5009524846569904,
      "toml": 0.4564154709067931,
      "txt": 0.43641461605374654,
      "virtual": 0.40125136335829215,
      "requirement": 0.3283283797605407,
      "environment": 0.2854323516606151
    }
  },
  "determining_threshold_values": {
    "title": "Determining Threshold Values",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "precision-recall_curve",
      "data_quality",
      "evaluation_metrics",
      "binary_classification",
      "cost-sensitive_analysis",
      "roc_(receiver_operating_characteristic)",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "In [[Binary Classification]] problems, a threshold value is used to convert predicted probabilities into discrete class labels. The choice of threshold significantly impacts the model's...",
    "TFIDF_Score": {
      "threshold": 0.4234085866591197,
      "class": 0.261307048696736,
      "choice": 0.22173575064758588,
      "imbalance": 0.2087127249032196,
      "receiver": 0.20432404338348825,
      "roc": 0.2003982339882724,
      "curve": 0.18529030934586874,
      "operating": 0.18288555713814364,
      "imbalanced": 0.18062663966396092,
      "analysis": 0.15810608076437369
    }
  },
  "devops": {
    "title": "DevOps",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "dataops",
      "ci-cd",
      "software_development_portal"
    ],
    "inlinks": [
      "software_development_portal",
      "machine_learning_operations",
      "software_development_life_cycle"
    ],
    "summary": "DevOps refers to practices for collaboration and automation between [[Software Development Portal]] (Dev) and IT operations (Ops) teams, aiming for faster, more reliable software delivery....",
    "TFIDF_Score": {
      "collaboration": 0.29691191392316074,
      "team": 0.2832345733904377,
      "software": 0.24954343568014012,
      "development": 0.24669673332665926,
      "operation": 0.24008878819366491,
      "delivery": 0.21841593187979041,
      "principle": 0.21841593187979041,
      "automation": 0.20955107952142238,
      "reliable": 0.18565911981157893,
      "continuous": 0.16264746975295336
    }
  },
  "difference_between_databricks_vs._snowflake": {
    "title": "Difference between Databricks vs. Snowflake",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "difference_between_snowflake_to_hadoop": {
    "title": "Difference between snowflake to hadoop",
    "tags": [
      "software_architecture",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "snowflake",
      "hadoop",
      "data_management"
    ],
    "inlinks": [],
    "summary": "Snowflake and Hadoop are both [[Data Management]] systems, but they serve different purposes and have distinct architectures and functionalities. In summary, Snowflake and Hadoop are...",
    "TFIDF_Score": {
      "hadoop": 0.5399264850875407,
      "snowflake": 0.5152793561457695,
      "scale": 0.18413225048444992,
      "cloud": 0.1736145256337544,
      "data": 0.14114774843195366,
      "requires": 0.12089985287532891,
      "excels": 0.12037489719260128,
      "hardware": 0.11449458813607437,
      "use": 0.11200862290338044,
      "query": 0.10312235329191731
    }
  },
  "differentation": {
    "title": "Differentation",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Forward Mode Automatic Differentiation uses dual numbers implemented in tensor flow see also Reverse Mode Automatic Differentiation Fast,Flexible,Exact",
    "TFIDF_Score": {
      "differentiation": 0.48305420702242313,
      "mode": 0.4298610299366032,
      "automatic": 0.3766678528507832,
      "dual": 0.24152710351121157,
      "tensor": 0.24152710351121157,
      "exact": 0.23296491443108952,
      "reverse": 0.23296491443108952,
      "implemented": 0.19345762829824234,
      "forward": 0.183814501078877,
      "fast": 0.17120954826514748
    }
  },
  "digital_transformation": {
    "title": "Digital Transformation",
    "tags": [
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "data_audit",
      "change_management",
      "digital_transformation"
    ],
    "inlinks": [
      "digital_transformation"
    ],
    "summary": "==\"Digital transformation starts with data centralisation\"== To digitally transform your department, you'll need to approach the process in a structured and strategic way that addresses...",
    "TFIDF_Score": {
      "digital": 0.3445108856642263,
      "technology": 0.29249294047732516,
      "transformation": 0.2727531569035012,
      "department": 0.18237714898176327,
      "data": 0.1615849714712531,
      "process": 0.1576406172696102,
      "change": 0.14784378541757037,
      "need": 0.12022561522062011,
      "leadership": 0.11551795690765415,
      "automation": 0.11485302862967155
    }
  },
  "digital_twin": {
    "title": "Digital twin",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "data_management",
      "energy"
    ],
    "inlinks": [],
    "summary": "[!Summary] A digital twin is a virtual representation of a physical object, system, or process that mirrors its real-world counterpart in real-time. This digital model...",
    "TFIDF_Score": {
      "twin": 0.6383228638066866,
      "digital": 0.5542672727131802,
      "physical": 0.2222537004593319,
      "system": 0.127578300061255,
      "real": 0.12674211341167094,
      "simulate": 0.11411385026447829,
      "turbine": 0.09671558542525555,
      "city": 0.09303376552064632,
      "simulation": 0.09303376552064632,
      "world": 0.08229216574203807
    }
  },
  "dimension_table": {
    "title": "Dimension Table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "star_schema",
      "facts",
      "dimension_table",
      "fact_table"
    ],
    "inlinks": [
      "components_of_the_database",
      "slowly_changing_dimension",
      "dimension_table",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "A dimension table is a key component of a [[star schema]] or snowflake schema in a data warehouse. It provides descriptive attributes (or dimensions) related...",
    "TFIDF_Score": {
      "dimension": 0.46657447332740504,
      "table": 0.4197071521595933,
      "product": 0.2237761235281191,
      "fact": 0.22357868414022228,
      "descriptive": 0.20667479164311772,
      "attribute": 0.1710662963891046,
      "data": 0.14154808524127357,
      "contain": 0.13952002706388983,
      "quarter": 0.12891237942188355,
      "contains": 0.1157920841759438
    }
  },
  "dimensional_modelling": {
    "title": "Dimensional Modelling",
    "tags": [
      "data_modeling"
    ],
    "aliases": null,
    "outlinks": [
      "facts",
      "performance_dimensions",
      "queries",
      "star_schema",
      "grain",
      "dimension_table",
      "dimensional_modelling",
      "data_warehouse",
      "fact_table"
    ],
    "inlinks": [
      "dimensional_modelling",
      "granularity"
    ],
    "summary": "Dimensional modeling is a design technique used in [[Data Warehouse]]used to structure data for efficient ==retrieval== and analysis. It is particularly well-suited for organizing data...",
    "TFIDF_Score": {
      "dimensional": 0.4776444704082065,
      "modeling": 0.38211557632656523,
      "fact": 0.21808536217032184,
      "warehouse": 0.19105778816328262,
      "dimension": 0.189629479859869,
      "data": 0.17258781596884148,
      "business": 0.16693054799960033,
      "foundational": 0.15718125736456146,
      "kimball": 0.15718125736456146,
      "table": 0.14621248544873577
    }
  },
  "dimensionality_reduction": {
    "title": "Dimensionality Reduction",
    "tags": [
      "ml_process",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "preprocessing",
      "principal_component_analysis",
      "data_visualisation",
      "linear_discriminant_analysis",
      "explain_the_curse_of_dimensionality",
      "t-sne"
    ],
    "inlinks": [
      "feature_selection",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "preprocessing",
      "unsupervised_learning",
      "vector_embedding",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "addressing_multicollinearity",
      "interview_notepad",
      "explain_the_curse_of_dimensionality",
      "factor_analysis",
      "feature_engineering",
      "t-sne",
      "feature_extraction",
      "learning_styles",
      "manifold_learning",
      "data_reduction"
    ],
    "summary": "Dimensionality reduction is a step in the [[Preprocessing]] phase of machine learning that helps simplify models, enhance interpretability, and improve computational efficiency. Its a technique...",
    "TFIDF_Score": {
      "dimensionality": 0.2530088098268473,
      "coordinate": 0.23424222645768586,
      "principal": 0.2124062100354274,
      "greatest": 0.20655293576097705,
      "component": 0.1980556277716901,
      "dimensional": 0.18830273265466843,
      "reduction": 0.18553062965466682,
      "computational": 0.18292168859113656,
      "reducing": 0.16800277671042846,
      "sne": 0.16557480807164235
    }
  },
  "dimensions": {
    "title": "Dimensions",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "facts",
      "olap_(online_analytical_processing)"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "Dimensions are the categorical buckets that can be used to segment, filter, or group—such as sales amount region, city, product, color, and distribution channel. Traditionally...",
    "TFIDF_Score": {
      "olap": 0.4040950163602125,
      "bucket": 0.223704846255021,
      "bus": 0.223704846255021,
      "segment": 0.223704846255021,
      "traditionally": 0.21698710352690212,
      "channel": 0.21130733297944881,
      "cube": 0.21130733297944881,
      "city": 0.20638729194012878,
      "color": 0.19816543932145175,
      "filter": 0.19144769659333288
    }
  },
  "directed_acyclic_graph_(dag)": {
    "title": "Directed Acyclic Graph (DAG)",
    "tags": [
      "math",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "apache_airflow",
      "prefect",
      "dagster"
    ],
    "inlinks": [
      "apache_airflow",
      "pyspark",
      "mathematics"
    ],
    "summary": "DAG stands for Directed Acyclic Graph. A DAG is a graph where information must travel along with a finite set of nodes connected by vertices....",
    "TFIDF_Score": {
      "travel": 0.34528525109572966,
      "graph": 0.3101432076491607,
      "dag": 0.30000835942389226,
      "node": 0.20385463882289995,
      "pipeline": 0.2024626245851122,
      "way": 0.18152059049836988,
      "circle": 0.17264262554786483,
      "idempotency": 0.17264262554786483,
      "vertex": 0.17264262554786483,
      "finite": 0.15551730237523972
    }
  },
  "directory_structure": {
    "title": "Directory Structure",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "To make a file tree ├── README.md <- The top-level README for developers using this project. ├── data │ ├── external <- Data from third...",
    "TFIDF_Score": {
      "script": 0.2464014946504014,
      "model": 0.22958425743696803,
      "data": 0.22331322692001973,
      "readme": 0.21286375409275327,
      "generated": 0.20600976843505828,
      "requirement": 0.20012271822692332,
      "figure": 0.18114462487420838,
      "txt": 0.17733562823563379,
      "notebook": 0.17392836463657066,
      "initial": 0.1567683278505856
    }
  },
  "distillation": {
    "title": "Distillation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "small_language_models",
      "pasted_image_20250130074219.png"
    ],
    "inlinks": [
      "llm",
      "deepseek",
      "small_language_models"
    ],
    "summary": "training smaller models with larger. [[Small Language Models]] ![[Pasted image 20250130074219.png]]",
    "TFIDF_Score": {
      "20250130074219": 0.5410975890613889,
      "larger": 0.34551617979309684,
      "model": 0.3141355534023965,
      "smaller": 0.3035209522697137,
      "small": 0.28169562845123847,
      "pasted": 0.27394560197348106,
      "png": 0.26917882724827563,
      "language": 0.25075234834036453,
      "image": 0.23894584549956996,
      "training": 0.22404940523771336
    }
  },
  "distributed_computing": {
    "title": "Distributed Computing",
    "tags": [
      "data_management",
      "data_processing",
      "cloud_computing"
    ],
    "aliases": null,
    "outlinks": [
      "kubernetes",
      "scalability",
      "apache_spark",
      "amazon_s3",
      "data_streaming",
      "edge_computing",
      "hadoop",
      "latency"
    ],
    "inlinks": [
      "batch_processing",
      "publish_and_subscribe",
      "map_reduce"
    ],
    "summary": "Distributed Computing is essential for managing massive data volumes by distributing tasks across multiple servers or machines. This enables scalability and efficient data processing. [[Hadoop]]...",
    "TFIDF_Score": {
      "distributed": 0.3927972782392027,
      "computing": 0.3290109873350547,
      "processing": 0.24498944929958894,
      "across": 0.2240924372691398,
      "data": 0.20980825457626773,
      "fault": 0.176411830001753,
      "storage": 0.17388147332757226,
      "cloud": 0.1707804978308139,
      "hadoop": 0.16995612749096256,
      "server": 0.1647673353943654
    }
  },
  "distributions": {
    "title": "Distributions",
    "tags": [
      "statistics",
      "drafting"
    ],
    "aliases": [
      "Distribution"
    ],
    "outlinks": [
      "violin_plot",
      "binomial",
      "poisson",
      "distribution_analysis.py",
      "bernoulli",
      "pasted_image_20250308191945.png",
      "gaussian_distribution",
      "hypothesis_testing",
      "uniform",
      "boxplot",
      "feature_distribution.py",
      "ml_tools"
    ],
    "inlinks": [
      "eda",
      "feature_selection",
      "standard_deviation",
      "precision-recall_curve",
      "gaussian_mixture_models",
      "variance",
      "t-test",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "covariance_structures",
      "data_selection_in_ml",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "statistics",
      "train-dev-test_sets",
      "data_analyst",
      "fitting_weights_and_biases_of_a_neural_network",
      "gaussian_distribution",
      "handling_different_distributions",
      "kmeans_vs_gmm",
      "ds_&_ml_portal",
      "gini_impurity_vs_cross_entropy",
      "boxplot"
    ],
    "summary": "In [[ML_Tools]] see: - [[Distribution_Analysis.py]] Discrete Distributions These distributions have probabilities concentrated on specific values. [[Uniform]] Distribution: All outcomes are equally likely. Example: Drawing a...",
    "TFIDF_Score": {
      "distribution": 0.5762876516751011,
      "tail": 0.30958378030503064,
      "plot": 0.22491787915985076,
      "theoretical": 0.1978449038114718,
      "quantiles": 0.18780233888632983,
      "quantile": 0.1691732446183469,
      "boxplot": 0.15827592304917742,
      "curve": 0.13364959995402567,
      "spread": 0.11053395111000598,
      "line": 0.10866729679483093
    }
  },
  "distribution_analysis.py": {
    "title": "Distribution_Analysis.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Distribution_Analysis.py The goodness-of-fit results represent the p-values from the Kolmogorov-Smirnov (KS) test, which assesses how well the data fits each distribution. Here's how to interpret...",
    "TFIDF_Score": {
      "fit": 0.5112179381603166,
      "distribution": 0.4170932657841513,
      "poor": 0.2372165566498332,
      "value": 0.18641991161787874,
      "logistic": 0.18480920856308436,
      "chi": 0.1812770104268073,
      "follow": 0.17152992867214298,
      "likely": 0.14809297260005663,
      "0502": 0.14725056402881745,
      "goodness": 0.14725056402881745
    }
  },
  "docker_image": {
    "title": "Docker Image",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "docker"
    ],
    "summary": "A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries,...",
    "TFIDF_Score": {
      "docker": 0.6869405088673484,
      "container": 0.4255904384164075,
      "nginx": 0.3355375443328674,
      "image": 0.17511190554286077,
      "server": 0.1430886737204712,
      "run": 0.14292080527623138,
      "running": 0.12694918336013022,
      "web": 0.12694918336013022,
      "8080": 0.12201365248467906,
      "bash": 0.11166221077857211
    }
  },
  "docker": {
    "title": "Docker",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "docker_image"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "Utilizes Docker images [[Docker Image]] to set up containers for consistent development and testing environments. Containers can include necessary dependencies like Python and pip. Tutorial:...",
    "TFIDF_Score": {
      "docker": 0.74204309709797,
      "container": 0.3380360013854451,
      "compose": 0.25865230095602504,
      "tutorial": 0.19811979079881598,
      "image": 0.18188309536496355,
      "dockerfile": 0.13729262124540206,
      "dockerignore": 0.13729262124540206,
      "init": 0.11928958427888177,
      "yaml": 0.11570737874893879,
      "started": 0.11267866712848172
    }
  },
  "documentation_&_meetings": {
    "title": "Documentation & Meetings",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "one_pager_template",
      "pull_request_template",
      "technical_design_doc_template",
      "feedback_template",
      "meeting_notes_template",
      "pdoc",
      "retrospective_template",
      "data_request_template",
      "postmortem_template",
      "mermaid",
      "experiment_plan_template",
      "1-on-1_template"
    ],
    "inlinks": [
      "pyright",
      "code_diagrams",
      "ipynb",
      "software_development_portal",
      "dbt",
      "fishbone_diagram",
      "data_principles",
      "data_engineer",
      "data_warehouse",
      "data_analyst"
    ],
    "summary": "Tools [[pdoc]] – Auto-generate Python API documentation [[Mermaid]] – Create diagrams and flowcharts from text in a Markdown-like syntax Templates Project & Technical Meetings [[Technical...",
    "TFIDF_Score": {
      "template": 0.5403720107879881,
      "meeting": 0.34696919576909535,
      "project": 0.18798180994618713,
      "review": 0.1817828097233743,
      "technical": 0.1711106031089003,
      "data": 0.16720604566867495,
      "session": 0.13509300269699703,
      "pager": 0.12022097889988498,
      "note": 0.11375943619998752,
      "proposal": 0.11324509843853817
    }
  },
  "dropout": {
    "title": "Dropout",
    "tags": [
      "deep_learning",
      "ml_optimisation"
    ],
    "aliases": null,
    "outlinks": [
      "regularisation",
      "overfitting",
      "neural_network"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network"
    ],
    "summary": "Dropout is a [[Regularisation]] technique used in [[Neural network]] training to prevent [[overfitting]]. It works by randomly dropping units (neurons) during training, which helps the...",
    "TFIDF_Score": {
      "neuron": 0.605724704756408,
      "dropout": 0.5188644142166102,
      "dropped": 0.1779473648344669,
      "training": 0.15644100580222445,
      "rate": 0.15534164257809524,
      "pas": 0.14046964617488572,
      "network": 0.13326929520366954,
      "iteration": 0.13105336298053752,
      "randomly": 0.12665975157490966,
      "layer": 0.10660187653144367
    }
  },
  "ds_&_ml_portal": {
    "title": "DS & ML Portal",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "model_ensemble",
      "recurrent_neural_networks",
      "data_quality",
      "attention_mechanism",
      "rag",
      "transformer",
      "principal_component_analysis",
      "bert",
      "precision",
      "recall",
      "distributions",
      "k-nearest_neighbours",
      "supervised_learning",
      "clustering",
      "linear_regression",
      "xgboost",
      "model_evaluation",
      "optimisation_techniques",
      "deep_learning",
      "outliers",
      "evaluation_metrics",
      "unsupervised_learning",
      "learning_rate",
      "sklearn",
      "classification",
      "lstm",
      "multicollinearity",
      "statistics",
      "cost_function",
      "hyperparameter_tuning",
      "regression",
      "hyperparameter",
      "optimisation_function",
      "overfitting",
      "anomaly_detection",
      "support_vector_machines",
      "batch_processing",
      "machine_learning_algorithms",
      "decision_tree",
      "random_forests",
      "gradient_descent",
      "vanishing_and_exploding_gradients_problem",
      "model_selection",
      "cross_entropy",
      "imbalanced_datasets",
      "gradient_boosting",
      "regularisation",
      "model_optimisation",
      "logistic_regression",
      "accuracy",
      "loss_function",
      "apache_spark",
      "correlation",
      "binary_classification",
      "neural_network",
      "interpretability",
      "dimensionality_reduction",
      "reinforcement_learning",
      "feature_engineering",
      "data_analysis",
      "ml_tools"
    ],
    "inlinks": [
      "orthogonalization",
      "machine_learning_operations"
    ],
    "summary": "Machine Learning Fundamentals [[ML_Tools]] [[Supervised Learning]] [[Unsupervised Learning]] [[Reinforcement learning]] [[Deep Learning]] Model Training and Optimisation [[Learning rate]] [[Overfitting]] [[Regularisation]] [[Hyperparameter]] [[Hyperparameter Tuning]] [[Model Optimisation]]...",
    "TFIDF_Score": {
      "learning": 0.3518784300377791,
      "optimisation": 0.34746398302889825,
      "model": 0.28431580713641635,
      "regression": 0.19612917446818906,
      "evaluation": 0.17998812827658017,
      "gradient": 0.1751670199457004,
      "machine": 0.1743123555219909,
      "neural": 0.16680129518227704,
      "classification": 0.15413214213625112,
      "network": 0.1535517209707639
    }
  },
  "duckdb_in_python": {
    "title": "DuckDB in python",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "duckdb"
    ],
    "summary": "To use DuckDB in Python, you can follow these steps to install the DuckDB library and perform basic operations such as creating a database, running...",
    "TFIDF_Score": {
      "duckdb": 0.6112289011188361,
      "conn": 0.35572440490049695,
      "step": 0.2461163035227709,
      "install": 0.20923706780509457,
      "csv": 0.20302953145808242,
      "execute": 0.1737038939363166,
      "table": 0.13087667398901265,
      "result": 0.12227619949310847,
      "reading": 0.11857480163349898,
      "insert": 0.11041090836086752
    }
  },
  "duckdb_vs_sqlite": {
    "title": "DuckDB vs SQLite",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "columnar_storage",
      "python",
      "sqlite",
      "duckdb"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "Choosing between [[DuckDB]] and [[SQLite]] for data processing in [[Python]] depends on your specific use case and requirements. While SQLite is an excellent choice for...",
    "TFIDF_Score": {
      "duckdb": 0.5075712351795905,
      "analytical": 0.38370890191796947,
      "sqlite": 0.35903710828911467,
      "data": 0.2672635953388883,
      "query": 0.18745192295671717,
      "storage": 0.17572204520317888,
      "science": 0.14479397810540193,
      "workload": 0.13508781638431913,
      "workflow": 0.11577208475543856,
      "columnar": 0.11460804829304536
    }
  },
  "duckdb": {
    "title": "DuckDB",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_transformation",
      "querying",
      "duckdb_vs_sqlite",
      "parquet",
      "duckdb_in_python",
      "columnar_storage",
      "data_analysis"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "vectorized_engine"
    ],
    "summary": "DuckDB is an open-source analytical database management system designed for efficient data processing and analysis. It is optimized for running complex queries on large datasets...",
    "TFIDF_Score": {
      "duckdb": 0.720849105691352,
      "science": 0.2056353914339571,
      "data": 0.1821916234783257,
      "query": 0.17747860353783784,
      "sql": 0.14837197198315266,
      "columnar": 0.14468048513924578,
      "lightweight": 0.1419006458288128,
      "analytical": 0.12109793096683391,
      "analysis": 0.11195385569735115,
      "easy": 0.11121199244787607
    }
  },
  "dummy_variable_trap": {
    "title": "Dummy variable trap",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "one-hot_encoding",
      "multicollinearity"
    ],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Key Takeaways: The dummy variable trap occurs due to [[multicollinearity]], where ==one dummy variable can be perfectly predicted from others.== Dropping one dummy variable avoids...",
    "TFIDF_Score": {
      "dummy": 0.5478285591060328,
      "west": 0.3241630817194454,
      "windsor": 0.3241630817194454,
      "variable": 0.3069641885704927,
      "princeton": 0.28814496152839597,
      "robbinsville": 0.28814496152839597,
      "category": 0.16027763172821924,
      "multicollinearity": 0.1379439195174713,
      "one": 0.13490106164729174,
      "perfectly": 0.12002599501581893
    }
  },
  "eda": {
    "title": "EDA",
    "tags": [
      "data_exploration",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "standard_deviation",
      "data_quality",
      "correlation",
      "standardised/outliers",
      "data_visualisation",
      "distributions",
      "data_transformation",
      "eda_pandas.py",
      "data_analysis",
      "ml_tools"
    ],
    "inlinks": [
      "preprocessing",
      "factor_analysis",
      "scientific_method",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Exploratory [[Data Analysis]] (EDA) is an approach to analyzing datasets to summarize their main characteristics, often utilizing visual methods. EDA helps users to: Understand the...",
    "TFIDF_Score": {
      "data": 0.2894034616776902,
      "plot": 0.2630486542064299,
      "eda": 0.24908462234524956,
      "statistical": 0.23949357981801064,
      "correlation": 0.21640363988234645,
      "chart": 0.18026341962042552,
      "analysis": 0.17783382555363444,
      "transformation": 0.16909925844017829,
      "summarize": 0.16605641489683304,
      "scatter": 0.16080549391463228
    }
  },
  "eda_pandas.py": {
    "title": "EDA_Pandas.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "eda"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "edge_machine_learning_models": {
    "title": "Edge Machine Learning Models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pytorch",
      "onnx"
    ],
    "inlinks": [
      "small_language_models",
      "deepseek"
    ],
    "summary": "Edge ML refers to deploying machine learning models directly on edge devices, such as IoT sensors, smartphones, or embedded systems, instead of relying on cloud-based...",
    "TFIDF_Score": {
      "edge": 0.6031865789094851,
      "device": 0.3874385596375016,
      "model": 0.17268601038745732,
      "cloud": 0.16687356653572513,
      "autonomous": 0.11928309228585374,
      "mobile": 0.11928309228585374,
      "limited": 0.11906823342416899,
      "iot": 0.11570108170715873,
      "real": 0.11532499442685974,
      "smart": 0.11004908835572116
    }
  },
  "education_and_training": {
    "title": "Education and Training",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Adaptive Learning Systems Overview: Adaptive learning systems use technology to tailor educational experiences to individual student needs. RL is instrumental in personalizing these systems. Applications:...",
    "TFIDF_Score": {
      "student": 0.564543505269857,
      "adaptive": 0.2713117807806106,
      "engagement": 0.21127133808797172,
      "system": 0.19451726264110822,
      "learning": 0.18011913700833052,
      "path": 0.16991895138248053,
      "feedback": 0.16807880403308764,
      "strategy": 0.14600901427454158,
      "level": 0.1263507593604962,
      "engaged": 0.12534203139740221
    }
  },
  "elastic_net": {
    "title": "Elastic Net",
    "tags": [
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "ridge",
      "lasso"
    ],
    "inlinks": [
      "embedded_methods"
    ],
    "summary": "This method combines both L1 ([[Lasso]]) and L2 ([[Ridge]]) regularization by adding both absolute and squared penalties to the loss function. It strikes a balance...",
    "TFIDF_Score": {
      "l1_ratio": 0.27490935604151234,
      "lambda_1": 0.27490935604151234,
      "lambda_2": 0.27490935604151234,
      "regularization": 0.2609992460432884,
      "elasticnet": 0.2589576076611532,
      "w_i": 0.2589576076611532,
      "elastic": 0.2476396620684912,
      "control": 0.22124062028124167,
      "loss": 0.21678470981814443,
      "net": 0.211591087466574
    }
  },
  "elt": {
    "title": "ELT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "elt",
      "bigquery",
      "dbt"
    ],
    "inlinks": [
      "elt",
      "etl",
      "data_transformation_in_data_engineering",
      "dbt"
    ],
    "summary": "ELT (Extract, Load, Transform) is a data integration approach that involves three main steps: Extract (E): Data is extracted from a source system. Load (L):...",
    "TFIDF_Score": {
      "etl": 0.43279160275340645,
      "elt": 0.3876104265082542,
      "loading": 0.24108482415237123,
      "cloud": 0.21120689992165928,
      "data": 0.20986797616574496,
      "destination": 0.19705067652259803,
      "transformation": 0.17836574553859186,
      "cost": 0.1767350290471248,
      "extract": 0.16642867836494157,
      "transform": 0.15840517494124445
    }
  },
  "embedded_methods": {
    "title": "Embedded Methods",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "gradient_boosting",
      "regularisation",
      "ridge",
      "filter_method",
      "lasso",
      "loss_function",
      "interpretability",
      "decision_tree",
      "random_forests",
      "elastic_net",
      "wrapper_methods"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Embedded methods for [[Feature Selection]] ==integrate feature selection directly into the model training process.== Embedded methods provide a convenient and efficient approach to feature selection...",
    "TFIDF_Score": {
      "embedded": 0.4313975058857071,
      "method": 0.37304970662394743,
      "feature": 0.35947079940008875,
      "selection": 0.3395299184772766,
      "model": 0.28924026481933535,
      "training": 0.28365350348697604,
      "regularization": 0.1576687900676964,
      "elastic": 0.11219873184414204,
      "wrapper": 0.10822126205862433,
      "importance": 0.10480007714247684
    }
  },
  "emergent_behavior": {
    "title": "emergent behavior",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "encoding_categorical_variables": {
    "title": "Encoding Categorical Variables",
    "tags": [
      "code_snippet",
      "regressor",
      "data_cleaning"
    ],
    "aliases": null,
    "outlinks": [
      "feature_engineering",
      "regression"
    ],
    "inlinks": [
      "naive_bayes",
      "data_transformation_in_machine_learning"
    ],
    "summary": "Overview Categorical variables need to be converted into numerical representations to be used in models, particularly in [[Regression]] analysis. This process is essential for transforming...",
    "TFIDF_Score": {
      "dummy": 0.3089323391447397,
      "category": 0.29267204253480517,
      "onehotencoder": 0.26856177509755313,
      "encoding": 0.24077407081744578,
      "col": 0.2123310872958855,
      "categorical": 0.2062317206487908,
      "variable": 0.19981726400148841,
      "labelencoder": 0.1918298393553951,
      "column": 0.16422834780550766,
      "binary_encoder": 0.16291722211380139
    }
  },
  "energy_abm": {
    "title": "Energy ABM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agent-based_modelling"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy ABM Complex Systems Understanding: Energy systems involve numerous stakeholders (producers, consumers, regulators) with diverse interests and behaviors. [[Agent-Based Modelling|ABM]] helps capture this complexity, providing...",
    "TFIDF_Score": {
      "abm": 0.5651762903268728,
      "energy": 0.2678990473089087,
      "behavior": 0.22542261843803338,
      "system": 0.1947352929894905,
      "producer": 0.19316173327277839,
      "consumer": 0.1837388630977282,
      "agent": 0.17859936487260578,
      "stakeholder": 0.16324761921093414,
      "scenario": 0.13128819258546137,
      "meter": 0.1254825246698319
    }
  },
  "energy_storage": {
    "title": "Energy Storage",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy Storage Battery farms exist. Stored energy can be traded. Stored energy can be stored using distributed system such as EV cars.",
    "TFIDF_Score": {
      "energy": 0.6081113682503055,
      "stored": 0.4851487059858965,
      "farm": 0.28483621175586277,
      "traded": 0.28483621175586277,
      "battery": 0.2565818211492218,
      "car": 0.22352630317799577,
      "exist": 0.21534647013256136,
      "distributed": 0.16590067212294304,
      "storage": 0.14688010784639285,
      "system": 0.11050874075143198
    }
  },
  "energy": {
    "title": "Energy",
    "tags": [
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "demand_forecasting",
      "energy_abm",
      "agent-based_modelling",
      "how_to_model_to_improve_demand_forecasting",
      "differential_equations",
      "network_design",
      "stochastic_modeling",
      "neural_network",
      "smart_grids",
      "energy_storage",
      "regression"
    ],
    "inlinks": [
      "digital_twin",
      "industries_of_interest",
      "interview_notepad"
    ],
    "summary": "Areas of interest: - [[Smart Grids]] - [[Energy Storage]] - [[Demand forecasting]] - [[Network Design]] - [[Energy ABM]] Questions: - [[How to model to improve...",
    "TFIDF_Score": {
      "energy": 0.7186405306455274,
      "consumption": 0.2438344778719222,
      "demand": 0.17618360801319657,
      "pricing": 0.1399466047213415,
      "smart": 0.1245359025283348,
      "system": 0.12054886032786863,
      "consumer": 0.11374163462790887,
      "pattern": 0.11128864203234469,
      "agent": 0.11056008163777345,
      "network": 0.10959970314685095
    }
  },
  "environment_variables": {
    "title": "Environment Variables",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Solution 1: Set Environment Variables Permanently (Recommended) This ensures that environment variables persist across sessions. On Windows (Permanent) Open Control Panel → System → Advanced...",
    "TFIDF_Score": {
      "variable": 0.6034520174981874,
      "click": 0.37015419463928007,
      "environment": 0.23148641750250157,
      "system": 0.17498047856501023,
      "name": 0.15361303745958346,
      "panel": 0.1503373588574097,
      "pg_password": 0.1503373588574097,
      "pg_user": 0.1503373588574097,
      "restarted": 0.1503373588574097,
      "permanent": 0.1416139608792812
    }
  },
  "epoch": {
    "title": "Epoch",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An epoch in machine learning is a single pass through the entire training dataset. The number of epochs, denoted as $N$, determines how many times...",
    "TFIDF_Score": {
      "epoch": 0.5836487546692285,
      "6250": 0.42499133956157353,
      "batch": 0.40004420506923377,
      "dataset": 0.1451030162281523,
      "execution": 0.13449574894757693,
      "entire": 0.13334806835641125,
      "applied": 0.12545635203683406,
      "model": 0.12336489049598616,
      "time": 0.12303467180037911,
      "learning": 0.11451028108511578
    }
  },
  "er_diagrams": {
    "title": "ER Diagrams",
    "tags": [
      "database",
      "database_design",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "why_use_er_diagrams",
      "mermaid"
    ],
    "inlinks": [
      "data_modelling",
      "why_use_er_diagrams",
      "implementing_database_schema",
      "types_of_database_schema",
      "conceptual_model",
      "relating_tables_together"
    ],
    "summary": "ER Diagrams are a visual representation of the database structure. Related: - [[Why use ER diagrams]] - [[Mermaid]] Example Entities are tables in the database....",
    "TFIDF_Score": {
      "department": 0.5252763657101479,
      "employee": 0.5171845714208912,
      "mermaid": 0.25514957686259265,
      "diagram": 0.21545057655755465,
      "many": 0.20529021793020957,
      "integer": 0.20298652962928584,
      "dept_id": 0.20183186584339774,
      "one": 0.19779746102212906,
      "belongs": 0.1700997179083951,
      "table": 0.14081038943119786
    }
  },
  "estimator": {
    "title": "Estimator",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "maximum_likelihood_estimation",
      "statistics",
      "statistical_tests"
    ],
    "summary": "Given a sample an estimator is a formula that approximates a population parameter i.e feature",
    "TFIDF_Score": {
      "approximates": 0.48907239793348545,
      "estimator": 0.45121207457923623,
      "population": 0.39537681128520535,
      "formula": 0.3236060399563002,
      "given": 0.31574094317941737,
      "sample": 0.2988979177535277,
      "parameter": 0.26573134475736204,
      "feature": 0.1911196099605119
    }
  },
  "etl_pipeline_example": {
    "title": "ETL Pipeline Example",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "\"domains\",\"country\",\"web_pages\",\"name\""
    ],
    "inlinks": [
      "etl"
    ],
    "summary": "Link [Github](https://github.com/syalanuj/youtube/blob/main/de_fundamentals_python/etl.py 1. Extract using a API Get data via api or download. 2. Transform Put into a pandas df. 3. Load Save df as...",
    "TFIDF_Score": {
      "university": 0.3056871167112485,
      "extract": 0.2590244649621371,
      "web_page": 0.24338804668920253,
      "load": 0.22890771874606688,
      "str": 0.20512240676718155,
      "api": 0.20364541025252592,
      "transform": 0.1972295452418026,
      "data": 0.17816293585854567,
      "def": 0.1649735575983199,
      "api_url": 0.1622586977928017
    }
  },
  "etl_vs_elt": {
    "title": "ETL vs ELT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl"
    ],
    "inlinks": [
      "elt",
      "etl",
      "data_transformation_in_data_engineering"
    ],
    "summary": "ETL (Extract, Transform, and Load) and ELT (Extract, Load, and Transform) are two paradigms for moving data from one system to another. ==ELT is most...",
    "TFIDF_Score": {
      "elt": 0.44022323053345225,
      "etl": 0.4213175420077123,
      "analyst": 0.2603830301747828,
      "destination": 0.2557686922305684,
      "data": 0.22287710163527663,
      "approach": 0.19814210018284772,
      "every": 0.1701188015709959,
      "going": 0.15237212108718742,
      "disadvantage": 0.13559314038045997,
      "demonstrates": 0.13274197205966878
    }
  },
  "etl": {
    "title": "ETL",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "dagster",
      "apache_airflow",
      "etl",
      "elt",
      "etl_pipeline_example",
      "data_transformation"
    ],
    "inlinks": [
      "etl_vs_elt",
      "fabric",
      "etl",
      "reverse_etl",
      "slowly_changing_dimension",
      "data_transformation_in_data_engineering",
      "data_warehouse"
    ],
    "summary": "ETL (Extract, Transform, Load) is a data integration process that involves moving data from one system to another. It consists of three main stages: Extract:...",
    "TFIDF_Score": {
      "etl": 0.5571123764538757,
      "elt": 0.3175153944206653,
      "data": 0.2813165806988088,
      "transform": 0.17301248249978005,
      "destination": 0.16141625456403783,
      "database": 0.15662479836766247,
      "extract": 0.136331903994407,
      "process": 0.12474978623476383,
      "load": 0.1204806084252669,
      "paradigm": 0.11995747313149159
    }
  },
  "etlt": {
    "title": "EtLT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "[!important] EtLT refers to Extract, “tweak”, Load, Transform, and can be thought of an extension to the ELT approach to data integration. When compared to...",
    "TFIDF_Score": {
      "etlt": 0.4353941458003669,
      "tweak": 0.4353941458003669,
      "elt": 0.34369523630814697,
      "light": 0.21769707290018345,
      "approach": 0.21657351931968538,
      "incorporates": 0.19477234552712847,
      "extracted": 0.18967405989815683,
      "extension": 0.18525772400656648,
      "thought": 0.18136223967463552,
      "destination": 0.17472537940274507
    }
  },
  "evaluating_language_models": {
    "title": "Evaluating Language Models",
    "tags": [
      "evaluation",
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "bradley-terry_model",
      "elo_rating_system",
      "prompting",
      "llm",
      "lmsys"
    ],
    "inlinks": [],
    "summary": "The [[LMSYS]] Chatbot Arena is a platform where various large language models ([[LLM]]s), including versions of GPT and other prominent models like LLaMA or Claude,...",
    "TFIDF_Score": {
      "model": 0.3558693743617188,
      "arena": 0.3079548181991567,
      "chatbot": 0.23096611364936753,
      "ranking": 0.2208715582831755,
      "rating": 0.2208715582831755,
      "gpt": 0.18871958076993442,
      "lmsys": 0.16346239356634112,
      "llama": 0.15397740909957836,
      "llm": 0.14790546208416702,
      "rank": 0.14202774189084705
    }
  },
  "evaluation_metrics": {
    "title": "Evaluation Metrics",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "ml_tools",
      "evaluation_metrics.py",
      "pasted_image_20241217073706.png",
      "accuracy",
      "pasted_image_20241222091831.png",
      "f1_score",
      "precision",
      "recall",
      "why_type_1_and_type_2_matter",
      "precision_or_recall",
      "confusion_matrix",
      "specificity"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "test_loss_when_evaluating_models",
      "forecasting_autoarima.py",
      "determining_threshold_values",
      "model_selection",
      "recall",
      "neural_network_classification",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "metric",
      "train-dev-test_sets",
      "imbalanced_datasets",
      "model_evaluation"
    ],
    "summary": "Description [[Confusion Matrix]] [[Accuracy]] [[Precision]] [[Recall]] [[Precision or Recall]] [[F1 Score]] [[Recall]] [[Specificity]] ![[Pasted image 20241222091831.png]] Resources: Link to good website describing these In [[ML_Tools]]...",
    "TFIDF_Score": {
      "predicts": 0.380148062328646,
      "spam": 0.37507417776131846,
      "negative": 0.2604366473620213,
      "positive": 0.2604366473620213,
      "metric": 0.23245373671922845,
      "model": 0.22629801902427835,
      "email": 0.18809462862523704,
      "occurs": 0.17789887655273587,
      "false": 0.1710061228482553,
      "type": 0.16890668423803984
    }
  },
  "event_driven_events": {
    "title": "Event Driven Events",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event_driven_microservices",
      "monolith_architecture",
      "data_lake",
      "api_driven_microservices",
      "business_observability"
    ],
    "inlinks": [
      "aws_lambda",
      "event_driven",
      "event_driven_microservices"
    ],
    "summary": "Events can be stored in a [[Data Lake]] and analysed to find patterns/predictions. [[Event Driven Microservices]] allow for [[Business observability]] [[Monolith Architecture]] [[Event Driven Microservices]]...",
    "TFIDF_Score": {
      "microservices": 0.5755052743641786,
      "event": 0.4477623717578056,
      "driven": 0.42617581012613975,
      "analysed": 0.23374027421392413,
      "monolith": 0.22017738172344947,
      "observability": 0.16634134764819553,
      "lake": 0.15843409368545866,
      "api": 0.14667976103079564,
      "allow": 0.13437929038809698,
      "stored": 0.13270643138224486
    }
  },
  "event_driven_microservices": {
    "title": "Event Driven Microservices",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "microservices",
      "software_architecture",
      "event_driven_events"
    ],
    "inlinks": [
      "event_driven",
      "event_driven_events"
    ],
    "summary": "Event-driven microservices refer to a [[software architecture]] pattern where [[microservices]] communicate and coordinate their actions through the production, detection, consumption, and reaction to [[Event Driven...",
    "TFIDF_Score": {
      "event": 0.7370221984621524,
      "service": 0.2084251865735922,
      "driven": 0.196417329034801,
      "microservices": 0.1894577076109822,
      "producer": 0.17767471356622,
      "broker": 0.17395900486312277,
      "consumer": 0.16900733555631448,
      "real": 0.11635067458483486,
      "architecture": 0.10180834271847217,
      "processing": 0.10063078856931934
    }
  },
  "event_driven": {
    "title": "Event Driven",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event_driven_microservices",
      "event-driven_architecture",
      "scalability",
      "event_driven_events"
    ],
    "inlinks": [],
    "summary": "Event-driven refers to a ==programming paradigm== or architectural style where the flow of the program is determined by events—changes in state or conditions that trigger...",
    "TFIDF_Score": {
      "event": 0.8168333215189357,
      "driven": 0.2591513002531121,
      "producer": 0.14065347472867948,
      "consumer": 0.13379207723766395,
      "component": 0.11681730204765878,
      "change": 0.11015528743816302,
      "action": 0.1078909919785322,
      "architecture": 0.10074377546684747,
      "trigger": 0.09059024569524339,
      "user": 0.0801756318819231
    }
  },
  "event-driven_architecture": {
    "title": "Event-Driven Architecture",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "publish_and_subscribe",
      "event_driven",
      "alternatives_to_batch_processing"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "everything": {
    "title": "Everything",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Can we search with descriptions ? Tips use \\ to match in paths i.e \\playground can copy file new window crl+ n Use | to...",
    "TFIDF_Score": {
      "search": 0.5857532005806326,
      "crl": 0.31374272269205905,
      "playground": 0.31374272269205905,
      "tip": 0.2574946234802551,
      "copy": 0.2332940928048949,
      "window": 0.22327511800404268,
      "syntax": 0.2176553886062351,
      "match": 0.21266144265164655,
      "path": 0.21266144265164655,
      "use": 0.19683078972847476
    }
  },
  "excel_&_sheets": {
    "title": "Excel & Sheets",
    "tags": [
      "software",
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "standardised/gsheets",
      "data_validation"
    ],
    "inlinks": [],
    "summary": "Links Google sheets example folder see [[standardised/GSheets|GSheets]] Excel Example folder: Desktop/Example_Examples Tools common to Excel and Sheets Vlookup Table | Product ID | Product Name...",
    "TFIDF_Score": {
      "excel": 0.2623459568146512,
      "data": 0.25475257657456724,
      "column": 0.24435069323798445,
      "tab": 0.2339517799054318,
      "consolidate": 0.225085627527666,
      "formula": 0.19908255339227637,
      "select": 0.18793649943600926,
      "click": 0.18473205820917113,
      "cell": 0.1805265101740932,
      "want": 0.16922017038343493
    }
  },
  "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.": {
    "title": "Explain different gradient descent algorithms, their advantages, and limitations.",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "deep_learning"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "explain_the_curse_of_dimensionality": {
    "title": "Explain the curse of dimensionality",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "regularisation",
      "ngrams",
      "multidimensional_scaling",
      "dimensionality_reduction",
      "language_models",
      "manifold_learning",
      "nlp"
    ],
    "inlinks": [
      "dimensionality_reduction"
    ],
    "summary": "The curse of dimensionality refers to the various phenomena that arise when working with data in high-dimensional spaces. Increased Data ==Sparsity==: As the number of...",
    "TFIDF_Score": {
      "curse": 0.3399130695804466,
      "dimensionality": 0.30940763152738254,
      "dimensional": 0.2631745852675501,
      "manifold": 0.2369262611053275,
      "space": 0.18607862630822908,
      "high": 0.15883118473094618,
      "data": 0.142639755206465,
      "distance": 0.14221328263623628,
      "difficult": 0.13951727396733485,
      "gram": 0.13002276078130873
    }
  },
  "exploration_vs._exploitation": {
    "title": "Exploration vs. Exploitation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "One of the major challenges in [[Reinforcement learning]] is balancing exploration (trying new actions) and exploitation (choosing the best-known actions). The ==epsilon-greedy strategy== is commonly...",
    "TFIDF_Score": {
      "action": 0.43019919832631204,
      "epsilon": 0.3986853442906397,
      "known": 0.2606272699302776,
      "best": 0.24270334988583944,
      "exploration": 0.23020438363334328,
      "exploiting": 0.22879415484351512,
      "greedy": 0.22879415484351512,
      "major": 0.21103821242526835,
      "exploitation": 0.20470084226718563,
      "trying": 0.1836319844708849
    }
  },
  "exploration": {
    "title": "Exploration",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "policy"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "f1_score": {
    "title": "F1 Score",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "precision",
      "recall",
      "harmonic_mean"
    ],
    "inlinks": [
      "confusion_matrix",
      "classification_report",
      "model_observability",
      "evaluation_metrics"
    ],
    "summary": "Definition The F1 Score is the harmonic mean of precision and recall. It provides a balanced view of both metrics and is particularly useful when:...",
    "TFIDF_Score": {
      "positive": 0.42392517737554536,
      "precision": 0.4141696520114052,
      "recall": 0.4141696520114052,
      "score": 0.3150454485767707,
      "false": 0.23858986171891944,
      "harmonic": 0.2328606240298961,
      "text": 0.22507227228483417,
      "true": 0.15296988172032075,
      "mean": 0.14898130867427561,
      "negative": 0.12112147925015582
    }
  },
  "fabric": {
    "title": "Fabric",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "t-sql",
      "powerbi",
      "etl",
      "synapse",
      "microsoft",
      "data_lake",
      "pyspark",
      "scala",
      "relational_database",
      "data_factory",
      "data_warehouse"
    ],
    "inlinks": [],
    "summary": "Fabric is a unified analytics platform that operates in the cloud, eliminating the need for local installations. It provides an integrated environment for data analysis,...",
    "TFIDF_Score": {
      "data": 0.4044328856120585,
      "powerbi": 0.2909375188603976,
      "fabric": 0.25113408784455543,
      "workspace": 0.23656191090335882,
      "analysis": 0.16944380286878963,
      "synapse": 0.1674227252297036,
      "cloud": 0.1526299132621719,
      "eliminating": 0.1454687594301988,
      "installation": 0.1454687594301988,
      "platform": 0.14531268684369883
    }
  },
  "fact_table": {
    "title": "Fact Table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "facts",
      "database_schema",
      "fact_table",
      "dimension_table",
      "granularity",
      "data_analysis",
      "data_warehouse"
    ],
    "inlinks": [
      "facts",
      "components_of_the_database",
      "dimension_table",
      "granularity",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "A fact table is a central component of a star [[Database Schema|schema]] or snowflake schema in a [[data warehouse]], it stores [[Facts]]. Essential for [[data...",
    "TFIDF_Score": {
      "fact": 0.5826633544817919,
      "table": 0.4557455505312819,
      "sale": 0.2427763977007466,
      "dimension": 0.2110988172708746,
      "foreign": 0.1514999952161726,
      "data": 0.1383319744081056,
      "schema": 0.12761330086902908,
      "store": 0.11510817743342493,
      "descriptive": 0.11221052132858887,
      "granularity": 0.1098510281951321
    }
  },
  "factor_analysis": {
    "title": "Factor Analysis",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "factor_analysis.py",
      "eda",
      "ml_tools"
    ],
    "inlinks": [],
    "summary": "Factor Analysis (FA) is a statistical method used for: - [[dimensionality reduction]], - [[EDA]] - or latent variable detection It identifies underlying relationships between observed...",
    "TFIDF_Score": {
      "factor": 0.5059774168686104,
      "latent": 0.4882596125796344,
      "observed": 0.32590388846045887,
      "variable": 0.32232365159914755,
      "variance": 0.25042940277068776,
      "unobserved": 0.1806753336065927,
      "shared": 0.12534142500669843,
      "unique": 0.11867495244063217,
      "common": 0.08838089928464259,
      "factor_analysis": 0.08509577998334651
    }
  },
  "factor_analysis.py": {
    "title": "Factor_Analysis.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "factor_analysis"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Factor_Analysis.py 1. Factor Loadings Table This table shows how strongly each feature (e.g., sepal length (cm)) is correlated with the two extracted factors (Factor 1...",
    "TFIDF_Score": {
      "factor": 0.8101172773482846,
      "specie": 0.20869931514330137,
      "sepal": 0.18240819606869185,
      "variance": 0.17642281816901836,
      "latent": 0.15634988234459302,
      "petal": 0.13029156862049418,
      "loading": 0.12041002295374983,
      "length": 0.11899027148138852,
      "dataset": 0.10535149667973298,
      "contribution": 0.10159655931650734
    }
  },
  "facts": {
    "title": "Facts",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fact_table"
    ],
    "inlinks": [
      "dimensions",
      "dimension_table",
      "granularity",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "Facts are quantitative data points that are typically stored in the [[Fact Table]]. They represent measurable events or metrics, such as sales revenue or quantities...",
    "TFIDF_Score": {
      "fact": 0.49148599996027814,
      "revenue": 0.31909211701305074,
      "measurable": 0.30778023109994307,
      "sold": 0.29853774733643373,
      "quantitative": 0.283954189964262,
      "quantity": 0.27264230405115436,
      "sale": 0.24574299998013907,
      "event": 0.22619249108925793,
      "represent": 0.2169500073257486,
      "stored": 0.20111469961226616
    }
  },
  "fastapi": {
    "title": "FastAPI",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fastapi_example.py",
      "ml_tools",
      "pydantic"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "model_deployment",
      "api",
      "pydantic"
    ],
    "summary": "FastAPI is a modern web framework for building APIs with Python. It is designed to be fast and easy to use, leveraging Python's type hints...",
    "TFIDF_Score": {
      "redoc": 0.30447598459841324,
      "fastapi_example": 0.28680861829225324,
      "uvicorn": 0.28680861829225324,
      "fastapi": 0.24407086022076757,
      "127": 0.1912057455281688,
      "8000": 0.1912057455281688,
      "swagger": 0.1912057455281688,
      "documentation": 0.1910687613022874,
      "support": 0.17753894059475256,
      "asynchronous": 0.17636691273561705
    }
  },
  "fastapi_example.py": {
    "title": "FastAPI_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fastapi"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/FastAPI_Example.py Explanation of New Features Path and Query Parameter Metadata: Added descriptions and constraints for better validation and autogenerated documentation. Nested Models: Demonstrated hierarchical data...",
    "TFIDF_Score": {
      "item": 0.5642066992266005,
      "curl": 0.33412337949409987,
      "127": 0.2797650744500067,
      "8000": 0.2797650744500067,
      "endpoint": 0.23654899668971835,
      "item_id": 0.23512385964399624,
      "get": 0.13966434190653626,
      "http": 0.13599214815783342,
      "json": 0.13433364933948286,
      "call": 0.13209945529101666
    }
  },
  "feature_engineering": {
    "title": "Feature Engineering",
    "tags": [
      "#ml_process",
      "#ml_optimisation",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "c1_w2_lab07_featureenglecture.png"
    ],
    "inlinks": [
      "feature_selection_and_creation",
      "ds_&_ml_portal",
      "model_optimisation",
      "preprocessing",
      "class_separability",
      "automated_feature_creation",
      "encoding_categorical_variables",
      "regression"
    ],
    "summary": "Its the term given to the iterative process of building good features for a better model. Its the process that makes relevant features (using formulas...",
    "TFIDF_Score": {
      "feature": 0.425113071721834,
      "bedroom": 0.26208590999632486,
      "footage": 0.26208590999632486,
      "picking": 0.26208590999632486,
      "square": 0.18859041420590236,
      "involve": 0.18460537950847916,
      "combining": 0.1792885445341863,
      "engineering": 0.16426577629741201,
      "capture": 0.15515396482036736,
      "c1_w2_lab07_featureenglecture": 0.13911518065713843
    }
  },
  "feature_evaluation": {
    "title": "Feature Evaluation",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Note Garbage in garbae out. It is the features that What is involved: Want to assess the usefulness of chosen features Measuring feature importance: Quantifying...",
    "TFIDF_Score": {
      "feature": 0.38249581300172003,
      "importance": 0.3159528966815438,
      "model": 0.27250188525137353,
      "done": 0.23767115586392953,
      "analysing": 0.18775348606952885,
      "garbae": 0.18775348606952885,
      "usefulness": 0.18775348606952885,
      "garbage": 0.17685899920868836,
      "permutation": 0.17685899920868836,
      "room": 0.16912923776744196
    }
  },
  "feature_extraction": {
    "title": "Feature Extraction",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "attention_mechanism",
      "interpretability",
      "dimensionality_reduction",
      "llm",
      "activation_atlases"
    ],
    "inlinks": [
      "convolutional_neural_networks"
    ],
    "summary": "Summary: In machine learning, Feature extraction is the process of transforming raw data into a set of useful features that can be effectively used by...",
    "TFIDF_Score": {
      "extraction": 0.4610606057695199,
      "feature": 0.43221170741594306,
      "atlas": 0.2246370742958955,
      "model": 0.15214921532525166,
      "data": 0.14799330157451748,
      "important": 0.14712220128402126,
      "attention": 0.14344130397115887,
      "mechanism": 0.14096720168225163,
      "dimensionality": 0.13758012831238384,
      "relationship": 0.13494455975582445
    }
  },
  "feature_importance": {
    "title": "Feature Importance",
    "tags": [
      "ml_process",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "gini_impurity",
      "feature_selection",
      "shapley_additive_explanations",
      "interpretability",
      "random_forests",
      "model_building",
      "xgboost",
      "local_interpretable_model-agnostic_explanations"
    ],
    "inlinks": [
      "shapley_additive_explanations",
      "feature_selection_vs_feature_importance",
      "model_evaluation"
    ],
    "summary": "Feature importance refers to ==techniques that assign scores to input features== (predictors) in a machine learning model to ==indicate their relative impact on the model's...",
    "TFIDF_Score": {
      "importance": 0.5191107692843191,
      "feature": 0.4189605430024691,
      "model": 0.37608541781641475,
      "agnostic": 0.160817121891418,
      "impurity": 0.160817121891418,
      "explanation": 0.15234096551519516,
      "tree": 0.13381645554298563,
      "method": 0.12318955320587663,
      "decrease": 0.12280520936602374,
      "forest": 0.11714814884090738
    }
  },
  "feature_scaling": {
    "title": "Feature Scaling",
    "tags": [
      "data_cleaning",
      "data_processing"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "naive_bayes",
      "principal_component_analysis",
      "decision_tree",
      "gradient_descent",
      "standardisation",
      "linear_discriminant_analysis",
      "normalisation",
      "pasted_image_20241224083928.png"
    ],
    "inlinks": [
      "clustering",
      "preprocessing",
      "why_and_when_is_feature_scaling_necessary"
    ],
    "summary": "Used in preparing data for machine learning models. Feature Scaling is a [[preprocessing]] step in machine learning that involves adjusting the range and distribution of...",
    "TFIDF_Score": {
      "scaling": 0.48111592779665346,
      "feature": 0.3313110085688123,
      "scale": 0.3234675329330813,
      "distance": 0.27468304720500003,
      "preprocessing": 0.1707476253392379,
      "algorithm": 0.15647599343116883,
      "principal": 0.14334621784781887,
      "range": 0.14178455403053783,
      "descent": 0.13012520130181432,
      "sklearn": 0.11728831229920936
    }
  },
  "feature_selection_and_creation": {
    "title": "Feature selection and creation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "feature_engineering"
    ],
    "inlinks": [],
    "summary": "[[Feature Selection]] [[Feature Engineering]] After the data is ready. Which features have the best value, which play the biggest role. Combining features to simplify the...",
    "TFIDF_Score": {
      "feature": 0.4773370201622769,
      "select": 0.2677130593814299,
      "control": 0.2514204069774839,
      "sensible": 0.23430763558477294,
      "stepwise": 0.23430763558477294,
      "regression": 0.21113082622617463,
      "biggest": 0.21106543819308404,
      "ready": 0.21106543819308404,
      "ask": 0.18387381037608994,
      "play": 0.18034092447409272
    }
  },
  "feature_selection_vs_feature_importance": {
    "title": "Feature Selection vs Feature Importance",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "feature_selection",
      "interpretability",
      "feature_importance"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Summary [[Feature Selection]] is about choosing which features to include in the model ==before training==, aiming to improve model performance and efficiency. [[Feature Importance]] is...",
    "TFIDF_Score": {
      "model": 0.4234177965839996,
      "feature": 0.39621874956790476,
      "aiming": 0.23943195425890823,
      "different": 0.231142771405568,
      "choosing": 0.20000100814342228,
      "serve": 0.1977437203208513,
      "stage": 0.1879903809341707,
      "role": 0.18464862552170203,
      "applied": 0.17223847703615866,
      "trained": 0.17106235264814557
    }
  },
  "feature_selection": {
    "title": "Feature Selection",
    "tags": [
      "ml_process",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "correlation",
      "variance",
      "filter_methods",
      "clustering",
      "heatmap",
      "principal_component_analysis",
      "svd",
      "dimensionality_reduction",
      "anova",
      "distributions",
      "feature_selection_vs_feature_importance",
      "wrapper_methods",
      "embedded_methods",
      "model_evaluation"
    ],
    "inlinks": [
      "feature_selection_and_creation",
      "regularisation",
      "ds_&_ml_portal",
      "ridge",
      "data_selection_in_ml",
      "lasso",
      "pca_principal_components",
      "correlation",
      "preprocessing",
      "filter_methods",
      "explain_the_curse_of_dimensionality",
      "feature_selection_vs_feature_importance",
      "p_values",
      "feature_importance",
      "linear_regression",
      "wrapper_methods",
      "logistic_regression_statsmodel_summary_table",
      "embedded_methods"
    ],
    "summary": "Purpose: The primary goal of feature selection is to identify and retain the most relevant features for model training while ==removing irrelevant or redundant ones==....",
    "TFIDF_Score": {
      "feature": 0.6368920996347339,
      "selection": 0.2556637904586179,
      "redundant": 0.18046637677399271,
      "model": 0.17015313496849002,
      "training": 0.1699005145599174,
      "correlation": 0.15400990396320066,
      "method": 0.1404519849866145,
      "low": 0.1365656239741836,
      "noisy": 0.13534978258049454,
      "variance": 0.12999746779614507
    }
  },
  "feature_distribution.py": {
    "title": "Feature_Distribution.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "feed_forward_neural_network": {
    "title": "Feed Forward Neural Network",
    "tags": [
      "deep_learning",
      "classifier"
    ],
    "aliases": [
      "FFNN"
    ],
    "outlinks": [
      "ridge",
      "overfitting",
      "recurrent_neural_networks",
      "loss_function",
      "neural_network",
      "supervised_learning",
      "forward_propagation"
    ],
    "inlinks": [
      "transformer",
      "types_of_neural_networks"
    ],
    "summary": "A Feedforward Neural Network (FFNN) is the simplest type of [[Neural network]]. In this model, connections between neurons do not form a cycle, allowing data...",
    "TFIDF_Score": {
      "network": 0.4045694503196978,
      "layer": 0.3883372762687526,
      "ffnns": 0.3440857144144136,
      "feedforward": 0.24308998077992036,
      "hidden": 0.1951059265746416,
      "neural": 0.17579147372071757,
      "shallow": 0.16205998718661357,
      "input": 0.15502504004111906,
      "recurrent": 0.12792832914601737,
      "sequential": 0.12792832914601737
    }
  },
  "feedback_template": {
    "title": "Feedback Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Praise: I really appreciate your work on this add here FYI: It's really not a big deal, but I'm letting you know just in case....",
    "TFIDF_Score": {
      "add": 0.5260607617899196,
      "really": 0.3859625099966116,
      "appreciate": 0.1929812549983058,
      "fairly": 0.1929812549983058,
      "fyi": 0.1929812549983058,
      "holding": 0.1929812549983058,
      "plea": 0.1929812549983058,
      "praise": 0.1929812549983058,
      "letting": 0.1817834243162753,
      "suggestion": 0.1817834243162753
    }
  },
  "filter_method": {
    "title": "Filter method",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "wrapper_methods",
      "embedded_methods"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "filter_methods": {
    "title": "filter methods",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "categorical",
      "correlation",
      "anova"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "For [[Feature Selection]] Pearson [[Correlation]] Coefficient: Measures the linear correlation between two continuous variables. Features with low correlation with the target variable are considered less...",
    "TFIDF_Score": {
      "variable": 0.5157600486608103,
      "categorical": 0.304973154392731,
      "target": 0.2671465576464266,
      "feature": 0.2617643787899529,
      "correlation": 0.2531939514498504,
      "mutual": 0.16746284232008246,
      "information": 0.15915265204575013,
      "chi": 0.158182208281151,
      "distribution": 0.13648324810795498,
      "calculates": 0.12922583719602623
    }
  },
  "firebase": {
    "title": "Firebase",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "aws"
    ],
    "inlinks": [],
    "summary": "Googles version of [[AWS]] Setup basics Project idea: Set up a basic emailer app.",
    "TFIDF_Score": {
      "basic": 0.5215525499785387,
      "emailer": 0.41201175241889876,
      "app": 0.3171154885327346,
      "aws": 0.2932083072749798,
      "setup": 0.2858283924917984,
      "google": 0.263088820951088,
      "idea": 0.25855101603659925,
      "version": 0.2543376265975337,
      "project": 0.24158860098983637,
      "set": 0.17572991483594427
    }
  },
  "fishbone_diagram": {
    "title": "Fishbone diagram",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "documentation_&_meetings",
      "pasted_image_20250312162034.png"
    ],
    "inlinks": [],
    "summary": "Fishbone diagram [[Documentation & Meetings]] Root cause analysis: [[Documentation & Meetings]] - 5 Y's - Fishbone diagram: start at issue at head - ![[Pasted image...",
    "TFIDF_Score": {
      "fishbone": 0.494101014068072,
      "meeting": 0.35650564703573845,
      "diagram": 0.3516271589516558,
      "documentation": 0.3100647456340719,
      "20250312162034": 0.247050507034036,
      "ownership": 0.23271528184493045,
      "entering": 0.2225442777091943,
      "people": 0.19014880466493347,
      "head": 0.18087291202335945,
      "cause": 0.1758135794758279
    }
  },
  "fitting_weights_and_biases_of_a_neural_network": {
    "title": "Fitting weights and biases of a neural network",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "regularisation",
      "model_parameters",
      "stochastic_gradient_descent",
      "ridge",
      "loss_function",
      "binary_classification",
      "learning_rate",
      "gradient_descent",
      "backpropagation",
      "distributions",
      "dropout",
      "forward_propagation",
      "optimisation_techniques",
      "ml_tools"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "For a neural network model, fitting weights and biases involves optimizing these [[Model Parameters]] so the model learns to map input features ($X$) to target...",
    "TFIDF_Score": {
      "weight": 0.3031660179623629,
      "initialization": 0.29998340195051154,
      "activation": 0.23076048491752837,
      "bias": 0.22214508718777468,
      "learning": 0.19696865435163666,
      "convergence": 0.19255613262992743,
      "rate": 0.17532950284657497,
      "loss": 0.16813519911645536,
      "network": 0.15041709927217972,
      "gradient": 0.14707812099388953
    }
  },
  "flask": {
    "title": "Flask",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "flask",
      "pasted_image_20240922202938.png"
    ],
    "inlinks": [
      "model_deployment",
      "flask",
      "jinja_template"
    ],
    "summary": "software web app framework for writing web pages uses decorators ![[Pasted image 20240922202938.png]] [[Flask]] Flask app example https://www.youtube.com/watch?v=wBCEDCiQh3Q&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl You can run a flask app in...",
    "TFIDF_Score": {
      "flask": 0.5657402688511861,
      "app": 0.501152169009907,
      "web": 0.19355981726928376,
      "20240922202938": 0.16278027315669077,
      "colab": 0.16278027315669077,
      "ngrok": 0.16278027315669077,
      "plcwfeusays2my8yuloa6jewb1": 0.16278027315669077,
      "publicly": 0.16278027315669077,
      "qbknsul": 0.16278027315669077,
      "wbcedciqh3q": 0.16278027315669077
    }
  },
  "folder_tree_diagram": {
    "title": "Folder Tree Diagram",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Links Simple method https://www.digitalcitizen.life/how-export-directory-tree-folder-windows/ More general https://superuser.com/questions/272699/how-do-i-draw-a-tree-file-structure Treeviz Graphviz tree /a /f >output.doc",
    "TFIDF_Score": {
      "tree": 0.4133610744822129,
      "272699": 0.254105750914988,
      "digitalcitizen": 0.254105750914988,
      "graphviz": 0.254105750914988,
      "superuser": 0.254105750914988,
      "treeviz": 0.254105750914988,
      "export": 0.22889967512326378,
      "http": 0.21480375773390412,
      "draw": 0.2141550659947698,
      "folder": 0.19941045686627584
    }
  },
  "forecasting_autoarima.py": {
    "title": "Forecasting_AutoArima.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "evaluation_metrics",
      "time_series_forecasting",
      "datasets",
      "ml_tools"
    ],
    "inlinks": [
      "forecasting_exponential_smoothing.py",
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_AutoArima.py ARIMA (AutoRegressive Integrated Moving Average) is a popular [[Time Series Forecasting]]method that models the autocorrelations within the data. It is particularly useful for datasets...",
    "TFIDF_Score": {
      "arima": 0.5499398441013812,
      "seasonal": 0.28631292884599846,
      "observation": 0.24504160315566662,
      "moving": 0.188781045510435,
      "lagged": 0.17442810382596102,
      "average": 0.15901632422310005,
      "autoregressive": 0.1571256697432518,
      "model": 0.15189700536093798,
      "forecasting": 0.11823105211918568,
      "datasets": 0.11748029454488568
    }
  },
  "forecasting_baseline.py": {
    "title": "Forecasting_Baseline.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Baseline.py Baseline methods are essential for establishing a performance benchmark. They provide insights into the data's underlying patterns and help in assessing the effectiveness of...",
    "TFIDF_Score": {
      "forecasting": 0.3569360959412845,
      "future": 0.3277141011763695,
      "forecast": 0.32413889120511535,
      "period": 0.29367482530088834,
      "baseline": 0.22877086122188856,
      "seasonal": 0.2160925941367436,
      "last": 0.21106113395760584,
      "naive": 0.21106113395760584,
      "value": 0.1904770520088252,
      "method": 0.15771928924041265
    }
  },
  "forecasting_exponential_smoothing.py": {
    "title": "Forecasting_Exponential_Smoothing.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "forecasting_autoarima.py",
      "time_series_forecasting",
      "datasets"
    ],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Exponential_Smoothing.py Exponential smoothing models are a set of [[Time Series Forecasting]] techniques that apply weighted averages of past observations, with the weights decaying exponentially over...",
    "TFIDF_Score": {
      "trend": 0.3989184313572811,
      "smoothing": 0.3773840311459779,
      "series": 0.2944420871167936,
      "exponential": 0.2630442766164718,
      "seasonality": 0.20885791785728303,
      "seasonal": 0.1972832074623538,
      "holt": 0.1602523315220664,
      "s": 0.1602523315220664,
      "description": 0.14838726353103152,
      "constant": 0.12575862816025815
    }
  },
  "foreign_key": {
    "title": "Foreign Key",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together",
      "turning_a_flat_file_into_a_database"
    ],
    "summary": "A foreign key is a field in one table that uniquely identifies a row in another table, linking to the primary key of that table....",
    "TFIDF_Score": {
      "departmentid": 0.5123997869564896,
      "table": 0.5060034616119388,
      "department": 0.2022413111900257,
      "employee": 0.19912581772074162,
      "foreign": 0.1962413213065462,
      "104": 0.13599088354679362,
      "dana": 0.13599088354679362,
      "departmentname": 0.13599088354679362,
      "employeeid": 0.13599088354679362,
      "employeename": 0.13599088354679362
    }
  },
  "forward_propagation": {
    "title": "Forward Propagation",
    "tags": [
      "deep_learning",
      "statistics"
    ],
    "aliases": [
      "feedforward pass",
      "forward pass"
    ],
    "outlinks": [
      "activation_function",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation"
    ],
    "inlinks": [
      "feed_forward_neural_network",
      "fitting_weights_and_biases_of_a_neural_network"
    ],
    "summary": "[!Summary] Forward propagation is the process by which input data moves through a neural network, layer by layer, to produce an output. During this process,...",
    "TFIDF_Score": {
      "propagation": 0.42707242250461747,
      "forward": 0.32502399575867724,
      "layer": 0.30575617425408974,
      "weight": 0.2859867720630902,
      "activation": 0.2443393863706332,
      "input": 0.21360236473042876,
      "network": 0.19112191629663178,
      "output": 0.1777157425720939,
      "bias": 0.16465191610113528,
      "relu": 0.15945109110250844
    }
  },
  "functional_programming": {
    "title": "functional programming",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pyright"
    ],
    "summary": "Functional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a...",
    "TFIDF_Score": {
      "programming": 0.5009943532640246,
      "declarative": 0.337739836409655,
      "functional": 0.3242852911597381,
      "threaten": 0.21066355306799298,
      "opposed": 0.19843969848610088,
      "mutable": 0.18976673563641025,
      "rise": 0.18303946301145177,
      "expressive": 0.17754288105451815,
      "imperative": 0.17754288105451815,
      "paradigm": 0.17754288105451815
    }
  },
  "fuzzywuzzy": {
    "title": "Fuzzywuzzy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing"
    ],
    "inlinks": [],
    "summary": "Tool used for correcting spelling with pandas. [[Data Cleansing]]",
    "TFIDF_Score": {
      "spelling": 0.5881188582328912,
      "correcting": 0.4714416995162431,
      "cleansing": 0.4446391115726479,
      "panda": 0.35476454079959496,
      "tool": 0.24565012017716897,
      "used": 0.17684070916660383,
      "data": 0.12915299314228337
    }
  },
  "gaussian_distribution": {
    "title": "Gaussian Distribution",
    "tags": null,
    "aliases": [
      "normally distributed data"
    ],
    "outlinks": [
      "distributions"
    ],
    "inlinks": [
      "distributions",
      "standardisation"
    ],
    "summary": "Common assumption for a [[Distributions]].",
    "TFIDF_Score": {
      "assumption": 0.6814171394123644,
      "common": 0.5266403500450773,
      "distribution": 0.5082525197374519
    }
  },
  "gaussian_mixture_models": {
    "title": "Gaussian Mixture Models",
    "tags": [
      "clustering"
    ],
    "aliases": [
      "GMM"
    ],
    "outlinks": [
      "gaussian_mixture_model_implementation.py",
      "kmeans_vs_gmm",
      "covariance_structures",
      "anomaly_detection",
      "pasted_image_20250126135722.png",
      "distributions",
      "k-means",
      "clustering",
      "covariance",
      "ml_tools"
    ],
    "inlinks": [
      "clustering",
      "covariance",
      "covariance_structures",
      "machine_learning_algorithms"
    ],
    "summary": "Gaussian Mixture Models (GMMs) represent data as a mixture of multiple Gaussian [[distributions]], with each cluster corresponding to a different Gaussian component. GMMs are more...",
    "TFIDF_Score": {
      "gmms": 0.6423369064926213,
      "gaussian": 0.292174532210266,
      "covariance": 0.27441639079425534,
      "cluster": 0.24068379721383806,
      "c_k": 0.23769001547194893,
      "distribution": 0.1683166809094171,
      "clustering": 0.13937271927315714,
      "pi_k": 0.11884500773597446,
      "elliptical": 0.1119489686670829,
      "mathcal": 0.1119489686670829
    }
  },
  "gaussian_model": {
    "title": "Gaussian Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241230202826.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "Gaussian Model (Univariate) Formula: $p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)$ Steps: Estimate $\\mu$ and $\\sigma^2$ from the data. Compute the...",
    "TFIDF_Score": {
      "sigma": 0.6638530931369828,
      "gaussian": 0.313548896082845,
      "frac": 0.2653965916589024,
      "exp": 0.21024276734822514,
      "multivariate": 0.18810302031623544,
      "probability": 0.18669450179145816,
      "covariance": 0.17178678849011564,
      "left": 0.1512853956115113,
      "threshold": 0.14518291877874653,
      "right": 0.14382352625225603
    }
  },
  "gaussian_mixture_model_implementation.py": {
    "title": "Gaussian_Mixture_Model_Implementation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/Gaussian_Mixture_Model_Implementation.py Follow-Up Questions How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency? What are the implications of choosing different...",
    "TFIDF_Score": {
      "gmms": 0.5256740015780222,
      "clustering": 0.34217844706065853,
      "gaussian_mixture_model_implementation": 0.2748495146562193,
      "covariance": 0.22457616990293802,
      "implication": 0.21362083181629424,
      "choosing": 0.20003242477860383,
      "compare": 0.18467763056801972,
      "computational": 0.17226553266676523,
      "follow": 0.16994514613066036,
      "scalability": 0.1590767096631211
    }
  },
  "general_linear_regression": {
    "title": "General Linear Regression",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "t-test",
      "linear_regression",
      "anova"
    ],
    "inlinks": [],
    "summary": "[[Linear Regression]] [[t-test]] - to compare means between two populations. [[ANOVA]] - tests",
    "TFIDF_Score": {
      "test": 0.5194340245063145,
      "anova": 0.446341226640355,
      "population": 0.38200220357233067,
      "compare": 0.3442159350363557,
      "linear": 0.28450173828471226,
      "two": 0.2660308789341125,
      "regression": 0.24502348262451532,
      "mean": 0.23410966543131484
    }
  },
  "generative_adversarial_networks": {
    "title": "Generative Adversarial Networks",
    "tags": null,
    "aliases": [
      "GAN"
    ],
    "outlinks": [],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Composed of two neural networks, a generator, and a discriminator, that compete against each other. GANs are used for tasks like generating realistic images or...",
    "TFIDF_Score": {
      "compete": 0.3602897366473411,
      "composed": 0.3602897366473411,
      "discriminator": 0.3602897366473411,
      "gans": 0.3602897366473411,
      "realistic": 0.32455071706115524,
      "generator": 0.30364473079069754,
      "generating": 0.2564003167159887,
      "video": 0.23006188891173748,
      "neural": 0.184070018365843,
      "two": 0.17624282347945885
    }
  },
  "generative_ai_from_theory_to_practice": {
    "title": "Generative AI From Theory to Practice",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "markov_chain",
      "call_summarisation",
      "pasted_image_20240524130607.png",
      "ngrams",
      "rag",
      "gan",
      "pasted_image_20240524131603.png",
      "pasted_image_20240524131311.png",
      "llm",
      "software_development_life_cycle",
      "tokenisation",
      "nlp"
    ],
    "inlinks": [],
    "summary": "Objective: How do LLMs work and operate. Enabling [[LLM]]'s at scale: Explore recent AI and Generative AI language models Steps Math on words: Turn words...",
    "TFIDF_Score": {
      "image": 0.32389332903371504,
      "word": 0.25271658402190006,
      "llm": 0.22121940614431881,
      "summariser": 0.18336571105347568,
      "transcript": 0.18336571105347568,
      "differnet": 0.1727258269606659,
      "genai": 0.16517670905814663,
      "use": 0.14379623456319787,
      "scale": 0.14183296032923032,
      "pasted": 0.13925093489043658
    }
  },
  "generative_ai": {
    "title": "Generative AI",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "inference_versus_prediction",
      "accessing_gen_ai_generated_content",
      "how_to_reduce_the_need_for_gen_ai_responses",
      "typical_output_formats_in_neural_networks",
      "guardrails"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "get_data": {
    "title": "Get data",
    "tags": [
      "data_collection"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "What is involved: df = pd.read_csv('Categorical.csv') Gather relevant data from appropriate sources, addressing any quality or privacy concerns. ```python Get textbook data using for example:...",
    "TFIDF_Score": {
      "replace": 0.3482191922449508,
      "text": 0.22275175473131764,
      "finditer": 0.1957248769055982,
      "read_file": 0.1957248769055982,
      "text_end": 0.1957248769055982,
      "text_start": 0.1957248769055982,
      "data": 0.19341845613185205,
      "filename": 0.17005967973596423,
      "obtaining": 0.16495287406761427,
      "would": 0.16277351108999463
    }
  },
  "gini_impurity_vs_cross_entropy": {
    "title": "Gini Impurity vs Cross Entropy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gini_impurity",
      "distributions",
      "cross_entropy"
    ],
    "inlinks": [
      "decision_tree"
    ],
    "summary": "When working with decision trees, both [[Gini Impurity]] and [[Cross Entropy]] are metrics used to evaluate the quality of a split. They help determine how...",
    "TFIDF_Score": {
      "impurity": 0.5324273068376406,
      "gini": 0.3993204801282304,
      "entropy": 0.3081431238068832,
      "cross": 0.254807789444004,
      "information": 0.12650152226429906,
      "tree": 0.12460345635467114,
      "efficiency": 0.11739981851031585,
      "class": 0.11531116238246121,
      "measure": 0.11287410097154488,
      "distribution": 0.10848288374276799
    }
  },
  "gini_impurity": {
    "title": "Gini Impurity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "regression_metrics",
      "classification",
      "decision_tree"
    ],
    "inlinks": [
      "gini_impurity_vs_cross_entropy",
      "feature_importance",
      "decision_tree"
    ],
    "summary": "Gini impurity is a metric used in decision trees to measure the degree or probability of misclassification in a dataset. It is associated with the...",
    "TFIDF_Score": {
      "impurity": 0.6061961645728832,
      "gini": 0.5304216440012728,
      "proportion": 0.18377333364257686,
      "p_i": 0.15711894229485227,
      "node": 0.1544655710870888,
      "yes": 0.14699810053493073,
      "tree": 0.1418675119864951,
      "class": 0.131287752282782,
      "split": 0.12534490105849805,
      "decision": 0.11553485104902406
    }
  },
  "gis": {
    "title": "GIS",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)",
      "shapefile",
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)"
    ],
    "inlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)",
      "shapefile"
    ],
    "summary": "Geographic information system. File formats: The Web Map Tile Service (WMTS) and Web Feature Server (WFS) are both specifications used in the field of Geographic...",
    "TFIDF_Score": {
      "web": 0.5028401434921118,
      "wfs": 0.4353505929307271,
      "geographic": 0.30548056122098827,
      "server": 0.28338397822374106,
      "serve": 0.24568849281765015,
      "tile": 0.21767529646536354,
      "wmts": 0.21767529646536354,
      "gi": 0.20365370748065886,
      "feature": 0.1640952023650983,
      "map": 0.15718535312430135
    }
  },
  "git": {
    "title": "Git",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_to_do_git_commit_messages_properly"
    ],
    "inlinks": [
      "debugging",
      "powershell_vs_bash",
      "how_to_do_git_commit_messages_properly"
    ],
    "summary": "tags: - software Do git bash here. git status git add . (adds all) git status git commit -m \"\" git push Notes https://www.youtube.com/watch?v=xnR0dlOqNVE Git...",
    "TFIDF_Score": {
      "git": 0.7795306088789424,
      "commit": 0.38962540115754707,
      "bash": 0.1896509485643088,
      "branch": 0.1694023483293683,
      "checkout": 0.12952012127788193,
      "reset": 0.12112866208763969,
      "master": 0.11858164383055779,
      "repo": 0.10793343439823495,
      "change": 0.08783194446661984,
      "last": 0.08652047291974262
    }
  },
  "gitlab-ci.yml": {
    "title": "gitlab-ci.yml",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The purpose of a gitlab-ci.yml file is to define and configure the GitLab CI/CD pipeline for automating tasks such as building, testing, and deploying your...",
    "TFIDF_Score": {
      "job": 0.479367892818675,
      "stage": 0.4322089915715368,
      "test": 0.2516739845426984,
      "build": 0.23013838682860863,
      "gitlab": 0.20813402006577938,
      "command": 0.2038213958587965,
      "deploy": 0.17047253166306603,
      "pipeline": 0.14046061932703655,
      "artifact": 0.12488041203946763,
      "execute": 0.11829836475293329
    }
  },
  "gitlab": {
    "title": "Gitlab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ci-cd"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "GitLab CI CD Tutorial for Beginners Crash Course Provides managed runners to execute [[CI-CD]] pipelines. Integrates with version control systems to automate the CI/CD process.",
    "TFIDF_Score": {
      "crash": 0.3358525079997443,
      "runner": 0.3358525079997443,
      "beginner": 0.30978812800216127,
      "gitlab": 0.30978812800216127,
      "course": 0.29261995979384625,
      "automate": 0.2572527999102536,
      "tutorial": 0.2572527999102536,
      "integrates": 0.2504394906378778,
      "managed": 0.2504394906378778,
      "execute": 0.220095139606117
    }
  },
  "google_cloud_platform": {
    "title": "Google Cloud Platform",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "kubernetes",
      "standardised/firebase",
      "mysql",
      "sql",
      "nosql",
      "bigquery"
    ],
    "inlinks": [],
    "summary": "Google Cloud Platform is a suite of cloud computing services offered by Google. It provides a range of services including computing, storage, and application development...",
    "TFIDF_Score": {
      "service": 0.35699871293776647,
      "google": 0.35347202784487675,
      "cloud": 0.28836959191866335,
      "gcp": 0.19993983714190236,
      "storage": 0.16311428393813307,
      "engine": 0.162641006971567,
      "largescale": 0.15815911154149181,
      "vms": 0.15815911154149181,
      "infrastructure": 0.1543188891257967,
      "web": 0.14104864245424598
    }
  },
  "google_my_maps_data_extraction": {
    "title": "Google My Maps Data Extraction",
    "tags": [],
    "aliases": null,
    "outlinks": [
      "**google_apps_script**"
    ],
    "inlinks": [],
    "summary": "Summary: This guide covers the key workflows and tools for managing and processing location data in Google Sheets and Google My Maps. Suppose we have...",
    "TFIDF_Score": {
      "google": 0.3889601417382932,
      "var": 0.2646291256981603,
      "kml": 0.25380552253442723,
      "sheet": 0.24996362714563455,
      "marker": 0.23907833430296022,
      "map": 0.23113340430267496,
      "coordinate": 0.23026337715003337,
      "result": 0.22057914600627435,
      "lat": 0.20304441802754178,
      "lng": 0.20304441802754178
    }
  },
  "gradient_boosting_regressor": {
    "title": "Gradient Boosting Regressor",
    "tags": [
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "boosting",
      "decision_tree",
      "model_ensemble"
    ],
    "inlinks": [],
    "summary": "https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor [[Boosting]] The GradientBoostingRegressor from the sklearn.ensemble module is a model used for regression tasks. It builds an [[Model Ensemble]] of [[Decision Tree]] in a...",
    "TFIDF_Score": {
      "default": 0.49132153077350865,
      "tree": 0.36502878005383255,
      "gradientboostingregressor": 0.20195496428785578,
      "sample": 0.17873448264477146,
      "ensemble": 0.1751567516117932,
      "overfitting": 0.14523299497840436,
      "loss": 0.1415601902302517,
      "used": 0.12145116111967323,
      "node": 0.11923317365206067,
      "number": 0.11738331543104506
    }
  },
  "gradient_boosting": {
    "title": "Gradient Boosting",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "heterogeneous_features",
      "overfitting",
      "model_ensemble",
      "weak_learners",
      "loss_function",
      "catboost",
      "learning_rate",
      "machine_learning_algorithms",
      "decision_tree",
      "gradient_descent",
      "boosting",
      "model_building",
      "xgboost",
      "lightgbm"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "lightgbm_vs_xgboost_vs_catboost",
      "catboost",
      "use_of_rnns_in_energy_sector",
      "boosting",
      "xgboost",
      "embedded_methods"
    ],
    "summary": "Gradient Boosting is a technique used for building predictive models [[Model Building]], particularly in tasks like regression and classification. It combines the concepts of [[Boosting]]...",
    "TFIDF_Score": {
      "weak": 0.4319133219371491,
      "learner": 0.3696023361658857,
      "model": 0.2800625427817962,
      "boosting": 0.23780771005926768,
      "sequentially": 0.19893658385624535,
      "gradient": 0.1882326360123812,
      "ensemble": 0.17116135358739468,
      "final": 0.1532586670739351,
      "tree": 0.14268092897918913,
      "residual": 0.13262438923749692
    }
  },
  "gradient_descent": {
    "title": "Gradient Descent",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "GD"
    ],
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "stochastic_gradient_descent",
      "loss_function",
      "mini-batch_gradient_descent",
      "learning_rate",
      "obsidian_epiqlato5w.png",
      "obsidian_m4mzgsax7d.png",
      "gradient_descent",
      "gradient_descent#",
      "pasted_image_20241224082847.png",
      "obsidian_fegflf5rkq.png",
      "batch_gradient_descent",
      "cost_function",
      "optimisation_techniques"
    ],
    "inlinks": [
      "z-normalisation",
      "stochastic_gradient_descent",
      "optimising_a_logistic_regression_model",
      "backpropagation",
      "linear_regression",
      "xgboost",
      "optimisation_techniques",
      "deep_learning",
      "momentum",
      "learning_rate",
      "standardisation",
      "lightgbm",
      "cost_function",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "pytorch",
      "gradient_descent",
      "feature_scaling",
      "gradient_boosting",
      "logistic_regression_in_sklearn_&_gradient_descent",
      "ds_&_ml_portal",
      "adam_optimizer",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Gradient descent is an [[Optimisation function]] used to minimize errors in a model by adjusting its parameters iteratively. It works by moving in the direction...",
    "TFIDF_Score": {
      "gradient": 0.5978622713534812,
      "descent": 0.5547876711935567,
      "theta": 0.21466085797184814,
      "stochastic": 0.2012727103029205,
      "batch": 0.1598363844391706,
      "function": 0.1450139883510644,
      "parameter": 0.13527467075749405,
      "cost": 0.1295351187409334,
      "mini": 0.1147201454777566,
      "update": 0.11334322889831673
    }
  },
  "gradio": {
    "title": "Gradio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "overview"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "Gradio is an open-source platform that simplifies the process of ==creating user interfaces== for machine learning models. It allows users to quickly build interactive demos...",
    "TFIDF_Score": {
      "gradio": 0.4837859307033995,
      "interface": 0.23778056458131547,
      "user": 0.22532732265427377,
      "model": 0.2236231892602901,
      "prototyping": 0.22312046844459404,
      "interactive": 0.205848152284538,
      "machine": 0.2056531006589348,
      "learning": 0.1845088566583449,
      "front": 0.12094648267584987,
      "slider": 0.12094648267584987
    }
  },
  "grain": {
    "title": "Grain",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "granularity"
    ],
    "inlinks": [
      "dimensional_modelling"
    ],
    "summary": "Grain - Definition: The level of detail or [[granularity]] of the data stored in the fact table. - Importance: Defining the grain is crucial as...",
    "TFIDF_Score": {
      "grain": 0.5046002233982609,
      "fact": 0.40289128462013146,
      "table": 0.27011320477317163,
      "granularity": 0.22787440565439976,
      "daily": 0.21953545597787652,
      "defining": 0.21259345778498873,
      "determines": 0.21259345778498873,
      "transaction": 0.19469210427381578,
      "detail": 0.19073145226521918,
      "record": 0.18541928623347148
    }
  },
  "grammar_method": {
    "title": "Grammar method",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "can understand the Grammar as a method for acceptable sentences.",
    "TFIDF_Score": {
      "acceptable": 0.5681847352197273,
      "grammar": 0.5681847352197273,
      "sentence": 0.42307011454765964,
      "understand": 0.3207163178039314,
      "method": 0.2692302638959713
    }
  },
  "granularity": {
    "title": "granularity",
    "tags": [
      "database",
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "semantic_layer",
      "facts",
      "dimensions",
      "olap",
      "fact_table",
      "dimensional_modelling",
      "business_intelligence"
    ],
    "inlinks": [
      "transaction",
      "choosing_the_number_of_clusters",
      "grain",
      "rollup",
      "fact_table"
    ],
    "summary": "Definition of Grain in [[Dimensional Modelling]] - The grain of a [[Fact Table]] defines what a single row in the table represents. It is the...",
    "TFIDF_Score": {
      "grain": 0.3979502740922526,
      "granularity": 0.39209876571805835,
      "sale": 0.3466233405293061,
      "level": 0.23084720019770502,
      "row": 0.2182610542239056,
      "fact": 0.20219694864209523,
      "transaction": 0.19541822278926088,
      "total": 0.1829007470362269,
      "store": 0.18260598532497632,
      "product": 0.1686462550844244
    }
  },
  "graph_analysis_plugin": {
    "title": "Graph Analysis Plugin",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_archive_graph_analysis"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "graph_neural_network": {
    "title": "Graph Neural Network",
    "tags": [],
    "aliases": [
      "GNN"
    ],
    "outlinks": [
      "recommender_systems"
    ],
    "inlinks": [
      "graphrag"
    ],
    "summary": "Resources: - How Graph Neural Networks Are Transforming Industries Use cases: - [[Recommender systems]] i.e. Uber, Pinterest (PinSage) - Traffic Prediction - Deepmind in google...",
    "TFIDF_Score": {
      "deepmind": 0.5593997090258549,
      "antibiotic": 0.186466569675285,
      "drug": 0.186466569675285,
      "gnome": 0.186466569675285,
      "graphcast": 0.186466569675285,
      "mit": 0.186466569675285,
      "pinsage": 0.186466569675285,
      "pinterest": 0.186466569675285,
      "uber": 0.186466569675285,
      "discovery": 0.1679699773276742
    }
  },
  "graphrag": {
    "title": "GraphRAG",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "graph database"
    ],
    "outlinks": [
      "how_to_search_within_a_graph",
      "rag",
      "graph_neural_network",
      "interpretability",
      "knowledge_graph",
      "neo4j",
      "text2cypher",
      "wikipedia_api.py",
      "graphrag",
      "named_entity_recognition",
      "ml_tools"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "text2cypher",
      "agentic_solutions",
      "relationships_in_memory",
      "graphrag"
    ],
    "summary": "[[GraphRAG]] is a [[RAG]] framework that utilizes [[Knowledge Graph]]s to enhance information retrieval and processing. A significant aspect of this framework is the use of...",
    "TFIDF_Score": {
      "graph": 0.650286698569324,
      "graphrag": 0.2673792140197564,
      "linkedin": 0.2171910270435965,
      "neo4j": 0.1956467153968489,
      "post": 0.16149978933290393,
      "rag": 0.15670849769975673,
      "entity": 0.1362945604985092,
      "retrieval": 0.13200058942020226,
      "knowledge": 0.12912945374719886,
      "framework": 0.11306973842397701
    }
  },
  "grep": {
    "title": "Grep",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "grep.png"
    ],
    "inlinks": [],
    "summary": "![[grep.png]]",
    "TFIDF_Score": {
      "grep": 0.8842626772783484,
      "png": 0.46698984739770044
    }
  },
  "gridseachcv": {
    "title": "GridSeachCv",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gridseachcv",
      "pasted_image_20240128194244.png",
      "hyperparameter"
    ],
    "inlinks": [
      "gridseachcv",
      "model_selection",
      "hyperparameter_tuning",
      "decision_tree"
    ],
    "summary": "Used [[GridSeachCv]] to search through the [[Hyperparameter]] space ```python rf_regressor = RandomForestRegressor(random_state=42) grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error') grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ Model Training...",
    "TFIDF_Score": {
      "rf_regressor": 0.49927812702836744,
      "grid_search": 0.37445859527127556,
      "hypers": 0.24963906351418372,
      "param_grid": 0.24963906351418372,
      "randomforestregressor": 0.24963906351418372,
      "best_params": 0.23515363608302195,
      "random_state": 0.21039063432939883,
      "x_train": 0.17318421576205537,
      "y_train": 0.17114220514461395,
      "give": 0.15410387336639497
    }
  },
  "groupby_vs_crosstab": {
    "title": "Groupby vs Crosstab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "groupby",
      "de_tools",
      "crosstab"
    ],
    "inlinks": [
      "groupby"
    ],
    "summary": "In pandas, [[Groupby]] and [[Crosstab]] serve related but distinct purposes for data ==aggregation== and summarization. groupby is more flexible for aggregation and transformations, whereas crosstab...",
    "TFIDF_Score": {
      "crosstab": 0.4891161070252875,
      "groupby": 0.4185879457879096,
      "aggregation": 0.3052542492013868,
      "count": 0.16743517831516383,
      "frequency": 0.16246779865312416,
      "exploring": 0.15816485866797336,
      "table": 0.1570947213836291,
      "de_tools": 0.14509907696697658,
      "categorical": 0.14251976613527823,
      "dataframe": 0.1413036149131702
    }
  },
  "groupby": {
    "title": "Groupby",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20250323081619.png",
      "de_tools",
      "groupby_vs_crosstab"
    ],
    "inlinks": [
      "aggregation",
      "melt",
      "handling_missing_data",
      "pd.grouper",
      "pandas_stack",
      "multi-level_index",
      "groupby_vs_crosstab"
    ],
    "summary": "Groupby is a versatile method in pandas used to group data based on one or more columns, and then perform aggregate functions on the grouped...",
    "TFIDF_Score": {
      "grouped": 0.4162767897236649,
      "category": 0.40496679635005134,
      "groupby": 0.37715317975081253,
      "de_tools": 0.2178935904536334,
      "dataframe": 0.21219398938365133,
      "sum": 0.1996360664262721,
      "group": 0.18747479059950023,
      "value": 0.18346574079277744,
      "20250323081619": 0.16906995524306836,
      "group_by": 0.15925958929717912
    }
  },
  "grouped_plots": {
    "title": "Grouped plots",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_visualisation",
      "pasted_image_20250402212849.png"
    ],
    "inlinks": [
      "melt"
    ],
    "summary": "Related: - [[Data Visualisation]] - pairplots ```python import seaborn as sns import matplotlib.pyplot as plt Load example dataset tips = sns.load_dataset(\"tips\") Facet Grid Example g...",
    "TFIDF_Score": {
      "sn": 0.5447204793012786,
      "tip": 0.40854035947595896,
      "plt": 0.2554212661863185,
      "import": 0.16652685982611057,
      "20250402212849": 0.16592782013486537,
      "facet": 0.16592782013486537,
      "facetgrid": 0.16592782013486537,
      "histplot": 0.16592782013486537,
      "map_dataframe": 0.16592782013486537,
      "pairplots": 0.16592782013486537
    }
  },
  "gru": {
    "title": "GRU",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "lstm",
      "recurrent_neural_networks"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "gsheets": {
    "title": "GSheets",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "query_gsheets"
    ],
    "inlinks": [],
    "summary": "Useful functions: - [[QUERY GSheets]] - ARRAYFORMULA - Indirect Accessing google sheets from a script: https://www.youtube.com/watch?v=zCEJurLGFRk",
    "TFIDF_Score": {
      "indirect": 0.35246079948148,
      "zcejurlgfrk": 0.35246079948148,
      "arrayformula": 0.3320090910775661,
      "gsheets": 0.3062429856998316,
      "sheet": 0.2892712859609874,
      "accessing": 0.2765949562297339,
      "youtube": 0.25082885085199935,
      "watch": 0.24451560699593405,
      "www": 0.23631813281199499,
      "google": 0.22506274547426483
    }
  },
  "guardrails": {
    "title": "Guardrails",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "prompting",
      "data_observability",
      "guardrails",
      "generative_ai"
    ],
    "inlinks": [
      "guardrails"
    ],
    "summary": "Controlling a [[Generative AI]] in business through the use of [[Guardrails]] ensures that the AI remains aligned with specific business goals and avoids unintended or...",
    "TFIDF_Score": {
      "guardrail": 0.4015121148855634,
      "business": 0.2842774789230358,
      "legal": 0.24112268828869735,
      "topic": 0.21365359176174986,
      "output": 0.2048553391030692,
      "input": 0.18089804103916457,
      "harmful": 0.18084201621652302,
      "specific": 0.1677067448076465,
      "compliance": 0.15451708563678648,
      "firm": 0.13383737162852113
    }
  },
  "hadoop": {
    "title": "Hadoop",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "data_lake",
      "batch_processing",
      "big_data",
      "apache_spark"
    ],
    "inlinks": [
      "map_reduce",
      "difference_between_snowflake_to_hadoop",
      "distributed_computing",
      "parquet",
      "databricks",
      "big_data"
    ],
    "summary": "Hadoop provides the backbone for distributed storage and computation. It uses HDFS (Hadoop Distributed File System) to split large datasets across clusters of servers, while...",
    "TFIDF_Score": {
      "hadoop": 0.3943450757455833,
      "processing": 0.32482461164590815,
      "distributed": 0.21699952066710085,
      "data": 0.21272500776230546,
      "hdfs": 0.21056987255215756,
      "mapreduce": 0.1883956795006483,
      "batch": 0.1870391248151601,
      "though": 0.18346430701674135,
      "structured": 0.16922099278486627,
      "large": 0.15794589135117415
    }
  },
  "handling_different_distributions": {
    "title": "Handling Different Distributions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "accuracy",
      "preprocessing",
      "distributions",
      "model_selection",
      "model_robustness",
      "datasets",
      "ml_tools"
    ],
    "inlinks": [
      "data_cleansing",
      "train-dev-test_sets"
    ],
    "summary": "Handling different [[distributions]] is needed for developing robust, fair, and accurate machine learning models that can adapt to a wide range of data environments. Importance...",
    "TFIDF_Score": {
      "distribution": 0.3666744777149893,
      "dev": 0.35704926630723927,
      "model": 0.3074438378386681,
      "set": 0.2409290219543301,
      "different": 0.22377714967080078,
      "data": 0.2015792191945452,
      "datasets": 0.190226752868349,
      "splitting": 0.1435817973982535,
      "test": 0.1348810466827655,
      "shuffling": 0.1330246686131285
    }
  },
  "handling_missing_data": {
    "title": "Handling Missing Data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "handling_missing_data_basic.ipynb",
      "groupby",
      "'var1',_'var2'",
      "data_transformation",
      "handling_missing_data.ipynb"
    ],
    "inlinks": [
      "data_cleansing",
      "pandas",
      "outliers",
      "xgboost"
    ],
    "summary": "Missing data can provide insights into the data collection process. It's important to determine whether the missing data is randomly distributed or specific to certain...",
    "TFIDF_Score": {
      "missing": 0.4981050672042992,
      "value": 0.3651867591525413,
      "fill": 0.2926196785954736,
      "var1": 0.24901600620503686,
      "imputation": 0.23298375398479415,
      "group": 0.2296410667727002,
      "isnull": 0.17069481251402627,
      "mean": 0.1671557167765398,
      "filling": 0.16323446169277792,
      "dataframe": 0.16245002171023432
    }
  },
  "handling_missing_data.ipynb": {
    "title": "Handling_Missing_Data.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data.ipynb",
    "TFIDF_Score": {
      "handling_missing_data": 0.45529532484291785,
      "investigating": 0.39668752347075587,
      "cleaning": 0.3206962412906583,
      "de_tools": 0.3114598420245139,
      "ipynb": 0.28736738232570463,
      "rhyslwells": 0.24254321299069412,
      "blob": 0.23942222138623664,
      "github": 0.23082451775610632,
      "exploration": 0.22905082452454412,
      "com": 0.21629382253252158
    }
  },
  "handling_missing_data_basic.ipynb": {
    "title": "Handling_Missing_Data_Basic.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data_Basic.ipynb",
    "TFIDF_Score": {
      "handling_missing_data_basic": 0.45529532484291785,
      "investigating": 0.39668752347075587,
      "cleaning": 0.3206962412906583,
      "de_tools": 0.3114598420245139,
      "ipynb": 0.28736738232570463,
      "rhyslwells": 0.24254321299069412,
      "blob": 0.23942222138623664,
      "github": 0.23082451775610632,
      "exploration": 0.22905082452454412,
      "com": 0.21629382253252158
    }
  },
  "handwritten_digit_classification": {
    "title": "Handwritten Digit Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241006124356.png"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "![[Pasted image 20241006124356.png|800]]",
    "TFIDF_Score": {
      "20241006124356": 0.6218364973425992,
      "800": 0.5857541337513248,
      "pasted": 0.31482190465696697,
      "png": 0.3093438641728379,
      "image": 0.27459972216427875
    }
  },
  "hash": {
    "title": "Hash",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_integrity"
    ],
    "inlinks": [
      "cryptography",
      "data_integrity"
    ],
    "summary": "A hash is a fixed-size string of characters that is generated from input data of any size using a hash function. Hashes are used to...",
    "TFIDF_Score": {
      "hash": 0.8604651449322787,
      "256": 0.1622367597047282,
      "input": 0.13579487157958905,
      "password": 0.13021249672694488,
      "sha": 0.12167756977854614,
      "data": 0.1040116950186111,
      "produce": 0.10168401375429749,
      "hello": 0.09942125100036099,
      "fixed": 0.0932013153578418,
      "size": 0.09145662587823311
    }
  },
  "heatmap": {
    "title": "Heatmap",
    "tags": [
      "code_snippet",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "correlation",
      "ml_tools",
      "heatmaps_dendrograms.py",
      "multicollinearity"
    ],
    "inlinks": [
      "feature_selection",
      "heatmaps_dendrograms.py",
      "pca_principal_components",
      "correlation",
      "multicollinearity"
    ],
    "summary": "Description A heatmap is a two-dimensional graphical representation of data where individual values are represented by colors. It is particularly useful for visualizing numerical data...",
    "TFIDF_Score": {
      "correlation": 0.5731900694208846,
      "heatmap": 0.3804515745539869,
      "present": 0.21991748390703383,
      "certainly": 0.19392149149662768,
      "attribute": 0.19299987803352095,
      "coefficient": 0.17418477393405635,
      "indicates": 0.16942213340810552,
      "graphical": 0.15218062982159475,
      "perfect": 0.15218062982159475,
      "color": 0.1492566854025712
    }
  },
  "heatmaps_dendrograms.py": {
    "title": "Heatmaps_Dendrograms.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "heatmap",
      "dendrograms"
    ],
    "inlinks": [
      "heatmap"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations\\Preprocess\\Correlation\\Heatmaps_Dendrograms.py See: - [[Heatmap]] - [[Dendrograms]]",
    "TFIDF_Score": {
      "heatmaps_dendrograms": 0.4176152829917157,
      "dendrograms": 0.3852055698902058,
      "heatmap": 0.3479130060717984,
      "preprocess": 0.31550329297028856,
      "correlation": 0.2912040635694398,
      "rhyslwells": 0.22247044281813091,
      "blob": 0.21960774311314282,
      "github": 0.21172158167317076,
      "exploration": 0.21009467851730176,
      "ml_tools": 0.2025326740487741
    }
  },
  "heterogeneous_features": {
    "title": "heterogeneous features",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing"
    ],
    "inlinks": [
      "gradient_boosting"
    ],
    "summary": "Description In machine learning, heterogeneous features refer to a situation where the input data contains a variety of different types of features. Let's break it...",
    "TFIDF_Score": {
      "feature": 0.5192340235944038,
      "heterogeneous": 0.44278293746826153,
      "image": 0.2170621398042214,
      "numerical": 0.20740890996349898,
      "text": 0.19890365747645722,
      "type": 0.17355082758335058,
      "homogeneous": 0.1475943124894205,
      "categorical": 0.13827260664233265,
      "learning": 0.1373469003520909,
      "machine": 0.1312170038951604
    }
  },
  "hierarchical_clustering": {
    "title": "Hierarchical Clustering",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "clustering"
    ],
    "summary": "Hierarchical clustering builds a treelike structure of clusters, with similar clusters merged together at higher levels. Hierarchical clustering builds a tree-like structure of clusters, with...",
    "TFIDF_Score": {
      "cluster": 0.5247773418681144,
      "merged": 0.3821739411457437,
      "hierarchical": 0.314588596079778,
      "together": 0.27354092567599625,
      "clustering": 0.26589737342643827,
      "higher": 0.2445790402208112,
      "build": 0.2420338046647621,
      "level": 0.22855900903818566,
      "treelike": 0.22673429611361928,
      "similar": 0.22462476280415683
    }
  },
  "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data": {
    "title": "High cross validation accuracy is not directly proportional to performance on unseen test data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "evaluation_metrics",
      "preprocessing",
      "cross_validation",
      "distributions",
      "data_leakage",
      "hyperparameter_tuning",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "Reasons a Model with High [[Cross Validation]] Accuracy May Perform Poorly on Unseen Test Data [[Data Leakage]]: - Information from test folds leaks into training,...",
    "TFIDF_Score": {
      "fold": 0.5351420094459893,
      "solution": 0.28940841440887044,
      "validation": 0.25292075836216216,
      "test": 0.252112760078338,
      "accuracy": 0.20097569906416254,
      "cross": 0.19512913168571802,
      "estimate": 0.17170009844188677,
      "tuning": 0.14895304092944006,
      "leakage": 0.14830627632687182,
      "model": 0.1277018469777794
    }
  },
  "how_businesses_use_gen_ai": {
    "title": "How businesses use Gen AI",
    "tags": [
      "business",
      "GenAI",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "transactional_journeys",
      "model_performance"
    ],
    "inlinks": [],
    "summary": "Businesses leverage generative AI to transform various operations, using models like OpenAI, Gemini (Google Cloud), Anthropic, and Meta models. These models provide services through cloud...",
    "TFIDF_Score": {
      "gateway": 0.6179153158107673,
      "model": 0.2715240788752176,
      "generative": 0.21901253229070947,
      "business": 0.1986835484382486,
      "gemini": 0.18707977859901012,
      "openai": 0.17622438394007367,
      "cloud": 0.15160013056901198,
      "service": 0.14075960072863672,
      "apis": 0.13498251494694213,
      "customer": 0.11945920980759511
    }
  },
  "how_do_we_evaluate_of_llm_outputs": {
    "title": "How do we evaluate of LLM Outputs",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm_evaluation_metrics",
      "llm",
      "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts",
      "prompt_engineering"
    ],
    "inlinks": [
      "llm"
    ],
    "summary": "Methods for assessing the quality and relevance of LLM-generated outputs, critical for improving model performance. The evaluation of [[LLM]] outputs involves various methodologies to assess...",
    "TFIDF_Score": {
      "llm": 0.4638705986350128,
      "evaluation": 0.33914650121067913,
      "prompt": 0.2973178744568605,
      "relevance": 0.23255425742028374,
      "output": 0.20177847775741833,
      "evaluating": 0.18988154135436044,
      "engineering": 0.1816035396336217,
      "quality": 0.15109141272322799,
      "informs": 0.14487414851787742,
      "judgment": 0.14487414851787742
    }
  },
  "how_do_you_do_the_data_selection": {
    "title": "how do you do the data selection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_do_you_do_the_data_selection",
      "data_selection"
    ],
    "inlinks": [
      "how_do_you_do_the_data_selection"
    ],
    "summary": "When you sample a dataset, [[how do you do the data selection]]? [[Data Selection]] A: By randomly sampling, by time period (use a feature)..",
    "TFIDF_Score": {
      "selection": 0.5671996047506624,
      "period": 0.3867988963407885,
      "sampling": 0.3525902020189123,
      "randomly": 0.34877180553995485,
      "sample": 0.276223236719504,
      "dataset": 0.23680465847903473,
      "data": 0.22846754277894124,
      "time": 0.2007896472045645,
      "feature": 0.17662109412014693,
      "use": 0.16317158166651707
    }
  },
  "how_is_reinforcement_learning_being_combined_with_deep_learning": {
    "title": "How is reinforcement learning being combined with deep learning",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning",
      "deep_learning"
    ],
    "inlinks": [],
    "summary": "The sources touch upon reinforcement learning as an area beyond the scope of their discussion. However, the combination of [[Reinforcement learning]] with [[Deep Learning]] has...",
    "TFIDF_Score": {
      "reinforcement": 0.3929762722867315,
      "learning": 0.3306370740377597,
      "combination": 0.2442576845523192,
      "area": 0.2272528890954842,
      "deep": 0.2090098841945396,
      "remarkable": 0.18406804830857854,
      "robotics": 0.18406804830857854,
      "touch": 0.18406804830857854,
      "advancement": 0.17338741075676364,
      "playing": 0.17338741075676364
    }
  },
  "how_is_schema_evolution_done_in_practice_with_sql": {
    "title": "How is schema evolution done in practice with SQL",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "how_llms_store_facts": {
    "title": "How LLMs store facts",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "multilayer_perceptrons",
      "johnson–lindenstrauss_lemma",
      "interpretability",
      "vector_embedding",
      "anthropic",
      "llm"
    ],
    "inlinks": [],
    "summary": "How might LLMs store facts Not solved How do [[Multilayer Perceptrons]] store facts? Different directions encode information in [[Vector Embedding]] space. MLP's are blocks of...",
    "TFIDF_Score": {
      "circuit": 0.3350583534106353,
      "pub": 0.3018221664282153,
      "fact": 0.23244286111756526,
      "transformer": 0.22231065724343924,
      "llm": 0.20211360548095525,
      "store": 0.1836811867082317,
      "vector": 0.1711794452981954,
      "acted": 0.16752917670531764,
      "autoencoder": 0.16752917670531764,
      "mlp": 0.16752917670531764
    }
  },
  "how_to_do_git_commit_messages_properly": {
    "title": "How to do git commit messages properly",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "git"
    ],
    "inlinks": [
      "git"
    ],
    "summary": "Structure of a goof [[Git]] Commit Message Subject Line Keep it short (50 characters or less). Use the imperative mood (e.g., \"Fix bug\" instead of...",
    "TFIDF_Score": {
      "commit": 0.28553028489474597,
      "fix": 0.28004420769846317,
      "message": 0.2323333393725578,
      "bug": 0.20395020349624712,
      "login": 0.1958483194657603,
      "add": 0.17002889540827484,
      "changed": 0.1631601627969977,
      "character": 0.16002526154197896,
      "subject": 0.15460045820230509,
      "line": 0.15037947892115963
    }
  },
  "how_to_model_to_improve_demand_forecasting": {
    "title": "How to model to improve demand forecasting",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "how_to_normalise_a_merged_table": {
    "title": "How to normalise a merged table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "normalised_schema"
    ],
    "inlinks": [
      "normalised_schema",
      "powerquery",
      "normalisation"
    ],
    "summary": "See: [[Normalised Schema]] Splitting out tables. Resource: Database Normalization for Beginners | How to Normalize Data w/ Power Query (full tutorial!)",
    "TFIDF_Score": {
      "normalize": 0.3689327896776871,
      "beginner": 0.35585404092794015,
      "normalised": 0.32140306054589146,
      "normalization": 0.2955066386128649,
      "tutorial": 0.2955066386128649,
      "splitting": 0.27760780244332134,
      "full": 0.2746014323022593,
      "power": 0.25488033430002444,
      "schema": 0.24891471137203952,
      "query": 0.1971315146737741
    }
  },
  "how_to_reduce_the_need_for_gen_ai_responses": {
    "title": "How to reduce the need for Gen AI responses",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "transactional_journeys",
      "caching",
      "generative_ai"
    ],
    "inlinks": [],
    "summary": "Reducing the need for frequent [[Generative AI]] (Gen AI) responses can be done by leveraging techniques such as [[caching]] and setting up predefined [[transactional journeys]]....",
    "TFIDF_Score": {
      "journey": 0.3277267044057853,
      "response": 0.3192715377166313,
      "predefined": 0.27214926813838436,
      "account": 0.26166899422507073,
      "reset": 0.22676741748477536,
      "caching": 0.22199909310563593,
      "cost": 0.19182492078336383,
      "botpress": 0.18859340726176393,
      "medium": 0.15117827832318356,
      "password": 0.142583431698296
    }
  },
  "how_to_search_within_a_graph": {
    "title": "How to search within a graph",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_to_search_within_a_graph",
      "knowledge_graph",
      "text2cypher",
      "pasted_image_20241004074458.png",
      "graphrag",
      "standardised/vector_embedding"
    ],
    "inlinks": [
      "graphrag",
      "vector_embedding",
      "how_to_search_within_a_graph"
    ],
    "summary": "[[How to search within a graph]] Vector Search with Graph Context [[standardised/Vector Embedding]] plays a crucial role in enhancing search capabilities: Comparison of Vector-Only vs....",
    "TFIDF_Score": {
      "graph": 0.566585394483065,
      "search": 0.36802062576453026,
      "context": 0.28463735182783406,
      "vector": 0.24169827830158477,
      "node": 0.20948176579285083,
      "traversal": 0.1993545969599319,
      "rag": 0.17067230919724827,
      "embedding": 0.16833679829125084,
      "prompt": 0.15242647917705962,
      "within": 0.118699116278218
    }
  },
  "how_to_use_sklearn_pipeline": {
    "title": "How to use Sklearn Pipeline",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "hugging_face": {
    "title": "Hugging Face",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "transformer"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "Hugging Face is open-source platform known for its contributions to natural language processing (NLP) and machine learning. It provides a comprehensive library called [[Transformer]], which...",
    "TFIDF_Score": {
      "hugging": 0.3517688291840401,
      "face": 0.33227412979442533,
      "transformer": 0.268622172865117,
      "nlp": 0.2499211640796044,
      "model": 0.23504094152326285,
      "community": 0.21180889784582546,
      "collaboration": 0.17908144857674468,
      "pre": 0.17234693182715033,
      "trained": 0.15826234651282609,
      "library": 0.15419272578986107
    }
  },
  "hyperparameter_tuning": {
    "title": "Hyperparameter Tuning",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "gridseachcv",
      "regularisation",
      "standardised/optuna",
      "hyperparameter_tuning_gridsearchcv.py",
      "random_forests",
      "cross_validation",
      "interpretable_decision_trees",
      "ml_tools"
    ],
    "inlinks": [
      "pycaret",
      "ds_&_ml_portal",
      "optuna",
      "xgboost",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "test_loss_when_evaluating_models",
      "hyperparameter"
    ],
    "summary": "Objective: - Tune the model’s hyperparameters to improve performance. For example, in regularized linear regression, the main hyperparameter to tune is the regularization strength (e.g.,...",
    "TFIDF_Score": {
      "tree": 0.258174375276578,
      "adjust": 0.25319862952956834,
      "number": 0.23349894931872608,
      "tune": 0.22903780536112864,
      "accuracy": 0.21750897059416657,
      "n_estimators": 0.21444700870777042,
      "min_samples_split": 0.20063337039453077,
      "objective": 0.19356772069186445,
      "hyperparameter": 0.17950412086798223,
      "train": 0.17568791719270274
    }
  },
  "hyperparameter": {
    "title": "Hyperparameter",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "neural_network",
      "k-nearest_neighbours",
      "model_parameters_vs_hyperparameters",
      "hyperparameter_tuning"
    ],
    "inlinks": [
      "weak_learners",
      "cross_validation",
      "k-means",
      "isolated_forest",
      "momentum",
      "learning_rate",
      "gridseachcv",
      "fitting_weights_and_biases_of_a_neural_network",
      "pycaret",
      "optuna",
      "catboost",
      "decision_tree",
      "random_forests",
      "ds_&_ml_portal",
      "model_optimisation",
      "neural_network",
      "adam_optimizer",
      "model_parameters_vs_hyperparameters",
      "kaggle_abalone_regression_example"
    ],
    "summary": "Hyperparameters are parameters set before training that control the learning process, such as: - the number of nodes in a [[Neural network]] - or k...",
    "TFIDF_Score": {
      "parameter": 0.45514822837879837,
      "hyperparameters": 0.4221792052731417,
      "knn": 0.25761404067126825,
      "neighbour": 0.2473514675496854,
      "found": 0.23896633476310494,
      "nearest": 0.21547300374187897,
      "hyperparameter": 0.1938571130556554,
      "node": 0.18973576907901127,
      "model": 0.18657271629205122,
      "tuning": 0.1813506362924308
    }
  },
  "hypothesis_testing": {
    "title": "Hypothesis testing",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "statistics",
      "p_values",
      "inference"
    ],
    "inlinks": [
      "testing",
      "statistical_assumptions",
      "t-test",
      "distributions",
      "statistical_tests",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "statistics",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Used to draw inferences about population parameters based on sample data. The process involves the formulation of two competing hypotheses: the null hypothesis ($H_0$) and...",
    "TFIDF_Score": {
      "hypothesis": 0.4862636326458794,
      "h_0": 0.47659711969321905,
      "effect": 0.32679243383006884,
      "testing": 0.21308567526781189,
      "value": 0.17239263179258327,
      "h_1": 0.15886570656440635,
      "small": 0.14473471684353273,
      "observed": 0.11462539014892745,
      "reject": 0.11223558531354211,
      "null": 0.1102112482539349
    }
  },
  "imbalanced_datasets": {
    "title": "Imbalanced Datasets",
    "tags": [
      "data_quality",
      "data_cleaning",
      "data_exploration"
    ],
    "aliases": [
      "Class Imbalance"
    ],
    "outlinks": [
      "imbalanced_datasets_smote.py",
      "anomaly_detection",
      "evaluation_metrics",
      "imbalanced_datasets",
      "bagging",
      "random_forests",
      "classification",
      "cost-sensitive_analysis",
      "transfer_learning",
      "smote_(synthetic_minority_over-sampling_technique)",
      "model_evaluation",
      "regression",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "data_selection_in_ml",
      "data_collection",
      "accuracy",
      "precision-recall_curve",
      "determining_threshold_values",
      "neural_network_classification",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "class_separability",
      "imbalanced_datasets"
    ],
    "summary": "Handling imbalanced datasets to ensure robustness of models is a common challenge in machine learning, particularly in classification tasks where one class significantly outnumbers the...",
    "TFIDF_Score": {
      "minority": 0.4037054572602298,
      "class": 0.3663861315750231,
      "resume": 0.250098102649205,
      "imbalanced": 0.23023800786344603,
      "majority": 0.1974246262136244,
      "dataset": 0.18129991435398948,
      "sample": 0.17623262946444368,
      "technique": 0.15693824607417853,
      "balanced": 0.15054802514498647,
      "model": 0.1348716868868449
    }
  },
  "imbalanced_datasets_smote.py": {
    "title": "Imbalanced_Datasets_SMOTE.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "support_vector_machines",
      "accuracy",
      "random_forests",
      "recall",
      "smote_(synthetic_minority_over-sampling_technique)",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "imbalanced_datasets"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Imbalanced_Datasets_SMOTE.py Demonstrating the Value of Resampling in Imbalanced Classification This example highlights the effectiveness of resampling techniques, such as [[SMOTE (Synthetic Minority Over-sampling Technique)|SMOTE]], in...",
    "TFIDF_Score": {
      "resampling": 0.49427631674473055,
      "minority": 0.4044078955184159,
      "class": 0.3373651595000047,
      "1000": 0.2487344427043351,
      "smote": 0.232928416806061,
      "imbalance": 0.20727870225361253,
      "avg": 0.17433943651094153,
      "recall": 0.16662486653378006,
      "synthetic": 0.1273319961076615,
      "majority": 0.11536474865173708
    }
  },
  "immutable_vs_mutable": {
    "title": "Immutable vs mutable",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "python"
    ],
    "inlinks": [
      "python"
    ],
    "summary": "[[Python]] list being mutable Side effect ```python def get_largest_numbers(numbers, n): numbers. sort() return numbers[-n:] nums [2, 3, 4, 1,34, 123, 321, 1] print(nums) largest =...",
    "TFIDF_Score": {
      "nums": 0.7338225964016846,
      "get_largest_numbers": 0.3669112982008423,
      "number": 0.23991944394271966,
      "print": 0.21814462666931456,
      "321": 0.18345564910042114,
      "mutable": 0.165257725680765,
      "largest": 0.15939930286336898,
      "python": 0.14772212166785437,
      "123": 0.14705980226110882,
      "sort": 0.14396752009777666
    }
  },
  "impact_of_multicollinearity_on_model_parameters": {
    "title": "Impact of multicollinearity on model parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "See https://youtu.be/StSAJIZuqws?t=655 ```R ) Monte Carlo Simulation: Multicollinearity & Harm results = expand_grid( rho = seq(0, 0.95, 0.05), rep = 1:1000 ) %>% mutate( sim...",
    "TFIDF_Score": {
      "sigma": 0.33637766091306803,
      "as_tibble": 0.30158227016893085,
      "mutate": 0.30158227016893085,
      "rho": 0.30158227016893085,
      "5v1": 0.15079113508446543,
      "5v2": 0.15079113508446543,
      "655": 0.15079113508446543,
      "byrow": 0.15079113508446543,
      "ceiling": 0.15079113508446543,
      "cor2cov": 0.15079113508446543
    }
  },
  "imperative": {
    "title": "imperative",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An imperative pipeline tells ==how to proceed== at each step in a procedural manner. In contrast, a declarative data pipeline does not tell the order...",
    "TFIDF_Score": {
      "declarative": 0.2945262283384483,
      "tell": 0.28279318402661147,
      "pipeline": 0.21544078472659667,
      "find": 0.19610555807827873,
      "way": 0.19315633461307843,
      "procedural": 0.18370927869531833,
      "best": 0.18356973938203658,
      "planner": 0.17304945892387896,
      "upstream": 0.17304945892387896,
      "approach": 0.17215633557322038
    }
  },
  "implementing_database_schema": {
    "title": "Implementing Database Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "many-to-many_relationships",
      "er_diagrams"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "To manage and create a database schema in SQLite, you can use the following commands: To view all commands used to create a database, execute:...",
    "TFIDF_Score": {
      "schema": 0.35514443463822176,
      "table": 0.32119989490286693,
      "foreign": 0.26830393674154884,
      "column": 0.26774999319313386,
      "null": 0.2579717437935157,
      "integer": 0.24932314775535666,
      "station_id": 0.21248991758072935,
      "key": 0.19397349276854656,
      "create": 0.17555020354292358,
      "station": 0.15936743818554702
    }
  },
  "in_ner_how_would_you_handle_ambiguous_entities": {
    "title": "In NER how would you handle ambiguous entities",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Handling ambiguous entities in Named Entity Recognition (NER) can be quite challenging. Here are some strategies that can be employed: Contextual Analysis: Utilize the surrounding...",
    "TFIDF_Score": {
      "entity": 0.58533194418911,
      "ambiguous": 0.3601955903852015,
      "disambiguation": 0.20727793236921416,
      "context": 0.19953659125604153,
      "model": 0.18050358006359643,
      "label": 0.17044668283204195,
      "classification": 0.14678081253383618,
      "feedback": 0.1389758351075791,
      "user": 0.13640933849407338,
      "correct": 0.1311932651760008
    }
  },
  "in-memory_format": {
    "title": "in-memory format",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The term \"in-memory format\" refers to the way data is stored and managed directly in a ==computer's RAM== (Random Access Memory) rather than on disk...",
    "TFIDF_Score": {
      "memory": 0.5122290674573173,
      "format": 0.4027740000310385,
      "ram": 0.27975852179224936,
      "data": 0.19566123732078106,
      "disk": 0.16686558297046286,
      "accessing": 0.15537686199872622,
      "optimized": 0.12424804280261953,
      "apache": 0.12222327647451112,
      "faster": 0.11689487811754731,
      "access": 0.10408802774689939
    }
  },
  "incremental_synchronization": {
    "title": "incremental synchronization",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "soft_deletion"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "industries_of_interest": {
    "title": "Industries of interest",
    "tags": [
      "career"
    ],
    "aliases": [],
    "outlinks": [
      "markov_decision_processes",
      "education_and_training",
      "reinforcement_learning",
      "telecommunications",
      "energy",
      "what_algorithms_or_models_are_used_within_the_telecommunication_sector",
      "what_algorithms_or_models_are_used_within_the_energy_sector"
    ],
    "inlinks": [],
    "summary": "Industries to investigate related to my background & interests: - [[Energy]] - [[Telecommunications]] - [[Education and Training]] Both Reinforcement Learning and Explainable AI offer exciting...",
    "TFIDF_Score": {
      "theoretical": 0.398716394977071,
      "algorithm": 0.3186389062620145,
      "mathematical": 0.24923261190533197,
      "telecommunication": 0.21308403995168487,
      "background": 0.1993581974885355,
      "sector": 0.1993581974885355,
      "energy": 0.1683398386096911,
      "reinforcement": 0.1683398386096911,
      "contribute": 0.16216787235132102,
      "analysis": 0.14364243987203898
    }
  },
  "inference_versus_prediction": {
    "title": "inference versus prediction",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "prediction",
      "llm",
      "generative_ai",
      "inference"
    ],
    "inlinks": [
      "inference"
    ],
    "summary": "[[inference]] is similar to prediction, ==but in the context of [[Generative AI]],== it is more specific to the application of a pre-trained model to ==produce...",
    "TFIDF_Score": {
      "inferencing": 0.550811850252551,
      "text": 0.26619458582623895,
      "image": 0.25821888797519854,
      "model": 0.21217080878579875,
      "generating": 0.2080658438991859,
      "generative": 0.20536550409601362,
      "produce": 0.17261427021547612,
      "refers": 0.1585360211760552,
      "novel": 0.1461854436529053,
      "distinction": 0.13770296256313774
    }
  },
  "inference": {
    "title": "inference",
    "tags": null,
    "aliases": [
      "inferencing"
    ],
    "outlinks": [
      "inference_versus_prediction"
    ],
    "inlinks": [
      "inference_versus_prediction",
      "small_language_models",
      "hypothesis_testing"
    ],
    "summary": "Inferencing involves prediction, but the output is more generative and creative in nature. [[inference versus prediction]]",
    "TFIDF_Score": {
      "creative": 0.4456975300827159,
      "inferencing": 0.41983571527946334,
      "prediction": 0.3988965699342521,
      "versus": 0.33141358643746255,
      "generative": 0.3130643368197147,
      "inference": 0.30919751151075225,
      "nature": 0.28720252201646224,
      "involves": 0.2050592045478827,
      "output": 0.19491356786549893
    }
  },
  "information_theory": {
    "title": "information theory",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "language_model_output_optimisation",
      "mathematics"
    ],
    "summary": "Information theory is a mathematical framework for quantifying the transmission, processing, and storage of information. Information theory has profound implications and applications across various domains,...",
    "TFIDF_Score": {
      "information": 0.4973533589725098,
      "compression": 0.4070876469480831,
      "theory": 0.38176953569163996,
      "uncertainty": 0.22242766635420766,
      "amount": 0.19757124761776607,
      "channel": 0.1744661344063213,
      "deal": 0.1276603578543687,
      "transmission": 0.11943709551573624,
      "quantifies": 0.10907701019761143,
      "entropy": 0.09501925037797411
    }
  },
  "input_is_not_properly_sanitized": {
    "title": "Input is Not Properly Sanitized",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Input is Not Properly Sanitized When we say that ==\"input is not properly sanitized,\"== it means that the input data from users or external sources...",
    "TFIDF_Score": {
      "malicious": 0.42024548334049594,
      "sanitization": 0.3627513101194442,
      "input": 0.27239112242686564,
      "sanitized": 0.21012274167024797,
      "properly": 0.18978031579185287,
      "injection": 0.18283540024314,
      "validating": 0.18283540024314,
      "proper": 0.1798240653614897,
      "executed": 0.174489138651282,
      "command": 0.14697779308382952
    }
  },
  "interoperable": {
    "title": "interoperable",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multi-level_index"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "interpretability": {
    "title": "interpretability",
    "tags": [
      "drafting",
      "model_explainability"
    ],
    "aliases": [
      "explainability",
      "interpretable"
    ],
    "outlinks": [],
    "inlinks": [
      "standard_deviation",
      "covariance_vs_correlation",
      "model_ensemble",
      "statistical_assumptions",
      "principal_component_analysis",
      "addressing_multicollinearity",
      "boosting",
      "xgboost",
      "clustering",
      "activation_function",
      "pdp_and_ice",
      "feature_importance",
      "regression_metrics",
      "pca_explained_variance_ratio",
      "text2cypher",
      "classification",
      "feature_selection_vs_feature_importance",
      "accessing_gen_ai_generated_content",
      "machine_learning_algorithms",
      "decision_tree",
      "model_observability",
      "how_llms_store_facts",
      "graphrag",
      "small_language_models",
      "model_interpretability",
      "regularisation",
      "ds_&_ml_portal",
      "feature_extraction",
      "embedded_methods"
    ],
    "summary": "Links Interpretability Importance https://christophm.github.io/interpretable-ml-book/index.html Interpretability Interpretability in machine learning (ML) is about understanding the reasoning behind a model's predictions. It involves making the model's decision-making...",
    "TFIDF_Score": {
      "explanation": 0.6417155492466424,
      "model": 0.2862794453756403,
      "interpretability": 0.18160032155261355,
      "prediction": 0.1789197503574609,
      "interpretable": 0.1429727126425394,
      "human": 0.13616319176333547,
      "understanding": 0.12779173467064553,
      "audience": 0.12005427575696864,
      "social": 0.11232096041133656,
      "trust": 0.1068340858540291
    }
  },
  "interpreting_logistic_regression_model_parameters": {
    "title": "Interpreting logistic regression model parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "How do this in terms of odds, probabilities ,odds ratio. [[Logistic Regression]]",
    "TFIDF_Score": {
      "odds": 0.800113644993776,
      "ratio": 0.34810391621486314,
      "logistic": 0.2889368923735124,
      "probability": 0.2567584298980295,
      "term": 0.21494010943430286,
      "regression": 0.20744432715515515
    }
  },
  "interquartile_range_(iqr)_detection": {
    "title": "Interquartile Range (IQR) Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "univariate_data"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_IQR.py Context: The IQR method is a robust and widely used statistical technique for identifying outliers, especially in [[univariate data]]. It is based on the...",
    "TFIDF_Score": {
      "iqr": 0.5125003655917227,
      "bound": 0.44278029716795414,
      "upper": 0.2656681783007725,
      "outlier": 0.23057617309460923,
      "text": 0.22376528034794246,
      "quartile": 0.17711211886718164,
      "lower": 0.17293212982095693,
      "percentile": 0.16570341344543005,
      "cdot": 0.15429470802367848,
      "anomaly": 0.1223594034699137
    }
  },
  "interview_notepad": {
    "title": "interview notepad",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "normalised_schema",
      "preprocessing",
      "dimensionality_reduction",
      "model_selection",
      "linear_regression",
      "energy"
    ],
    "inlinks": [],
    "summary": "Tell about a recent project of yours;; Collaborating in the image matching kaggle competition. Obviously S2DS project too. What are some areas in this business...",
    "TFIDF_Score": {
      "project": 0.40738821275556175,
      "science": 0.3719668201444584,
      "data": 0.21970679388605918,
      "stay": 0.14487991897710284,
      "business": 0.13281563104828484,
      "team": 0.1231283696551238,
      "area": 0.12008829141453378,
      "variable": 0.1115520210728023,
      "s2ds": 0.11116334513434006,
      "people": 0.10694967525880293
    }
  },
  "ipynb": {
    "title": "ipynb",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "documentation_&_meetings",
      "nbconvert"
    ],
    "inlinks": [],
    "summary": "Printing without code : [[Documentation & Meetings]]/ [[nbconvert]] https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html jupyter nbconvert stock_analysis.ipynb --no-input --to pdf jupyter nbconvert --to html --no-input --no-prompt phi_analysis.ipynb --clear -output jupyter...",
    "TFIDF_Score": {
      "nbconvert": 0.5548671923275926,
      "jupyter": 0.45337518246517283,
      "html": 0.30648337345320015,
      "ipynb": 0.26266039408477226,
      "49907455": 0.1472617312820339,
      "exclude_input": 0.1472617312820339,
      "phi_analysis": 0.1472617312820339,
      "printing": 0.1472617312820339,
      "stock_analysis": 0.1472617312820339,
      "templateexporter": 0.1472617312820339
    }
  },
  "isolated_forest": {
    "title": "Isolated Forest",
    "tags": [
      "anomaly_detection",
      "data_quality"
    ],
    "aliases": [
      "iForest",
      "anomaly isolation"
    ],
    "outlinks": [
      "model_ensemble",
      "anomaly_detection_with_clustering",
      "support_vector_machines",
      "standardised/outliers",
      "random_forests",
      "dbscan",
      "hyperparameter"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "model_ensemble",
      "anomaly_detection_with_clustering",
      "unsupervised_learning",
      "anomaly_detection_in_time_series",
      "model_observability"
    ],
    "summary": "Isolation Forest (iForest) is an [[Model Ensemble]]-based method used for anomaly detection. It operates by isolating data points using a series of random binary splits....",
    "TFIDF_Score": {
      "anomaly": 0.489239809266029,
      "isolation": 0.3781724621881671,
      "path": 0.2997365161135616,
      "tree": 0.29306811085216483,
      "length": 0.26947384356475046,
      "forest": 0.21769004255980826,
      "isolate": 0.17704053958261612,
      "shorter": 0.17076441341409762,
      "split": 0.16948532310520661,
      "average": 0.11944721652379126
    }
  },
  "java_vs_javascript": {
    "title": "Java vs JavaScript",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "java",
      "javascript"
    ],
    "inlinks": [],
    "summary": "Difference Between [[Java]] and [[JavaScript]] Although their names are similar, Java and JavaScript are fundamentally different languages designed for different purposes. Below is a comparison...",
    "TFIDF_Score": {
      "end": 0.315231996025282,
      "java": 0.2919700386149244,
      "front": 0.2680852524173348,
      "javascript": 0.21516732777742384,
      "android": 0.21344944672192637,
      "web": 0.2115082041910781,
      "compiled": 0.19227628195725152,
      "typed": 0.19227628195725152,
      "node": 0.16802594272556917,
      "multithreading": 0.1422996311479509
    }
  },
  "javascript": {
    "title": "JavaScript",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "java_vs_javascript",
      "quartz",
      "cryptography",
      "react",
      "strongly_vs_weakly_typed_language"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "jinja_template": {
    "title": "jinja template",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "dbt",
      "quartz",
      "pasted_image_20240922201606.png",
      "flask",
      "pasted_image_20240922202345.png"
    ],
    "inlinks": [
      "quartz"
    ],
    "summary": "Resources LINK Practical jinja2 works with python 3. ![[Pasted image 20240922201606.png]] Renders templates with variable substitutions You can use tags too. ![[Pasted image 20240922202345.png]] Get...",
    "TFIDF_Score": {
      "jinja": 0.3676113133479737,
      "flask": 0.35457941122552206,
      "template": 0.34393157189089496,
      "render": 0.2720613991634219,
      "get": 0.15352509317161506,
      "pasted": 0.1377385988668959,
      "20240922201606": 0.13603069958171096,
      "20240922202345": 0.13603069958171096,
      "jinja2": 0.13603069958171096,
      "popularized": 0.13603069958171096
    }
  },
  "johnson–lindenstrauss_lemma": {
    "title": "Johnson–Lindenstrauss lemma",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm"
    ],
    "inlinks": [
      "how_llms_store_facts",
      "mathematics"
    ],
    "summary": "Johnson–Lindenstrauss lemma math https://youtu.be/9-Jl0dxWQs8?list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&t=1125 THe number of vectors that can be fit into a spaces grows exponentially. Useful for [[LLM]] in storing ideas. Plotting M>N...",
    "TFIDF_Score": {
      "torch": 0.4455581283827295,
      "big_matrix": 0.425703995851438,
      "num_vectors": 0.33110310788445174,
      "plt": 0.254842060196214,
      "dot_product": 0.23650221991746553,
      "loss": 0.19893127772986244,
      "norm": 0.18920177593397242,
      "tqdm": 0.1419013319504793,
      "diff": 0.12782539815043314,
      "vector": 0.12082766600254692
    }
  },
  "joining_datasets": {
    "title": "Joining Datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sql_joins",
      "de_tools"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ],
    "summary": "Joining Datasets In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ```python Merge df1 = pd.DataFrame({'key': ['A', 'B'], 'value': [1, 2]}) df2 = pd.DataFrame({'key': ['A', 'B'], 'value': [3, 4]})...",
    "TFIDF_Score": {
      "df1": 0.5281381058444511,
      "df2": 0.5281381058444511,
      "join": 0.21724177005651213,
      "concat": 0.21125524233778042,
      "set_index": 0.18355356468985923,
      "joining": 0.1780415445210609,
      "key": 0.175314915851378,
      "inplace": 0.17338119787207934,
      "merge": 0.16934422198038104,
      "de_tools": 0.1361305241636616
    }
  },
  "json_to_yaml": {
    "title": "Json to Yaml",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "yaml",
      "json"
    ],
    "inlinks": [],
    "summary": "[[Json]] [[yaml]] JSON { \"json\": [ \"rigid\", \"better for data interchange\" ], \"yaml\": [ \"slim and flexible\", \"better for configuration\" ], \"object\": { \"key\": \"value\",...",
    "TFIDF_Score": {
      "alias": 0.5997698434575405,
      "yaml": 0.32196642140492104,
      "baz": 0.2546862290352637,
      "json": 0.24394364717504888,
      "bar": 0.22655942962566608,
      "line": 0.18421005965800724,
      "paragraph": 0.17993095303726211,
      "break": 0.17668562852288258,
      "alias_reuse": 0.12734311451763186,
      "denote": 0.12734311451763186
    }
  },
  "json": {
    "title": "Json",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "json_to_yaml",
      "yaml",
      "normalisation_of_data",
      "software_development_portal",
      "structured_data",
      "pydantic",
      "rest_api",
      "semi-structured_data",
      "multi-level_index"
    ],
    "summary": "Stands for javascript object notation records separated by commas keys & strings wrapped by double quotes good choice for data transport JSON data embedded inside...",
    "TFIDF_Score": {
      "string": 0.4159600205548896,
      "record": 0.3147365439256677,
      "structured": 0.27984193986190004,
      "semi": 0.2484309375693174,
      "json": 0.2098243626171118,
      "data": 0.180402478893148,
      "name": 0.16787819339930055,
      "_airbyte_data": 0.1642983214367117,
      "transport": 0.1642983214367117,
      "airbyte": 0.15476483185086426
    }
  },
  "junction_tables": {
    "title": "Junction Tables",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "justfile": {
    "title": "Justfile",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "Justfile is a command runner designed to streamline workflows by allowing users to define simple, reusable commands for common tasks. This approach minimizes the cognitive...",
    "TFIDF_Score": {
      "justfile": 0.4500924063902433,
      "command": 0.4467691398577209,
      "application": 0.28588483327058417,
      "deploy": 0.2092549948986423,
      "deployment": 0.18008744761564216,
      "npm": 0.1731124639962474,
      "echo": 0.17306285528432205,
      "run": 0.17221310526473202,
      "environment": 0.1697848878742223,
      "install": 0.1639836358709967
    }
  },
  "k-means": {
    "title": "K-means",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "ml_tools",
      "unsupervised_learning",
      "k_means.py",
      "pasted_image_20241230200255.png",
      "wcss_and_elbow_method",
      "hyperparameter"
    ],
    "inlinks": [
      "kmeans_vs_gmm",
      "model_parameters",
      "gaussian_mixture_models",
      "unsupervised_learning",
      "machine_learning_algorithms",
      "algorithms",
      "why_and_when_is_feature_scaling_necessary",
      "dbscan",
      "clustering"
    ],
    "summary": "K-means clustering is an [[Unsupervised Learning]] algorithm that partitions data into (k) clusters. Each data point is assigned to the cluster with the nearest centroid....",
    "TFIDF_Score": {
      "centroid": 0.6282941391881934,
      "cluster": 0.4313670985599162,
      "mean": 0.19255165093436535,
      "initial": 0.15515514002326242,
      "updated": 0.14690298009773126,
      "algorithm": 0.13389564435193607,
      "number": 0.1299936815762292,
      "partition": 0.12954912206807703,
      "unsupervised": 0.10916085461866645,
      "convergence": 0.10473034054774079
    }
  },
  "k-nearest_neighbours": {
    "title": "K-nearest neighbours",
    "tags": [
      "classifier"
    ],
    "aliases": [
      "KNN"
    ],
    "outlinks": [
      "parametric_vs_non-parametric_models",
      "classification",
      "recommender_systems",
      "k-nearest_neighbours",
      "regression"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "unsupervised_learning",
      "standardisation",
      "classification",
      "k-nearest_neighbours",
      "supervised_learning",
      "hyperparameter"
    ],
    "summary": "K-nearest Neighbors is a non-parametric method used for both [[classification]] and [[regression]] tasks. It classifies a sample by a majority vote of its neighbors, assigning...",
    "TFIDF_Score": {
      "neighbor": 0.47720826791987503,
      "knn": 0.3865614380436997,
      "nearest": 0.323326919483958,
      "parametric": 0.2651157043999306,
      "point": 0.21225733424827137,
      "non": 0.1545719438451599,
      "data": 0.13615684498714412,
      "distance": 0.13574975539594392,
      "new": 0.1224910669771933,
      "classifies": 0.11971344366738505
    }
  },
  "kaggle_abalone_regression_example": {
    "title": "Kaggle Abalone regression example",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cross_validation",
      "hyperparameter"
    ],
    "inlinks": [],
    "summary": "Task: For each model as we tune the hyperparameters what happens to the (RMSLE) metric (scatter metric against hyperparameter). Using Root Mean Squared Logarithmic Error...",
    "TFIDF_Score": {
      "repeatedstratifiedkfold": 0.32874198112791014,
      "hyperparameter": 0.2644045985915521,
      "result": 0.23808788953022464,
      "class": 0.21995254629693567,
      "rmsle": 0.21916132075194011,
      "stratifiedkfold": 0.21916132075194011,
      "model": 0.19085197616081054,
      "fold": 0.19042292736023606,
      "blog": 0.1798698668904124,
      "post": 0.1629648683610425
    }
  },
  "kernelling": {
    "title": "Kernelling",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "support_vector_machines",
      "kernelling"
    ],
    "inlinks": [
      "support_vector_machines",
      "kernelling"
    ],
    "summary": "[[Kernelling]] is a technique where the [[Support Vector Machines|SVM]] uses a kernel function to map the dataset into a higher-dimensional space, making it easier to...",
    "TFIDF_Score": {
      "kernel": 0.6073873334385987,
      "trick": 0.2466891064184467,
      "kernelling": 0.23237485167489963,
      "dimensional": 0.22489262950242606,
      "higher": 0.19957827040814768,
      "space": 0.19081346257860032,
      "svm": 0.17799206436815812,
      "dimension": 0.14880758597294744,
      "original": 0.14772124499753636,
      "low": 0.14368218250040027
    }
  },
  "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)": {
    "title": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS) Data Type: [[Web Map Tile Service (WMTS)]]: Serves image tiles (raster data). [[Web...",
    "TFIDF_Score": {
      "wfs": 0.4839468623610829,
      "wmts": 0.42345350456594755,
      "map": 0.30577970825234435,
      "tile": 0.24197343118054146,
      "geographic": 0.22638667396773046,
      "web": 0.1996321336541461,
      "interactivity": 0.1814800733854061,
      "feature": 0.13680932035459384,
      "data": 0.13272688098091248,
      "rendered": 0.12651620570929228
    }
  },
  "kmeans_vs_gmm": {
    "title": "Kmeans vs GMM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "distributions",
      "k-means",
      "covariance_structures"
    ],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "Key Differences Between [[k-Means]] and GMM Cluster Shape k-Means: Assumes clusters are spherical and equidistant from their centroids. GMM: Models clusters as Gaussian distributions, allowing...",
    "TFIDF_Score": {
      "cluster": 0.5511828295561093,
      "gmm": 0.4894355735360309,
      "shape": 0.24992850267856398,
      "spherical": 0.24471778676801545,
      "mean": 0.223667675401835,
      "covariance": 0.19995590433632665,
      "overlapping": 0.1504840433343899,
      "centroid": 0.14596508406850484,
      "probability": 0.14487209510673527,
      "distribution": 0.12264542234055205
    }
  },
  "knowledge_graph_vs_rag_setup": {
    "title": "Knowledge graph vs RAG setup",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "semantic_search",
      "generative",
      "knowledge_graph",
      "rag"
    ],
    "inlinks": [],
    "summary": "Comparison: Knowledge Graph vs. RAG Setup ==Knowledge Graphs are structured representations of entities and their relationships, designed primarily for querying, reasoning, and storing factual information.==...",
    "TFIDF_Score": {
      "knowledge": 0.4096783621540108,
      "graph": 0.3438518244928281,
      "rag": 0.33145074605487806,
      "retrieval": 0.24429263680378516,
      "structured": 0.22821007156830692,
      "generation": 0.21620525253999695,
      "generative": 0.2016701957240283,
      "reasoning": 0.2016701957240283,
      "factual": 0.19956926840267947,
      "setup": 0.15934340729372823
    }
  },
  "knowledge_graph": {
    "title": "Knowledge Graph",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "explainability",
      "pasted_image_20240921154214.png"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "accessing_gen_ai_generated_content",
      "knowledge_graphs_with_obsidian",
      "knowledge_graph_vs_rag_setup",
      "graphrag"
    ],
    "summary": "[!Summary] Knowledge graphs (KGs) enable large language models (LLMs) to generate more accurate, trustworthy AI outputs. Neo4j is leader in this space and make use...",
    "TFIDF_Score": {
      "knowledge": 0.474578885433384,
      "graph": 0.3983242238229211,
      "kg": 0.2506356683655475,
      "reasoning": 0.1868947458364657,
      "relationship": 0.1798167696972602,
      "person": 0.17338850983120974,
      "entity": 0.16697073811072977,
      "llm": 0.16050139892234302,
      "company": 0.14201416098565306,
      "barack": 0.13303739328941688
    }
  },
  "knowledge_graphs_with_obsidian": {
    "title": "Knowledge Graphs with Obsidian",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "llm",
      "knowledge_graph",
      "rag"
    ],
    "inlinks": [],
    "summary": "[!Summary] Llama Index is a python package that can be used to create Knowledge graphs (KGs). There exists a method to integrate with Obsidian. This...",
    "TFIDF_Score": {
      "obsidian": 0.48237285088648835,
      "graph": 0.342737785051394,
      "knowledge": 0.3402925437317191,
      "rag": 0.27531407054002316,
      "tutorial": 0.20648555290501738,
      "llama": 0.17971606355838113,
      "llm": 0.1726291381312941,
      "set": 0.12206048662070222,
      "index": 0.11187024218903774,
      "note": 0.1083193799027606
    }
  },
  "knowledge_work": {
    "title": "Knowledge Work",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "scientific_method"
    ],
    "inlinks": [
      "thinking_systems"
    ],
    "summary": "Knowledge work refers to tasks that primarily involve handling or using information and require cognitive skills rather than manual labor. It is characterized by problem-solving,...",
    "TFIDF_Score": {
      "knowledge": 0.4470338161474995,
      "solving": 0.28935753523601726,
      "work": 0.22980428147371626,
      "skill": 0.21122705576895254,
      "development": 0.20725382444827575,
      "information": 0.2069600102812297,
      "scientific": 0.20569805800511523,
      "research": 0.18948648669538432,
      "problem": 0.17815777029086166,
      "analyzing": 0.15350055966231899
    }
  },
  "kubernetes": {
    "title": "kubernetes",
    "tags": [
      "data_orchestration",
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "google_cloud_platform"
    ],
    "summary": "It’s a platform that allows you to run and orchestrate container workloads. Kubernetes has become the de-facto standard for your cloud-native apps to (auto-) scale-out...",
    "TFIDF_Score": {
      "kubernetes": 0.4942994359060922,
      "infrastructure": 0.24670453912637852,
      "cloud": 0.23050349725613478,
      "run": 0.19744586183320642,
      "facto": 0.18963283242180248,
      "orchestrate": 0.17862929557967838,
      "zoo": 0.17862929557967838,
      "lock": 0.1647664786353641,
      "yaml": 0.15981862510326553,
      "container": 0.15563527454905404
    }
  },
  "k_means.py": {
    "title": "K_Means.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "k-means"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/KMeans/K_Means.py Key Concepts Used in the Script Data Loading: The script reads data from a CSV file (penguins.csv) and uses a sample dataset with random...",
    "TFIDF_Score": {
      "clustering": 0.44608579266248227,
      "cluster": 0.44019937749274524,
      "feature": 0.1845061926173415,
      "centroid": 0.18318814755380872,
      "script": 0.17775652457811641,
      "csv": 0.15683181946819483,
      "number": 0.14213069307246246,
      "preprocessing": 0.1331244383947252,
      "optimal": 0.1283293762541328,
      "data": 0.11933364093562603
    }
  },
  "label_encoding": {
    "title": "Label encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "labelling_data": {
    "title": "Labelling data",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Possible missing labelling or bias in the data, or under-represented data. Construction of the data set comes from the group collecting it. Examples: - ImageNet",
    "TFIDF_Score": {
      "construction": 0.3739655555165099,
      "imagenet": 0.3739655555165099,
      "labelling": 0.3576210972513418,
      "collecting": 0.3055626721355825,
      "come": 0.30014769009384074,
      "missing": 0.26341001429719213,
      "data": 0.2615490060461877,
      "represented": 0.25824151554605485,
      "bias": 0.2412828425487428,
      "possible": 0.2327873003539253
    }
  },
  "lambda_architecture": {
    "title": "lambda architecture",
    "tags": [
      "data_modeling",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "batch_processing",
      "data_streaming",
      "design_pattern"
    ],
    "inlinks": [],
    "summary": "Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. [[Data Streaming]] This...",
    "TFIDF_Score": {
      "batch": 0.4649790111283792,
      "processing": 0.35889490884157077,
      "stream": 0.23649215795857811,
      "data": 0.2350373694971115,
      "layer": 0.2322932778547971,
      "architecture": 0.22693412472669827,
      "lambda": 0.2178536276772553,
      "real": 0.2074795663556531,
      "view": 0.2016919026402062,
      "latency": 0.17348775218026288
    }
  },
  "langchain": {
    "title": "Langchain",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agentic_solutions",
      "python",
      "llm",
      "pandas_dataframe_agent"
    ],
    "inlinks": [
      "vector_database"
    ],
    "summary": "[[Python]] framework For building apps with [[LLM]] and interaction with them and combining models. Its end to end, through composability Example: [[Pandas Dataframe Agent]] Modules...",
    "TFIDF_Score": {
      "agent": 0.37711075311269693,
      "end": 0.3353981964873164,
      "llm": 0.31965194352710125,
      "composability": 0.2649550820882069,
      "agentic": 0.2386728045070428,
      "apps": 0.21239052692587876,
      "chain": 0.19398137582482225,
      "call": 0.18855537655634846,
      "module": 0.18380952667293599,
      "combining": 0.1707341025261894
    }
  },
  "language_model_output_optimisation": {
    "title": "Language Model Output Optimisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "attention_mechanism",
      "language_models",
      "information_theory",
      "cross_entropy"
    ],
    "inlinks": [],
    "summary": "What techniques from [[information theory]] can be used to measure and optimize the amount of information conveyed by an language model? In information theory, several...",
    "TFIDF_Score": {
      "information": 0.37692534804139943,
      "entropy": 0.3060488973005089,
      "optimize": 0.2774205228960139,
      "model": 0.26500013039194953,
      "language": 0.24174970431557546,
      "theory": 0.200758709774358,
      "conveyed": 0.19562655598329123,
      "measure": 0.1921833893809721,
      "distribution": 0.18470674944966306,
      "perplexity": 0.1699742515327475
    }
  },
  "language_models_large_(llms)_vs_small_(slms)": {
    "title": "Language Models Large (LLMs) vs Small (SLMs)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm",
      "slm"
    ],
    "inlinks": [],
    "summary": "Overview Language models can be categorized into large language models ([[LLM]]) and small language models ([[SLM]]). While LLMs boast extensive general-purpose knowledge and capabilities, SLMs...",
    "TFIDF_Score": {
      "task": 0.2781488731434886,
      "llm": 0.24867514538077407,
      "slms": 0.24756925174853034,
      "specific": 0.22958731711716726,
      "extensive": 0.2115309266827765,
      "language": 0.19104104762118443,
      "general": 0.19066096397810375,
      "resource": 0.18899473845799244,
      "domain": 0.1724656725852407,
      "purpose": 0.1482306611187957
    }
  },
  "language_models": {
    "title": "Language Models",
    "tags": [
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "llm",
      "small_language_models"
    ],
    "inlinks": [
      "small_language_models",
      "vector_embedding",
      "language_model_output_optimisation",
      "explain_the_curse_of_dimensionality",
      "memory",
      "llm"
    ],
    "summary": "A language model is a machine learning model that is designed to understand, generate, and predict human language. It does this by analyzing large amounts...",
    "TFIDF_Score": {
      "word": 0.38483224347623585,
      "language": 0.3234932742591287,
      "predict": 0.2747559823692756,
      "generate": 0.2710550674367615,
      "coherent": 0.23268832367569384,
      "text": 0.2118554706941991,
      "model": 0.2026316790329863,
      "slm": 0.19610490173302625,
      "phrase": 0.190971735627897,
      "assigning": 0.16559272668085498
    }
  },
  "lasso": {
    "title": "Lasso",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection"
    ],
    "inlinks": [
      "regularisation",
      "elastic_net",
      "embedded_methods",
      "regression"
    ],
    "summary": "L1 Regularization (Lasso Regression): In L1 regularization, a penalty proportional to the absolute value of the coefficients is added to the loss function. The L1...",
    "TFIDF_Score": {
      "lasso": 0.3523086743130925,
      "coefficient": 0.32424865959654875,
      "regularization": 0.2570425012530756,
      "absolute": 0.25164905308078034,
      "penalty": 0.24418326004479088,
      "proportional": 0.24388544837786222,
      "zero": 0.22454585991238235,
      "selection": 0.1845083385625026,
      "feature": 0.1838538072777408,
      "exactly": 0.18291408628339667
    }
  },
  "latency": {
    "title": "Latency",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "performance_dimensions",
      "data_ingestion"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "lbfgs": {
    "title": "LBFGS",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "logistic_regression",
      "model_parameters",
      "sklearn",
      "cost_function"
    ],
    "inlinks": [],
    "summary": "LBFGS stands for Limited-memory Broyden-Fletcher-Goldfarb-Shanno, which is an [[Optimisation function]]optimization algorithm used to find the minimum of a function. In the context of [[logistic regression]],...",
    "TFIDF_Score": {
      "lbfgs": 0.5396685412824244,
      "function": 0.2571816455969142,
      "logistic": 0.2436062555074649,
      "memory": 0.22317759123101977,
      "hessian": 0.20237570298090912,
      "limited": 0.2020111734206683,
      "cost": 0.1579393344959092,
      "differentiable": 0.14626854347772267,
      "matrix": 0.1449515275854961,
      "regression": 0.13991909541051192
    }
  },
  "learning_rate": {
    "title": "learning rate",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241216204925.png",
      "loss_function",
      "standardised/optuna",
      "gradient_descent",
      "adam_optimizer",
      "hyperparameter"
    ],
    "inlinks": [
      "gradient_boosting",
      "z-normalisation",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "momentum",
      "adam_optimizer",
      "weak_learners",
      "gradient_descent",
      "linear_regression",
      "adaptive_learning_rates",
      "xgboost",
      "model_parameters_vs_hyperparameters",
      "optimisation_techniques"
    ],
    "summary": "Description The learning rate is a [[Hyperparameter]] in machine learning that ==determines the step size at which a model's parameters are updated during training==. It...",
    "TFIDF_Score": {
      "rate": 0.5324395432551112,
      "learning": 0.37799958148975293,
      "convergence": 0.22740404765102157,
      "training": 0.20107787010612266,
      "model": 0.16445775860247155,
      "minimum": 0.1611038491811635,
      "parameter": 0.15283744939200072,
      "hyperparameter": 0.14646747132418875,
      "optimization": 0.12893509632748232,
      "adjusted": 0.11519708553019573
    }
  },
  "learning_styles": {
    "title": "Learning Styles",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "continuous",
      "unsupervised_learning",
      "dimensionality_reduction",
      "classification",
      "categorical",
      "_pasted_image_20240112101344.png",
      "clustering",
      "regression"
    ],
    "inlinks": [],
    "summary": "What does the data look like [[continuous]] or [[categorical]]? ![[ Pasted image 20240112101344.png|500]] [[Unsupervised Learning]] [[Regression]] [[Classification]] [[Unsupervised Learning]] [[Dimensionality Reduction]] [[Clustering]]",
    "TFIDF_Score": {
      "unsupervised": 0.5311613794198637,
      "20240112101344": 0.3627510792927024,
      "learning": 0.2606404676172985,
      "look": 0.2548015602606761,
      "500": 0.22959751669925646,
      "categorical": 0.22959751669925646,
      "dimensionality": 0.22216875905714026,
      "reduction": 0.21722094597246888,
      "clustering": 0.2127039465242626,
      "continuous": 0.19776958038179393
    }
  },
  "lemmatization": {
    "title": "lemmatization",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "nlp",
      "normalisation_of_text"
    ],
    "summary": "Lemmatization is the process of ==reducing a word to its base or root== form, known as the \"lemma.\" Unlike stemming, which simply cuts off word...",
    "TFIDF_Score": {
      "word": 0.6277063238288546,
      "lemmatization": 0.29305122073048245,
      "root": 0.22050982635995864,
      "form": 0.1977182892555553,
      "run": 0.16936254107620186,
      "cut": 0.16266078241520607,
      "lemmatized": 0.16266078241520607,
      "morphological": 0.16266078241520607,
      "ran": 0.15322231182329252,
      "language": 0.1507586579502077
    }
  },
  "lightgbm_vs_xgboost_vs_catboost": {
    "title": "LightGBM vs XGBoost vs CatBoost",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "gradient_boosting",
      "regularisation",
      "catboost",
      "xgboost",
      "lightgbm"
    ],
    "inlinks": [],
    "summary": "This table summarizes the key differences and strengths of each [[Gradient Boosting]] framework. | Feature/Aspect | [[LightGBM]] (LGBM) | [[XGBoost]] | [[CatBoost]] | | ---------------------------------...",
    "TFIDF_Score": {
      "categorical": 0.3279693648319905,
      "growth": 0.23929515525922818,
      "datasets": 0.23266556606353378,
      "boosting": 0.21073717338509623,
      "regularization": 0.1967816188991943,
      "preprocessing": 0.19041463820950236,
      "speed": 0.1810835501668094,
      "competitive": 0.18009021855723806,
      "feature": 0.1759390453938666,
      "competition": 0.17468220091090123
    }
  },
  "lightgbm": {
    "title": "LightGBM",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "LGBM"
    ],
    "outlinks": [
      "gradient_descent"
    ],
    "inlinks": [
      "lightgbm_vs_xgboost_vs_catboost",
      "gradient_boosting",
      "optuna",
      "time_series_forecasting"
    ],
    "summary": "LightGBM is a gradient boosting framework that is designed for speed and efficiency. It is particularly well-suited for handling large datasets and high-dimensional data. Tree...",
    "TFIDF_Score": {
      "leaf": 0.3163178262727874,
      "wise": 0.3163178262727874,
      "tree": 0.2674635600052613,
      "lightgbm": 0.24942265681055936,
      "speed": 0.22983370792734095,
      "growth": 0.2277875023916351,
      "dimensional": 0.1798688815293027,
      "gpu": 0.17773029664935316,
      "learning": 0.17720416669556396,
      "rate": 0.16224311250341994
    }
  },
  "linear_discriminant_analysis": {
    "title": "Linear Discriminant Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dimensionality_reduction",
      "feature_scaling"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "linear_regression": {
    "title": "Linear Regression",
    "tags": [
      "regressor"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "p-values_in_linear_regression_in_sklearn",
      "linearity",
      "loss_function",
      "learning_rate",
      "gradient_descent",
      "ordinary_least_squares",
      "pasted_image_20240124135607.png",
      "adjusted_r_squared",
      "r_squared",
      "pasted_image_20240117145455_1.png",
      "f-statistic",
      "model_evaluation"
    ],
    "inlinks": [
      "logistic_regression",
      "ds_&_ml_portal",
      "model_parameters",
      "outliers",
      "p-values_in_linear_regression_in_sklearn",
      "ridge",
      "machine_learning_algorithms",
      "pytorch",
      "general_linear_regression",
      "interview_notepad",
      "maximum_likelihood_estimation",
      "supervised_learning",
      "regression"
    ],
    "summary": "Description Linear regression assumes [[linearity]] between the input features and the target variable. Assumes that the relationship between the independent variable(s) and the dependent variable...",
    "TFIDF_Score": {
      "variable": 0.3275988933340803,
      "sse": 0.3007579454870943,
      "dependent": 0.22385801673847758,
      "coefficient": 0.2199237335999659,
      "regression": 0.2105957841694632,
      "hat": 0.210530561840966,
      "linear": 0.2095946047871916,
      "ssr": 0.2003261652699826,
      "b_0": 0.18870214261610666,
      "ldots": 0.18045476729225657
    }
  },
  "linked_list": {
    "title": "Linked List",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A linked list is a linear data structure in which elements (called nodes) are linked together using pointers. Unlike arrays, linked lists do not store...",
    "TFIDF_Score": {
      "node": 0.45700107052463373,
      "linked": 0.3621468088555213,
      "current": 0.2988354822361119,
      "list": 0.29880839226610667,
      "self": 0.2927673921231639,
      "last": 0.262516661246353,
      "none": 0.23363329452550607,
      "next": 0.2334639077947314,
      "head": 0.17437293071765214,
      "pointer": 0.16826419140436658
    }
  },
  "llm_evaluation_metrics": {
    "title": "LLM Evaluation Metrics",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm_evaluation_metrics",
      "llm"
    ],
    "inlinks": [
      "llm_evaluation_metrics",
      "how_do_we_evaluate_of_llm_outputs"
    ],
    "summary": "[[LLM Evaluation Metrics]] - BLEU, - ROUGE, - perplexity which quantify the similarity between generated text and reference outputs. [[LLM]]",
    "TFIDF_Score": {
      "llm": 0.44952003438191296,
      "bleu": 0.37260094931928794,
      "rouge": 0.37260094931928794,
      "quantify": 0.3356407162872051,
      "perplexity": 0.3237421788805937,
      "reference": 0.24982171281642795,
      "similarity": 0.2472201068545902,
      "generated": 0.22645293223278679,
      "evaluation": 0.20540904528835618,
      "metric": 0.1745845787471009
    }
  },
  "llm": {
    "title": "LLM",
    "tags": [
      "language_models"
    ],
    "aliases": [
      "LLMs",
      "Large Language Models"
    ],
    "outlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "attention_mechanism",
      "distillation",
      "transformer",
      "chain_of_thought",
      "reinforcement_learning",
      "language_models",
      "relationships_in_memory",
      "memory",
      "mixture_of_experts",
      "standardised/vector_embedding",
      "transfer_learning"
    ],
    "inlinks": [
      "knowledge_graphs_with_obsidian",
      "rag",
      "attention_mechanism",
      "langchain",
      "how_do_we_evaluate_of_llm_outputs",
      "deep_learning",
      "johnson–lindenstrauss_lemma",
      "inference_versus_prediction",
      "agent-based_modelling",
      "language_models_large_(llms)_vs_small_(slms)",
      "scaling_agentic_systems",
      "comparing_llm",
      "generative_ai_from_theory_to_practice",
      "language_models",
      "how_llms_store_facts",
      "small_language_models",
      "llm_evaluation_metrics",
      "evaluating_language_models",
      "vector_database",
      "deepseek",
      "feature_extraction",
      "neural_scaling_laws"
    ],
    "summary": "A Large Language Model (LLM) is a type of language model designed for language understanding and generation. They can perform a variety of tasks, including:...",
    "TFIDF_Score": {
      "llm": 0.6036600022964076,
      "language": 0.23187648904181232,
      "word": 0.2298696670271277,
      "model": 0.1936589353021266,
      "memory": 0.19177625828992717,
      "generation": 0.1569979402486765,
      "deterministic": 0.1502438601676515,
      "trained": 0.1466979837263321,
      "architecture": 0.1379218002162766,
      "known": 0.13422746800448287
    }
  },
  "load_balancing": {
    "title": "Load Balancing",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Load balancing is a technique used to distribute incoming network traffic across multiple servers. This helps ensure both reliability and performance by preventing any single...",
    "TFIDF_Score": {
      "traffic": 0.4561544680159174,
      "server": 0.36533475260431447,
      "overwhelmed": 0.31152589779341644,
      "incoming": 0.280624017916275,
      "becoming": 0.27067583459898875,
      "distribute": 0.25567523309663204,
      "balancing": 0.21356925617929867,
      "preventing": 0.21356925617929867,
      "reliability": 0.19387147334234917,
      "much": 0.18266737630215724
    }
  },
  "local_interpretable_model-agnostic_explanations": {
    "title": "Local Interpretable Model-agnostic Explanations",
    "tags": [],
    "aliases": [
      "LIME"
    ],
    "outlinks": [],
    "inlinks": [
      "feature_importance",
      "model_interpretability"
    ],
    "summary": "LIME explains individual predictions ==by approximating the model locally== with an interpretable model and calculating the feature importance based on the surrogate model. Key Points...",
    "TFIDF_Score": {
      "lime": 0.7197472031972203,
      "cholesterol": 0.18438568718158405,
      "model": 0.17840928344288207,
      "explainer": 0.17368661848049524,
      "patient": 0.16609550843012777,
      "prediction": 0.16502406497081948,
      "approximating": 0.12292379145438936,
      "lime_tabular": 0.12292379145438936,
      "feature_names": 0.11579107898699681,
      "risk": 0.11474848508691356
    }
  },
  "logical_model": {
    "title": "Logical Model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Logical Model - Customer: CustomerID, Name, Email - Order: OrderID, OrderDate, CustomerID - Book: BookID, Title, Author - Order-Book Relationship: OrderID, BookID Logical Model -...",
    "TFIDF_Score": {
      "bookid": 0.38168056440107595,
      "customerid": 0.38168056440107595,
      "orderid": 0.38168056440107595,
      "book": 0.3325488086253041,
      "logical": 0.30129433994779303,
      "order": 0.21971247718302858,
      "orderdate": 0.19084028220053798,
      "relationship": 0.18255599610488327,
      "author": 0.16627440431265206,
      "title": 0.15064716997389652
    }
  },
  "logistic_regression_does_not_predict_probabilities": {
    "title": "Logistic Regression does not predict probabilities",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "what_is_the_difference_between_odds_and_probability"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "In logistic regression, the model predicts the ==odds of an event happening rather than directly predicting probabilities.== The odds are defined as: $$ \\text{Odds} =...",
    "TFIDF_Score": {
      "odds": 0.736626875494739,
      "log": 0.26380437298237813,
      "b_0": 0.1996509559536748,
      "text": 0.19297334836036417,
      "b_1": 0.19092505414520813,
      "frac": 0.18901964107920446,
      "probability": 0.1772890274613974,
      "success": 0.16024142429843935,
      "logistic": 0.13300528092225933,
      "explanation": 0.1308378052164262
    }
  },
  "logistic_regression_in_sklearn_&_gradient_descent": {
    "title": "Logistic regression in sklearn & Gradient Descent",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "gradient_descent"
    ],
    "inlinks": [],
    "summary": "Logistic regression in sklearn & Gradient Descent sklearn's Logistic Regression implementation does not use [[Gradient Descent]] by default. Instead, it uses more sophisticated optimization techniques...",
    "TFIDF_Score": {
      "solver": 0.3579829023638768,
      "gradient": 0.33157540091650434,
      "logistic": 0.2585494397678311,
      "descent": 0.25640513057058256,
      "newton": 0.25360786863950896,
      "sag": 0.20600462028466635,
      "regression": 0.18562743624876363,
      "datasets": 0.18499682056432398,
      "sklearn": 0.1733330173284201,
      "efficient": 0.1530659667323175
    }
  },
  "logistic_regression_statsmodel_summary_table": {
    "title": "Logistic Regression Statsmodel Summary table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "mle",
      "sklearn",
      "pasted_image_20240124095916.png"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "Statsmodel has this summary table unlike [[Sklearn]] Explanation of summary The dependent variable is 'duration'. The model used is a Logit regression (logistic in common...",
    "TFIDF_Score": {
      "duration": 0.3430927120357333,
      "significant": 0.2043080206700077,
      "summary": 0.18871028812424742,
      "0051": 0.18211358488393334,
      "20240124095916": 0.18211358488393334,
      "518": 0.18211358488393334,
      "converged": 0.18211358488393334,
      "lingo": 0.18211358488393334,
      "pseudo": 0.18211358488393334,
      "mle": 0.17154635601786666
    }
  },
  "logistic_regression": {
    "title": "Logistic Regression",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "ml_tools",
      "model_parameters",
      "support_vector_machines",
      "interpreting_logistic_regression_model_parameters",
      "binary_classification",
      "regression_logistic_metrics.ipynb",
      "linear_regression",
      "confusion_matrix",
      "logistic_regression_does_not_predict_probabilities",
      "logistic_regression_statsmodel_summary_table",
      "regression",
      "model_evaluation"
    ],
    "inlinks": [
      "ridge",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "model_parameters",
      "interpreting_logistic_regression_model_parameters",
      "statistical_assumptions",
      "optimising_a_logistic_regression_model",
      "machine_learning_algorithms",
      "lbfgs",
      "statistics",
      "roc_(receiver_operating_characteristic)",
      "regression"
    ],
    "summary": "==Logistic regression models the log-odds of the probability as a linear function of the input features.== It models the probability of an input belonging to...",
    "TFIDF_Score": {
      "logistic": 0.5314458330514246,
      "mathbf": 0.39887013459386933,
      "regression": 0.3375297515496786,
      "probability": 0.2179659316168746,
      "odds": 0.19810816803852038,
      "log": 0.18243630919575166,
      "function": 0.17533174795361528,
      "class": 0.16344969066769205,
      "mid": 0.1628617203406593,
      "y_i": 0.1533676271814419
    }
  },
  "looker_studio": {
    "title": "Looker Studio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/gsheets",
      "google",
      "powerbi",
      "postgresql"
    ],
    "inlinks": [
      "data_visualisation"
    ],
    "summary": "Looker studio is [[Google]] version of [[PowerBI]], but its free. Connectors to data Can connect to data sources i.e: - [[standardised/GSheets|GSheets]] - [[PostgreSQL]] Data Modelling...",
    "TFIDF_Score": {
      "looker": 0.4107187571278734,
      "gsheets": 0.3961586866175328,
      "studio": 0.3578057284629772,
      "data": 0.25031866482190557,
      "blend": 0.22797323355449753,
      "blending": 0.22797323355449753,
      "connector": 0.22797323355449753,
      "powerbi": 0.1980793433087664,
      "free": 0.1827455235733759,
      "postgresql": 0.1695172664071518
    }
  },
  "loss_function": {
    "title": "Loss function",
    "tags": [
      "deep_learning",
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "loss_versus_cost_function",
      "classification",
      "cross_entropy",
      "mean_squared_error",
      "cost_function",
      "regression",
      "model_evaluation"
    ],
    "inlinks": [
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "optimising_a_logistic_regression_model",
      "linear_regression",
      "xgboost",
      "learning_rate",
      "feed_forward_neural_network",
      "cost_function",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "ridge",
      "pytorch",
      "gradient_descent",
      "typical_output_formats_in_neural_networks",
      "cross_entropy",
      "gradient_boosting",
      "regularisation",
      "ds_&_ml_portal",
      "loss_versus_cost_function",
      "model_parameters_vs_hyperparameters",
      "neural_scaling_laws",
      "embedded_methods",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Loss functions are used in training machine learning models. Also known as a [[cost function]], error function, or objective function. Serves as a metric for...",
    "TFIDF_Score": {
      "function": 0.33006805081022605,
      "model": 0.2776694087405708,
      "loss": 0.25144002445699093,
      "measure": 0.23493330370675788,
      "error": 0.20849692031932054,
      "training": 0.19804083048613563,
      "used": 0.19175311221225347,
      "actual": 0.1909361981160782,
      "predicted": 0.1882512436764442,
      "evaluation": 0.17578057886489826
    }
  },
  "loss_versus_cost_function": {
    "title": "Loss versus Cost function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "loss_function",
      "model_parameters_vs_hyperparameters",
      "cost_function",
      "model_optimisation"
    ],
    "inlinks": [
      "loss_function",
      "cost_function"
    ],
    "summary": "In machine learning, the terms \"loss function\" and \"cost function\" are often used interchangeably, but they can have slightly different meanings depending on the context:...",
    "TFIDF_Score": {
      "function": 0.4492629752119639,
      "loss": 0.32594344508979556,
      "cost": 0.23648531200786585,
      "model": 0.22496548599512334,
      "performing": 0.20362194854393856,
      "optimisation": 0.19795076224756789,
      "training": 0.1925412750908417,
      "interchangeably": 0.15500096156012658,
      "measure": 0.15227283433072952,
      "parameter": 0.14634886162279756
    }
  },
  "lstm": {
    "title": "LSTM",
    "tags": [
      "deep_learning",
      "time_series",
      "code_snippet",
      "drafting"
    ],
    "aliases": [
      "LSTM vs. Transformer",
      "RNN vs. Transformer"
    ],
    "outlinks": [
      "rnn",
      "recurrent_neural_networks",
      "keras",
      "attention_mechanism",
      "transformer",
      "pytorch",
      "bert",
      "vanishing_and_exploding_gradients_problem",
      "gru",
      "lstm",
      "pasted_image_20241015211424.png",
      "nlp"
    ],
    "inlinks": [
      "ai_engineer",
      "ds_&_ml_portal",
      "recurrent_neural_networks",
      "attention_mechanism",
      "anomaly_detection_in_time_series",
      "transformers_vs_rnns",
      "lstm",
      "named_entity_recognition"
    ],
    "summary": "What is LSTM LSTM (Long Short-Term Memory) networks are a specialized type of Recurrent Neural Network (RNN) designed to overcome the [[vanishing and exploding gradients...",
    "TFIDF_Score": {
      "lstm": 0.5322052438979484,
      "transformer": 0.2711014269656401,
      "long": 0.22424011082409673,
      "cell": 0.21847106900352864,
      "sequential": 0.18696860453942837,
      "gate": 0.1776392875109833,
      "lstms": 0.1776392875109833,
      "attention": 0.16055779525616987,
      "sequence": 0.15281732629015238,
      "memory": 0.14455642515091727
    }
  },
  "machine_learning_algorithms": {
    "title": "Machine Learning Algorithms",
    "tags": [
      "ml_process",
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "data_quality",
      "gaussian_mixture_models",
      "principal_component_analysis",
      "k-means",
      "linear_regression",
      "supervised_learning",
      "clustering",
      "scalability",
      "unsupervised_learning",
      "bias_and_variance",
      "classification",
      "manifold_learning",
      "regression",
      "overfitting",
      "support_vector_machines",
      "naive_bayes",
      "support_vector_regression",
      "decision_tree",
      "random_forests",
      "logistic_regression",
      "interpretability",
      "algorithms",
      "dimensionality_reduction",
      "random_forest_regression"
    ],
    "inlinks": [
      "gradient_boosting",
      "z-normalisation",
      "ds_&_ml_portal",
      "use_of_rnns_in_energy_sector",
      "model_building",
      "supervised_learning",
      "pandas_stack",
      "machine_learning"
    ],
    "summary": "Machine learning [[Algorithms]] are used to automate tasks, extract insights, and make more informed decisions. Choosing the right algorithm for a specific problem involves understanding...",
    "TFIDF_Score": {
      "algorithm": 0.5256162649843407,
      "learning": 0.3644741386830294,
      "machine": 0.3124938035607699,
      "regression": 0.17580266050350665,
      "data": 0.17137978704786005,
      "strength": 0.15378064728495544,
      "common": 0.1527002580870961,
      "limitation": 0.14818353698733755,
      "include": 0.1468137239367865,
      "automate": 0.11261620617645769
    }
  },
  "machine_learning_operations": {
    "title": "Machine Learning Operations",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "MLOPs"
    ],
    "outlinks": [
      "devops",
      "model_observability",
      "ds_&_ml_portal"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Machine Learning Operations (MLOps) is a set of practices and tools designed to streamline the entire lifecycle of machine learning models, from development to deployment...",
    "TFIDF_Score": {
      "mlops": 0.5272812701343917,
      "model": 0.3883694881680471,
      "retraining": 0.21142129839065082,
      "deployment": 0.20485562331243254,
      "remain": 0.14094753226043388,
      "machine": 0.1339352199574163,
      "environment": 0.12875740645266465,
      "streamline": 0.12872161155052467,
      "data": 0.12854376990096522,
      "developed": 0.12066860345024728
    }
  },
  "machine_learning": {
    "title": "Machine Learning",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "tensorflow",
      "data_ingestion"
    ],
    "summary": "Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed...",
    "TFIDF_Score": {
      "machine": 0.38505753344099114,
      "learning": 0.3454678048385311,
      "programmed": 0.30194117601149156,
      "artificial": 0.26307385736317496,
      "explicitly": 0.24671243436418147,
      "become": 0.21974943508132977,
      "historical": 0.21726925896406626,
      "predicting": 0.21726925896406626,
      "intelligence": 0.21491632865385424,
      "outcome": 0.19481266163655175
    }
  },
  "maintainable_code": {
    "title": "Maintainable Code",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "testing",
      "pyright",
      "pydantic"
    ],
    "inlinks": [
      "pyright"
    ],
    "summary": "[[Pydantic]] : runtine analysis [[Pyright]]: static analysis [[Testing]] Want robust and reliable Python applications.",
    "TFIDF_Score": {
      "runtine": 0.43602194576552883,
      "pyright": 0.39277065305669967,
      "pydantic": 0.35785150900287227,
      "analysis": 0.35302837902510775,
      "static": 0.2892998319516637,
      "reliable": 0.27134892358521395,
      "robust": 0.2556672989542825,
      "want": 0.25067276002245176,
      "testing": 0.2339335096397685,
      "application": 0.1784952582991555
    }
  },
  "makefile": {
    "title": "Makefile",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "A Makefile is a special file used by the make build automation tool to manage the build process of a project. It defines a set...",
    "TFIDF_Score": {
      "target": 0.41107236776438905,
      "file": 0.4026465822407274,
      "cpp": 0.3223621210841796,
      "executable": 0.2903853396568784,
      "object": 0.21698930458044452,
      "objs": 0.19341727265050773,
      "clean": 0.18114542144182086,
      "makefile": 0.17423120379412704,
      "rule": 0.17249239369557187,
      "build": 0.17205718760497676
    }
  },
  "manifold_learning": {
    "title": "Manifold Learning",
    "tags": [
      "deleted",
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "pasted_image_20240127124620.png"
    ],
    "inlinks": [
      "explain_the_curse_of_dimensionality",
      "machine_learning_algorithms",
      "principal_component_analysis"
    ],
    "summary": "Manifold learning is a powerful approach for high-dimensional data exploration, focusing on uncovering the lower-dimensional manifold that the data resides on. These algorithms aim to...",
    "TFIDF_Score": {
      "manifold": 0.5519776870472916,
      "dimensional": 0.40875305306076687,
      "distance": 0.2761005275464704,
      "isomap": 0.1979776630383219,
      "high": 0.18501805070015812,
      "space": 0.17340627295493002,
      "geodesic": 0.16813844504675854,
      "data": 0.16615710262032665,
      "learning": 0.1661127078159517,
      "structure": 0.1509921365127503
    }
  },
  "many-to-many_relationships": {
    "title": "Many-to-Many Relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "many-to-many_relationships"
    ],
    "inlinks": [
      "many-to-many_relationships",
      "relating_tables_together",
      "implementing_database_schema"
    ],
    "summary": "Many-to-Many Relationships Occurs when multiple records in one table are associated with multiple records in another table. Need to use a junction table (also known...",
    "TFIDF_Score": {
      "table": 0.4656520961239502,
      "course": 0.32867173123154725,
      "junction": 0.30178454079619327,
      "student": 0.2885948104604577,
      "enrollment": 0.2640614731966691,
      "many": 0.24439798401576876,
      "course_id": 0.20023401350049067,
      "student_id": 0.20023401350049067,
      "relationship": 0.18042763567406753,
      "foreign": 0.17336833038368285
    }
  },
  "map_reduce": {
    "title": "map reduce",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "real-time_processing",
      "apache_spark",
      "batch_processing",
      "distributed_computing",
      "hadoop"
    ],
    "inlinks": [],
    "summary": "MapReduce is a programming model and processing technique used for processing and generating large data sets with a parallel, distributed algorithm on a cluster. [[Distributed...",
    "TFIDF_Score": {
      "mapreduce": 0.3944554758555687,
      "count": 0.2485914436971299,
      "function": 0.22148497725914593,
      "map": 0.21746516964363682,
      "reduce": 0.20687362425925568,
      "processing": 0.20403150501539938,
      "data": 0.19088368629032398,
      "word": 0.18430270201536053,
      "intermediate": 0.16905234679524372,
      "key": 0.1553658248534323
    }
  },
  "markov_chain": {
    "title": "Markov chain",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "Is a stochastic model that describes a sequence of events in which the probability of each event depends only on the state attained in the...",
    "TFIDF_Score": {
      "event": 0.6901863460126815,
      "attained": 0.36029009124279593,
      "describes": 0.288811981722076,
      "stochastic": 0.25307292696171607,
      "previous": 0.2366532751845969,
      "depends": 0.22421882519125322,
      "sequence": 0.21897085277266612,
      "state": 0.20209957279064644,
      "probability": 0.20091422042423696,
      "model": 0.10458365504296775
    }
  },
  "markov_decision_processes": {
    "title": "Markov Decision Processes",
    "tags": [
      "model_algorithm"
    ],
    "aliases": [
      "MDP"
    ],
    "outlinks": [
      "markov_decision_processes"
    ],
    "inlinks": [
      "industries_of_interest",
      "reinforcement_learning",
      "markov_decision_processes"
    ],
    "summary": "Markov Decision Process ([[Markov Decision Processes|MDP]]) is a formal framework for decision-making where outcomes depend solely on the current state (Markov property). \\ architecture [[Markov...",
    "TFIDF_Score": {
      "markov": 0.6309410306692343,
      "decision": 0.3967162989661392,
      "mdp": 0.2994572995722601,
      "process": 0.26245887529308226,
      "state": 0.16797628850639254,
      "probability": 0.16699107568118784,
      "framework": 0.15589759389548527,
      "formal": 0.13487628347363223,
      "solely": 0.1300948894820806,
      "depend": 0.12618820613384688
    }
  },
  "master_data_management": {
    "title": "master data management",
    "tags": [
      "data_storage",
      "data_governance",
      "data_management"
    ],
    "aliases": [
      "mdm"
    ],
    "outlinks": [
      "data_governance",
      "source_of_truth",
      "data_management",
      "data_quality"
    ],
    "inlinks": [
      "data_management",
      "single_source_of_truth"
    ],
    "summary": "Master data management is a method to ==centralize== master data. It's the bridge between the business that maintain the data and know them best and...",
    "TFIDF_Score": {
      "master": 0.5109187200628014,
      "data": 0.4130366500637866,
      "mdm": 0.2606552911007287,
      "consistent": 0.18007590973647183,
      "organization": 0.16743017457096238,
      "across": 0.16024935393360182,
      "management": 0.13818590999000713,
      "ensuring": 0.13765158502147,
      "stewardship": 0.13628404683616005,
      "maintain": 0.12640087222175977
    }
  },
  "master_observability_datadog": {
    "title": "Master Observability Datadog",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "lambdas"
    ],
    "inlinks": [
      "model_observability"
    ],
    "summary": "what happens in prod, pre prod. monitoring web frontend. how is infrastructure working in prod Datadog agents tagging profile how it was working versus other...",
    "TFIDF_Score": {
      "prod": 0.6883730761384619,
      "working": 0.28106508075679776,
      "frontend": 0.229457692046154,
      "datadog": 0.2066965858693621,
      "tagging": 0.2066965858693621,
      "happens": 0.17660804014354667,
      "profile": 0.17660804014354667,
      "lambda": 0.17347830783979137,
      "logging": 0.1706210861042143,
      "versus": 0.1706210861042143
    }
  },
  "mathematical_reasoning_in_transformers": {
    "title": "Mathematical Reasoning in Transformers",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "grok",
      "transformer",
      "gpt-f"
    ],
    "inlinks": [
      "symbolic_computation",
      "reasoning_tokens"
    ],
    "summary": "transformer-based models that address mathematical reasoning either through pretraining, hybrid systems, or fine-tuning on specific mathematical tasks Challenges: General-purpose transformers [[Transformer|Transformer]] are trained primarily on...",
    "TFIDF_Score": {
      "math": 0.47000843432553496,
      "transformer": 0.4636164523999359,
      "mathematical": 0.35057774302949235,
      "reasoning": 0.2103466458176954,
      "grokking": 0.19964156346584938,
      "problem": 0.1419123016195397,
      "specific": 0.12508179266287914,
      "trained": 0.11706250060581,
      "model": 0.11590240705054987,
      "aristo": 0.09982078173292469
    }
  },
  "mathematics": {
    "title": "Mathematics",
    "tags": [
      "portal",
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "directed_acyclic_graph_(dag)",
      "information_theory",
      "big_o_notation",
      "johnson–lindenstrauss_lemma"
    ],
    "inlinks": [],
    "summary": "[[Johnson–Lindenstrauss lemma]] [[Big O Notation]] [[Directed Acyclic Graph (DAG)]] [[information theory]]",
    "TFIDF_Score": {
      "lindenstrauss": 0.34441612864853216,
      "acyclic": 0.3322065009385908,
      "dag": 0.3322065009385908,
      "directed": 0.3322065009385908,
      "johnson": 0.3322065009385908,
      "lemma": 0.3322065009385908,
      "notation": 0.3137959274773324,
      "theory": 0.29427993316197787,
      "big": 0.2591599534530012,
      "graph": 0.2289527087419621
    }
  },
  "maximum_likelihood_estimation": {
    "title": "Maximum Likelihood Estimation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "parametric_vs_non-parametric_models",
      "estimator",
      "model_parameters",
      "linear_regression"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resource: - https://www.youtube.com/watch?v=YevSE6bRhTo Used to infer [[Model Parameters]] from collected data for example in [[Linear Regression]] ($\\beta_0,\\beta_1$). Definition: Likelihood Why is it a good tool...",
    "TFIDF_Score": {
      "mle": 0.5347800485508777,
      "likelihood": 0.3219139983208927,
      "parameter": 0.2680161739775742,
      "estimator": 0.22754585837743996,
      "parametric": 0.218481111735421,
      "value": 0.15401555755852075,
      "true": 0.14637733249595494,
      "asymptotical": 0.1419306041042646,
      "beta_1": 0.1419306041042646,
      "regularity": 0.1419306041042646
    }
  },
  "mean_absolute_error": {
    "title": "mean absolute error",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "mean_squared_error": {
    "title": "Mean Squared Error",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "loss_function",
      "cross_entropy.py"
    ],
    "summary": "==Measures numerical proximity.==",
    "TFIDF_Score": {
      "proximity": 0.7351656024205303,
      "numerical": 0.5355355723454941,
      "measure": 0.4156118234245062
    }
  },
  "melt": {
    "title": "Melt",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "de_tools",
      "grouped_plots",
      "data_visualisation",
      "groupby",
      "database_techniques",
      "normalisation",
      "data_transformation",
      "turning_a_flat_file_into_a_database",
      "multi-level_index"
    ],
    "inlinks": [],
    "summary": "In pandas, the melt function is used to ==transform ([[Data Transformation]]) a DataFrame from a wide format to a long format==. This is especially useful...",
    "TFIDF_Score": {
      "melt": 0.38161707459755484,
      "format": 0.339636511669353,
      "column": 0.31255920604604037,
      "english_score": 0.2862128059481661,
      "math_score": 0.2862128059481661,
      "science_score": 0.2862128059481661,
      "long": 0.25524882720031355,
      "wide": 0.21517106672667405,
      "dataframe": 0.2095426806892908,
      "df_wide": 0.14310640297408306
    }
  },
  "memory_caching": {
    "title": "Memory Caching",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Memory Caching - Use in-memory caches to store frequently accessed data closer to the user, reducing latency.",
    "TFIDF_Score": {
      "memory": 0.4793955313599642,
      "cache": 0.3513816274123924,
      "caching": 0.32718894849039876,
      "closer": 0.315215632889901,
      "accessed": 0.31002396962720385,
      "frequently": 0.31002396962720385,
      "latency": 0.292858990764009,
      "store": 0.22856483703965608,
      "reducing": 0.2260783736087171,
      "user": 0.1829215899397017
    }
  },
  "memory": {
    "title": "Memory",
    "tags": null,
    "aliases": [
      "What is LLM memory",
      "context"
    ],
    "outlinks": [
      "memory",
      "language_models",
      "semantic_relationships"
    ],
    "inlinks": [
      "memory",
      "llm"
    ],
    "summary": "Memory in large [[language models]] (LLMs) involves managing context windows to enhance reasoning capabilities without the high costs associated with traditional training methods. The goal...",
    "TFIDF_Score": {
      "memory": 0.5427056089961944,
      "context": 0.3844628084244614,
      "forgetting": 0.2736032858138098,
      "interaction": 0.26479355207187394,
      "turn": 0.2523698579245535,
      "llm": 0.1752093403514251,
      "multi": 0.12951702293530998,
      "relevant": 0.11687658302674706,
      "evolving": 0.11641663651082972,
      "immediate": 0.10798967657178776
    }
  },
  "merge": {
    "title": "Merge",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_join_vs_merge",
      "data_transformation_with_pandas"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "mermaid": {
    "title": "Mermaid",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings",
      "er_diagrams",
      "code_diagrams"
    ],
    "summary": "Step 1. Ask Chatgpt to make er diagram with mermaidcode. Step 2. Use https://mermaid.js.org/ Use obsidians built in feature: ```mermaid erDiagram SAMPLING_POINT { string notation...",
    "TFIDF_Score": {
      "string": 0.7467042724181557,
      "primary": 0.20184365698247636,
      "key": 0.19580849591261743,
      "int": 0.19457687860198003,
      "foreign": 0.18240387081693224,
      "determinand": 0.1685357829812123,
      "sampling_point": 0.1685357829812123,
      "result": 0.14647230174612122,
      "mermaid": 0.1420384686173305,
      "notation": 0.13832052456653088
    }
  },
  "metadata_handling": {
    "title": "Metadata Handling",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parquet"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "methods_for_handling_outliers": {
    "title": "Methods for Handling Outliers",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "outliers"
    ],
    "summary": "Trimming Description: Removing data points identified as outliers based on criteria such as being beyond a certain number of standard deviations from the mean or...",
    "TFIDF_Score": {
      "var1": 0.4034206124997762,
      "capping": 0.2516316124076838,
      "flooring": 0.2516316124076838,
      "lower_quantile": 0.2516316124076838,
      "upper_quantile": 0.2516316124076838,
      "winsorizing": 0.2516316124076838,
      "description": 0.23300083080498502,
      "quantile": 0.22667095932878595,
      "percentile": 0.21206991328412061,
      "extreme": 0.2017103062498881
    }
  },
  "metric": {
    "title": "Metric",
    "tags": [
      "business"
    ],
    "aliases": [
      "Measure",
      "KPI"
    ],
    "outlinks": [
      "regression_metrics",
      "evaluation_metrics"
    ],
    "inlinks": [
      "semantic_layer",
      "cosine_similarity"
    ],
    "summary": "Metrics in Machine Learning [[Evaluation Metrics]] [[Regression Metrics]] Metrics in business A metric, also called KPI or (calculated) measure, are terms that serve as the...",
    "TFIDF_Score": {
      "metric": 0.6226903195769626,
      "business": 0.23523070058422796,
      "financial": 0.21959768592011267,
      "calculated": 0.19030302862111653,
      "organization": 0.1708816957721883,
      "measure": 0.14506268833853114,
      "kpi": 0.1390935002590525,
      "mapped": 0.1390935002590525,
      "measured": 0.13301430960524946,
      "bus": 0.12829892299792744
    }
  },
  "microsoft_access": {
    "title": "Microsoft Access",
    "tags": [
      "software",
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "database"
    ],
    "inlinks": [],
    "summary": "Tasks [ ] How to update (or more so insert) into multiple related tables. How to insert existing data into a [[Database]]. SQL triggers? [...",
    "TFIDF_Score": {
      "form": 0.4030705620523736,
      "user": 0.3491629157452241,
      "table": 0.24676910271160496,
      "access": 0.2091925470354479,
      "insert": 0.2081807243059361,
      "excel": 0.1914069702981399,
      "make": 0.17024061033782614,
      "interface": 0.16376009848697418,
      "querying": 0.16002291613151987,
      "control": 0.14232826937890006
    }
  },
  "mini-batch_gradient_descent": {
    "title": "Mini-batch gradient descent",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gradient_descent"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "mixture_of_experts": {
    "title": "Mixture of Experts",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "llm"
    ],
    "summary": "Different parts of the network focusing on parts of the questions Routing, distribution activating",
    "TFIDF_Score": {
      "part": 0.5308677438146044,
      "activating": 0.4814827531831257,
      "routing": 0.43372196569428945,
      "focusing": 0.3162575748132289,
      "question": 0.2426788157389444,
      "distribution": 0.22730327641234316,
      "network": 0.22644731197309143,
      "different": 0.19074073407598083
    }
  },
  "ml_engineer": {
    "title": "ML Engineer",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "ML Engineer - Configures and optimizes production ML models. - Monitors the performance and accuracy of ML models in production environments.",
    "TFIDF_Score": {
      "production": 0.5534314726460694,
      "configures": 0.42559717360117133,
      "optimizes": 0.36217833650492837,
      "engineer": 0.3097448970242753,
      "monitor": 0.3097448970242753,
      "model": 0.26230168141373916,
      "environment": 0.23189795790475,
      "accuracy": 0.20640368575485174,
      "performance": 0.15602046200036326
    }
  },
  "mnist": {
    "title": "MNIST",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "datasets"
    ],
    "inlinks": [
      "neural_scaling_laws"
    ],
    "summary": "[[Datasets]]",
    "TFIDF_Score": {
      "datasets": 1.0
    }
  },
  "model_building": {
    "title": "Model Building",
    "tags": "ml_optimisation evaluation",
    "aliases": "Build",
    "outlinks": [
      "model_parameters",
      "preprocessing",
      "machine_learning_algorithms",
      "model_selection",
      "parametric_vs_nonparametric_models",
      "train-dev-test_sets"
    ],
    "inlinks": [
      "gradient_boosting",
      "train-dev-test_sets",
      "feature_importance"
    ],
    "summary": "The Model Building phase follows the [[Preprocessing]] phase, where data is organized and prepared for analysis. This phase focuses on selecting and setting up the...",
    "TFIDF_Score": {
      "phase": 0.47189013290409726,
      "model": 0.4605346605339471,
      "setting": 0.18605770268166633,
      "selection": 0.17299422219209473,
      "nonparametric": 0.15865392617620394,
      "best": 0.15853341805562723,
      "problem": 0.15036911441868858,
      "appropriateness": 0.14291621511121547,
      "data": 0.1393638660117064,
      "tradeoff": 0.13784980377017597
    }
  },
  "model_cascading": {
    "title": "Model Cascading",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "small_language_models"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "model_deployment": {
    "title": "Model Deployment",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": [
      "Deployment"
    ],
    "outlinks": [
      "streamlit.io",
      "scalability",
      "pycaret",
      "preprocessing",
      "api",
      "gradio",
      "fastapi",
      "model_observability",
      "flask",
      "sklearn_pipeline"
    ],
    "inlinks": [
      "testing",
      "pycaret",
      "continuous_delivery_-_deployment"
    ],
    "summary": "Deploying a machine learning model involves moving it from a development environment to a production environment where it can make predictions on new data. Steps...",
    "TFIDF_Score": {
      "aws": 0.3201884760860472,
      "environment": 0.29690727764792285,
      "model": 0.2798618799646576,
      "azure": 0.23896873034883123,
      "deployment": 0.19682727096467795,
      "must": 0.19385905643120846,
      "joblib": 0.19282451397672173,
      "account": 0.17835970467477572,
      "gcp": 0.1625085082387979,
      "deploying": 0.15825474834338152
    }
  },
  "model_ensemble": {
    "title": "Model Ensemble",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "stacking",
      "isolated_forest",
      "interpretability",
      "comparing_ensembles.py",
      "bagging",
      "boosting",
      "ml_tools"
    ],
    "inlinks": [
      "gradient_boosting",
      "stacking",
      "ds_&_ml_portal",
      "model_optimisation",
      "isolated_forest",
      "weak_learners",
      "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy",
      "regularisation_of_tree_based_models",
      "bagging",
      "random_forests",
      "gradient_boosting_regressor",
      "boosting",
      "classification",
      "xgboost",
      "comparing_ensembles.py"
    ],
    "summary": "Ensemble models in machine learning are techniques that ==combine the predictions of multiple individual models== to improve overall performance. Ensemble methods can achieve better accuracy...",
    "TFIDF_Score": {
      "ensemble": 0.5628920422510019,
      "model": 0.38820330638230316,
      "doctor": 0.23600438151883146,
      "opinion": 0.23600438151883146,
      "prediction": 0.17601873453626166,
      "method": 0.1570785880666318,
      "final": 0.13745894623641627,
      "individual": 0.13399188208906546,
      "bagging": 0.1291288582726811,
      "diagnosis": 0.12612224570933991
    }
  },
  "model_evaluation_vs_model_optimisation": {
    "title": "Model Evaluation vs Model Optimisation",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "model_optimisation",
      "model_evaluation"
    ],
    "inlinks": [],
    "summary": "[[Model Evaluation]]focuses on assessing a model's performance, while [[Model Optimisation]]aims to improve that performance through various techniques. Iterative Process: Model evaluation and optimization are often...",
    "TFIDF_Score": {
      "model": 0.42289500708739547,
      "evaluation": 0.34420714564262916,
      "optimization": 0.33155023477503,
      "iterative": 0.28536295571978715,
      "feedback": 0.27908676422494993,
      "gained": 0.1808332753152479,
      "conversely": 0.16332642446128007,
      "evaluated": 0.16332642446128007,
      "refine": 0.16332642446128007,
      "effort": 0.15237397646378406
    }
  },
  "model_evaluation": {
    "title": "Model Evaluation",
    "tags": [
      "evaluation",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "regression_metrics",
      "overfitting",
      "evaluation_metrics",
      "cross_validation",
      "feature_importance"
    ],
    "inlinks": [
      "feature_selection",
      "logistic_regression",
      "ds_&_ml_portal",
      "model_optimisation",
      "test_loss_when_evaluating_models",
      "wrapper_methods",
      "loss_function",
      "cross_validation",
      "neural_network_in_practice",
      "model_selection",
      "linear_regression",
      "model_evaluation_vs_model_optimisation",
      "train-dev-test_sets",
      "imbalanced_datasets"
    ],
    "summary": "Assess the model's performance using various metrics to ensure it meets the desired accuracy and reliability. Appropriate evaluation metrics are used based on the problem...",
    "TFIDF_Score": {
      "metric": 0.39842426569098244,
      "performance": 0.25168599334128183,
      "assess": 0.2417947717730611,
      "bias": 0.22148334034485714,
      "squared": 0.21225602056392473,
      "model": 0.21156731109266833,
      "evaluation": 0.20090126914649978,
      "accuracy": 0.16648110130064442,
      "regression": 0.1641883291606578,
      "generalization": 0.1595791014427898
    }
  },
  "model_interpretability": {
    "title": "Model Interpretability",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "confidence_interval",
      "shapley_additive_explanations",
      "interpretability",
      "local_interpretable_model-agnostic_explanations",
      "p_values"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Model [[interpretability]] tools are crucial in ensuring that machine learning models are transparent, explainable, and understandable to stakeholders, particularly in industries where decisions need to...",
    "TFIDF_Score": {
      "model": 0.36477371036572404,
      "explanation": 0.2737886070984245,
      "surrogate": 0.26635066497352666,
      "global": 0.21042133406797575,
      "decision": 0.19585661636377455,
      "behavior": 0.17705821867344096,
      "prediction": 0.1653953117736726,
      "churn": 0.14784039272523575,
      "counterfactual": 0.14784039272523575,
      "customer": 0.1416045333027883
    }
  },
  "model_observability": {
    "title": "Model Observability",
    "tags": [
      "deleted",
      "#model_explainability"
    ],
    "aliases": [
      "Observability"
    ],
    "outlinks": [
      "data_drift",
      "isolated_forest",
      "accuracy",
      "data_observability",
      "interpretability",
      "f1_score",
      "performance_drift",
      "roc_(receiver_operating_characteristic)",
      "precision",
      "recall",
      "validation",
      "model_validation",
      "data_lineage",
      "master_observability_datadog"
    ],
    "inlinks": [
      "model_validation",
      "model_deployment",
      "machine_learning_operations",
      "business_observability"
    ],
    "summary": "Monitor the model's performance over time (in production). Similar to [[Model Validation]]. In the context of machine learning (ML), Observability refers to the ability to...",
    "TFIDF_Score": {
      "observability": 0.4125617352241516,
      "model": 0.37021708488354393,
      "drift": 0.31845916566672494,
      "monitoring": 0.2221090831260749,
      "performance": 0.2001908652823615,
      "pipeline": 0.16996485086158947,
      "time": 0.1566413762348292,
      "data": 0.1400407196092887,
      "prediction": 0.1297126588908378,
      "monitor": 0.1192307050442036
    }
  },
  "model_optimisation": {
    "title": "Model Optimisation",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "Optimisation"
    ],
    "outlinks": [
      "hyperparameter",
      "model_parameters",
      "model_ensemble",
      "cross_validation",
      "feature_engineering",
      "model_evaluation"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "loss_versus_cost_function",
      "momentum",
      "cross_validation",
      "model_evaluation_vs_model_optimisation",
      "cost_function"
    ],
    "summary": "Model optimization is a step in the machine learning workflow aimed at enhancing a model's performance by fine-tuning its parameters and hyperparameters. The goal is...",
    "TFIDF_Score": {
      "model": 0.44449701091256616,
      "performance": 0.2266222464363832,
      "squared": 0.19111884440314636,
      "ability": 0.18745787358966398,
      "tuning": 0.18516675204893998,
      "metric": 0.15374906688911263,
      "accuracy": 0.14990234722675316,
      "improve": 0.1463470202747117,
      "hyperparameters": 0.14368767198032556,
      "error": 0.14304203120039122
    }
  },
  "model_parameters_tuning": {
    "title": "Model Parameters Tuning",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "pasted_image_20241231142918.png",
      "cost_function",
      "optimisation_techniques"
    ],
    "inlinks": [
      "regularisation",
      "model_parameters"
    ],
    "summary": "To find optimal [[Model Parameters]]. Finding Optimal Model Parameters Parameter Space Exploration: It's useful to visualize slices of the parameter space by selecting two parameters...",
    "TFIDF_Score": {
      "parameter": 0.48595833056516696,
      "model": 0.35313131807914644,
      "function": 0.2324881558427909,
      "optimal": 0.16574682239773908,
      "data": 0.16440338026060897,
      "error": 0.16317532226377138,
      "find": 0.14984127351584237,
      "axis": 0.14149918472676326,
      "degree": 0.13319197164384508,
      "visualize": 0.12548667323358972
    }
  },
  "model_parameters_vs_hyperparameters": {
    "title": "Model parameters vs hyperparameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "loss_function",
      "neural_network",
      "learning_rate",
      "gradient_descent",
      "hyperparameter"
    ],
    "inlinks": [
      "loss_versus_cost_function",
      "hyperparameter"
    ],
    "summary": "Model parameters and hyperparameters serve different roles: [[Model Parameters]] - These are the internal variables of the model that are learned from the training data....",
    "TFIDF_Score": {
      "model": 0.3936878213588269,
      "parameter": 0.284565944938241,
      "learned": 0.2174596262169545,
      "hyperparameters": 0.19796488867442452,
      "configuration": 0.19604762326213657,
      "search": 0.18756323880611034,
      "training": 0.1871920603901169,
      "random": 0.1652236437596624,
      "process": 0.15849135518372814,
      "neural": 0.15397816290163455
    }
  },
  "model_parameters": {
    "title": "Model Parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "deep_learning",
      "support_vector_machines",
      "model_parameters_tuning",
      "decision_tree",
      "k-means",
      "linear_regression",
      "clustering",
      "optimisation_techniques"
    ],
    "inlinks": [
      "optimising_a_logistic_regression_model",
      "lbfgs",
      "model_parameters_tuning",
      "model_validation",
      "cost_function",
      "train-dev-test_sets",
      "hyperparameter",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "forecasting_autoarima.py",
      "gradient_descent",
      "maximum_likelihood_estimation",
      "cost-sensitive_analysis",
      "logistic_regression",
      "model_optimisation",
      "loss_function",
      "neural_network",
      "model_building",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Model parameters are also called weights and biases. These parameters are adjusted during the training process to optimize the model's performance on the given task....",
    "TFIDF_Score": {
      "parameter": 0.27023851324066805,
      "bias": 0.2609261084932774,
      "weight": 0.2589754791526936,
      "coefficient": 0.2570848787992853,
      "intercept": 0.23490204134729492,
      "tree": 0.23279699728792183,
      "neuron": 0.22943262546213422,
      "regression": 0.19342773917532036,
      "decision": 0.18958650948132824,
      "model": 0.16616279154252386
    }
  },
  "model_preparation": {
    "title": "Model preparation",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "python model = model.fit(X_train, y_train) print(model) y_pred = model.predict(X_test) print(accuracy_score(y_expect, y_pred))",
    "TFIDF_Score": {
      "y_pred": 0.5133194155294402,
      "print": 0.3888997747086629,
      "model": 0.37974822258893604,
      "y_expect": 0.32705761170243486,
      "accuracy_score": 0.2841708912053712,
      "x_test": 0.24726737438311536,
      "x_train": 0.22689243900515924,
      "y_train": 0.22421715611389229,
      "predict": 0.1930931341443118,
      "fit": 0.1892440039385689
    }
  },
  "model_selection": {
    "title": "Model Selection",
    "tags": [
      "ml_process",
      "deleted",
      "evaluation"
    ],
    "aliases": [
      "Selection"
    ],
    "outlinks": [
      "gridseachcv",
      "evaluation_metrics",
      "random_search",
      "cross_validation",
      "machine_learning_operations",
      "model_interpretability",
      "model_evaluation"
    ],
    "inlinks": [
      "handling_different_distributions",
      "regularisation",
      "ds_&_ml_portal",
      "pycaret",
      "parsimonious",
      "interview_notepad",
      "model_building",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Model selection is an integral part of building a [[Machine Learning Operations]] to ensure that the best performing model is chosen for a given task,...",
    "TFIDF_Score": {
      "model": 0.5261610333213766,
      "validation": 0.29774051425855896,
      "evaluation": 0.1903371174538359,
      "best": 0.17249951747783077,
      "unseen": 0.17181021330855445,
      "candidate": 0.14999376741139384,
      "set": 0.14725972164516166,
      "evaluate": 0.14525203362616027,
      "cross": 0.14356731825601163,
      "selection": 0.14117569131757776
    }
  },
  "model_validation": {
    "title": "Model Validation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_observability",
      "model_parameters",
      "performance_drift"
    ],
    "inlinks": [
      "cross_validation",
      "model_observability"
    ],
    "summary": "Model Validation refers to the process of evaluating a machine learning model's performance on a separate dataset (often called the validation set) to ensure it...",
    "TFIDF_Score": {
      "model": 0.5379867713679132,
      "validation": 0.38054048054511386,
      "observability": 0.282631123526932,
      "performance": 0.18285802856903852,
      "monitoring": 0.16906544416343902,
      "ensure": 0.12405777326704125,
      "well": 0.11969776888634942,
      "step": 0.11578809353805845,
      "ongoing": 0.11502371547582502,
      "generalizes": 0.11156961181587093
    }
  },
  "momentum": {
    "title": "Momentum",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "model_optimisation",
      "learning_rate",
      "gradient_descent",
      "cost_function",
      "momentum.py",
      "ml_tools"
    ],
    "inlinks": [
      "adaptive_learning_rates",
      "adam_optimizer"
    ],
    "summary": "Momentum is an [[Model Optimisation|Optimisation]] technique used to accelerate the [[Gradient Descent]] algorithm by incorporating the concept of inertia. It helps in reducing oscillations and...",
    "TFIDF_Score": {
      "momentum": 0.5244023185599183,
      "gradient": 0.3047204944140831,
      "theta": 0.2991654608865511,
      "beta": 0.2839798861942855,
      "velocity": 0.25581043898986894,
      "update": 0.21663437218279086,
      "alpha": 0.14775582351906463,
      "inertia": 0.14198994309714275,
      "v_t": 0.14198994309714275,
      "previous": 0.13989720739184466
    }
  },
  "momentum.py": {
    "title": "Momentum.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "momentum"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Momentum.py",
    "TFIDF_Score": {
      "momentum": 0.49041661434359085,
      "optimisation": 0.3815605454363969,
      "rhyslwells": 0.2998511783436051,
      "blob": 0.29599276070883507,
      "github": 0.28536359680540074,
      "exploration": 0.28317081639754654,
      "ml_tools": 0.2729785592967627,
      "com": 0.26739959760204707,
      "main": 0.2548629847208914,
      "http": 0.2525620275114223
    }
  },
  "mongodb": {
    "title": "MongoDB",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "monolith_architecture": {
    "title": "Monolith Architecture",
    "tags": [
      "software_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "microservices",
      "software_architecture"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "A monolith, in the context of [[software architecture]], refers to a ==single, unified application where all components and functionalities are interconnected and interdependent==. In a...",
    "TFIDF_Score": {
      "application": 0.42530538561664283,
      "part": 0.3524559032120449,
      "monolithic": 0.23975101232574464,
      "monolith": 0.22583934384844795,
      "architecture": 0.2202849395075281,
      "single": 0.18864016161374636,
      "call": 0.17061889152138157,
      "function": 0.1654540474849185,
      "component": 0.1532586536235343,
      "codebase": 0.15055956256563197
    }
  },
  "monte_carlo_simulation": {
    "title": "Monte Carlo Simulation",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resources: - https://www.youtube.com/watch?v=r7cn3WS5x9c Algorithms that use repeated random sampling. Monte Carlo: random How does the randomness in data generation impact the randomness of the paramaeter...",
    "TFIDF_Score": {
      "randomness": 0.33106333897570855,
      "study": 0.33106333897570855,
      "random": 0.3133222584727552,
      "simulation": 0.3054348939493294,
      "paramaeter": 0.1905135049342138,
      "r7cn3ws5x9c": 0.1905135049342138,
      "approximate": 0.16553166948785428,
      "coverage": 0.16553166948785428,
      "repeated": 0.16553166948785428,
      "carlo": 0.16056083766372944
    }
  },
  "multi-agent_reinforcement_learning": {
    "title": "Multi-Agent Reinforcement Learning",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "multi-head_attention": {
    "title": "Multi-head attention",
    "tags": [
      "deleted",
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "attention_mechanism"
    ],
    "inlinks": [
      "attention_mechanism",
      "transformer"
    ],
    "summary": "Summary ==Aggregates different perspectives== This approach allows the model to attend to different parts of the input sequence simultaneously, capturing various aspects of the context...",
    "TFIDF_Score": {
      "head": 0.7263183596709935,
      "attention": 0.38303364553414204,
      "different": 0.20107412192501034,
      "capture": 0.1801180263629138,
      "multi": 0.15088529126206598,
      "context": 0.13325745565776362,
      "input": 0.11434012975439631,
      "focus": 0.10758123698020716,
      "word": 0.10175045958487673,
      "model": 0.09375842833017671
    }
  },
  "multi-level_index": {
    "title": "Multi-level index",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "interoperable",
      "groupby",
      "json",
      "pandas_stack"
    ],
    "inlinks": [
      "melt",
      "pd.grouper",
      "data_transformation_with_pandas",
      "pandas_stack",
      "structuring_and_organizing_data"
    ],
    "summary": "Multi-level indexing in pandas—also called hierarchical indexing—enables you to work with higher-dimensional data in a 2D DataFrame. It's particularly useful for working with grouped or...",
    "TFIDF_Score": {
      "multiindex": 0.3025013929230106,
      "store": 0.28629089501434174,
      "stack": 0.273215594942587,
      "product": 0.20145122729116022,
      "level": 0.17547820458591903,
      "feb": 0.16397634754728305,
      "jan": 0.16397634754728305,
      "dataframe": 0.1638588710929559,
      "slicing": 0.15680963251316823,
      "multi": 0.15524468796461996
    }
  },
  "multicollinearity": {
    "title": "Multicollinearity",
    "tags": [
      "code_snippet",
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "correlation",
      "impact_of_multicollinearity_on_model_parameters",
      "heatmap",
      "addressing_multicollinearity",
      "clustering"
    ],
    "inlinks": [
      "ridge",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "correlation",
      "dummy_variable_trap",
      "heatmap",
      "addressing_multicollinearity",
      "statistics",
      "regression"
    ],
    "summary": "When two or more regressors are in [[Correlation]] Multicollinearity refers to the ==instability== of a model due to ==highly correlated independent variables.== It occurs when...",
    "TFIDF_Score": {
      "multicollinearity": 0.5330622436597192,
      "variable": 0.35945881102406163,
      "coefficient": 0.241311937873974,
      "independent": 0.20409791368160715,
      "correlated": 0.19669002793107834,
      "vif": 0.18979946239886653,
      "estimated": 0.18150412785222628,
      "effect": 0.17004075869298474,
      "model": 0.1364721730808529,
      "lead": 0.13422535601122237
    }
  },
  "multinomial_naive_bayes": {
    "title": "Multinomial Naive bayes",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "naive_bayes"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "mysql": {
    "title": "MySql",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "database_management_system_(dbms)",
      "google_cloud_platform"
    ],
    "summary": "MySQL has more ==granularity== with types than SQLite. For example, an integer could be TINYINT, SMALLINT, MEDIUMINT, INT or BIGINT based on the size of...",
    "TFIDF_Score": {
      "integer": 0.32750039070639403,
      "tag": 0.2991574852628085,
      "store": 0.26777487457390026,
      "size": 0.25937652215829404,
      "bigint": 0.244228083908197,
      "mediumint": 0.244228083908197,
      "relational_database": 0.244228083908197,
      "smallint": 0.244228083908197,
      "tinyint": 0.23005663118640424,
      "number": 0.2129309050035534
    }
  },
  "naive_bayes": {
    "title": "Naive Bayes",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "multinomial_naive_bayes",
      "pasted_image_20240118111554.png",
      "naive_bayes",
      "pasted_image_20240116184108.png",
      "encoding_categorical_variables",
      "gaussian_naive_bayes",
      "bag_of_words"
    ],
    "inlinks": [
      "naive_bayes",
      "machine_learning_algorithms",
      "feature_scaling",
      "classification",
      "supervised_learning"
    ],
    "summary": "Can values for X,y be categroical ? [[Encoding Categorical Variables]] BernoulliNB() Why Naive Bayes?;;Order doesn't matter, features are independent. Treated it as a [[Bag of...",
    "TFIDF_Score": {
      "accident": 0.48312366638151294,
      "raining": 0.48312366638151294,
      "night": 0.35254970249461753,
      "urban": 0.2822919620415806,
      "frac": 0.27947471631492615,
      "summer": 0.22197573860772213,
      "bayes": 0.17146325947011504,
      "naive": 0.14653707294019047,
      "autumn": 0.14363136027558493,
      "rural": 0.14363136027558493
    }
  },
  "named_entity_recognition": {
    "title": "Named Entity Recognition",
    "tags": [
      "NLP",
      "model_algorithm"
    ],
    "aliases": [
      "NER",
      "Entity Recognition"
    ],
    "outlinks": [
      "in_ner_how_would_you_handle_ambiguous_entities",
      "what_are_the_challenges_of_ner_in_multilingual_contexts",
      "structured_data",
      "backpropagation",
      "lstm",
      "why_is_named_entity_recognition_(ner)_a_challenging_task",
      "how_does_the_choice_of_training_data_affect_the_performance_of_ner_models",
      "nlp"
    ],
    "inlinks": [
      "bert",
      "graphrag"
    ],
    "summary": "Named Entity Recognition (NER) is a subtask of [[NLP|Natural Language Processing]] (NLP) that involves identifying and classifying key entities in text into predefined categories such...",
    "TFIDF_Score": {
      "ner": 0.6637204661788494,
      "entity": 0.27455887754663816,
      "inc": 0.17500837709203707,
      "recognition": 0.17242899526063366,
      "nlp": 0.1620507728345804,
      "apple": 0.14028838825837747,
      "extraction": 0.1346997183285122,
      "named": 0.12292839384154766,
      "text": 0.11950475700255671,
      "sentence": 0.10982355101865526
    }
  },
  "nbconvert": {
    "title": "nbconvert",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "ipynb"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "neo4j": {
    "title": "neo4j",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "graphrag",
      "text2cypher"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "network_design": {
    "title": "Network Design",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Mixed-Integer Programming: Handles problems where some variables must be integers, commonly used in optimizing network design and capacity planning. How to systems interact.",
    "TFIDF_Score": {
      "integer": 0.5009653455774957,
      "mixed": 0.324598712202615,
      "capacity": 0.30660975527506595,
      "planning": 0.27351380130184133,
      "optimizing": 0.2561156064211494,
      "interact": 0.23855265510419643,
      "commonly": 0.22880512090920613,
      "must": 0.22535467076636578,
      "programming": 0.2221134710716863,
      "design": 0.21081616398165712
    }
  },
  "neural_network_classification": {
    "title": "Neural Network Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "deep_learning",
      "data_quality",
      "evaluation_metrics",
      "neural_network",
      "choosing_the_number_of_clusters",
      "classification",
      "clustering",
      "choosing_a_threshold",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "Choosing Thresholds/Clusters in [[Neural network]] [[Classification]] When working with [[Deep Learning|neural networks]], the output is often a probability distribution across different classes. To make a...",
    "TFIDF_Score": {
      "threshold": 0.4926080877797446,
      "class": 0.42224110087906114,
      "cluster": 0.2921289066554944,
      "probability": 0.28153670571072864,
      "classification": 0.23834257488179808,
      "choosing": 0.17305782826867772,
      "clustering": 0.14801765012399204,
      "belonging": 0.14183038109292131,
      "classified": 0.1381178838607262,
      "based": 0.11956802144602705
    }
  },
  "neural_network_in_practice": {
    "title": "Neural network in Practice",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "model_evaluation",
      "neural_network",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "ml_tools"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "This guide provides practical insights into building and using [[Neural network]]. Refer to [[ML_Tools]] for more details: Neural_Net_Build.py Softmax Placement at the End Numerical stability...",
    "TFIDF_Score": {
      "loss": 0.47871396494057167,
      "logits": 0.2859210486269726,
      "history": 0.2821285705182009,
      "model": 0.22027158677960018,
      "softmax": 0.19779876054394133,
      "progress": 0.17211185835564033,
      "output": 0.1659276762537683,
      "function": 0.15710325518461446,
      "from_logits": 0.1517668703898562,
      "compilation": 0.1429605243134863
    }
  },
  "neural_network": {
    "title": "Neural network",
    "tags": [
      "#deep_learning",
      "drafting"
    ],
    "aliases": [
      "Neural Network"
    ],
    "outlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "model_parameters",
      "deep_learning",
      "neural_network",
      "types_of_neural_networks",
      "neural_network_in_practice",
      "normalisation",
      "activation_function",
      "optimisation_techniques",
      "hyperparameter"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "batch_normalisation",
      "neural_network_classification",
      "energy",
      "activation_function",
      "deep_learning",
      "use_cases_for_a_simple_neural_network_like",
      "classification",
      "feed_forward_neural_network",
      "dropout",
      "hyperparameter",
      "ai_engineer",
      "ridge",
      "pytorch",
      "typical_output_formats_in_neural_networks",
      "deep_q-learning",
      "neural_network_in_practice",
      "optimising_neural_networks",
      "ds_&_ml_portal",
      "regularisation",
      "over_parameterised_models",
      "neural_network",
      "word2vec",
      "types_of_neural_networks",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "A [[Neural network|Neural Network]] is a computational model inspired by biological neural networks in the human brain. It consists of layers of interconnected nodes (neurons)...",
    "TFIDF_Score": {
      "layer": 0.43765181473033005,
      "network": 0.43314803056561274,
      "neural": 0.3467013311150889,
      "neuron": 0.27199269290332495,
      "weight": 0.23391678976626637,
      "input": 0.17471150530417703,
      "receives": 0.12255510929152046,
      "bias": 0.11783933726977557,
      "node": 0.11447189454960567,
      "hidden": 0.10994111053472562
    }
  },
  "neural_scaling_laws": {
    "title": "Neural Scaling Laws",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "entropy_of_natural_language",
      "loss_function",
      "pasted_image_20241017072233.png",
      "mnist",
      "pasted_image_20241017072732.png",
      "resolution_limited_scaling",
      "cross_entropy",
      "validation_loss",
      "llm",
      "pasted_image_20241017074030.png",
      "pasted_image_20241017073743.png",
      "cross_entropy_loss",
      "compute_efficent_frontier",
      "intrinsic_dimension_of_natural_language"
    ],
    "inlinks": [],
    "summary": "Even scaled model cannot cross tthe [[compute efficent frontier]] ![[Pasted image 20241017072233.png|500]] [[validation loss]] Neural scaling laws. That is error rates scale with compute,model size...",
    "TFIDF_Score": {
      "manifold": 0.34053146012290114,
      "loss": 0.30537862818224437,
      "cross": 0.2300430834306025,
      "law": 0.2242563084641062,
      "entropy": 0.22255581587407036,
      "image": 0.21987095735053527,
      "500": 0.21009281322775666,
      "scaling": 0.20830004694353102,
      "llm": 0.2002293893427907,
      "pasted": 0.16805113767360969
    }
  },
  "ngrams": {
    "title": "Ngrams",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "text_classification",
      "nlp"
    ],
    "inlinks": [
      "nlp",
      "explain_the_curse_of_dimensionality",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "N-grams are used in NLP that allow for the analysis of text data by breaking it down into smaller, manageable sequences. An N-gram is a...",
    "TFIDF_Score": {
      "gram": 0.7368800893160297,
      "love": 0.3635663010280627,
      "word": 0.20042838572909566,
      "bigram": 0.17123509526213626,
      "text": 0.1655078961766031,
      "token": 0.12936604245002395,
      "item": 0.12188207814700537,
      "sentence": 0.1140749456485188,
      "nlp": 0.11221596176740573,
      "sequence": 0.1104810053211678
    }
  },
  "nlp": {
    "title": "NLP",
    "tags": [
      "NLP"
    ],
    "aliases": [
      "Natural Language Processing"
    ],
    "outlinks": [
      "summarisation",
      "ngrams",
      "preprocessing",
      "part_of_speech_tagging",
      "tf-idf",
      "one_hot_encoding",
      "grammar_method",
      "nltk",
      "standardised/vector_embedding",
      "normalisation_of_text",
      "bag_of_words",
      "lemmatization"
    ],
    "inlinks": [
      "ngrams",
      "rag",
      "recurrent_neural_networks",
      "attention_mechanism",
      "transformer",
      "generative_ai_from_theory_to_practice",
      "bert",
      "sentence_similarity",
      "explain_the_curse_of_dimensionality",
      "lstm",
      "normalisation",
      "named_entity_recognition",
      "tokenisation"
    ],
    "summary": "Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It encompasses various techniques and models to process and analyze large...",
    "TFIDF_Score": {
      "word": 0.48372144469366796,
      "text": 0.3709113708839945,
      "nltk": 0.26410435414920624,
      "import": 0.18870266703129654,
      "bag": 0.15431475639733472,
      "document": 0.15122611299441724,
      "language": 0.14522156511608644,
      "frequency": 0.13566368876084414,
      "porterstemmer": 0.11807578675067622,
      "wordnetlemmatizer": 0.11807578675067622
    }
  },
  "nltk": {
    "title": "nltk",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf",
      "nlp"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "node.js": {
    "title": "Node.JS",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cryptography"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "non-parametric_tests": {
    "title": "Non-parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "normalisation_of_data": {
    "title": "Normalisation of data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "json",
      "postgresql",
      "api"
    ],
    "inlinks": [
      "normalised_schema",
      "data_transformation",
      "normalisation"
    ],
    "summary": "Normalization is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from...",
    "TFIDF_Score": {
      "destination": 0.4763444659185664,
      "normalization": 0.30306653032875186,
      "source": 0.2878302498026208,
      "relational": 0.2847097915418007,
      "json": 0.2682131700274889,
      "appropriate": 0.24798755082019713,
      "format": 0.21361701087816445,
      "nest": 0.21001838429577097,
      "typed": 0.18918556452229918,
      "structuring": 0.17236608493784772
    }
  },
  "normalisation_of_text": {
    "title": "Normalisation of Text",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "stemming",
      "tokenisation",
      "lemmatization"
    ],
    "inlinks": [
      "normalisation",
      "nlp"
    ],
    "summary": "[[Preprocessing]] in NLP tasks is called Normalization involves reducing words to their base or root form, converting them to lowercase, and removing stop words. Processes...",
    "TFIDF_Score": {
      "temp": 0.6291922547169911,
      "word": 0.34931054764700453,
      "token": 0.28987931933667493,
      "stem": 0.2557987754285905,
      "nltk": 0.190718103561112,
      "porter_stemmer": 0.17053251695239363,
      "wordnetlemmatizer": 0.12789938771429524,
      "text": 0.1236216124414389,
      "stemming": 0.12230944453958334,
      "stopwords": 0.11443086213666719
    }
  },
  "normalisation_vs_standardisation": {
    "title": "Normalisation vs Standardisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241219071120.png",
      "normalisation",
      "standardisation"
    ],
    "inlinks": [
      "batch_normalisation",
      "normalisation"
    ],
    "summary": "Key Differences: [[Normalisation]] changes the range of the data, while standardisation changes the data distribution. Normalisation is preferred when the data does not follow a...",
    "TFIDF_Score": {
      "standardisation": 0.44717635210654655,
      "normalisation": 0.41114599361228044,
      "20241219071120": 0.2849150554317699,
      "distribution": 0.26901119581416966,
      "change": 0.2576139581452617,
      "data": 0.25027343833780646,
      "whereas": 0.22839063238697924,
      "normally": 0.21185829845892856,
      "gaussian": 0.20012842086458388,
      "preferred": 0.20012842086458388
    }
  },
  "normalisation": {
    "title": "Normalisation",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "z-normalisation",
      "normalised_schema",
      "batch_normalisation",
      "data_engineering",
      "normalisation_of_data",
      "standardisation",
      "normalisation_vs_standardisation",
      "how_to_normalise_a_merged_table",
      "nlp",
      "normalisation_of_text"
    ],
    "inlinks": [
      "anomaly_detection",
      "melt",
      "neural_network",
      "normalisation_vs_standardisation",
      "feature_scaling"
    ],
    "summary": "Standardizing data distributions for consistency. In ML: - [[Z-Normalisation]] - [[Standardisation]] - [[Normalisation vs Standardisation]] - [[Batch Normalisation]] In [[Data Engineering]]: - [[Normalisation of data]]...",
    "TFIDF_Score": {
      "normalisation": 0.5095282009739545,
      "category": 0.33829885832906864,
      "groupby": 0.3150640775363452,
      "transform": 0.257515159070592,
      "print": 0.2519139646911255,
      "mean": 0.24319485941965863,
      "standardisation": 0.2216720734210153,
      "value": 0.20435008898538165,
      "ntransformed": 0.141236697309467,
      "subtracted": 0.141236697309467
    }
  },
  "normalised_schema": {
    "title": "Normalised Schema",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "normalisation_of_data",
      "how_to_normalise_a_merged_table"
    ],
    "inlinks": [
      "snowflake_schema",
      "why_use_er_diagrams",
      "interview_notepad",
      "normalisation",
      "types_of_database_schema",
      "data_transformation",
      "how_to_normalise_a_merged_table"
    ],
    "summary": "In a normalized schema, data is organized into multiple related tables to minimize redundancy and dependency, and improve data integrity. This approach is often used...",
    "TFIDF_Score": {
      "attribute": 0.2874826011237935,
      "database": 0.2669972782694252,
      "denormalization": 0.25996984983387433,
      "dependency": 0.20909171245909927,
      "integrity": 0.20756527888800105,
      "table": 0.20152337988250188,
      "1nf": 0.1733132332225829,
      "2nf": 0.1733132332225829,
      "prime": 0.1733132332225829,
      "normal": 0.17248956067427607
    }
  },
  "nosql": {
    "title": "NoSQL",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "sql_vs_nosql",
      "data_storage",
      "google_cloud_platform"
    ],
    "summary": "(Not Only SQL):** ==Non-relational== database management systems offering flexibility and scalability for unstructured or document-based data. NoSQL Databases: Accommodate unstructured data and can be represented...",
    "TFIDF_Score": {
      "unstructured": 0.4146359457313382,
      "document": 0.35608090016398813,
      "database": 0.2598248260655588,
      "accommodate": 0.24874676549055996,
      "nosql": 0.24874676549055996,
      "theory": 0.22717023851657428,
      "offering": 0.22314447596714995,
      "based": 0.2097023103344785,
      "relational": 0.20005926944212465,
      "data": 0.19444832600796735
    }
  },
  "notebooklm": {
    "title": "NotebookLM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "notebooklm",
      "data_archive"
    ],
    "inlinks": [
      "notebooklm"
    ],
    "summary": "https://www.youtube.com/watch?v=EOmgC3-hznM key topics chat interface takes into account resources. save to note- to dave. how to select and folders - from obsidian [[Data Archive]] for...",
    "TFIDF_Score": {
      "note": 0.4804942907471489,
      "folder": 0.2656582062786304,
      "project": 0.1984982289395973,
      "dave": 0.16926213162038195,
      "eomgc3": 0.16926213162038195,
      "faq": 0.16926213162038195,
      "getter": 0.16926213162038195,
      "hznm": 0.16926213162038195,
      "muiltple": 0.16926213162038195,
      "notebooklm": 0.16926213162038195
    }
  },
  "npy_files_a_numpy_array_storage": {
    "title": "npy Files A NumPy Array storage",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A .npy file is a binary file format specifically designed to store a single NumPy array. NumPy, or Numerical Python, is a powerful library in...",
    "TFIDF_Score": {
      "npy": 0.7178200275962611,
      "numpy": 0.3344326785685302,
      "file": 0.2934305983403889,
      "array": 0.26842237535868774,
      "my_array": 0.1957690984353439,
      "format": 0.16593630509340054,
      "load": 0.11047311073834302,
      "write": 0.08489585653123312,
      "save": 0.08410095039579135,
      "storing": 0.08410095039579135
    }
  },
  "olap_(online_analytical_processing)": {
    "title": "OLAP (online analytical processing)",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [
      "OLAP"
    ],
    "outlinks": [
      "excel_pivot_table"
    ],
    "inlinks": [
      "dimensions"
    ],
    "summary": "OLAP, or Online Analytical Processing, is a category of database technology. OLAP systems allow organizations to gain insights by examining data across various dimensions, such...",
    "TFIDF_Score": {
      "olap": 0.5318746949252631,
      "sale": 0.2686789286154585,
      "dimension": 0.233621573569277,
      "dicing": 0.19364569637717788,
      "slicing": 0.17443696897904304,
      "data": 0.17010113480381256,
      "monthly": 0.1632005837645591,
      "cube": 0.15892870836769613,
      "pivot": 0.15196419855007517,
      "level": 0.14640308907975552
    }
  },
  "oltp_(online_transactional_processing)": {
    "title": "oltp (online transactional processing)",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "oltp": {
    "title": "OLTP",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "row-based_storage",
      "database"
    ],
    "summary": "In online transaction processing (OLTP), information systems typically facilitate and manage transaction-oriented applications. It's the opposite of OLAP (Online Analytical Processing).",
    "TFIDF_Score": {
      "online": 0.4777052734735701,
      "transaction": 0.44391067804753526,
      "processing": 0.28861737298549517,
      "oltp": 0.2789929506048221,
      "opposite": 0.26536415579452394,
      "olap": 0.2597842431798304,
      "oriented": 0.24236346498437997,
      "facilitate": 0.2355841338340658,
      "analytical": 0.21744006675526892,
      "manage": 0.17668846051380016
    }
  },
  "one_pager_template": {
    "title": "One Pager Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Proposal: [Project name] About this doc Metadata about this document. Describe the scope and current status. This doc is a proposal for [feature or change]....",
    "TFIDF_Score": {
      "name": 0.35051640742866397,
      "option": 0.328575650107354,
      "sign": 0.24780739365913243,
      "risk": 0.24398262118320704,
      "doc": 0.21527231582652592,
      "con": 0.18464956818045036,
      "pro": 0.18464956818045036,
      "proposal": 0.18464956818045036,
      "proposed": 0.17657931380464745,
      "happen": 0.16088058874032762
    }
  },
  "one-hot_encoding": {
    "title": "One-hot encoding",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "one_hot_encoding.py",
      "dummy_variable_trap",
      "why_does_label_encoding_give_different_predictions_from_one-hot_encoding",
      "label_encoding",
      "ml_tools"
    ],
    "inlinks": [
      "dummy_variable_trap",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ],
    "summary": "Related terms: - Why do we need to drop one of the dummy columns? [[Dummy variable trap]]: ==Dummy variables & One-hot encoding are fundamentally different...",
    "TFIDF_Score": {
      "hot": 0.46743757361803684,
      "encoding": 0.4314367676518359,
      "one": 0.31470098122673223,
      "cat_variables": 0.21894533576111438,
      "category": 0.21851307809118653,
      "categorical": 0.18477061637474246,
      "dummy": 0.1845225960464591,
      "column": 0.18392280389000837,
      "numerical": 0.13857796228105684,
      "green": 0.1314846700504617
    }
  },
  "one_hot_encoding.py": {
    "title": "One_hot_encoding.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Explorations\\Preprocess\\One_hot_encoding\\One_hot_encoding.py This script demonstrates how to preprocess categorical variables and apply linear regression for house price prediction. Key steps include: Data Loading: It loads a...",
    "TFIDF_Score": {
      "dummy": 0.5026424064049123,
      "variable": 0.3546641749699737,
      "price": 0.3491060629695669,
      "house": 0.23904417217891918,
      "one_hot_encoding": 0.18726782506642345,
      "town": 0.17908313771739878,
      "trap": 0.17273460099457616,
      "evaluates": 0.15301424472979017,
      "preprocess": 0.1414785758140288,
      "feature": 0.13500238291206063
    }
  },
  "optimisation_function": {
    "title": "Optimisation function",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "loss_function",
      "gradient_descent",
      "optimisation_techniques"
    ],
    "inlinks": [
      "logistic_regression_in_sklearn_&_gradient_descent",
      "optimisation_function",
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "model_parameters_tuning",
      "gradient_descent",
      "lbfgs"
    ],
    "summary": "Optimization functions adjust the [[Model Parameters]] to minimize the [[Loss function]], which measures how well the model performs. This is a fundamental step in training...",
    "TFIDF_Score": {
      "function": 0.4268142057231858,
      "parameter": 0.3244175728168214,
      "gradient": 0.276518862292713,
      "loss": 0.27094961696078645,
      "optimisation": 0.21940281978794685,
      "model": 0.19947611788659272,
      "optimization": 0.18245445806579969,
      "negligible": 0.17179852024819753,
      "described": 0.15475692828623178,
      "lbfgs": 0.14927076105206835
    }
  },
  "optimisation_techniques": {
    "title": "Optimisation techniques",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "stochastic_gradient_descent",
      "standardised/optuna",
      "learning_rate",
      "sklearn",
      "gradient_descent",
      "adaptive_learning_rates",
      "adam_optimizer",
      "cost_function"
    ],
    "inlinks": [
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "deep_learning",
      "model_parameters",
      "neural_network",
      "model_parameters_tuning",
      "gradient_descent"
    ],
    "summary": "Optimisation techniques - [[Adam Optimizer]] - RMSprop - [[Stochastic Gradient Descent]] - [[standardised/Optuna]] [[Gradient Descent]] - Iteratively updates parameters using the gradient of the [[Cost...",
    "TFIDF_Score": {
      "gradient": 0.42184337064897537,
      "descent": 0.3914505270268426,
      "solver": 0.27326402456358595,
      "tuning": 0.17747597648115504,
      "rate": 0.17241353593246897,
      "size": 0.1670060471294743,
      "update": 0.15994683728900222,
      "parameter": 0.1484745756004947,
      "approximation": 0.13663201228179297,
      "eliminating": 0.13663201228179297
    }
  },
  "optimising_a_logistic_regression_model": {
    "title": "Optimising a Logistic Regression Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "logistic_regression",
      "model_parameters",
      "ridge",
      "loss_function",
      "sklearn",
      "gradient_descent",
      "cost_function"
    ],
    "inlinks": [],
    "summary": "Optimising a [[Logistic Regression]] Model In sklearn, the logistic regression model uses an optimization algorithm to find the best parameters (intercept and coefficients) that minimize...",
    "TFIDF_Score": {
      "solver": 0.43656186743264885,
      "logistic": 0.3153022826044869,
      "theta": 0.2694695415012955,
      "newton": 0.26241601094405176,
      "loss": 0.19210366032980497,
      "function": 0.18913242090883095,
      "stop": 0.18744000781717984,
      "optimization": 0.16978561416989332,
      "lbfgs": 0.15874976997550866,
      "iteration": 0.1584395324080714
    }
  },
  "optimising_neural_networks": {
    "title": "Optimising Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "neural_network",
      "deep_learning"
    ],
    "inlinks": [
      "orthogonalization"
    ],
    "summary": "[[Deep Learning]] Ways to improve in using a [[Neural network]] more data, bigger network, diverse training set, try dropout, change network architechure. ==Need strategies that...",
    "TFIDF_Score": {
      "try": 0.4298069494745134,
      "network": 0.4141554165892181,
      "architechure": 0.29353213703088205,
      "bigger": 0.27649979277425857,
      "whats": 0.27649979277425857,
      "dropout": 0.23035046369275217,
      "towards": 0.2149034747372567,
      "diverse": 0.21179042636727038,
      "strategy": 0.17096554726281152,
      "deep": 0.16665336143877912
    }
  },
  "optuna": {
    "title": "Optuna",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "catboost",
      "xgboost",
      "lightgbm",
      "hyperparameter_tuning",
      "hyperparameter"
    ],
    "inlinks": [],
    "summary": "Optuna is a [[hyperparameter]] optimization framework used to automatically tune hyperparameters for machine learning models. Optuna automates the process of tuning hyperparameters by defining an...",
    "TFIDF_Score": {
      "trial": 0.43591941201169854,
      "hyperparameters": 0.34231065015011924,
      "objective": 0.33899541392724547,
      "optuna": 0.2749597905589583,
      "study": 0.22640475454615447,
      "optimization": 0.2174352362560275,
      "function": 0.20037508676506444,
      "model": 0.17288752404260815,
      "batch_size": 0.14025918779258062,
      "hyperparameter": 0.13472820528234491
    }
  },
  "ordinary_least_squares": {
    "title": "Ordinary Least Squares",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pytorch",
      "linear_regression"
    ],
    "summary": "Derivation of Coefficients: - OLS derives the coefficients by setting the partial derivatives of the SSE with respect to each coefficient to zero. This results...",
    "TFIDF_Score": {
      "coefficient": 0.6098755816508112,
      "matrix": 0.2535292103276801,
      "vector": 0.20813226673118493,
      "derivation": 0.20369400794906403,
      "derives": 0.20369400794906403,
      "ols": 0.18348853607683596,
      "sse": 0.18348853607683596,
      "solved": 0.17698382701070942,
      "expressed": 0.17166909272220646,
      "intercept": 0.16717554890829617
    }
  },
  "orthogonalization": {
    "title": "Orthogonalization",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "regularisation",
      "ds_&_ml_portal",
      "optimising_neural_networks",
      "adam_optimizer"
    ],
    "inlinks": [],
    "summary": "link When training ML model need Orthogonalization in order to Determine what to tune, and observe the effect it has. Each button does one thing....",
    "TFIDF_Score": {
      "bigger": 0.38294232417586405,
      "set": 0.3467850008216573,
      "training": 0.33666068847966446,
      "try": 0.2976336266426876,
      "work": 0.24849942289971558,
      "network": 0.19119681333502422,
      "effect": 0.1715384293388486,
      "link": 0.15113369987280095,
      "size": 0.14391564911607954,
      "cost": 0.13783245162772426
    }
  },
  "outliers": {
    "title": "Outliers",
    "tags": [
      "statistics",
      "anomaly_detection",
      "data_cleaning"
    ],
    "aliases": [
      "anomalies",
      "Handling Outliers"
    ],
    "outlinks": [
      "handling_missing_data",
      "methods_for_handling_outliers",
      "anomaly_detection",
      "linear_regression"
    ],
    "inlinks": [
      "ds_&_ml_portal"
    ],
    "summary": "Outliers are data points that differ significantly from other observations in the dataset. They can skew and mislead the training of machine learning models, especially...",
    "TFIDF_Score": {
      "outlier": 0.53451754054714,
      "model": 0.26461022633015,
      "class": 0.25413103839387985,
      "regression": 0.22816962662346862,
      "removing": 0.21079952827146034,
      "squared": 0.17698107000016428,
      "handling": 0.15743348550685637,
      "classification": 0.14344943190532988,
      "boundary": 0.14229052534039885,
      "point": 0.1337460417190931
    }
  },
  "over_parameterised_models": {
    "title": "Over parameterised models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "neural_network"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Neural network]] Universal approximation theory",
    "TFIDF_Score": {
      "universal": 0.5714858364757752,
      "approximation": 0.5271347488693294,
      "theory": 0.4669540729225451,
      "neural": 0.30995416928211356,
      "network": 0.2853334925446608
    }
  },
  "overfitting": {
    "title": "Overfitting",
    "tags": [
      "model_architecture"
    ],
    "aliases": [
      "model overfitting",
      "high variance models"
    ],
    "outlinks": [
      "cross_validation",
      "bias_and_variance",
      "regularisation"
    ],
    "inlinks": [
      "gradient_boosting",
      "ridge",
      "ds_&_ml_portal",
      "parsimonious",
      "batch_normalisation",
      "machine_learning_algorithms",
      "decision_tree",
      "bagging",
      "bias_and_variance",
      "cross_validation",
      "feed_forward_neural_network",
      "dropout",
      "model_evaluation"
    ],
    "summary": "[!Summary] Overfitting in machine learning occurs when a model captures not only the underlying patterns in the training data ==but also the noise==, leading to...",
    "TFIDF_Score": {
      "overfitting": 0.42170594623733687,
      "model": 0.32783101732072306,
      "l_1": 0.22644971301087752,
      "l_2": 0.22644971301087752,
      "regularization": 0.21994477845620253,
      "training": 0.2158314053240632,
      "validation": 0.19978090547555838,
      "cross": 0.1926644676443066,
      "high": 0.15295449645658435,
      "mitigating": 0.15096647534058502
    }
  },
  "p_values": {
    "title": "p values",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection"
    ],
    "inlinks": [
      "statistics",
      "p-values_in_linear_regression_in_sklearn",
      "hypothesis_testing",
      "model_interpretability"
    ],
    "summary": "A p-value is a measure of the evidence against a null hypothesis. p-values indicate whether an effect exists Used in [[Feature Selection]]",
    "TFIDF_Score": {
      "evidence": 0.41538370840112815,
      "exists": 0.342885293336149,
      "value": 0.3335923318500398,
      "null": 0.3199002206663526,
      "hypothesis": 0.2971438861848805,
      "indicate": 0.2971438861848805,
      "effect": 0.29186179576617965,
      "whether": 0.2761288374586256,
      "selection": 0.25140247903361196,
      "measure": 0.22650449268023
    }
  },
  "p-values_in_linear_regression_in_sklearn": {
    "title": "p-values in linear regression in sklearn",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "p_values",
      "linear_regression"
    ],
    "inlinks": [
      "sklearn",
      "linear_regression"
    ],
    "summary": "Question How to include [[p values]] in sklearn for a [[Linear Regression]]? import scipy.stats as stat. You can modify the class of LinearRegression() from sklearn...",
    "TFIDF_Score": {
      "f_regression": 0.578056292193467,
      "regression": 0.21703216206806283,
      "sklearn": 0.2161682720688861,
      "array": 0.19814572918009757,
      "gpa": 0.19268543073115568,
      "p_values": 0.19268543073115568,
      "statistic": 0.18455802226961066,
      "linear": 0.15120026875140513,
      "simple": 0.14834656004296046,
      "value": 0.13939466748440796
    }
  },
  "pandas_dataframe_agent": {
    "title": "Pandas Dataframe Agent",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "langchain"
    ],
    "summary": "Example: https://github.com/AssemblyAI/youtube-tutorials/tree/main/pandas-dataframe-agent Follow: https://www.youtube.com/watch?v=ZIfzpmO8MdA&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=7 Can as pandas questions to a dataframe. Types of questions: - what is the max value of \"col1\"",
    "TFIDF_Score": {
      "youtube": 0.3561595907878495,
      "dataframe": 0.3140614429556982,
      "panda": 0.3018930233663497,
      "question": 0.25224880001277866,
      "assemblyai": 0.25023495839031706,
      "plcwfeusays2kc31f4_ed1jxlkdmu6tlrm": 0.25023495839031706,
      "zifzpmo8mda": 0.25023495839031706,
      "col1": 0.2254128467135315,
      "com": 0.22395876091351197,
      "http": 0.21153165241273889
    }
  },
  "pandas_join_vs_merge": {
    "title": "Pandas join vs merge",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "merge"
    ],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": "In pandas, both .join() and pd.merge() are used to combine DataFrames, but they differ in syntax, defaults, and use cases. [[Merge]] is better than Join....",
    "TFIDF_Score": {
      "join": 0.611874619315337,
      "merge": 0.42927144350503493,
      "index": 0.3140045529091871,
      "column": 0.2699107874649896,
      "suffix": 0.17850396028835505,
      "default": 0.17765561876751604,
      "style": 0.14309048116834497,
      "syntax": 0.12383505985714212,
      "behavior": 0.106890926938859,
      "requires": 0.10073033961833901
    }
  },
  "pandas_pivot_table": {
    "title": "Pandas Pivot Table",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "aggregation",
      "pandas_stack"
    ],
    "summary": "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html Pivot Table: Summarize Data python df = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': ['one', 'two', 'one'], 'C': [1, 2, 3]}) pivot_table = df.pivot_table(values='C', index='A', columns='B',...",
    "TFIDF_Score": {
      "pivot_table": 0.6632535350043715,
      "foo": 0.3171327592142679,
      "de_tools": 0.2268602706451393,
      "panda": 0.21236626235722145,
      "aggfunc": 0.17602746241664172,
      "pydata": 0.17602746241664172,
      "http": 0.1488016711755105,
      "pivot": 0.1381382222760363,
      "one": 0.13800708818971236,
      "org": 0.1330831232592942
    }
  },
  "pandas_stack": {
    "title": "Pandas Stack",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "de_tools",
      "pandas_stack.py",
      "pandas_pivot_table",
      "pandas_common.py",
      "machine_learning_algorithms",
      "groupby",
      "data_transformation",
      "multi-level_index"
    ],
    "inlinks": [
      "multi-level_index",
      "data_transformation_with_pandas"
    ],
    "summary": "Tool for reshaping data, particularly when you need to pivot a DataFrame ([[Pandas Pivot Table]]) from a wide format to a long format. See: -...",
    "TFIDF_Score": {
      "dataframe": 0.5065818532277895,
      "stack": 0.2923844801139147,
      "format": 0.2526435083653629,
      "original": 0.22310734565966822,
      "stacking": 0.21581665210122314,
      "wide": 0.20007261928845296,
      "index": 0.1820566179284682,
      "aggregation": 0.16836224864628563,
      "column": 0.15649150514864366,
      "reshaping": 0.15289222137394615
    }
  },
  "pandas": {
    "title": "Pandas",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_selection",
      "handling_missing_data",
      "pandas_common.py",
      "data_transformation",
      "ml_tools"
    ],
    "inlinks": [
      "pyspark",
      "data_transformation_with_pandas"
    ],
    "summary": "In [[ML_Tools]] see: - [[Pandas_Common.py]] Areas: - [[Handling Missing Data]] - [[Data Selection]] - [[Data Transformation]]",
    "TFIDF_Score": {
      "pandas_common": 0.4976116303369031,
      "missing": 0.3665216836169219,
      "data": 0.3639321849632619,
      "area": 0.3410050810509725,
      "selection": 0.30116924408081813,
      "handling": 0.2862088631779632,
      "transformation": 0.2835288020539274,
      "ml_tools": 0.2523583661121567,
      "see": 0.22939412973000464
    }
  },
  "pandas_common.py": {
    "title": "Pandas_Common.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack",
      "pandas"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Common.py",
    "TFIDF_Score": {
      "pandas_common": 0.5143854735883211,
      "utility": 0.4180669099567267,
      "rhyslwells": 0.28654517674298946,
      "blob": 0.28285797774910554,
      "github": 0.2727004867358482,
      "exploration": 0.2706050117305536,
      "ml_tools": 0.26086503962676755,
      "com": 0.25553364632131,
      "main": 0.2435533500502288,
      "http": 0.2413544986269743
    }
  },
  "pandas_stack.py": {
    "title": "Pandas_Stack.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Stack.py",
    "TFIDF_Score": {
      "pandas_stack": 0.5313622680819112,
      "utility": 0.41298981315440386,
      "rhyslwells": 0.2830653088895077,
      "blob": 0.27942288805379784,
      "github": 0.26938875185268857,
      "exploration": 0.2673187247582316,
      "ml_tools": 0.2576970370248301,
      "com": 0.25243038933606304,
      "main": 0.24059558442638881,
      "http": 0.23842343634000193
    }
  },
  "parametric_tests": {
    "title": "Parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "parametric_vs_non-parametric_models": {
    "title": "parametric vs non-parametric models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "statistics",
      "bernoulli",
      "support_vector_machines",
      "regression"
    ],
    "inlinks": [
      "maximum_likelihood_estimation",
      "k-nearest_neighbours"
    ],
    "summary": "Parametric Models In [[Statistics]] Definition: Models that summarize data with a ==set of parameters of fixed size, regardless of the number of data points.== Characteristics:...",
    "TFIDF_Score": {
      "parametric": 0.5592368265824722,
      "model": 0.34273051638202906,
      "data": 0.21939664432051656,
      "non": 0.20378442770465777,
      "fixed": 0.19659381411013338,
      "assumption": 0.17245573980133316,
      "linear": 0.14253831027745198,
      "generally": 0.13980920664561805,
      "mapping": 0.13298899447844922,
      "assumes": 0.12759136715706812
    }
  },
  "parametric_vs_non-parametric_tests": {
    "title": "parametric vs non-parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "statistical_assumptions",
      "non-parametric_tests_",
      "parametric_tests_"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Parametric tests ]] are statistical tests that make ==assumptions about the distribution== of the data. For example, a t-test assumes that the data is normally...",
    "TFIDF_Score": {
      "parametric": 0.6100750392010238,
      "test": 0.5677989241050452,
      "assumption": 0.25084409614125946,
      "non": 0.22230973857753072,
      "distribution": 0.18709852827447887,
      "powerful": 0.17530487136530526,
      "data": 0.14505527622334827,
      "statistical": 0.14404723636475322,
      "make": 0.11303657226606735,
      "generally": 0.10167917320017063
    }
  },
  "parquet": {
    "title": "Parquet",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "hadoop",
      "cloud_providers",
      "apache_spark",
      "data_storage",
      "metadata_handling",
      "big_data",
      "schema_evolution"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "A Parquet file is a columnar storage file format specifically designed for storing large amounts of data efficiently. It is commonly used in [[Big Data]]...",
    "TFIDF_Score": {
      "parquet": 0.563984621355025,
      "row": 0.2668298689542682,
      "column": 0.25399508655639996,
      "file": 0.22888719779839845,
      "storage": 0.18899019239462994,
      "big": 0.18631500974994017,
      "read": 0.17106158959589263,
      "data": 0.17102901321027106,
      "columnar": 0.14380531054296627,
      "format": 0.13979166381664435
    }
  },
  "parsimonious": {
    "title": "parsimonious",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_selection",
      "overfitting"
    ],
    "inlinks": [
      "adjusted_r_squared"
    ],
    "summary": "Parsimonious refers to a principle in [[Model Selection]] and statistical modeling that emphasizes ==simplicity==. In the context of regression and other statistical models, a parsimonious...",
    "TFIDF_Score": {
      "parsimonious": 0.6410890706042316,
      "model": 0.35414604356784096,
      "simplicity": 0.20450179771037647,
      "balance": 0.1930498100998506,
      "good": 0.18537232499250428,
      "overfitting": 0.16450644952391358,
      "fit": 0.11765693035045113,
      "statistical": 0.11085886434402682,
      "complexity": 0.10294574008039115,
      "fewest": 0.10166925725464351
    }
  },
  "part_of_speech_tagging": {
    "title": "Part of speech tagging",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "Part of speech tagging : assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text Part-of-speech tagging involves...",
    "TFIDF_Score": {
      "speech": 0.447908495505542,
      "part": 0.3266046401350342,
      "pos_tag": 0.29622161671498815,
      "adjective": 0.27903321409857396,
      "noun": 0.26683784836193325,
      "tagging": 0.26683784836193325,
      "verb": 0.26683784836193325,
      "assigning": 0.2108062168259537,
      "etc": 0.2108062168259537,
      "category": 0.177382076783302
    }
  },
  "pca_explained_variance_ratio": {
    "title": "PCA Explained Variance Ratio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "pca_explained_variance_ratio"
    ],
    "inlinks": [
      "pca_explained_variance_ratio",
      "principal_component_analysis"
    ],
    "summary": "[[PCA Explained Variance Ratio]] - The variance explained by each principal component is printed using pca.explained_variance_ratio_. - The sum of the explained variances is calculated...",
    "TFIDF_Score": {
      "variance": 0.6026368275123206,
      "explained": 0.42903573252696714,
      "component": 0.37899400991172044,
      "principal": 0.24838910830508623,
      "total": 0.1860263386546721,
      "pca": 0.18508816135566952,
      "ratio": 0.17431544972789625,
      "dataset": 0.14994444499786158,
      "first": 0.13908119836391863,
      "explain": 0.10927105131546341
    }
  },
  "pca_principal_components": {
    "title": "PCA Principal Components",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "correlation",
      "pca_analysis.ipynb",
      "heatmap",
      "pasted_image_20250317093551.png"
    ],
    "inlinks": [
      "principal_component_analysis"
    ],
    "summary": "The principal components (or the new axes that explain the most variance) are stored in pca.components_ and displayed as a DataFrame for easier reading Interpretating...",
    "TFIDF_Score": {
      "sepal": 0.5146316407225937,
      "pc1": 0.4080729111688845,
      "pc2": 0.2856510378182191,
      "length": 0.27975789082888086,
      "width": 0.2679305635198607,
      "loading": 0.1981671068212237,
      "principal": 0.19583052358021658,
      "petal": 0.1470376116350268,
      "component": 0.13694987052662982,
      "heatmap": 0.12809470916407936
    }
  },
  "pca-based_anomaly_detection": {
    "title": "PCA-Based Anomaly Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pca_based_anomaly_detection.py",
      "ml_tools"
    ],
    "inlinks": [
      "anomaly_detection",
      "principal_component_analysis"
    ],
    "summary": "For implementation, see: [[ML_Tools]]: - [[PCA_Based_Anomaly_Detection.py]]",
    "TFIDF_Score": {
      "pca_based_anomaly_detection": 0.7468471166698086,
      "implementation": 0.4501434927855433,
      "ml_tools": 0.3622016477968641,
      "see": 0.3292418359778436
    }
  },
  "pca_analysis.ipynb": {
    "title": "PCA_Analysis.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "principal_component_analysis"
    ],
    "inlinks": [
      "principal_component_analysis",
      "pca_principal_components"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Analysis.ipynb This script performs Principal Component Analysis (PCA) on the Iris dataset to reduce its dimensionality while preserving key variance. See also [[Principal Component Analysis|PCA]]...",
    "TFIDF_Score": {
      "pca": 0.6526834352977344,
      "component": 0.3084140808291812,
      "variance": 0.27740248836149084,
      "principal": 0.24500810526118613,
      "contribution": 0.15061925429947717,
      "loading": 0.1487588752142744,
      "computes": 0.14700486315671168,
      "explained": 0.14700486315671168,
      "plot": 0.12840446058151414,
      "feature": 0.12134539470792845
    }
  },
  "pca_based_anomaly_detection.py": {
    "title": "PCA_Based_Anomaly_Detection.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pca-based_anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Based_Anomaly_Detection.py",
    "TFIDF_Score": {
      "pca_based_anomaly_detection": 0.4960182825432073,
      "preprocess": 0.3747358104204087,
      "pca": 0.36987237870288686,
      "rhyslwells": 0.26423699384934896,
      "blob": 0.26083685154389286,
      "github": 0.2514701439241648,
      "exploration": 0.24953780633475173,
      "ml_tools": 0.24055611284357376,
      "com": 0.23563978043108927,
      "main": 0.2245921770197277
    }
  },
  "pd.grouper": {
    "title": "pd.Grouper",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "groupby",
      "multi-level_index"
    ],
    "inlinks": [],
    "summary": "pd.Grouper is a utility in pandas used with .groupby() to flexibly group data by a specific column, often useful for time-based grouping, multi-index grouping, or...",
    "TFIDF_Score": {
      "grouper": 0.34615160637722964,
      "grouping": 0.33790363921219047,
      "none": 0.2716437928859455,
      "sort": 0.2716437928859455,
      "index": 0.2706274981206047,
      "group": 0.25588894974648807,
      "level": 0.23262491085133324,
      "freq": 0.217377327970307,
      "multi": 0.20580209253412124,
      "axis": 0.17446874961236486
    }
  },
  "pdoc": {
    "title": "pdoc",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pdoc"
    ],
    "inlinks": [
      "documentation_&_meetings",
      "pdoc"
    ],
    "summary": "PDOC is a documentation generator specifically designed for Python projects. Here are some key features and details: Automatic Documentation: It scans your Python code and...",
    "TFIDF_Score": {
      "documentation": 0.5917010272001042,
      "pdoc": 0.3618543129909771,
      "html": 0.2180420148576676,
      "doc": 0.20454067398665515,
      "project": 0.18429402645148532,
      "folder": 0.16443209848441948,
      "repository": 0.15580566562983098,
      "web": 0.14533940883785218,
      "file": 0.13958279976580426,
      "directory": 0.13439396559177463
    }
  },
  "pdp_and_ice": {
    "title": "PDP and ICE",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241204203413.png",
      "interpretability",
      "pasted_image_20241204203338.png"
    ],
    "inlinks": [],
    "summary": "link: https://scikit-learn.org/1.5/modules/partial_dependence.html#h2009 [[interpretability|interpretable]] Regression example: ![[Pasted image 20241204203338.png]] Categorical Example ![[Pasted image 20241204203413.png]]",
    "TFIDF_Score": {
      "pasted": 0.2994011346838765,
      "20241204203338": 0.29568868960227457,
      "20241204203413": 0.29568868960227457,
      "h2009": 0.29568868960227457,
      "partial_dependence": 0.29568868960227457,
      "png": 0.29419142242265517,
      "image": 0.26114913601531337,
      "org": 0.22355133559545315,
      "scikit": 0.2198694012773568,
      "html": 0.20513061176249053
    }
  },
  "percentile_detection": {
    "title": "Percentile Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_percentile.py",
    "TFIDF_Score": {
      "outliers_percentile": 0.5292861313082923,
      "preprocess": 0.37666665990450166,
      "outlier": 0.31035400139772285,
      "rhyslwells": 0.2655984913338832,
      "blob": 0.26218082958450484,
      "github": 0.2527658594233481,
      "exploration": 0.2508235653447663,
      "ml_tools": 0.24179559312131482,
      "com": 0.23685392900142926,
      "main": 0.2257494021289142
    }
  },
  "performance_dimensions": {
    "title": "Performance Dimensions",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "availability",
      "scalability",
      "accessibility",
      "data_quality",
      "speed",
      "usability",
      "simplicity",
      "latency",
      "reusability",
      "data_integrity",
      "performance",
      "interoperability",
      "data_compatibility",
      "flexibility",
      "cost-efficiency"
    ],
    "inlinks": [
      "dimensional_modelling",
      "data_principles",
      "data_lifecycle_management"
    ],
    "summary": "Efficiency & Performance - [[Cost-efficiency]]: Ensuring that the solutions used are cost-effective and provide value for money. - [[Speed]]: The ability to process and analyze...",
    "TFIDF_Score": {
      "data": 0.3804468778296771,
      "ensuring": 0.36056043184129166,
      "authorized": 0.20398762477084179,
      "accessibility": 0.18250657946761556,
      "usability": 0.18250657946761556,
      "system": 0.16803358455206197,
      "interoperability": 0.16667578815241224,
      "quality": 0.15955630483418656,
      "performance": 0.14956040797831036,
      "user": 0.14251340032994292
    }
  },
  "performance_drift": {
    "title": "Performance Drift",
    "tags": [
      "deleted",
      "data_quality",
      "deleted",
      "model_explainability"
    ],
    "aliases": [
      "model degradation",
      "accuracy drift",
      "concept drift"
    ],
    "outlinks": [
      "data_drift",
      "data_observability",
      "pasted_image_20250113072251.png"
    ],
    "inlinks": [
      "data_drift",
      "model_validation",
      "model_observability",
      "transfer_learning"
    ],
    "summary": "Not [[Data Drift]] TL;DR. Data drift is a change in the input data. Concept drift is a change in input-output relationships. Both often happen simultaneously....",
    "TFIDF_Score": {
      "drift": 0.652136545347994,
      "shift": 0.22843093787070742,
      "change": 0.21467973545159366,
      "input": 0.21394506117302658,
      "model": 0.20676156556920922,
      "data": 0.18249227033263765,
      "recession": 0.17807310160901366,
      "performance": 0.1639793020522957,
      "monitoring": 0.15161069903263844,
      "retraining": 0.15007632330104154
    }
  },
  "physical_model": {
    "title": "Physical Model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Physical Model (for a SQL database): ```sql CREATE TABLE Customer ( CustomerID INT PRIMARY KEY, Name VARCHAR(100), Email VARCHAR(100) ); CREATE TABLE Order ( OrderID...",
    "TFIDF_Score": {
      "bookid": 0.38833844642830023,
      "orderid": 0.38833844642830023,
      "int": 0.3807679021766169,
      "customerid": 0.31067075714264014,
      "varchar": 0.29709264774346617,
      "100": 0.22610272729343114,
      "primary": 0.19749413802162846,
      "table": 0.1917453940963564,
      "key": 0.19158902833855845,
      "foreign": 0.1784732588447206
    }
  },
  "poetry": {
    "title": "Poetry",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dependency_manager"
    ],
    "inlinks": [
      "virtual_environments",
      "dependency_manager"
    ],
    "summary": "Modern version of setting up dependencies instead of requirements.txt ([[dependency manager]]) Primary Purpose: Poetry is a [[dependency manager]] and packaging tool for Python projects. Main...",
    "TFIDF_Score": {
      "poetry": 0.5742488531664243,
      "dependency": 0.33647484904435343,
      "package": 0.2501340949985767,
      "project": 0.23362338185475995,
      "publishing": 0.22518521666120905,
      "lock": 0.20770935175892793,
      "version": 0.196761655856863,
      "virtual": 0.17248502275450397,
      "python": 0.16041091763945817,
      "pypi": 0.15937106067029824
    }
  },
  "policy": {
    "title": "Policy",
    "tags": [
      "question",
      "#question"
    ],
    "aliases": [
      "policies"
    ],
    "outlinks": [
      "sarsa",
      "exploration",
      "q-learning",
      "exploitation",
      "reinforcement_learning"
    ],
    "inlinks": [
      "agent-based_modelling",
      "q-learning",
      "reinforcement_learning"
    ],
    "summary": "In [[reinforcement learning]] (RL), a policy is a strategy or a rule that defines the actions an agent takes in a given state to achieve...",
    "TFIDF_Score": {
      "policy": 0.5458153240978799,
      "move": 0.4234912065197741,
      "agent": 0.41738818901602587,
      "action": 0.26636227002207835,
      "right": 0.2616499572141824,
      "goal": 0.18700075599418667,
      "state": 0.15184289116160654,
      "position": 0.09507027742310882,
      "grid": 0.09389601350469429,
      "sarsa": 0.0812813324780235
    }
  },
  "positional_encoding": {
    "title": "Positional Encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "postgresql": {
    "title": "PostgreSQL",
    "tags": [
      "relational_database",
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "adding_a_database_to_postgresql",
      "pasted_image_20250329081752.png",
      "tableau"
    ],
    "inlinks": [
      "normalisation_of_data",
      "data_engineering_portal",
      "tableau",
      "looker_studio",
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": "Installation How to set up a Postgres database on your Windows 10 PC In [[Tableau]] can connect to a database here. There are plugins. Spatial...",
    "TFIDF_Score": {
      "pgadmin": 0.4461106521956103,
      "database": 0.4169082612609248,
      "20250329081752": 0.23679549951440138,
      "connecting": 0.23679549951440138,
      "hosted": 0.22305532609780515,
      "plugins": 0.22305532609780515,
      "installation": 0.20574475481601373,
      "postgres": 0.20574475481601373,
      "tableau": 0.18981753631280474,
      "spatial": 0.18582617108041066
    }
  },
  "powerbi": {
    "title": "PowerBI",
    "tags": [
      "software",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_visualisation",
      "looker_studio",
      "fabric",
      "semantic_layer"
    ],
    "summary": "tutorial Business analytics tool for data visualization and reporting.",
    "TFIDF_Score": {
      "tutorial": 0.5020964094367856,
      "reporting": 0.4330681073319159,
      "analytics": 0.41084595754198594,
      "visualization": 0.38362955181937214,
      "business": 0.3695230316467895,
      "tool": 0.29066204378299904,
      "data": 0.152818459524274
    }
  },
  "powerquery": {
    "title": "Powerquery",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "how_to_normalise_a_merged_table"
    ],
    "inlinks": [],
    "summary": "[[How to normalise a merged table]]",
    "TFIDF_Score": {
      "normalise": 0.6832810188163537,
      "merged": 0.6392673628688693,
      "table": 0.3527949660864889
    }
  },
  "powershell_versus_cmd": {
    "title": "Powershell versus cmd",
    "tags": [
      "#software"
    ],
    "aliases": null,
    "outlinks": [
      "command_prompt",
      "powershell"
    ],
    "inlinks": [],
    "summary": "PowerShell and Command Prompt (cmd) are both command-line interfaces available on Windows systems, but they differ significantly in their capabilities, syntax, and scripting abilities. Here...",
    "TFIDF_Score": {
      "powershell": 0.4992762514008991,
      "file": 0.40525197951631087,
      "command": 0.34918599134900336,
      "cmd": 0.25551503252955465,
      "txt": 0.21217651942036908,
      "scripting": 0.208099835347407,
      "object": 0.13270430649927714,
      "net": 0.13006239709212938,
      "copy": 0.12565332492869297,
      "path": 0.11454047992402182
    }
  },
  "powershell_vs_bash": {
    "title": "Powershell vs Bash",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "bash",
      "windows_subsystem_for_linux",
      "powershell",
      "git"
    ],
    "inlinks": [
      "command_line"
    ],
    "summary": "The choice between [[PowerShell]] and [[Bash]] largely depends on the user's needs and the environment in which they are working. Here are some considerations for...",
    "TFIDF_Score": {
      "bash": 0.44859854903050184,
      "window": 0.37375753791714883,
      "powershell": 0.35920018525135156,
      "linux": 0.30421972047413515,
      "unix": 0.24736169708101266,
      "community": 0.20607567568398572,
      "remote": 0.1436800741005406,
      "system": 0.13584181128012962,
      "environment": 0.1347816103466302,
      "scripting": 0.13474414069975282
    }
  },
  "powershell": {
    "title": "PowerShell",
    "tags": [
      "software"
    ],
    "aliases": [
      "Why is Powershell better than cmd"
    ],
    "outlinks": [
      ".net",
      "command_prompt",
      "batch_files"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_prompt",
      "command_line",
      "powershell_vs_bash"
    ],
    "summary": "Why is Powershell better than [[Command Prompt|cmd]]? PowerShell is often considered better than Command Prompt (cmd) for several reasons: Object-Oriented: PowerShell is built on the...",
    "TFIDF_Score": {
      "powershell": 0.7493815677098051,
      "net": 0.26772383954098544,
      "script": 0.1896400117205855,
      "command": 0.18497849393599544,
      "cmd": 0.16436213285573018,
      "scripting": 0.13386191977049272,
      "vbscript": 0.13043990156863358,
      "window": 0.12377013524659097,
      "cmdlets": 0.1228710631757073,
      "file": 0.1086173532171132
    }
  },
  "precision_or_recall": {
    "title": "Precision or Recall",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "pasted_image_20240116211130.png",
      "classification",
      "precision",
      "recall"
    ],
    "inlinks": [
      "precision",
      "precision-recall_curve",
      "evaluation_metrics"
    ],
    "summary": "[[Precision]] and [[Recall]] are two fundamental metrics used to evaluate the performance of a [[Classification]] model, particularly in binary classification tasks. They are related through...",
    "TFIDF_Score": {
      "recall": 0.5066964252051318,
      "precision": 0.45039682240456164,
      "spam": 0.33027056512145286,
      "email": 0.23187671177048677,
      "positive": 0.21403834107762068,
      "important": 0.17166246472939173,
      "false": 0.15810793381696192,
      "task": 0.1414778901490757,
      "classification": 0.12373812312088929,
      "trade": 0.11157839115503293
    }
  },
  "precision-recall_curve": {
    "title": "Precision-Recall Curve",
    "tags": [],
    "aliases": [
      "PR Curve"
    ],
    "outlinks": [
      "auc",
      "binary_classification",
      "roc_pr_example.py",
      "precision",
      "recall",
      "distributions",
      "precision_or_recall",
      "pasted_image_20241231172749.png",
      "roc_(receiver_operating_characteristic)",
      "imbalanced_datasets",
      "ml_tools"
    ],
    "inlinks": [
      "determining_threshold_values",
      "choosing_a_threshold"
    ],
    "summary": "A [[precision]]-[[recall]] curve is a graphical representation used to evaluate the performance of a [[Binary Classification]] model, particularly in scenarios where the classes are imbalanced....",
    "TFIDF_Score": {
      "curve": 0.49630075088972975,
      "precision": 0.4758278652226971,
      "recall": 0.4758278652226971,
      "class": 0.2676133570523253,
      "positive": 0.22612372433500605,
      "threshold": 0.10673895542440476,
      "roc": 0.09472365540686653,
      "scenario": 0.08584238228934377,
      "imbalanced": 0.08537807560634772,
      "performance": 0.08499679950451644
    }
  },
  "precision": {
    "title": "Precision",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "classification_report",
      "accuracy",
      "pasted_image_20241222091831.png",
      "classification",
      "precision_or_recall"
    ],
    "inlinks": [
      "classification_report",
      "ds_&_ml_portal",
      "precision-recall_curve",
      "evaluation_metrics",
      "f1_score",
      "model_observability",
      "precision_or_recall",
      "confusion_matrix"
    ],
    "summary": "Precision Score is a metric used to evaluate the [[Accuracy]] of a [[Classification]] model, specifically focusing on the positive class. ==How many retrieved items are...",
    "TFIDF_Score": {
      "positive": 0.6798265524727212,
      "precision": 0.39737392307894137,
      "predicted": 0.2184465364181612,
      "instance": 0.20283683209025977,
      "false": 0.1487943473972205,
      "many": 0.1254468782592753,
      "classification": 0.11644914226280555,
      "metric": 0.11557748920362138,
      "accuracy": 0.11268580205883784,
      "20241222091831": 0.11109947837710829
    }
  },
  "preprocessing": {
    "title": "Preprocessing",
    "tags": [
      "#ml_optimisation",
      "data_transformation",
      "data_cleaning",
      "data_collection",
      "portal"
    ],
    "aliases": [
      "Data Preprocessing",
      "Feature Preprocessing",
      "Preprocess"
    ],
    "outlinks": [
      "data_cleansing",
      "eda",
      "feature_selection",
      "data_collection",
      "dimensionality_reduction",
      "feature_scaling",
      "feature_engineering",
      "data_transformation",
      "data_reduction"
    ],
    "inlinks": [
      "data_pipeline",
      "handling_different_distributions",
      "heterogeneous_features",
      "pycaret",
      "model_deployment",
      "data_lifecycle_management",
      "standardisation",
      "dimensionality_reduction",
      "feature_scaling",
      "interview_notepad",
      "model_building",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "clustermap",
      "nlp",
      "normalisation_of_text"
    ],
    "summary": "Data Preprocessing Data Preprocessing refers to the overall process of cleaning and transforming raw data into a format that is suitable for analysis and modelling....",
    "TFIDF_Score": {
      "feature": 0.43446547956042986,
      "preprocessing": 0.3562209395245491,
      "data": 0.28100058215561446,
      "scaling": 0.21899442735529825,
      "model": 0.20259928683562853,
      "transforming": 0.17033070393793362,
      "normalization": 0.1678633277314721,
      "contribute": 0.15949597908246924,
      "often": 0.1497607884692449,
      "raw": 0.14478544810278765
    }
  },
  "prevention_is_better_than_the_cure": {
    "title": "Prevention Is Better Than The Cure",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "change_management",
      "data_observability",
      "data_quality"
    ],
    "inlinks": [
      "data_quality"
    ],
    "summary": "To ensure data products are effective essential to prioritize prevention over remediation of [[Data Quality]] Prevention Preventing data quality issues is the most effective strategy....",
    "TFIDF_Score": {
      "quality": 0.5494340913951263,
      "data": 0.3901321781093112,
      "issue": 0.333117441168936,
      "remediation": 0.19739215704757956,
      "problem": 0.12472297225292833,
      "alerting": 0.12395892109348489,
      "prevention": 0.1143388874581444,
      "arise": 0.10800233329355315,
      "effective": 0.10703448444560763,
      "trust": 0.10548762684842339
    }
  },
  "primary_key": {
    "title": "Primary Key",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": "A primary key (PK) is a unique identifier for each record in a database table. Uniqueness: No two records can have the same primary key...",
    "TFIDF_Score": {
      "primary": 0.5549099683107874,
      "record": 0.3944855234694917,
      "key": 0.30761032116565845,
      "book": 0.2535144908735275,
      "serf": 0.23353438854925515,
      "null": 0.21429105696500353,
      "isbn": 0.15444657743436488,
      "immutability": 0.14548474006034476,
      "uniqueness": 0.14548474006034476,
      "uniquely": 0.139126215252222
    }
  },
  "principal_component_analysis": {
    "title": "Principal Component Analysis",
    "tags": [
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [
      "PCA"
    ],
    "outlinks": [
      "pca_explained_variance_ratio",
      "pca-based_anomaly_detection",
      "unsupervised_learning",
      "pca_principal_components",
      "variance",
      "interpretability",
      "pca_analysis.ipynb",
      "dimensionality_reduction",
      "t-sne",
      "manifold_learning",
      "ml_tools"
    ],
    "inlinks": [
      "feature_selection",
      "covariance_structures",
      "ds_&_ml_portal",
      "unsupervised_learning",
      "pca_analysis.ipynb",
      "machine_learning_algorithms",
      "standardisation",
      "addressing_multicollinearity",
      "dimensionality_reduction",
      "feature_scaling",
      "t-sne"
    ],
    "summary": "PCA is a tool for [[Dimensionality Reduction]] in [[Unsupervised Learning]] to reduce the dimensionality of data. It transforms the original data into a new coordinate...",
    "TFIDF_Score": {
      "pca": 0.5234950100958652,
      "linear": 0.3898805791245817,
      "principal": 0.28385153707341265,
      "component": 0.23820657090812683,
      "variance": 0.22955836101089153,
      "capture": 0.18471196006113144,
      "technique": 0.16315972469509052,
      "non": 0.1393512918306143,
      "manifold": 0.13592557131941102,
      "sne": 0.13276070534076767
    }
  },
  "probability_in_other_fields": {
    "title": "Probability in other fields",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "problem_definition": {
    "title": "Problem Definition",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "scientific_method"
    ],
    "summary": "What is involved: Clearly articulate the problem you're trying to solve and the outcomes you expect. Follow up questions What assumption can we make based...",
    "TFIDF_Score": {
      "problem": 0.3165388746881389,
      "outcome": 0.3044699855164326,
      "success": 0.2525002382827975,
      "desired": 0.234591680219744,
      "machine": 0.2006003453863373,
      "learning": 0.1799756009216978,
      "question": 0.16833342805224452,
      "expect": 0.16698952923549662,
      "quality": 0.1640503946833872,
      "context": 0.160752864804488
    }
  },
  "programming_languages": {
    "title": "programming languages",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "prompt_engineering": {
    "title": "Prompt Engineering",
    "tags": [
      "language_models",
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "how_can_prompt_engineering_be_integrated_into_existing_nlp_workflows_to_enhance_performance",
      "prompt_retrievers",
      "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts"
    ],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "agent-based_modelling"
    ],
    "summary": "Prompt engineering is a technique in the field of natural language processing (NLP), particularly when working with large language models (LLMs). It involves designing and...",
    "TFIDF_Score": {
      "prompt": 0.7572541973663396,
      "response": 0.1808567320994553,
      "daslam": 0.16024786100634403,
      "uprise": 0.16024786100634403,
      "guide": 0.14176558377386153,
      "ambiguity": 0.13923472760315586,
      "llm": 0.12888609851156,
      "engineering": 0.12614601878272755,
      "model": 0.1085376774340883,
      "enhance": 0.100560838465494
    }
  },
  "prompt_extracting_information_from_blog_posts": {
    "title": "Prompt Extracting information from blog posts",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Prompt Extracting information from blog posts You are an expert in summarizing and analyzing online content. Your assignment involves producing a 200-word summary of the...",
    "TFIDF_Score": {
      "url": 0.36412322228705607,
      "crawling": 0.27309241671529205,
      "follow": 0.2651009809313291,
      "live": 0.2372820954554638,
      "summary": 0.23582053967278577,
      "article": 0.23015663484098248,
      "section": 0.23015663484098248,
      "200": 0.2241321437324189,
      "text": 0.20720178238248124,
      "point": 0.2003389789392794
    }
  },
  "prompting": {
    "title": "Prompting",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "pasted_image_20240910072458.png"
    ],
    "inlinks": [
      "evaluating_language_models",
      "ai_engineer",
      "guardrails",
      "rag"
    ],
    "summary": "Pre set prompts are most useful when they are easily accessible. In obsidian copilot can select prompts with \"/\". We tag prompts with #prompt How...",
    "TFIDF_Score": {
      "prompt": 0.7885798613662867,
      "20240910072458": 0.20396066047531586,
      "copilot": 0.19212574447884903,
      "persona": 0.19212574447884903,
      "obsidian": 0.17189382195069305,
      "write": 0.13267222795721265,
      "accessible": 0.13142997689438113,
      "pre": 0.130238444340052,
      "500": 0.12909365077777396,
      "easily": 0.12491675261898313
    }
  },
  "proportion_test": {
    "title": "Proportion Test",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Proportion Test The proportion test is used to compare proportions between groups. It can be categorized into: - One-Sample Proportion Test: Compares the proportion of...",
    "TFIDF_Score": {
      "proportion": 0.800129071373012,
      "sample": 0.30244175498059367,
      "test": 0.27199729463673683,
      "compare": 0.2703687052163985,
      "success": 0.2153025855280047,
      "two": 0.13930485094557415,
      "categorized": 0.12826476667786943,
      "population": 0.1000161339216265,
      "independent": 0.0865387633468861,
      "group": 0.07894472879938605
    }
  },
  "publish_and_subscribe": {
    "title": "Publish and Subscribe",
    "tags": [
      "event_driven",
      "data_streaming"
    ],
    "aliases": null,
    "outlinks": [
      "event-driven_architecture",
      "scalability",
      "batch_processing",
      "distributed_computing",
      "apache_kafka",
      "data_streaming"
    ],
    "inlinks": [
      "apache_kafka",
      "data_streaming"
    ],
    "summary": "The Publish-Subscribe (Pub-Sub) model is a messaging pattern that enables real-time data distribution by decoupling message producers from consumers. This architecture is widely used in...",
    "TFIDF_Score": {
      "producer": 0.3802643341085012,
      "consumer": 0.36171417206659084,
      "topic": 0.28393115137933794,
      "activity": 0.27139960930438106,
      "user": 0.23843510912852264,
      "event": 0.22083533996170732,
      "decoupling": 0.19762311251760437,
      "receive": 0.16655266774130728,
      "message": 0.15773952854407663,
      "data": 0.15189558451418475
    }
  },
  "pull_request_template": {
    "title": "Pull Request Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Tl;dr 1-liner if the context of the change is long Context A few sentences on the high level context for the change. Link to relevant...",
    "TFIDF_Score": {
      "change": 0.39919056054178426,
      "link": 0.328264082571463,
      "callout": 0.2943303266729291,
      "plan": 0.279280372810721,
      "context": 0.23611482023317495,
      "screenshots": 0.19622021778195275,
      "ticket": 0.19622021778195275,
      "test": 0.18741383554220772,
      "design": 0.16609151818171142,
      "describes": 0.1572920025528908
    }
  },
  "push-down": {
    "title": "Push-Down",
    "tags": [
      "database"
    ],
    "aliases": [
      "Query Pushdown"
    ],
    "outlinks": [
      "querying"
    ],
    "inlinks": [],
    "summary": "Query pushdown aims to execute as much work as possible in the source databases. Push-downs or query pushdowns push transformation logic to the source database....",
    "TFIDF_Score": {
      "query": 0.3545478105250585,
      "database": 0.3242219311015083,
      "logic": 0.31044162114484253,
      "transformation": 0.302456312051542,
      "pushdown": 0.2946424477999232,
      "source": 0.26920503376273636,
      "sql": 0.23712135101940623,
      "push": 0.2361882031087933,
      "semantic": 0.20199466195650356,
      "layer": 0.16626764910381187
    }
  },
  "pycaret": {
    "title": "PyCaret",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "preprocessing",
      "model_deployment",
      "model_selection",
      "pycaret_example.py",
      "hyperparameter_tuning",
      "ml_tools"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "PyCaret is an open-source, low-code Python library designed to simplify machine learning workflows. It allows users to build, evaluate, and deploy machine learning models with...",
    "TFIDF_Score": {
      "pycaret": 0.8072622468295287,
      "machine": 0.19695820766090774,
      "learning": 0.1767079299487922,
      "model": 0.17450769226033963,
      "deployment": 0.16736115833420204,
      "coding": 0.12619422984870676,
      "modular": 0.08762000560786098,
      "build": 0.08751055990235761,
      "apis": 0.07886622557469522,
      "end": 0.06918291574120199
    }
  },
  "pycaret_anomaly.ipynb": {
    "title": "Pycaret_Anomaly.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/Pycaret_Anomaly.ipynb",
    "TFIDF_Score": {
      "pycaret_anomaly": 0.4825632855336353,
      "preprocess": 0.3645707229911022,
      "ipynb": 0.3045780191530324,
      "outlier": 0.3003875700106718,
      "rhyslwells": 0.2570692984494287,
      "blob": 0.25376138843895474,
      "github": 0.24464876222599816,
      "exploration": 0.24276884124581516,
      "ml_tools": 0.2340307852642166,
      "com": 0.22924781333507738
    }
  },
  "pycaret_example.py": {
    "title": "Pycaret_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pycaret"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Pycaret/Pycaret_Example.py",
    "TFIDF_Score": {
      "pycaret_example": 0.4872996863314089,
      "pycaret": 0.44948200179813,
      "deployment": 0.31683361609807587,
      "rhyslwells": 0.2595924560678384,
      "blob": 0.25625207863167687,
      "github": 0.24705001119648326,
      "exploration": 0.24515163862767506,
      "ml_tools": 0.23632781786337767,
      "com": 0.2314979006469776,
      "main": 0.2206444828062726
    }
  },
  "pydantic": {
    "title": "Pydantic",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "declarative",
      "debugging",
      "json",
      "pydantic.py",
      "pydantic_more.py",
      "data_validation",
      "fastapi",
      "object-oriented_programming",
      "type_checking",
      "ml_tools"
    ],
    "inlinks": [
      "fastapi",
      "pyright_vs_pydantic",
      "maintainable_code",
      "data_validation"
    ],
    "summary": "Pydantic is a Python library used for [[data validation]] and settings management using Python type annotations. It provides a way to define data models with...",
    "TFIDF_Score": {
      "pydantic": 0.6749337382833147,
      "oop": 0.30838830606977863,
      "data": 0.21069430329527675,
      "type": 0.1905471441840518,
      "validation": 0.13789617705239549,
      "validating": 0.1295292803399435,
      "error": 0.10456023101486388,
      "class": 0.10316722089390566,
      "defining": 0.10034671609362186,
      "model": 0.09946422188740722
    }
  },
  "pydantic.py": {
    "title": "Pydantic.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic.py Explanation: BaseModel: This is the base class for creating data models in Pydantic. You define your model by subclassing BaseModel and specifying fields with...",
    "TFIDF_Score": {
      "pydantic": 0.35998731280718405,
      "constraint": 0.2940890912002738,
      "validator": 0.29241620175261507,
      "basemodel": 0.2754486101803818,
      "list": 0.25896153544819406,
      "optional": 0.21408646583558327,
      "field": 0.1835008482763704,
      "used": 0.17585250928736548,
      "raised": 0.14620810087630753,
      "subclassing": 0.14620810087630753
    }
  },
  "pydantic_more.py": {
    "title": "Pydantic_More.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic_More.py Key Features Demonstrated in the Script: Nested Models: Use of the Friend model inside the User model. Custom Validators: Validating age and email fields...",
    "TFIDF_Score": {
      "parsing": 0.3181101649615399,
      "model": 0.2940838296863546,
      "field": 0.21192150897966738,
      "adminuser": 0.1688528508385563,
      "created_at": 0.1688528508385563,
      "parse_raw": 0.1688528508385563,
      "stripping": 0.1688528508385563,
      "validators": 0.1688528508385563,
      "alias": 0.15905508248076994,
      "friend": 0.15905508248076994
    }
  },
  "pyright_vs_pydantic": {
    "title": "Pyright vs Pydantic",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fastapi",
      "pyright",
      "data_validation",
      "pydantic"
    ],
    "inlinks": [],
    "summary": "While [[Pyright]] and [[Pydantic]] serve different roles in Python development, they complement each other well. Pyright helps ensure that the code adheres to ==type constraints...",
    "TFIDF_Score": {
      "pyright": 0.47200453907280443,
      "pydantic": 0.4300411327804195,
      "runtime": 0.3524537516101571,
      "type": 0.29732845173532774,
      "code": 0.17471837023039016,
      "error": 0.1631550645057618,
      "adheres": 0.134858439735087,
      "hint": 0.1300776783080016,
      "validation": 0.12910334709694687,
      "data": 0.11506807902985351
    }
  },
  "pyright": {
    "title": "Pyright",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [
      "functional_programming",
      "documentation_&_meetings",
      "maintainable_code",
      "debugging",
      "type_checking"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "maintainable_code"
    ],
    "summary": "Pyright is a ==static type checker== for Python that enhances code reliability by enforcing type constraints ==at compile-time.== It utilizes type hints to identify potential...",
    "TFIDF_Score": {
      "pyright": 0.5061310205371657,
      "type": 0.4463559062588906,
      "checker": 0.21170514795025458,
      "code": 0.209832798789625,
      "enforcing": 0.2024524082148663,
      "hint": 0.19527542569960468,
      "compile": 0.18015866427253435,
      "runtime": 0.17637039980097907,
      "constraint": 0.15068793781494086,
      "static": 0.1491186961626572
    }
  },
  "pyspark": {
    "title": "PySpark",
    "tags": [
      "#data_orchestration",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "directed_acyclic_graph_(dag)",
      "pandas",
      "sql",
      "apache_spark"
    ],
    "inlinks": [
      "fabric"
    ],
    "summary": "Python API for [[Apache Spark]], a ==distributed processing framework== for big data analysis and machine learning on clusters. Part of [[Apache Spark]] [[Directed Acyclic Graph...",
    "TFIDF_Score": {
      "spark": 0.38286954853571115,
      "apache": 0.3406874013533704,
      "interlinked": 0.2759466130443241,
      "querier": 0.2759466130443241,
      "acyclic": 0.2397620240230156,
      "dag": 0.2397620240230156,
      "directed": 0.2397620240230156,
      "tutorial": 0.19910205206972922,
      "big": 0.18704244140330115,
      "local": 0.1778169225966372
    }
  },
  "pytest": {
    "title": "Pytest",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "testing_unittest.py",
      "testing_pytest.py",
      "toml"
    ],
    "summary": "@pytest.fixture Explanation @pytest.fixture is a decorator in pytest used to define reusable test setup functions. It allows tests to use shared resources without redundant code....",
    "TFIDF_Score": {
      "fixture": 0.5147973105278524,
      "pytest": 0.46902942714617324,
      "sample_data": 0.46142163237562,
      "test": 0.1949420152221787,
      "test_example": 0.1632817110693366,
      "assert": 0.15380721079187334,
      "doe": 0.1308881806608648,
      "john": 0.12813594531017422,
      "age": 0.11954332300641235,
      "code": 0.11433518915445125
    }
  },
  "python_click": {
    "title": "Python Click",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "click_implementation.py",
      "ml_tools"
    ],
    "inlinks": [],
    "summary": "Python Click, or \"Command Line Interface Creation Kit,\" is a library for building command-line interfaces (CLIs). It supports arbitrary nesting of commands, automatic help page...",
    "TFIDF_Score": {
      "click": 0.6762373797628536,
      "cli": 0.27272860396993015,
      "ctx": 0.25871557105051374,
      "command": 0.22653899575265116,
      "pprint": 0.176562263579965,
      "pass_context": 0.13732620500663942,
      "def": 0.13297514518540107,
      "download": 0.13226935136869541,
      "key": 0.13024370389112266,
      "paragraph": 0.12935778552525687
    }
  },
  "python": {
    "title": "Python",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "immutable_vs_mutable"
    ],
    "inlinks": [
      "testing",
      "langchain",
      "tool.ruff",
      "duckdb_vs_sqlite",
      "immutable_vs_mutable"
    ],
    "summary": "dynamic language lower learning, support object orientated [[Immutable vs mutable]]",
    "TFIDF_Score": {
      "orientated": 0.47158454190720445,
      "mutable": 0.4509735691852516,
      "immutable": 0.42192403470540746,
      "dynamic": 0.3115588998604098,
      "lower": 0.29355348582196933,
      "object": 0.28082352365910496,
      "language": 0.232000978845647,
      "support": 0.2189385568404175,
      "learning": 0.179855425970196
    }
  },
  "pytorch_vs_tensorflow": {
    "title": "Pytorch vs Tensorflow",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "tensorflow",
      "pytorch",
      "keras"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "[[Tensorflow]] is widely adopted but pytorch picking up Dynamic vs static graph Tensorboard is better than [[pytorch]] visualization Plain tensorflow looks pretty much like a...",
    "TFIDF_Score": {
      "pytorch": 0.4964327530100878,
      "tensorflow": 0.3175226837704906,
      "contrib": 0.22602235268854648,
      "pretty": 0.22602235268854648,
      "rescue": 0.22602235268854648,
      "tensorboard": 0.22602235268854648,
      "picking": 0.21290729632836922,
      "better": 0.2079794511930669,
      "adopted": 0.20360201575411274,
      "plain": 0.20360201575411274
    }
  },
  "pytorch": {
    "title": "PyTorch",
    "tags": [
      "software"
    ],
    "aliases": [
      "PyTorch"
    ],
    "outlinks": [
      "parallelism",
      "deep_learning",
      "stochastic_gradient_descent",
      "use_cases_for_a_simple_neural_network_like",
      "loss_function",
      "neural_network",
      "pytorch",
      "gradient_descent",
      "numpy",
      "1.0,_2.0",
      "ordinary_least_squares",
      "linear_regression",
      "tensorflow"
    ],
    "inlinks": [
      "pytorch_vs_tensorflow",
      "deep_learning",
      "recurrent_neural_networks",
      "vector_embedding",
      "pytorch",
      "edge_machine_learning_models",
      "lstm",
      "transfer_learning"
    ],
    "summary": "Text Generation With LSTM Recurrent Neural Networks in Python with Keras want for [[PyTorch]] Open-source [[Deep Learning]] framework with dynamic computational graphs, emphasizing flexibility and...",
    "TFIDF_Score": {
      "torch": 0.41379417587233996,
      "tensor": 0.4093541135985289,
      "partial": 0.2748533407483115,
      "gradient": 0.26819164850468596,
      "derivative": 0.2615190544232838,
      "gpu": 0.20467705679926446,
      "element": 0.1591942604665356,
      "loss": 0.14334006433069185,
      "pas": 0.1351632261745291,
      "backward": 0.1307595272116419
    }
  },
  "q-learning": {
    "title": "Q-Learning",
    "tags": [
      "regressor",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "exploration_vs._exploitation",
      "bellman_equations",
      "policy",
      "pasted_image_20250220133556.png",
      "reinforcement_learning"
    ],
    "inlinks": [
      "deep_q-learning",
      "reinforcement_learning",
      "policy"
    ],
    "summary": "Q-learning is a value-based, model-free [[Reinforcement learning]] algorithm where the agent learns the optimal [[policy]] by updating Q-values based on the rewards received. It is...",
    "TFIDF_Score": {
      "a_t": 0.43810098822087046,
      "s_t": 0.43810098822087046,
      "action": 0.27458553162119165,
      "state": 0.260884427494508,
      "reward": 0.2433196899676557,
      "learning": 0.19493283811153583,
      "max_": 0.1550293136660721,
      "value": 0.1401913182201734,
      "gamma": 0.13470048288473074,
      "received": 0.12165984498382786
    }
  },
  "quartz": {
    "title": "Quartz",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "javascript",
      "jinja_template",
      "vim"
    ],
    "inlinks": [
      "jinja_template"
    ],
    "summary": "[[Vim]]: telescope? Search preview feature? https://www.youtube.com/watch?v=v5LGaczJaf0 How does quartz work of a software level: - Transforming text. Think [[jinja template]]. - Manipulating markdown notes -...",
    "TFIDF_Score": {
      "markdown": 0.39014046521609247,
      "existed": 0.21655106286776538,
      "telescope": 0.21655106286776538,
      "v5lgaczjaf0": 0.21655106286776538,
      "vim": 0.21655106286776538,
      "manipulating": 0.20398558268147338,
      "preview": 0.20398558268147338,
      "quartz": 0.20398558268147338,
      "jinja": 0.19507023260804623,
      "go": 0.18815494984593606
    }
  },
  "query_gsheets": {
    "title": "QUERY GSheets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/gsheets"
    ],
    "inlinks": [
      "gsheets"
    ],
    "summary": "In [[standardised/GSheets]] I want to use query, but I also want to remove certain rows based on a range of keys , can I do...",
    "TFIDF_Score": {
      "filter": 0.43592649377351006,
      "query": 0.3950487611183512,
      "x10": 0.35175026527007114,
      "match": 0.2384237576048995,
      "col3": 0.23450017684671412,
      "col4": 0.23450017684671412,
      "col2": 0.22089319063834503,
      "isna": 0.22089319063834503,
      "col1": 0.21123888028224364,
      "remove": 0.14715662509266586
    }
  },
  "query_optimisation": {
    "title": "Query Optimisation",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "database_index",
      "queries",
      "querying",
      "transaction",
      "query_optimisation"
    ],
    "inlinks": [
      "query_optimisation"
    ],
    "summary": "[[Querying]] can be optimised for time, ==space efficiency==, and concurrency of queries. Optimizing SQL [[Querying]]: - Timing queries - [[Database Index|Indexing]] - Managing [[Transaction]] -...",
    "TFIDF_Score": {
      "index": 0.45685635207502523,
      "query": 0.4218954490689784,
      "transaction": 0.261197527702069,
      "space": 0.20088643836203712,
      "timing": 0.18348137896185512,
      "vacuum": 0.18348137896185512,
      "search": 0.18182929515576177,
      "querying": 0.1762459720415419,
      "database": 0.17147069297776465,
      "concurrency": 0.1561405590402449
    }
  },
  "query_plan": {
    "title": "Query Plan",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "What is expected to happen to the query plan if there is [[Database Index|Indexing]]?",
    "TFIDF_Score": {
      "happen": 0.4795532789959025,
      "indexing": 0.4585384884401759,
      "plan": 0.41582349763215987,
      "expected": 0.35246618688339293,
      "index": 0.34261722658983607,
      "query": 0.2812430802896106,
      "database": 0.2571872449738041
    }
  },
  "querying": {
    "title": "Querying",
    "tags": [
      "database",
      "data_analysis",
      "data_exploration"
    ],
    "aliases": [
      "Queries",
      "Query"
    ],
    "outlinks": [
      "sql_injection",
      "sql_joins",
      "de_tools"
    ],
    "inlinks": [
      "sql_groupby",
      "database_index",
      "sqlite",
      "common_table_expression",
      "soft_deletion",
      "sql",
      "query_optimisation",
      "database_techniques",
      "sql_window_functions",
      "duckdb",
      "views",
      "columnar_storage",
      "data_storage",
      "data_warehouse",
      "push-down"
    ],
    "summary": "Querying is the process of asking questions of data. Querying makes use of keys primary and foreign within tables. Useful Links - CS50 SQL Course...",
    "TFIDF_Score": {
      "querying": 0.4719251321098161,
      "sql": 0.3357933423705116,
      "de_tools": 0.30248042793517316,
      "sqlite": 0.20981871473097047,
      "ipynb": 0.1860550372839575,
      "rhyslwells": 0.15703378083740835,
      "cs50": 0.15646889017841115,
      "parameterised_queries": 0.15646889017841115,
      "blob": 0.15501310540573354,
      "query": 0.15062514676286495
    }
  },
  "quicksort": {
    "title": "QuickSort",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "recursive_algorithm"
    ],
    "inlinks": [],
    "summary": "[[Recursive Algorithm]] Quicksort Algorithm in Five Lines of Code! - Computerphile Fast algorithm (compared to say Insertion Sort) 1) Pick pivot value 2) Divide remaining...",
    "TFIDF_Score": {
      "arr": 0.5605196307881686,
      "pivot": 0.43986991736908454,
      "indent": 0.35199680536178557,
      "print": 0.24993986189308295,
      "middle": 0.2029247005724528,
      "quick_sort": 0.18683987692938953,
      "sorted_array": 0.14012990769704214,
      "depth": 0.13480942017984404,
      "left": 0.12664401402437928,
      "sorted": 0.12622969071501625
    }
  },
  "r_squared": {
    "title": "R squared",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "adjusted_r_squared",
      "regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression"
    ],
    "inlinks": [
      "regression_metrics",
      "linear_regression",
      "adjusted_r_squared"
    ],
    "summary": "R², or the coefficient of determination, ==measures the proportion of variance in the dependent variable that is explained by the independent variables== in a [[regression]]...",
    "TFIDF_Score": {
      "y_i": 0.3312144596669658,
      "value": 0.30533257194576724,
      "hat": 0.2534634121176407,
      "variable": 0.2509850818161195,
      "model": 0.24502892068587928,
      "bar": 0.2002401958355017,
      "variability": 0.2002401958355017,
      "explained": 0.19289864351803987,
      "regression": 0.1901564512763145,
      "actual": 0.16849133923845183
    }
  },
  "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression": {
    "title": "R-squared metric not always a good indicator of model performance in regression",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cross_validation",
      "adjusted_r_squared"
    ],
    "inlinks": [
      "r_squared"
    ],
    "summary": "R-squared (R²) is a commonly used metric for assessing the performance of regression models, but it is not always a reliable indicator of model quality....",
    "TFIDF_Score": {
      "model": 0.44322923020607163,
      "predictor": 0.31046705230217275,
      "adjusted": 0.23285028922662956,
      "assumption": 0.20709451113111763,
      "performance": 0.1506505876224017,
      "relationship": 0.14741623842722001,
      "dependent": 0.14625310017264434,
      "high": 0.14401813442136444,
      "still": 0.13465418826829592,
      "squared": 0.1270491607412903
    }
  },
  "r": {
    "title": "R",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "race_conditions": {
    "title": "Race Conditions",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "rag": {
    "title": "RAG",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "llm",
      "rag",
      "transformer",
      "bert",
      "prompting",
      "pasted_image_20240928194559.png",
      "pasted_image_20241017165540.png",
      "generative",
      "nlp",
      "hallucinating"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "accessing_gen_ai_generated_content",
      "knowledge_graphs_with_obsidian",
      "rag",
      "generative_ai_from_theory_to_practice",
      "knowledge_graph_vs_rag_setup",
      "relationships_in_memory",
      "graphrag",
      "small_language_models"
    ],
    "summary": "Rag is a framework the help [[LLM]] be more up to date. RAG grounds the Gen AI in external data. [!Summary] Given a question sometimes...",
    "TFIDF_Score": {
      "rag": 0.4053652285239971,
      "llm": 0.2711195976434526,
      "external": 0.2558158665653611,
      "document": 0.20333969823258946,
      "knowledge": 0.20041513241113834,
      "generative": 0.19731458975232394,
      "date": 0.1883441145023674,
      "retrieval": 0.17072606106575441,
      "capital": 0.16854546805669324,
      "france": 0.16854546805669324
    }
  },
  "random_forest_regression": {
    "title": "Random Forest Regression",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Random Forest Regression : Like random forests for classification, random forest regression combines multiple regression trees to improve prediction accuracy.",
    "TFIDF_Score": {
      "forest": 0.6047407145193653,
      "random": 0.5237872976941014,
      "regression": 0.43047302833531664,
      "combine": 0.1808205984569979,
      "tree": 0.17269640993092622,
      "classification": 0.15035381127794656,
      "accuracy": 0.14549475837461917,
      "prediction": 0.14252122929656627,
      "improve": 0.14204396894136523,
      "multiple": 0.13421138695998494
    }
  },
  "random_forests": {
    "title": "Random Forests",
    "tags": [
      "classifier",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240128194716.png",
      "model_ensemble",
      "decision_tree",
      "random_forests",
      "bagging",
      "pasted_image_20240118145117.png",
      "hyperparameter"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "ada_boosting",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "regularisation_of_tree_based_models",
      "machine_learning_algorithms",
      "why_and_when_is_feature_scaling_necessary",
      "bagging",
      "random_forests",
      "feature_importance",
      "hyperparameter_tuning",
      "embedded_methods",
      "imbalanced_datasets"
    ],
    "summary": "A random forest is an [[Model Ensemble]] of [[Decision Tree]]s. Take many decision trees decisions to get better result. What is the Random Forest method;;...",
    "TFIDF_Score": {
      "tree": 0.5181943273291497,
      "forest": 0.41385389830742825,
      "random": 0.3308801375482345,
      "randomly": 0.2023404445552754,
      "decision": 0.19989945929013408,
      "number": 0.10962972666242721,
      "feature": 0.10246697162570849,
      "n_jobs": 0.10059472797223928,
      "susceptible": 0.10059472797223928,
      "data": 0.09940932030208185
    }
  },
  "react": {
    "title": "React",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "javascript",
      "dashboarding"
    ],
    "inlinks": [],
    "summary": "React is a [[JavaScript]] library developed by Meta for building user interfaces, particularly in web development. Related to: - [[Dashboarding]] Core Concepts React's component-based architecture...",
    "TFIDF_Score": {
      "react": 0.39375445439374435,
      "dom": 0.31984511087106915,
      "interface": 0.19744253871444015,
      "library": 0.1827228398805353,
      "update": 0.1626627991577419,
      "cs": 0.15992255543553457,
      "dashboarding": 0.15992255543553457,
      "parent": 0.15992255543553457,
      "shadcn": 0.15992255543553457,
      "tailwind": 0.15992255543553457
    }
  },
  "reasoning_tokens": {
    "title": "Reasoning tokens",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "mathematical_reasoning_in_transformers"
    ],
    "inlinks": [],
    "summary": "Transformers rely on pattern recognition and language-based reasoning. Thus, reasoning tokens serve as a mechanism for token-based logical progression, allowing models like ChatGPT to simulate...",
    "TFIDF_Score": {
      "token": 0.502722474404358,
      "reasoning": 0.42531256634237125,
      "logical": 0.3001605876507492,
      "mathematical": 0.21265628317118562,
      "continuity": 0.19012216181251196,
      "model": 0.1757625489925233,
      "chatgpt": 0.17536742229617924,
      "transformer": 0.13391629422147655,
      "recognition": 0.13257260533616133,
      "step": 0.1323996061096584
    }
  },
  "recall": {
    "title": "Recall",
    "tags": [
      "evaluation"
    ],
    "aliases": [
      "sensitivity"
    ],
    "outlinks": [
      "classification",
      "pasted_image_20241222091831.png",
      "evaluation_metrics"
    ],
    "inlinks": [
      "classification_report",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "precision-recall_curve",
      "evaluation_metrics",
      "f1_score",
      "model_observability",
      "precision_or_recall",
      "confusion_matrix",
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "Recall Score is a [[Evaluation Metrics]] used to evaluate the performance of a [[Classification]] model, focusing on the model's ability to identify all relevant instances...",
    "TFIDF_Score": {
      "recall": 0.4584580047163095,
      "positive": 0.4357383795887146,
      "instance": 0.2600186539182418,
      "email": 0.23602683085314655,
      "actual": 0.21301713033925623,
      "model": 0.2065204370762482,
      "spam": 0.1792969242766732,
      "missing": 0.15735122056876436,
      "identifying": 0.13548256281533183,
      "relevant": 0.12723727845062335
    }
  },
  "recommender_systems": {
    "title": "Recommender systems",
    "tags": [
      "evaluation",
      "model_algorithm"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf",
      "graph_neural_network",
      "k-nearest_neighbours"
    ],
    "summary": "Crab on Python A recommender system, or recommendation system, is a type of information filtering system that aims to predict the preferences or interests of...",
    "TFIDF_Score": {
      "item": 0.4095667515595103,
      "user": 0.3350026619505,
      "recommendation": 0.27169719378502727,
      "preference": 0.26537731859270847,
      "recommender": 0.26537731859270847,
      "system": 0.23699538987225163,
      "filtering": 0.22711120131376575,
      "content": 0.17909163774907913,
      "based": 0.16275313694619634,
      "similar": 0.15129302005205497
    }
  },
  "recurrent_neural_networks": {
    "title": "Recurrent Neural Networks",
    "tags": [
      "deep_learning",
      "time_series"
    ],
    "aliases": [
      "RNN",
      "RNNs"
    ],
    "outlinks": [
      "neural_network",
      "use_of_rnns_in_energy_sector",
      "pytorch",
      "transformer",
      "gated_recurrent_unit",
      "pasted_image_20241219073017.png",
      "time_series",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation",
      "gru",
      "lstm",
      "pasted_image_20241219073440.png",
      "nlp"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "reward_function",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "vanishing_and_exploding_gradients_problem",
      "feed_forward_neural_network",
      "lstm"
    ],
    "summary": "Recurrent Neural Networks (RNNs) are a type of [[neural network]] designed to process sequential data by maintaining a memory of previous inputs through hidden states....",
    "TFIDF_Score": {
      "rnns": 0.4897040396689064,
      "sequence": 0.2307695635424788,
      "long": 0.22575011075790793,
      "hidden": 0.2050496620172066,
      "variant": 0.17392794944144943,
      "lstm": 0.16699917997539557,
      "recurrent": 0.1613379631117339,
      "network": 0.15306792050096726,
      "state": 0.1521351541099483,
      "gru": 0.14658800492254392
    }
  },
  "recursive_algorithm": {
    "title": "Recursive Algorithm",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "common_table_expression",
      "quicksort",
      "algorithms"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "regression_metrics": {
    "title": "Regression Metrics",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "interpretability",
      "standardised/outliers",
      "variance",
      "adjusted_r_squared",
      "r_squared"
    ],
    "inlinks": [
      "gini_impurity",
      "metric",
      "adjusted_r_squared",
      "model_evaluation"
    ],
    "summary": "These metrics provide various ways to evaluate the performance of regression models. Evaluating Regression Models These metric provide: Comprehensive Evaluation: Each metric provides a different...",
    "TFIDF_Score": {
      "error": 0.3106538198050013,
      "hat": 0.3020893006694964,
      "rmse": 0.2549576473020193,
      "mse": 0.23865543440957915,
      "mae": 0.2185351262588737,
      "text": 0.2099145084917287,
      "variance": 0.20917203803324255,
      "y_i": 0.19737827968239335,
      "value": 0.19711755940780598,
      "metric": 0.19641587992061812
    }
  },
  "regression": {
    "title": "Regression",
    "tags": [
      "statistics",
      "regressor"
    ],
    "aliases": [
      "regressive models",
      "predictive regression",
      "linear models"
    ],
    "outlinks": [
      "logistic_regression",
      "ridge",
      "linearity",
      "lasso",
      "multicollinearity",
      "polynomial_regression",
      "feature_engineering",
      "linear_regression",
      "supervised_learning",
      "regression"
    ],
    "inlinks": [
      "supervised_learning",
      "parametric_vs_non-parametric_models",
      "logistic_regression",
      "ds_&_ml_portal",
      "use_cases_for_a_simple_neural_network_like",
      "loss_function",
      "machine_learning_algorithms",
      "typical_output_formats_in_neural_networks",
      "classification",
      "regression",
      "k-nearest_neighbours",
      "energy",
      "r_squared",
      "learning_styles",
      "encoding_categorical_variables",
      "imbalanced_datasets"
    ],
    "summary": "[!Summary] [[Regression]] analysis is a statistical method used to ==predict== a continuous variable based on one or more predictor variables. The most common form, [[Linear...",
    "TFIDF_Score": {
      "regression": 0.4488354073649543,
      "linear": 0.335026221650725,
      "non": 0.23949044545376125,
      "variable": 0.22215455487447341,
      "polynomial": 0.1854814349713151,
      "linearity": 0.1799115218387263,
      "claim": 0.13405808570790478,
      "regressor": 0.13405808570790478,
      "logarithmic": 0.12819897180118445,
      "predict": 0.12603399120957764
    }
  },
  "regression_logistic_metrics.ipynb": {
    "title": "Regression_Logistic_Metrics.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Regression_Logistic_Metrics.ipynb",
    "TFIDF_Score": {
      "regression_logistic_metrics": 0.4547006053380129,
      "model_evaluation": 0.4348275583221475,
      "ipynb": 0.2869920150025248,
      "selection": 0.2631704708282764,
      "rhyslwells": 0.24222639625290385,
      "blob": 0.23910948137509067,
      "github": 0.23052300830624053,
      "exploration": 0.22875163192245376,
      "classification": 0.22788269098335107,
      "ml_tools": 0.22051810180646783
    }
  },
  "regularisation_of_tree_based_models": {
    "title": "Regularisation of Tree based models",
    "tags": [
      "ml_process",
      "ml_optimisation",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "regularisation",
      "model_ensemble",
      "decision_tree",
      "random_forests",
      "hyperparameters"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "Tree models, such as Random Forests and Gradient Boosting, can also be regularized, although they don’t use L1 or L2 regularization directly. Instead, they are...",
    "TFIDF_Score": {
      "tree": 0.5275196871816462,
      "model": 0.25818932081056856,
      "leaf": 0.17825005583363177,
      "regularization": 0.17592812434858374,
      "decisiontreeclassifier": 0.16677388650239572,
      "depth": 0.16044177370853752,
      "split": 0.1597995652712233,
      "x_train": 0.15426327561039696,
      "y_train": 0.15244436130985514,
      "random": 0.1523775638525917
    }
  },
  "regularisation": {
    "title": "Regularisation",
    "tags": [
      "deleted",
      "ml_process",
      "data_visualization",
      "statistics",
      "ml_optimisation",
      "model_explainability"
    ],
    "aliases": [
      "Regulation in ML",
      "Regularisation techniques"
    ],
    "outlinks": [
      "feature_selection",
      "ridge",
      "when_and_why_not_to_us_regularisation",
      "lasso",
      "loss_function",
      "interpretability",
      "neural_network",
      "regularisation_of_tree_based_models",
      "model_parameters_tuning",
      "model_selection",
      "regularisation.py",
      "ml_tools"
    ],
    "inlinks": [
      "orthogonalization",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "overfitting",
      "lightgbm_vs_xgboost_vs_catboost",
      "regularisation_of_tree_based_models",
      "backpropagation",
      "bias_and_variance",
      "explain_the_curse_of_dimensionality",
      "xgboost",
      "dropout",
      "hyperparameter_tuning",
      "embedded_methods"
    ],
    "summary": "Regularization is a technique in machine learning that reduces the risk of overfitting by adding a penalty to the [[Loss function]] during model training. This...",
    "TFIDF_Score": {
      "regularization": 0.43798429543699835,
      "lambda": 0.2853653160160563,
      "model": 0.25565065707781426,
      "coefficient": 0.2260224706821844,
      "l_2": 0.21863651677647966,
      "penalty": 0.18155910772339906,
      "sparsity": 0.1700040693287217,
      "overfitting": 0.1696482594365073,
      "zero": 0.16695798868930373,
      "loss": 0.16535801579822876
    }
  },
  "regularisation.py": {
    "title": "Regularisation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "regularisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Regularisation/Regularisation.py",
    "TFIDF_Score": {
      "regularisation": 0.6937435958060042,
      "optimisation": 0.315332266215025,
      "rhyslwells": 0.2478053685718295,
      "blob": 0.24461666473091084,
      "github": 0.23583242751946787,
      "exploration": 0.23402025269271856,
      "ml_tools": 0.22559708743657156,
      "com": 0.2209864780447922,
      "main": 0.21062587185068493,
      "http": 0.20872429670093814
    }
  },
  "reinforcement_learning": {
    "title": "Reinforcement learning",
    "tags": [
      "field",
      "reinforcement_learning"
    ],
    "aliases": null,
    "outlinks": [
      "exploration_vs._exploitation",
      "markov_decision_processes",
      "sarsa",
      "bellman_equations",
      "q-learning",
      "policy",
      "deep_q-learning",
      "reinforcement_learning",
      "multi-agent_reinforcement_learning"
    ],
    "inlinks": [
      "industries_of_interest",
      "demand_forecasting",
      "exploration_vs._exploitation",
      "ds_&_ml_portal",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "llm",
      "sarsa",
      "q-learning",
      "policy",
      "deep_q-learning",
      "reinforcement_learning",
      "cost_function"
    ],
    "summary": "Reinforcement Learning ( [[Reinforcement learning|RL]]) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent...",
    "TFIDF_Score": {
      "action": 0.4355873261688608,
      "agent": 0.43239279149492577,
      "reward": 0.4086941519731759,
      "environment": 0.20047655706828663,
      "learning": 0.18709756105947628,
      "print": 0.15481692180217219,
      "reinforcement": 0.15442599696247347,
      "info": 0.13915750148280606,
      "next_state": 0.13019820526682643,
      "policy": 0.12354079756997878
    }
  },
  "relating_tables_together": {
    "title": "Relating Tables Together",
    "tags": [
      "data_integrity",
      "database_design"
    ],
    "aliases": [],
    "outlinks": [
      "primary_key",
      "er_diagrams",
      "junction_tables",
      "data_integrity",
      "many-to-many_relationships",
      "foreign_key"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "Implementing these concepts, database tables can be effectively related, ensuring [[Data Integrity]], efficient retrieval, and easy maintenance. Resources: - LINK Notes on Relating Database Tables...",
    "TFIDF_Score": {
      "table": 0.47344138634340316,
      "record": 0.3545388810506532,
      "many": 0.2823706829995245,
      "one": 0.21765169223358874,
      "relationship": 0.20846110872170145,
      "department": 0.2064292167790705,
      "employee": 0.20324920932678323,
      "foreign": 0.20030498229383586,
      "key": 0.1843072602623046,
      "cascade": 0.17433676314167307
    }
  },
  "relational_database": {
    "title": "Relational Database",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fabric"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "relationships_in_memory": {
    "title": "Relationships in memory",
    "tags": [
      "memory_management",
      "language_models"
    ],
    "aliases": [
      "Managing LLM memory"
    ],
    "outlinks": [
      "graphrag",
      "vector_database",
      "semantic_relationships",
      "rag"
    ],
    "inlinks": [
      "llm"
    ],
    "summary": "In managing the memory of a large language model (LLM), several key concepts and techniques play a crucial role in forming and maintaining relationships between...",
    "TFIDF_Score": {
      "memory": 0.5027472919312765,
      "llm": 0.2840408038253452,
      "relationship": 0.21214892891493958,
      "correlating": 0.2018035715352483,
      "point": 0.1776502853676242,
      "information": 0.16664013092538357,
      "piece": 0.16562403176580415,
      "metadata": 0.13678672221208105,
      "enhancing": 0.13255284369056464,
      "management": 0.12849773856149402
    }
  },
  "requirements.txt": {
    "title": "requirements.txt",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dependency_manager"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "rest_api": {
    "title": "REST API",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "crud",
      "json"
    ],
    "inlinks": [
      "api"
    ],
    "summary": "REST API REST stands for Representational State Transfer. It is a ==standardized== software architecture style used for API communication between a client and a server....",
    "TFIDF_Score": {
      "flavor": 0.6804484657574876,
      "api": 0.34936686113781473,
      "endpoint": 0.2687372610914243,
      "rest": 0.20853377428401496,
      "body": 0.1564003307130112,
      "response": 0.1396287942556408,
      "operation": 0.13275369968668665,
      "confirmation": 0.11144567830817696,
      "get": 0.10472159569173059,
      "put": 0.09917345375135615
    }
  },
  "reverse_etl": {
    "title": "reverse etl",
    "tags": [
      "data_transformation"
    ],
    "aliases": [
      "Data Activation",
      "Operational Analytics"
    ],
    "outlinks": [
      "etl"
    ],
    "inlinks": [],
    "summary": "Reverse [[ETL]] is the flip side of the ETL/ELT. With Reverse ETL, the data warehouse becomes the source rather than the destination. Data is taken...",
    "TFIDF_Score": {
      "reverse": 0.5288069530961222,
      "etl": 0.5197443072122729,
      "warehouse": 0.22193565051257066,
      "destination": 0.18405362532802855,
      "data": 0.1603845185305031,
      "beauchemin": 0.12172276927988902,
      "formerly": 0.12172276927988902,
      "maxime": 0.12172276927988902,
      "salesforce": 0.12172276927988902,
      "crm": 0.11465974670520411
    }
  },
  "reward_function": {
    "title": "Reward Function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "recurrent_neural_networks"
    ],
    "inlinks": [
      "cost_function"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]]",
    "TFIDF_Score": {
      "rnn": 0.637920026191777,
      "recurrent": 0.5628367660570796,
      "neural": 0.38670834376506424,
      "network": 0.3559908310903127
    }
  },
  "ridge": {
    "title": "Ridge",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "L2"
    ],
    "outlinks": [
      "feature_selection",
      "logistic_regression",
      "overfitting",
      "ridge",
      "loss_function",
      "neural_network",
      "linear_regression",
      "multicollinearity"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "regularisation",
      "ridge",
      "optimising_a_logistic_regression_model",
      "feed_forward_neural_network",
      "elastic_net",
      "embedded_methods",
      "regression"
    ],
    "summary": "L2 Regularization, also known as Ridge Regularization, adds a penalty term proportional to the square of the weights to the [[loss function]]. This technique enhances...",
    "TFIDF_Score": {
      "ridge": 0.4644167146150108,
      "regularization": 0.37281540595171847,
      "lambda": 0.34255836827527414,
      "coefficient": 0.32558640201142863,
      "penalty": 0.22884463390053333,
      "model": 0.1709806149649274,
      "loss": 0.14291938649727698,
      "weight": 0.13665865086452278,
      "regression": 0.12248375311187826,
      "penalizing": 0.11455845240741157
    }
  },
  "roc_(receiver_operating_characteristic)": {
    "title": "ROC (Receiver Operating Characteristic)",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "logistic_regression",
      "ml_tools",
      "recall",
      "roc_curve.py",
      "specificity"
    ],
    "inlinks": [
      "precision-recall_curve",
      "auc",
      "determining_threshold_values",
      "model_observability",
      "choosing_a_threshold"
    ],
    "summary": "ROC (Receiver Operating Characteristic) is a graphical representation of a classifier's performance across different thresholds, showing the trade-off between sensitivity (true positive rate) and specificity...",
    "TFIDF_Score": {
      "threshold": 0.34980516854120175,
      "plt": 0.34492066402863447,
      "roc": 0.310428597625771,
      "fpr": 0.28257913124903683,
      "tpr": 0.28257913124903683,
      "curve": 0.223242084952564,
      "positive": 0.2195713269024437,
      "specificity": 0.19468688480926422,
      "y_probs": 0.17925503152112146,
      "roc_curve": 0.16885367159684708
    }
  },
  "roc_curve.py": {
    "title": "ROC_Curve.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/ROC_Curve.py Overview This script demonstrates how to compute and interpret Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC) scores using Random...",
    "TFIDF_Score": {
      "auroc": 0.49782701502404436,
      "roc": 0.2873740666630948,
      "bayes": 0.23833606158894355,
      "random": 0.2274266953905933,
      "classifier": 0.2066626151515381,
      "curve": 0.2066626151515381,
      "naive": 0.1995314473788694,
      "fpr": 0.1868520645378715,
      "tpr": 0.1868520645378715,
      "model": 0.18063419615132845
    }
  },
  "rollup": {
    "title": "rollup",
    "tags": [
      "database"
    ],
    "aliases": null,
    "outlinks": [
      "database",
      "granularity"
    ],
    "inlinks": [
      "business_intelligence"
    ],
    "summary": "Rollup refers to aggregating data to a higher level of [[granularity]], such as summarizing hourly data into daily totals. [[Database]]",
    "TFIDF_Score": {
      "hourly": 0.38963635857991674,
      "rollup": 0.38963635857991674,
      "summarizing": 0.34860545643650553,
      "granularity": 0.32460391731185073,
      "daily": 0.31272519963185325,
      "aggregating": 0.3075745542930943,
      "total": 0.2595714760437846,
      "refers": 0.22429218967044415,
      "higher": 0.2230962888138505,
      "level": 0.20848338698751562
    }
  },
  "row-based_storage": {
    "title": "Row-based Storage",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "crud",
      "columnar_storage",
      "oltp"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Data is stored in consecutive rows, allows [[CRUD]] Row-based storage is well-suited for transactional systems ([[OLTP]]) Less efficient than [[Columnar Storage]] in largedatasets. Row-based Storage...",
    "TFIDF_Score": {
      "row": 0.5855382254072561,
      "2024": 0.31763250687342715,
      "storage": 0.2356391288930371,
      "transactional": 0.20073294603312136,
      "record": 0.1750748116371783,
      "order_amount": 0.1721784413490214,
      "together": 0.16538875460743416,
      "inserting": 0.16465324736450074,
      "customer_id": 0.15881625343671357,
      "stored": 0.15566439886013245
    }
  },
  "sarsa": {
    "title": "Sarsa",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "policy"
    ],
    "summary": "SARSA stands for State-Action-Reward-State-Action SARSA is another value-based [[Reinforcement learning]] algorithm, differing from Q-learning in that it updates the Q-values based on the action actually...",
    "TFIDF_Score": {
      "a_t": 0.4509613438343922,
      "s_t": 0.4509613438343922,
      "sarsa": 0.28750114945256416,
      "policy": 0.28391319869568343,
      "action": 0.28264592791775545,
      "state": 0.22378552581023523,
      "reward": 0.18784671340525824,
      "learning": 0.1433250339790727,
      "gamma": 0.1386545851528986,
      "taken": 0.12792098527971182
    }
  },
  "scala": {
    "title": "Scala",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "java",
      "big_data",
      "lambdas",
      "apache_spark"
    ],
    "inlinks": [
      "fabric",
      "big_data"
    ],
    "summary": "[!Summary] Scala is a functional programming language primarily used for [[big data]] processing, particularly with frameworks like [[Apache Spark]]. It is known for its concise...",
    "TFIDF_Score": {
      "scala": 0.5601310792483353,
      "java": 0.28859407576861057,
      "programming": 0.17421883136699648,
      "code": 0.1641511842148754,
      "type": 0.1629516316846538,
      "big": 0.15889738243635584,
      "data": 0.14157080848570336,
      "functional": 0.13532273348937007,
      "concise": 0.13292463241713173,
      "framework": 0.1220411121990025
    }
  },
  "scalability": {
    "title": "Scalability",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "performance_dimensions",
      "publish_and_subscribe",
      "spreadsheets_vs_databases",
      "distributed_computing",
      "model_deployment",
      "machine_learning_algorithms",
      "event_driven",
      "data_ingestion",
      "apache_kafka"
    ],
    "summary": "Scalability refers to the capability of a system, network, or process to handle a growing amount of work or its potential to accommodate growth. Key...",
    "TFIDF_Score": {
      "scaling": 0.3182776963467075,
      "adding": 0.25251041924596884,
      "server": 0.2379174974915883,
      "scalability": 0.22121283790581067,
      "cost": 0.20635171063720759,
      "system": 0.19677548455127183,
      "single": 0.19155103771096652,
      "vertical": 0.1827511990105874,
      "node": 0.1796649589776768,
      "horizontal": 0.17627262870605415
    }
  },
  "scaling_agentic_systems": {
    "title": "Scaling Agentic Systems",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agentic_solutions",
      "llm",
      "small_language_models"
    ],
    "inlinks": [],
    "summary": "[[Agentic solutions]] propose an improvement over traditional Large Language Model ([[LLM]]) usage by employing networks of Small Language Models (SLMs). These systems aim to strike...",
    "TFIDF_Score": {
      "slms": 0.2921419302633312,
      "model": 0.25104029812499007,
      "task": 0.24069990636148195,
      "system": 0.209707223797678,
      "teacher": 0.20366251707983693,
      "agentic": 0.1947612868422208,
      "slm": 0.1822157074467733,
      "llm": 0.1630260724287873,
      "specific": 0.15803812825102062,
      "language": 0.15029087825567874
    }
  },
  "scaling_server": {
    "title": "Scaling Server",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Scaling Server - Horizontal Scaling: Adding more servers, preferred for scalability. - Vertical Scaling: Adding more resources (memory, CPU) to existing servers.",
    "TFIDF_Score": {
      "scaling": 0.5404474545622399,
      "server": 0.5049910321672354,
      "adding": 0.35731012806939244,
      "vertical": 0.2585986531498361,
      "horizontal": 0.24943127387047082,
      "cpu": 0.2356080260467432,
      "preferred": 0.20164569113778913,
      "existing": 0.17582063831740183,
      "memory": 0.16504195926725787,
      "scalability": 0.15651153659074366
    }
  },
  "scheduled_tasks": {
    "title": "Scheduled Tasks",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "cron_jobs_",
      "unix"
    ],
    "inlinks": [],
    "summary": "Similar to [[Cron jobs ]]in [[Unix]] Using schtasks (Command-Line) Windows provides schtasks, a command-line tool to schedule tasks. Example Commands Run a Python script every...",
    "TFIDF_Score": {
      "schtasks": 0.5865904310158588,
      "copyedit": 0.35195425860951524,
      "cmd": 0.26609013921531954,
      "dailybackup": 0.23463617240634352,
      "command": 0.2139046452231234,
      "minute": 0.21136138573245591,
      "daily": 0.17739342614354636,
      "delete": 0.17739342614354636,
      "path": 0.1590413523986939,
      "python": 0.14170026942962896
    }
  },
  "schema_evolution": {
    "title": "Schema Evolution",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "database_schema",
      "acid_transaction"
    ],
    "inlinks": [
      "parquet"
    ],
    "summary": "[[Database Schema|Schema]] Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might...",
    "TFIDF_Score": {
      "schema": 0.31775772023095833,
      "evolution": 0.29375462884671855,
      "table": 0.24317326444597218,
      "de_tools": 0.2246047219371054,
      "even": 0.2087198951070544,
      "column": 0.17567986626273346,
      "enlarging": 0.1742773167683832,
      "reorder": 0.1742773167683832,
      "rewrite": 0.1742773167683832,
      "schema_evolution": 0.1742773167683832
    }
  },
  "scientific_method": {
    "title": "Scientific Method",
    "tags": [
      "field",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "eda",
      "problem_definition"
    ],
    "inlinks": [
      "knowledge_work",
      "thinking_systems",
      "data_science"
    ],
    "summary": "Step 1: Start with Data Collect Data: Gather all relevant data sources that might be useful for your analysis. Understand Data: Familiarize yourself with the...",
    "TFIDF_Score": {
      "question": 0.3298847158159821,
      "step": 0.2862285210015749,
      "data": 0.23356286719153,
      "experiment": 0.18556022334628716,
      "finding": 0.16636326110820118,
      "analysis": 0.1656007709225331,
      "assess": 0.16284790020990633,
      "stakeholder": 0.15965258065796778,
      "hypothesis": 0.1581577042165311,
      "formulate": 0.14739466129392462
    }
  },
  "search": {
    "title": "Search",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "security": {
    "title": "Security",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "inlinks": [
      "checksum",
      "cryptography",
      "data_principles",
      "deepseek",
      "software_design_patterns",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "[[Common Security Vulnerabilities in Software Development]]",
    "TFIDF_Score": {
      "vulnerability": 0.5991582233210555,
      "security": 0.4310122272857392,
      "software": 0.4071040082528337,
      "development": 0.4024599111831344,
      "common": 0.35711374898186543
    }
  },
  "semantic_layer": {
    "title": "semantic layer",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "metric",
      "powerbi",
      "data_virtualization",
      "tableau"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "A Semantic Layer is much more flexible and makes the most sense on top of transformed data in a Data Warehouse. A semantic layer in...",
    "TFIDF_Score": {
      "semantic": 0.3590588157511898,
      "layer": 0.3283907747676823,
      "data": 0.3067108345422431,
      "user": 0.3063793764357879,
      "business": 0.2472142823840028,
      "technical": 0.1656552345515591,
      "access": 0.15296651702673317,
      "tool": 0.14584167105197948,
      "warehouse": 0.14147265003519255,
      "security": 0.1374298485692143
    }
  },
  "semantic_relationships": {
    "title": "Semantic Relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "vector_embedding",
      "word2vec",
      "relationships_in_memory",
      "memory",
      "syntactic_relationships"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "semi-structured_data": {
    "title": "semi-structured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "json",
      "xml",
      "structured_data"
    ],
    "inlinks": [],
    "summary": "Semi-structured data is data that lacks a rigid structure and that does not conform directly to a data model, but that has tags, metadata, or...",
    "TFIDF_Score": {
      "structured": 0.5800528131534999,
      "semi": 0.4634500384549296,
      "data": 0.35897852486348863,
      "string": 0.19399424384099523,
      "relatively": 0.16379551272287785,
      "often": 0.1315321955165947,
      "json": 0.13047638091444588,
      "record": 0.13047638091444588,
      "contains": 0.12235792374459727,
      "name": 0.10439273512374206
    }
  },
  "sentence_similarity": {
    "title": "Sentence Similarity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "nlp"
    ],
    "inlinks": [
      "bert"
    ],
    "summary": "Sentence similarity refers to the degree to which two sentences are alike in meaning. It is a crucial concept in natural language processing ([[NLP]]) tasks...",
    "TFIDF_Score": {
      "sentence": 0.6525511371979336,
      "similarity": 0.44643835871495263,
      "word": 0.16860660987946083,
      "cosine": 0.14710015417707578,
      "embeddings": 0.13873715866163208,
      "bert": 0.13645213046302698,
      "semantic": 0.1258041067489782,
      "comparing": 0.12053424189188763,
      "measure": 0.12018410116086903,
      "lexical": 0.11020204729788013
    }
  },
  "shapefile": {
    "title": "shapefile",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "A shapefile is a popular geospatial vector data format for geographic information system (GIS) software. It is widely used for storing the location, shape, and...",
    "TFIDF_Score": {
      "file": 0.44991961518123097,
      "shapefile": 0.3470195211793848,
      "spatial": 0.2891000633746224,
      "shapefiles": 0.286530120912308,
      "gi": 0.2759790328352299,
      "geometry": 0.24559724649626397,
      "attribute": 0.1901122561239072,
      "park": 0.163731497664176,
      "geospatial": 0.15423089830194878,
      "data": 0.15281308186371548
    }
  },
  "shapley_additive_explanations": {
    "title": "SHapley Additive exPlanations",
    "tags": [],
    "aliases": [
      "SHAP"
    ],
    "outlinks": [
      "feature_importance"
    ],
    "inlinks": [
      "use_of_rnns_in_energy_sector",
      "feature_importance",
      "model_interpretability"
    ],
    "summary": "SHAP provides a unified approach to measure [[Feature Importance]] by computing the contribution of each feature to each prediction, based on game theory. Key Points...",
    "TFIDF_Score": {
      "shap": 0.6681018577523038,
      "feature": 0.19703381923065424,
      "applicant": 0.19343368045695367,
      "shap_values": 0.19343368045695367,
      "importance": 0.18083976002422847,
      "loan": 0.1680689254668654,
      "explanation": 0.15921054970758056,
      "model": 0.1497312068048255,
      "prediction": 0.14426830312049865,
      "value": 0.13993597474011035
    }
  },
  "sharepoint": {
    "title": "Sharepoint",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "microsoft"
    ],
    "inlinks": [],
    "summary": "SharePoint is a web-based collaboration platform developed by [[Microsoft]]. It is primarily used for creating intranet sites, document management, and team collaboration, providing a centralized...",
    "TFIDF_Score": {
      "site": 0.4177873530336717,
      "team": 0.32987611724228383,
      "document": 0.31438911450335577,
      "collaboration": 0.29640494446155013,
      "communication": 0.24215752982830205,
      "intranet": 0.22336525352742323,
      "sharepoint": 0.22336525352742323,
      "microsoft": 0.2180429922899667,
      "share": 0.16353224421747503,
      "news": 0.140269584421017
    }
  },
  "silhouette_analysis": {
    "title": "Silhouette Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "pasted_image_20241231172459.png",
      "pasted_image_20241231172403.png"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters"
    ],
    "summary": "Sklearn link Silhouette analysis is a technique used to evaluate the quality of clustering results. It provides a measure of how similar an object is...",
    "TFIDF_Score": {
      "silhouette": 0.5986096469700035,
      "cluster": 0.5374185754952793,
      "score": 0.2855347403892167,
      "point": 0.23667967064156165,
      "average": 0.17825694010183576,
      "clustering": 0.11465368864005848,
      "number": 0.1065477570723608,
      "distance": 0.09632580812771503,
      "close": 0.09449971137404171,
      "neighboring": 0.092093791841539
    }
  },
  "single_source_of_truth": {
    "title": "Single Source of Truth",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "single_source_of_truth",
      "master_data_management",
      "database",
      "data_lakehouse",
      "data_warehouse"
    ],
    "inlinks": [
      "data_integration",
      "business_intelligence",
      "single_source_of_truth"
    ],
    "summary": "Sending data from across an enterprise into a centralized system such as a: [[Database]] [[Data Warehouse]] [[Data Lakehouse]] [[Data Lakehouse]] [[master data management]] results in...",
    "TFIDF_Score": {
      "lakehouse": 0.44488230436342197,
      "data": 0.28110539587551187,
      "flowing": 0.2560116965826412,
      "single": 0.24172121243555467,
      "data_storage": 0.24115649403472048,
      "centralized": 0.22244115218171098,
      "sending": 0.21011367164933767,
      "truth": 0.20522142360067439,
      "accessing": 0.20090615499581635,
      "master": 0.20090615499581635
    }
  },
  "sklearn_datasets": {
    "title": "sklearn datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "make a dataframe by ```python ds = datasets.load_dataset() df = pd.DataFrame(ds.data,columns=ds.feature_names) df.head() add target column df['target'] = ds.target",
    "TFIDF_Score": {
      "target": 0.5684762884125972,
      "dataframe": 0.4289545672515671,
      "column": 0.3445289999016906,
      "feature_names": 0.32194657542405003,
      "load_dataset": 0.32194657542405003,
      "head": 0.25022600213981006,
      "add": 0.18633542784249138,
      "datasets": 0.15346239229785952,
      "make": 0.14622091049155436,
      "python": 0.13760338185703602
    }
  },
  "sklearn_pipiline": {
    "title": "Sklearn Pipiline",
    "tags": [
      "code_snippet",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "```python Naivebayesfor email prediction from sklearn.pipeline import Pipeline clf = Pipeline([ ('vectorizer', CountVectorizer()), ('nb', MultinomialNB()) ]) clf.fit(X_train, y_train) clf.score(X_test,y_test) clf.predict(user_input) ```",
    "TFIDF_Score": {
      "clf": 0.7554880728330234,
      "pipeline": 0.33224290115880517,
      "multinomialnb": 0.18887201820825586,
      "naivebayesfor": 0.18887201820825586,
      "vectorizer": 0.18887201820825586,
      "countvectorizer": 0.16410542918912166,
      "user_input": 0.1591774266355994,
      "y_test": 0.14821803640628417,
      "x_test": 0.14279407164290653,
      "x_train": 0.13102778023734643
    }
  },
  "sklearn": {
    "title": "Sklearn",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "p-values_in_linear_regression_in_sklearn",
      "sklearn"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "transformed_target_regressor",
      "sklearn",
      "lbfgs",
      "logistic_regression_statsmodel_summary_table",
      "optimisation_techniques"
    ],
    "summary": "Terms of interest: Also called Scikit-learn. train_test_split X and y are separate things (y is the target variable/column) and X is multiple is columns used...",
    "TFIDF_Score": {
      "pickle": 0.33560252672732127,
      "sklearn": 0.22937379227525287,
      "column": 0.2061018306907639,
      "out_file": 0.20445640583605867,
      "pipe": 0.20445640583605867,
      "skileanr": 0.20445640583605867,
      "to_numpy": 0.20445640583605867,
      "018": 0.19259272397520233,
      "data_cleaning": 0.19259272397520233,
      "bad": 0.1776462524649669
    }
  },
  "slowly_changing_dimension": {
    "title": "Slowly Changing Dimension",
    "tags": [
      "database"
    ],
    "aliases": [
      "SCD"
    ],
    "outlinks": [
      "etl",
      "database",
      "dimension_table"
    ],
    "inlinks": [],
    "summary": "A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a Data Warehouse. It is...",
    "TFIDF_Score": {
      "membership": 0.3023352912416601,
      "customer": 0.286926405152555,
      "dimension": 0.2710521311912803,
      "city": 0.2572838816623572,
      "doe": 0.2572838816623572,
      "john": 0.25187387603237027,
      "address": 0.19974202366192775,
      "scd": 0.19257545891285138,
      "status": 0.15437032899741435,
      "current": 0.14318516899110473
    }
  },
  "small_language_models": {
    "title": "Small Language Models",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": [
      "SLM"
    ],
    "outlinks": [
      "rag",
      "contrastive_decoding",
      "interpretability",
      "distillation",
      "data_synthesis:",
      "model_cascading",
      "edge_machine_learning_models",
      "inference",
      "bert",
      "language_models",
      "llm"
    ],
    "inlinks": [
      "agentic_solutions",
      "scaling_agentic_systems",
      "distillation",
      "language_models"
    ],
    "summary": "[[LLM|LLMs]] dominate many general-purpose NLP tasks, small [[Language Models]] have their own place in specialized tasks, where they excel due to computational efficiency, [[interpretability]], and...",
    "TFIDF_Score": {
      "llm": 0.372478427928396,
      "slms": 0.27811647265804096,
      "sm": 0.23155666916699144,
      "model": 0.22405133529599455,
      "task": 0.21872848929452976,
      "cascading": 0.21812048141575274,
      "domain": 0.19374596875513836,
      "specific": 0.1934368293497963,
      "computational": 0.1822798242333403,
      "interpretability": 0.17528891462259877
    }
  },
  "smart_grids": {
    "title": "Smart Grids",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [
      "rl"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Smart Grids Want adaptive grid that can handle the volatility of energy coming on or off. This occurs more often due to the variety of...",
    "TFIDF_Score": {
      "energy": 0.40985268985566403,
      "grid": 0.3995368909859006,
      "electricity": 0.32550040054594453,
      "demand": 0.261249176313972,
      "renewable": 0.2001596084535985,
      "wind": 0.18906695022746442,
      "smart": 0.18466475016567732,
      "distribution": 0.16313131392782726,
      "balancing": 0.15793038472720167,
      "source": 0.1578593246258781
    }
  },
  "smote_(synthetic_minority_over-sampling_technique)": {
    "title": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "tags": [],
    "aliases": [
      "SMOTE"
    ],
    "outlinks": [],
    "inlinks": [
      "imbalanced_datasets_smote.py",
      "imbalanced_datasets"
    ],
    "summary": "SMOTE (Synthetic Minority Over-sampling Technique) Generate synthetic samples for the minority class by interpolating between existing samples. SMOTE: This technique generates synthetic samples for the...",
    "TFIDF_Score": {
      "minority": 0.4945867028919193,
      "synthetic": 0.46717717056540886,
      "smote": 0.34184287385466733,
      "sample": 0.3022680001317158,
      "existing": 0.23241819535138122,
      "class": 0.19042805782150118,
      "interpolating": 0.18974303941017143,
      "interpolation": 0.18974303941017143,
      "female": 0.17873310775423112,
      "resume": 0.17873310775423112
    }
  },
  "smss": {
    "title": "SMSS",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "microsoft sql server management.",
    "TFIDF_Score": {
      "microsoft": 0.6197034985530633,
      "server": 0.4963214003288276,
      "sql": 0.45413006498343333,
      "management": 0.404225834818691
    }
  },
  "snowflake_schema": {
    "title": "Snowflake Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "normalised_schema",
      "star_schema"
    ],
    "inlinks": [
      "types_of_database_schema"
    ],
    "summary": "Snowflake Schema - Description: A more [[Normalised Schema]] normalized form of a star schema where dimension tables are further broken down into additional tables. -...",
    "TFIDF_Score": {
      "schema": 0.5226039302577844,
      "table": 0.31995019007742537,
      "star": 0.27571573995776527,
      "snowflake": 0.248170033671537,
      "redundancy": 0.24159728036377906,
      "dimension": 0.20747882076979282,
      "additional": 0.19649513007220942,
      "complicate": 0.17197632957286885,
      "normalizes": 0.17197632957286885,
      "query": 0.16555341992977407
    }
  },
  "snowflake": {
    "title": "Snowflake",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_warehouse",
      "cloud"
    ],
    "inlinks": [
      "databricks_vs_snowflake",
      "difference_between_snowflake_to_hadoop"
    ],
    "summary": "Snowflake Architecture: Cloud-Native: Snowflake is a fully managed, cloud-native data warehousing service. It operates entirely on cloud platforms like AWS, Azure, and Google [[Cloud]]. Separation...",
    "TFIDF_Score": {
      "snowflake": 0.3492202641128555,
      "structured": 0.274793964463823,
      "data": 0.255093492612306,
      "native": 0.2383386762271591,
      "cloud": 0.2353275355300516,
      "semi": 0.21955461728661543,
      "storage": 0.19966711223007572,
      "compute": 0.1726567069594806,
      "architecture": 0.16009421006392355,
      "fully": 0.1490104099903362
    }
  },
  "soft_deletion": {
    "title": "Soft Deletion",
    "tags": [
      "data_integrity",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "querying",
      "data_integrity",
      "incremental_synchronization",
      "database_schema"
    ],
    "inlinks": [
      "views",
      "database_techniques"
    ],
    "summary": "Soft deletion is a technique used in databases to ==mark records as deleted without physically removing them from the database==. This approach is particularly useful...",
    "TFIDF_Score": {
      "is_deleted": 0.47178231841825025,
      "record": 0.40884624740173914,
      "deleted": 0.4051984467387395,
      "user": 0.26612514166589946,
      "soft": 0.25477463074565176,
      "synchronization": 0.18213590043737296,
      "flag": 0.15867127420957958,
      "incremental": 0.12142393362491531,
      "boolean": 0.11711942840196232,
      "sql": 0.10847982172070104
    }
  },
  "software_design_patterns": {
    "title": "Software Design Patterns",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "security",
      "database"
    ],
    "inlinks": [],
    "summary": "10 Design Patterns Explained in 10 Minutes Software Design Patterns What Are Software Design Patterns? Software design patterns provide reusable solutions to common software design...",
    "TFIDF_Score": {
      "pattern": 0.31040012626978597,
      "javascript": 0.30500787525147893,
      "const": 0.29557192560312834,
      "mediator": 0.2465407295037476,
      "observer": 0.2465407295037476,
      "participant": 0.22412793591249786,
      "console": 0.21106230269503318,
      "constructor": 0.20171514232124807,
      "object": 0.18858203930388096,
      "iterator": 0.17930234872999828
    }
  },
  "software_development_life_cycle": {
    "title": "Software Development Life Cycle",
    "tags": [
      "#data_orchestration"
    ],
    "aliases": [
      "sdlc"
    ],
    "outlinks": [
      "devops"
    ],
    "inlinks": [
      "debugging",
      "ci-cd",
      "generative_ai_from_theory_to_practice",
      "data_lifecycle_management"
    ],
    "summary": "A structured approach like the Software Development Life Cycle (SDLC) ensures ==systematic progression through various stages of development==. SDLC remains relevant today by outlining the...",
    "TFIDF_Score": {
      "sdlc": 0.3256495828446352,
      "activity": 0.29814721037515496,
      "product": 0.28264392296660623,
      "stage": 0.24481912291676597,
      "development": 0.20944641097941016,
      "phase": 0.20178987034917054,
      "agile": 0.1886316637934576,
      "purpose": 0.17563964469789856,
      "waterfall": 0.1628247914223176,
      "design": 0.12251004785746654
    }
  },
  "software_development_portal": {
    "title": "Software Development Portal",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "tool.ruff",
      "makefile",
      "documentation_&_meetings",
      "devops",
      "json",
      "tool.uv",
      "toml",
      "justfile"
    ],
    "inlinks": [
      "devops",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Tools: - [[tool.uv]] - [[tool.ruff]] File types: - [[Justfile]] - [[TOML]] - [[Makefile]] - [[Json]] Practices: - [[Testing]] - [[Documentation & Meetings]] Related to: -...",
    "TFIDF_Score": {
      "inlinks": 0.42754585527693806,
      "file": 0.42722110600207563,
      "length": 0.2931077349934771,
      "tool": 0.2678710840329161,
      "backlinks": 0.21377292763846903,
      "desc": 0.20136865009286728,
      "justfile": 0.20136865009286728,
      "makefile": 0.19256767511320552,
      "devops": 0.18574110856614204,
      "ruff": 0.18574110856614204
    }
  },
  "sparsecategorialcrossentropy_or_categoricalcrossentropy": {
    "title": "SparseCategorialCrossentropy or CategoricalCrossEntropy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "loss_function",
      "one-hot_encoding",
      "tensorflow",
      "cross_entropy"
    ],
    "inlinks": [
      "neural_network_in_practice"
    ],
    "summary": "To understand the differences and use cases for SparseCategoricalCrossentropy and CategoricalCrossentropy in [[TensorFlow]], let's break down each one: CategoricalCrossentropy Use Case: This [[loss function]] is...",
    "TFIDF_Score": {
      "label": 0.5450041877400255,
      "sparsecategoricalcrossentropy": 0.3121573115313578,
      "class": 0.2993242840397123,
      "hot": 0.28299728594420037,
      "encoded": 0.26564230880965295,
      "categoricalcrossentropy": 0.24972584922508623,
      "integer": 0.2221879911940979,
      "one": 0.18186677367592993,
      "loss": 0.13937093474436707,
      "use": 0.12473978242932422
    }
  },
  "specificity": {
    "title": "Specificity",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "confusion_matrix",
      "roc_(receiver_operating_characteristic)",
      "evaluation_metrics"
    ],
    "summary": "Specificity, also known as the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. It indicates how well...",
    "TFIDF_Score": {
      "specificity": 0.5155191169286156,
      "negative": 0.4845096810128742,
      "correctly": 0.2711707025533996,
      "instance": 0.21684150565226076,
      "treatment": 0.1862976366663531,
      "identified": 0.149523945428923,
      "medical": 0.14479580274993067,
      "unnecessary": 0.14269831996942822,
      "proportion": 0.13891897223467337,
      "positive": 0.12112742025321856
    }
  },
  "spreadsheets_vs_databases": {
    "title": "Spreadsheets vs Databases",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "scalability",
      "data_management"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "Compared to spreadsheets, databases offer: [[Scalability]]: Databases are designed to handle large volumes of data, making them suitable for applications with millions or even billions...",
    "TFIDF_Score": {
      "spreadsheet": 0.47982146237596845,
      "database": 0.2691245404667426,
      "update": 0.23321475509915643,
      "tag": 0.18723650071067774,
      "lag": 0.15285732117340264,
      "unwieldy": 0.15285732117340264,
      "handle": 0.14599706990259673,
      "billion": 0.14398770116277756,
      "data_storage": 0.14398770116277756,
      "million": 0.14398770116277756
    }
  },
  "sql_groupby": {
    "title": "SQL Groupby",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying",
      "sql_groupby",
      "sql"
    ],
    "inlinks": [
      "sql_groupby"
    ],
    "summary": "The [[SQL]] GROUP BY clause is used to group rows that have the same values in specified columns into summary rows, like \"total sales per...",
    "TFIDF_Score": {
      "sale": 0.480316466846559,
      "group": 0.41339273550718425,
      "clause": 0.2776482494922115,
      "product": 0.2773503622990769,
      "region": 0.2586319436866087,
      "amount": 0.2078953422626099,
      "sql": 0.200018648692948,
      "select": 0.18255457923585347,
      "count": 0.15840848248129794,
      "per": 0.15840848248129794
    }
  },
  "sql_injection": {
    "title": "SQL Injection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "querying",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "SQL injection is a code injection technique that targets applications using SQL databases. It occurs when a malicious user injects harmful SQL code into a...",
    "TFIDF_Score": {
      "sql": 0.42948328299956157,
      "user": 0.3316945750854599,
      "statement": 0.3080633478938156,
      "injection": 0.3026040505100577,
      "query": 0.23546254929381594,
      "prepared": 0.21899561852648947,
      "password": 0.20173603367337178,
      "account": 0.18511289987031876,
      "select": 0.17784466070432078,
      "malicious": 0.15456280506130923
    }
  },
  "sql_joins": {
    "title": "SQL Joins",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "pasted_image_20250323083319.png"
    ],
    "inlinks": [
      "querying",
      "joining_datasets",
      "database_techniques"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ![[Pasted image 20250323083319.png|800]]",
    "TFIDF_Score": {
      "de_tools": 0.4783982892577949,
      "20250323083319": 0.3712031050788861,
      "800": 0.34966386532550264,
      "joining": 0.3128422917599676,
      "ipynb": 0.22069629137339566,
      "transformation": 0.19052367203465476,
      "pasted": 0.18793182621947305,
      "rhyslwells": 0.18627161917828328,
      "png": 0.18466172926288313,
      "blob": 0.18387471780786058
    }
  },
  "sql_vs_nosql": {
    "title": "SQL vs NoSQL",
    "tags": [
      "#question",
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "nosql"
    ],
    "inlinks": [],
    "summary": "[[NoSQL]]",
    "TFIDF_Score": {
      "nosql": 1.0
    }
  },
  "sql_window_functions": {
    "title": "SQL Window functions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying",
      "sql_window_functions"
    ],
    "inlinks": [
      "sql_window_functions"
    ],
    "summary": "SQL Window Functions are a feature in SQL that allow you to perform calculations across a set of table rows that are somehow related to...",
    "TFIDF_Score": {
      "salary": 0.3750516807142129,
      "department": 0.30986939750579123,
      "window": 0.29656184373175654,
      "row": 0.26698953323441105,
      "sale": 0.26018776981608616,
      "clause": 0.21724769896421842,
      "partition": 0.21724769896421842,
      "function": 0.207060876993092,
      "employee": 0.15254795571167493,
      "sql": 0.13414792048555957
    }
  },
  "sql": {
    "title": "SQL",
    "tags": [
      "software",
      "database",
      "query_language"
    ],
    "aliases": [],
    "outlinks": [
      "querying",
      "database_techniques",
      "database"
    ],
    "inlinks": [
      "sql_groupby",
      "data_engineering_tools",
      "declarative",
      "sqlite",
      "dbt",
      "pyspark",
      "sqlalchemy",
      "unstructured_data",
      "google_cloud_platform"
    ],
    "summary": "Structured Query Language (SQL) is the standard language for interacting with relational databases, enabling efficient data [[Querying]] and manipulation. It serves as a common interface...",
    "TFIDF_Score": {
      "quote": 0.44343911854732376,
      "sql": 0.26411155850375045,
      "language": 0.22812492037458962,
      "database": 0.2166757885517619,
      "use": 0.20588818966458272,
      "querying": 0.19796442857179958,
      "structured": 0.18632472227306823,
      "name": 0.16766531372816473,
      "column": 0.16541054508521638,
      "capitalization": 0.16408998125906618
    }
  },
  "sqlalchemy_vs._sqlite3": {
    "title": "SQLAlchemy vs. sqlite3",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sqlite",
      "sqlalchemy"
    ],
    "inlinks": [
      "sqlalchemy"
    ],
    "summary": "SQLAlchemy vs. sqlite3: Which One Should You Use? The choice between [[SQLAlchemy]] and [[SQLite]] depends on your specific needs. Here’s a comparison based on key...",
    "TFIDF_Score": {
      "sqlalchemy": 0.5150057038138598,
      "sqlite3": 0.45171705858369354,
      "orm": 0.30293052486464217,
      "use": 0.2353802648077326,
      "sql": 0.191710306702006,
      "transaction": 0.19166269286428803,
      "database": 0.17300598400476347,
      "query": 0.1547901670186979,
      "sqlite": 0.143747019648216,
      "raw": 0.13342351536695915
    }
  },
  "sqlalchemy": {
    "title": "SQLAlchemy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sqlalchemy_vs._sqlite3",
      "sql"
    ],
    "inlinks": [
      "sqlalchemy_vs._sqlite3"
    ],
    "summary": "What is SQLAlchemy? SQLAlchemy is a Python SQL toolkit and ==Object Relational Mapper== (ORM) that provides tools to interact with databases in a more Pythonic...",
    "TFIDF_Score": {
      "sqlalchemy": 0.45761571646568616,
      "sql": 0.30283901696834475,
      "orm": 0.2658501428474813,
      "database": 0.24844759986119544,
      "query": 0.2445173805348561,
      "session": 0.19028370089245222,
      "customer": 0.18021483339442976,
      "python": 0.15907804002800252,
      "string": 0.14290459862343866,
      "connection": 0.1416851635445473
    }
  },
  "sqlite_studio": {
    "title": "SQLite Studio",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "sqlite"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "sqlite": {
    "title": "SQLite",
    "tags": [
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "sqlite_studio",
      "querying",
      "concurrency",
      "sql",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "transaction",
      "duckdb_vs_sqlite",
      "database_management_system_(dbms)",
      "views",
      "data_storage",
      "sqlalchemy_vs._sqlite3"
    ],
    "summary": "Lightweight [[Database Management System (DBMS)|DBMS]] used in various applications (phone apps, desktop apps, websites). Note [[SQLite Studio]] exists To get in terminal enter: sqlite3 database.db...",
    "TFIDF_Score": {
      "dbms": 0.41044770189023216,
      "apps": 0.4008909131630235,
      "note": 0.28393684061579044,
      "database": 0.22012551738986236,
      "phone": 0.21074010240595104,
      "sqlite3": 0.21074010240595104,
      "concurrency": 0.20044545658151175,
      "desktop": 0.20044545658151175,
      "enter": 0.1962306140441374,
      "studio": 0.1962306140441374
    }
  },
  "stacking": {
    "title": "Stacking",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "stacking",
      "model_ensemble"
    ],
    "inlinks": [
      "stacking",
      "model_ensemble"
    ],
    "summary": "What is [[Stacking]]?;; is an [[Model Ensemble]] combines predictions of multiple base models ==by training a meta-model== on the outputs of the base models.",
    "TFIDF_Score": {
      "base": 0.5452395995728138,
      "model": 0.46699667526609057,
      "meta": 0.34946012517341196,
      "stacking": 0.34946012517341196,
      "ensemble": 0.2616229946709835,
      "combine": 0.2283498451136509,
      "prediction": 0.17998336977641236,
      "output": 0.1758912129294979,
      "multiple": 0.16948925999760597,
      "training": 0.16653690772678859
    }
  },
  "standard_deviation": {
    "title": "Standard deviation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/outliers",
      "variance",
      "distributions",
      "interpretability"
    ],
    "inlinks": [
      "eda",
      "z-test",
      "t-test"
    ],
    "summary": "Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a set of data values. It indicates how much individual...",
    "TFIDF_Score": {
      "deviation": 0.5974703669453363,
      "standard": 0.48899300666735834,
      "centimeter": 0.17990487123397816,
      "x_i": 0.15162010081693383,
      "fall": 0.14118103362894058,
      "sigma": 0.13377439758788284,
      "direct": 0.13171362543609005,
      "data": 0.13169249874965663,
      "mean": 0.12907392549211563,
      "observation": 0.1263677615468251
    }
  },
  "standardisation": {
    "title": "Standardisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "principal_component_analysis",
      "gradient_descent",
      "gaussian_distribution",
      "k-nearest_neighbours",
      "data_transformation"
    ],
    "inlinks": [
      "anomaly_detection",
      "normalisation_vs_standardisation",
      "feature_scaling",
      "statistical_tests",
      "normalisation"
    ],
    "summary": "Standardisation is a [[Preprocessing|data preprocessing]] technique used to [[Data Transformation]] features so that they have a mean of 0 and a standard deviation of 1....",
    "TFIDF_Score": {
      "deviation": 0.37604227525180567,
      "standardisation": 0.2764471904793607,
      "mean": 0.2653772203441314,
      "standard": 0.2564730305503706,
      "feature": 0.2392192737431313,
      "data": 0.23208088449054828,
      "algorithm": 0.19771793554322287,
      "centred": 0.1659158606742025,
      "preprocessing": 0.16181332631624515,
      "rescales": 0.15866437769584776
    }
  },
  "star_schema": {
    "title": "Star Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "star_schema"
    ],
    "inlinks": [
      "snowflake_schema",
      "star_schema",
      "dimension_table",
      "types_of_database_schema",
      "dimensional_modelling"
    ],
    "summary": "Star Schema - This schema consists of a central fact table surrounded by dimension tables. The fact table contains quantitative data, while the dimension tables...",
    "TFIDF_Score": {
      "table": 0.4563923117693164,
      "fact": 0.38899348554650265,
      "schema": 0.3407850679148172,
      "dimension": 0.3382374295962676,
      "star": 0.33710928723908445,
      "central": 0.20526001257668763,
      "easy": 0.1691187147981338,
      "surrounded": 0.14018016742698935,
      "query": 0.13494476932671792,
      "understand": 0.13337115848573028
    }
  },
  "statistical_assumptions": {
    "title": "Statistical Assumptions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "interpretability",
      "assumption_of_normality",
      "statistical_tests",
      "hypothesis_testing",
      "data_analysis"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_tests",
      "statistical_tests",
      "statistics"
    ],
    "summary": "Statistical assumptions are essential conditions that must be met for various statistical methods and models to produce valid results. Necessary for robustness and reliability of...",
    "TFIDF_Score": {
      "assumption": 0.7041396815960002,
      "statistical": 0.27993617438712665,
      "test": 0.2452088520811628,
      "normality": 0.18588841782525617,
      "residual": 0.12939859057038788,
      "homoscedasticity": 0.1209170120314251,
      "violation": 0.1209170120314251,
      "alternative": 0.11601177839671635,
      "analysis": 0.10393205705126025,
      "method": 0.10252420778007411
    }
  },
  "statistical_tests": {
    "title": "Statistical Tests",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "estimator",
      "z-test",
      "statistical_assumptions",
      "t-test",
      "standardisation",
      "chi-squared_test",
      "hypothesis_testing",
      "proportion_test"
    ],
    "inlinks": [
      "statistical_assumptions",
      "statistics"
    ],
    "summary": "Statistical tests are methods used to determine if there is a significant difference between groups or if a relationship exists between variables. Each test has...",
    "TFIDF_Score": {
      "test": 0.6950277977394786,
      "statistical": 0.3173840114507772,
      "statistic": 0.278797143610205,
      "deviation": 0.20714332205508995,
      "population": 0.20445495501211813,
      "hypothesis": 0.18756521204156798,
      "centered": 0.13110068382223844,
      "chi": 0.11944522120476238,
      "estimator": 0.11666408066414875,
      "standardisation": 0.11421094085168827
    }
  },
  "statistics": {
    "title": "Statistics",
    "tags": [
      "statistics",
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "confidence_interval",
      "statistical_assumptions",
      "type_1_error_and_power",
      "t-test",
      "adaptive_decision_analysis",
      "distributions",
      "tidyverse",
      "parametric_vs_non-parametric_tests",
      "expectation_maximisation_algorithm",
      "r",
      "bootstrap",
      "statistical_tests",
      "hypothesis_testing",
      "multicollinearity",
      "central_limit_theorem",
      "maximum_likelihood_estimation",
      "likelihood_ratio",
      "correlation_vs_causation",
      "proportional_hazard_model",
      "markov_chain",
      "monte_carlo_simulation",
      "logistic_regression",
      "estimator",
      "over_parameterised_models",
      "casual_inference",
      "univariate_vs_multivariate",
      "p_values"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "covariance_structures",
      "ds_&_ml_portal",
      "data_analyst",
      "tf-idf",
      "hypothesis_testing",
      "data_analysis",
      "data_science"
    ],
    "summary": "Statistics want to understand the world. The world is made of probabilities, we model probabilities with functions, and we model functions with parameters. \"Observe data...",
    "TFIDF_Score": {
      "theorem": 0.2667652043032837,
      "model": 0.22540034473383047,
      "test": 0.21190096260983401,
      "power": 0.20710283678031666,
      "statistical": 0.18143333015455018,
      "parametric": 0.17075871745621604,
      "estimation": 0.16773264302757646,
      "likelihood": 0.16773264302757646,
      "mean": 0.14325610507025635,
      "hypothesis": 0.14296280347704823
    }
  },
  "stemming": {
    "title": "Stemming",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "normalisation_of_text"
    ],
    "summary": "Shorting words to the key term.",
    "TFIDF_Score": {
      "shorting": 0.7827738719187329,
      "word": 0.431530928735542,
      "term": 0.3654158943046414,
      "key": 0.25984100364556545
    }
  },
  "stochastic_gradient_descent": {
    "title": "Stochastic Gradient Descent",
    "tags": [],
    "aliases": [
      "SGD"
    ],
    "outlinks": [
      "gradient_descent"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "deep_learning",
      "pytorch",
      "gradient_descent",
      "optimisation_techniques"
    ],
    "summary": "[[Gradient Descent]]",
    "TFIDF_Score": {
      "descent": 0.7573933640030072,
      "gradient": 0.6529588747878446
    }
  },
  "storage_layer_object_store": {
    "title": "storage layer object store",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Object Store"
    ],
    "outlinks": [
      "cloud_providers",
      "s3_bucket"
    ],
    "inlinks": [],
    "summary": "A storage layer or object storage are services from the three big [[Cloud Providers]], AWS S3,[[S3 bucket]] Azure Blob Storage, and Google Cloud Storage. The...",
    "TFIDF_Score": {
      "storage": 0.45562058302121194,
      "cloud": 0.26849706930102185,
      "object": 0.24780989028531886,
      "store": 0.24218641852369277,
      "configurable": 0.2208897494163753,
      "exceptionally": 0.2208897494163753,
      "solid": 0.2080725148441241,
      "bucket": 0.19192470899253247,
      "provider": 0.1670006335642938,
      "azure": 0.16425010039379676
    }
  },
  "stored_procedures": {
    "title": "Stored Procedures",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "Stored procedures are a way to automate SQL statements, allowing them to be executed repeatedly without rewriting the code. Demonstration with the Boston MFA Database...",
    "TFIDF_Score": {
      "procedure": 0.6216666173943919,
      "delimiter": 0.2718146678503993,
      "collection": 0.2580893273847893,
      "deleted": 0.23131117955203054,
      "sell": 0.21641880685496576,
      "sql": 0.2128729707469977,
      "stored": 0.20478689889388724,
      "call": 0.17968360592268243,
      "create": 0.13622558939073492,
      "title": 0.13410464891404716
    }
  },
  "strongly_vs_weakly_typed_language": {
    "title": "Strongly vs Weakly typed language",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "java",
      "javascript"
    ],
    "inlinks": [],
    "summary": "A strongly typed programming language is one where ==types== are strictly enforced. This means that once a variable is assigned a type, it cannot be...",
    "TFIDF_Score": {
      "type": 0.3694708386026423,
      "typed": 0.335159723401918,
      "language": 0.2873685925000037,
      "text": 0.22583693673229355,
      "conversion": 0.22343981560127868,
      "strongly": 0.1988349710382542,
      "string": 0.19624489510574936,
      "runtime": 0.19465399279039441,
      "error": 0.1892260975244557,
      "mismatch": 0.17523881156914578
    }
  },
  "structured_data": {
    "title": "structured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "xml",
      "json",
      "database",
      "database_schema"
    ],
    "inlinks": [
      "data_engineering_portal",
      "data_lake",
      "database",
      "semi-structured_data",
      "named_entity_recognition"
    ],
    "summary": "Structured data refers to data that has been formatted into a well-defined schema ([[Database Schema]]). An example would be data that is stored with precisely...",
    "TFIDF_Score": {
      "structured": 0.55226939930587,
      "data": 0.39810084968924125,
      "unstructured": 0.31057267845567915,
      "semi": 0.20056894236162834,
      "example": 0.1905853554014835,
      "record": 0.16940019992507785,
      "age": 0.1618555751089915,
      "easily": 0.1353985950850539,
      "defined": 0.12963035689920957,
      "formatted": 0.11948731842560757
    }
  },
  "structuring_and_organizing_data": {
    "title": "Structuring and organizing data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "multi-level_index"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Structuring and organizing data. In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/multi_index.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Related terms: - [[Multi-level index]]",
    "TFIDF_Score": {
      "de_tools": 0.472835865413108,
      "ipynb": 0.2908402873176389,
      "transformation": 0.25107789157009075,
      "rhyslwells": 0.24547440694993214,
      "multi_index": 0.24459137274691267,
      "blob": 0.2423156973997733,
      "github": 0.23361408842167133,
      "exploration": 0.23181895967426108,
      "com": 0.21890778619784235,
      "main": 0.20864463622737234
    }
  },
  "summarisation": {
    "title": "Summarisation",
    "tags": [
      "NLP"
    ],
    "aliases": [],
    "outlinks": [
      "extraction",
      "abstraction"
    ],
    "inlinks": [
      "bert",
      "nlp"
    ],
    "summary": "Summarization in NLP Summarization in natural language processing (NLP) is the process of condensing a text document into a shorter version while retaining its main...",
    "TFIDF_Score": {
      "sentence": 0.6149040276677478,
      "summarization": 0.34287975229384465,
      "word": 0.2859827830699308,
      "text": 0.26239579342601954,
      "original": 0.17257773450234767,
      "summary": 0.1493189800350494,
      "unsupervised": 0.12659915521273907,
      "frequency": 0.12476526539658554,
      "assigning": 0.12305795482351171,
      "score": 0.12242971202888626
    }
  },
  "supervised_learning": {
    "title": "Supervised Learning",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "support_vector_machines",
      "naive_bayes",
      "machine_learning_algorithms",
      "decision_tree",
      "classification",
      "pasted_image_20241012152141.png",
      "linear_regression",
      "k-nearest_neighbours",
      "regression"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "data_transformation_in_machine_learning",
      "machine_learning_algorithms",
      "decision_tree",
      "backpropagation",
      "classification",
      "feed_forward_neural_network",
      "active_learning",
      "regression",
      "transfer_learning"
    ],
    "summary": "Supervised learning is a type of machine learning where an algorithm learns from ==labeled data== to make predictions or decisions. In supervised learning, the training...",
    "TFIDF_Score": {
      "supervised": 0.3162814911225928,
      "house": 0.3007890694123506,
      "labeled": 0.28880653223784875,
      "label": 0.2879860321284773,
      "output": 0.2625554202299142,
      "spam": 0.22695079324804293,
      "price": 0.21085432741506188,
      "learning": 0.18872539138034475,
      "input": 0.16905747253073714,
      "algorithm": 0.1684831485196748
    }
  },
  "support_vector_classifier_(svc)": {
    "title": "Support Vector Classifier (SVC)",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "support_vector_machines"
    ],
    "inlinks": [],
    "summary": "Overview Support Vector Classifiers (SVCs) are a fundamental component of [[Support Vector Machines|SVM]]s, designed to find the optimal hyperplane that separates data into distinct classes....",
    "TFIDF_Score": {
      "hyperplane": 0.5138449105310451,
      "svc": 0.42125849623796136,
      "margin": 0.24170823771669905,
      "separation": 0.20652309262948565,
      "svm": 0.19360271261088413,
      "space": 0.18448764537917456,
      "class": 0.17952899334063555,
      "linearly": 0.1554264182593663,
      "separable": 0.1554264182593663,
      "kernel": 0.14681283158029862
    }
  },
  "support_vector_machines": {
    "title": "Support Vector Machines",
    "tags": [
      "classifier",
      "clustering"
    ],
    "aliases": [
      "SVM"
    ],
    "outlinks": [
      "pasted_image_20240128193838.png",
      "kernelling",
      "classification",
      "pasted_image_20240128193726.png",
      "svm_example.py",
      "ml_tools"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "logistic_regression",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "model_parameters",
      "unsupervised_learning",
      "machine_learning_algorithms",
      "support_vector_regression",
      "kernelling",
      "why_and_when_is_feature_scaling_necessary",
      "classification",
      "supervised_learning",
      "support_vector_classifier_(svc)"
    ],
    "summary": "Support Vector Machines (SVM) are a type of supervised learning algorithm primarily used for [[classification]] tasks, though they can also be adapted for regression. The...",
    "TFIDF_Score": {
      "margin": 0.4957500844021992,
      "hyperplane": 0.38715050426863973,
      "svm": 0.22690534913143603,
      "dimensional": 0.1911296468767873,
      "separation": 0.1815361745381978,
      "space": 0.16216676288002368,
      "700": 0.15724037478159977,
      "classification": 0.14846327157462558,
      "kernelling": 0.14811642596491897,
      "data": 0.13812218235529633
    }
  },
  "support_vector_regression": {
    "title": "Support Vector Regression",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "support_vector_machines"
    ],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Support Vector Regression use similar principles to [[Support Vector Machines|SVM]]s but for predicting continuous variables.",
    "TFIDF_Score": {
      "vector": 0.4801655980866774,
      "support": 0.41101883170494097,
      "principle": 0.34404691032411144,
      "svm": 0.33906311620587876,
      "predicting": 0.31852606432578695,
      "continuous": 0.2562009051214075,
      "similar": 0.2327771248750088,
      "regression": 0.21172156275342247,
      "variable": 0.20958644865809226,
      "machine": 0.18817029347325354
    }
  },
  "svm_example.py": {
    "title": "SVM_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "support_vector_machines"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main\\Explorations/Build/Classifiers/SVM/SVM_Example.py Overview Objective: To classify Iris flowers using SVM and explore various hyperparameters like kernel type, regularization (C), and gamma. Dataset: The Iris dataset contains...",
    "TFIDF_Score": {
      "specie": 0.325132821481244,
      "margin": 0.3044720004834051,
      "flower": 0.2936784009252466,
      "petal": 0.2435776003867241,
      "sepal": 0.2435776003867241,
      "ccc": 0.20279998983946415,
      "svm": 0.19510002890919984,
      "kernel": 0.16644181123132096,
      "iris": 0.1591480652474219,
      "boundary": 0.14244962119120086
    }
  },
  "symbolic_computation": {
    "title": "Symbolic computation",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "mathematical_reasoning_in_transformers",
      "mathematica"
    ],
    "inlinks": [],
    "summary": "[[Mathematical Reasoning in Transformers]] Summary of Wolfram Alpha’s Approach: Uses symbolic computation with precise algorithms. Leverages predefined mathematical rules for various domains. Provides step-by-step solutions...",
    "TFIDF_Score": {
      "wolfram": 0.5002160969372679,
      "alpha": 0.36839546222900876,
      "symbolic": 0.3463034517258008,
      "mathematical": 0.2869250902654979,
      "exact": 0.1774600051369777,
      "solution": 0.15675356399209175,
      "numerical": 0.15512610237458252,
      "rule": 0.1457168728116323,
      "problem": 0.13550365804778572,
      "computation": 0.1281686488555379
    }
  },
  "sympy": {
    "title": "Sympy",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "backpropagation"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "syntactic_relationships": {
    "title": "syntactic relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "semantic_relationships"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "Syntactic relationships refer to the structural connections between words or phrases in a sentence, focusing on grammar and the arrangement of words. They determine how...",
    "TFIDF_Score": {
      "word": 0.3952259750277053,
      "phrase": 0.35303326333068363,
      "relationship": 0.32300190415109364,
      "concerned": 0.2701276358146724,
      "meaning": 0.2457390883897269,
      "syntactic": 0.23535550888712245,
      "semantic": 0.19659589379048537,
      "interpretation": 0.1883605998997857,
      "sentence": 0.1799560734121041,
      "form": 0.17428654286172163
    }
  },
  "t-sne": {
    "title": "t-SNE",
    "tags": [
      "data_visualization",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "t-sne",
      "principal_component_analysis",
      "pasted_image_20241015211844.png"
    ],
    "inlinks": [
      "dimensionality_reduction",
      "vector_embedding",
      "t-sne",
      "principal_component_analysis"
    ],
    "summary": "t-SNE (t-distributed Stochastic Neighbor Embedding) is a [[Dimensionality Reduction]] technique used primarily for visualizing high-dimensional data. Unlike methods such as [[Principal Component Analysis|PCA]] (Principal Component...",
    "TFIDF_Score": {
      "sne": 0.6296735359547655,
      "plt": 0.28451257646519995,
      "dimensional": 0.22466095976517014,
      "tsne": 0.1848262375278818,
      "iris": 0.18130386823988162,
      "local": 0.14887495995033306,
      "import": 0.1391201288185565,
      "x_tsne": 0.13861967814591133,
      "similarity": 0.12263189955541397,
      "space": 0.1191355618619809
    }
  },
  "t-test": {
    "title": "T-test",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard_deviation",
      "variance",
      "distributions",
      "hypothesis_testing"
    ],
    "inlinks": [
      "general_linear_regression",
      "statistics",
      "statistical_tests"
    ],
    "summary": "The T-test is a statistical method ==used to determine if there is a significant difference between the means of two groups, especially when the population...",
    "TFIDF_Score": {
      "sample": 0.5361933544158426,
      "test": 0.4568389936119065,
      "two": 0.2599694480876132,
      "mean": 0.22877554950765,
      "unknown": 0.1914932344480088,
      "deviation": 0.1891034412357607,
      "population": 0.18664920107929758,
      "variance": 0.17679112337663846,
      "standard": 0.15476961773650447,
      "distribution": 0.15053551483547498
    }
  },
  "tableau": {
    "title": "Tableau",
    "tags": [
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "data_visualisation",
      "postgresql"
    ],
    "inlinks": [
      "data_visualisation",
      "semantic_layer",
      "postgresql"
    ],
    "summary": "Next Steps Load a [[PostgreSQL]] database and perform analytics as an example. Resources Tableau How-To Videos Tutorial Link Example Usage Video Features Can publish to...",
    "TFIDF_Score": {
      "online": 0.38216797800191166,
      "dashboard": 0.3590200647786716,
      "video": 0.3382177660848879,
      "embed": 0.23856367307273757,
      "blog": 0.21735424919850901,
      "publish": 0.21735424919850901,
      "tableau": 0.21229341287518436,
      "postgresql": 0.19692629577542817,
      "visualisation": 0.19692629577542817,
      "tutorial": 0.19108398900095583
    }
  },
  "technical_debt": {
    "title": "Technical Debt",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Technical debt refers to the concept in software development where developers take shortcuts or make suboptimal decisions to spped up the delivery of a project....",
    "TFIDF_Score": {
      "debt": 0.6180652748060863,
      "technical": 0.39062525240164325,
      "codebase": 0.19389386611820356,
      "refactoring": 0.19389386611820356,
      "developer": 0.13657296853394044,
      "spa": 0.1292625774121357,
      "address": 0.12809868485756276,
      "area": 0.12706499584063927,
      "shortcut": 0.12361305496121726,
      "development": 0.11347508677865674
    }
  },
  "technical_design_doc_template": {
    "title": "Technical Design Doc Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "[Project name] Design Doc About this doc Metadata about this document. Describe the scope and current status. This doc describes the technical approach, milestones, and...",
    "TFIDF_Score": {
      "name": 0.3917556767903356,
      "milestone": 0.2878084721597153,
      "doc": 0.23391640081865375,
      "sign": 0.21541539476987565,
      "scope": 0.20977676466331596,
      "project": 0.1873439731827695,
      "approach": 0.17964538305485456,
      "alternative": 0.17325188699140573,
      "date": 0.1713756454184047,
      "flag": 0.1504380088589843
    }
  },
  "telecommunications": {
    "title": "Telecommunications",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Network Optimization Overview: In telecommunications, RL is used to enhance network performance, optimize resource allocation, and manage traffic efficiently. Applications: Traffic Management: RL algorithms can...",
    "TFIDF_Score": {
      "allocation": 0.3483320302356516,
      "network": 0.3478329239626709,
      "traffic": 0.3384169067037985,
      "resource": 0.29667758751660617,
      "qos": 0.18489461457725828,
      "telecommunication": 0.16655395266097137,
      "bandwidth": 0.16064957831128793,
      "service": 0.15650490854558463,
      "scheduling": 0.14821329074468445,
      "demand": 0.13978701075360533
    }
  },
  "tensorflow": {
    "title": "Tensorflow",
    "tags": [
      "deep_learning",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "pytorch_vs_tensorflow",
      "handwritten_digit_classification",
      "machine_learning"
    ],
    "inlinks": [
      "pytorch",
      "pytorch_vs_tensorflow",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "deep_learning"
    ],
    "summary": "Open sourced by Google Based on a dataflow graph Text summarization with TensorFlow Open-source library for numerical computation and large-scale [[Machine Learning]], focusing on static...",
    "TFIDF_Score": {
      "tensorflow": 0.4671410813839529,
      "dataflow": 0.44336706907819107,
      "graph": 0.2654950451036276,
      "open": 0.2487004151045608,
      "handwritten": 0.22168353453909553,
      "sourced": 0.22168353453909553,
      "digit": 0.1996935876242807,
      "summarization": 0.16484034637145026,
      "pytorch": 0.16230100849051532,
      "static": 0.14708665449398906
    }
  },
  "terminal_commands": {
    "title": "Terminal commands",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "jupyter nbconvert K-Means_VideoGames_Raw.ipynb --to python --no-prompt",
    "TFIDF_Score": {
      "means_videogames_raw": 0.541488261983232,
      "nbconvert": 0.5100681436196717,
      "jupyter": 0.41677042881769394,
      "prompt": 0.3489290022653549,
      "ipynb": 0.3219381777976467,
      "python": 0.21800853587947205
    }
  },
  "test_loss_when_evaluating_models": {
    "title": "Test Loss When Evaluating Models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "accuracy",
      "loss_function",
      "evaluation_metrics",
      "standardised/outliers",
      "model_selection",
      "hyperparameter_tuning",
      "model_evaluation"
    ],
    "inlinks": [],
    "summary": "Test loss is used for [[Model Evaluation]] to assess how well a model generalizes to unseen data, which is essential for evaluating its performance in...",
    "TFIDF_Score": {
      "loss": 0.5283845168962337,
      "accuracy": 0.331612527897588,
      "model": 0.3079603528505287,
      "test": 0.29332582114030287,
      "outlier": 0.19644808299201136,
      "high": 0.17204213199078147,
      "prediction": 0.14992395809915324,
      "low": 0.13008962022768655,
      "might": 0.10708176507010779,
      "correct": 0.1060252824461589
    }
  },
  "testing": {
    "title": "Testing",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "types_of_computational_bugs",
      "unittest",
      "model_deployment",
      "python",
      "hypothesis_testing",
      "pytest",
      "common_security_vulnerabilities_in_software_development",
      "continuous_integration"
    ],
    "inlinks": [
      "debugging",
      "software_development_portal",
      "maintainable_code",
      "hypothesis_testing"
    ],
    "summary": "Testing in coding projects refers to the systematic process of evaluating software to ensure it meets specified requirements and functions correctly. It enhances software robustness,...",
    "TFIDF_Score": {
      "testing": 0.6010781525256845,
      "software": 0.25724937721245206,
      "test": 0.25177686433727114,
      "practice": 0.1919202861070288,
      "best": 0.16462960709707328,
      "expected": 0.15901314402567512,
      "bug": 0.1551503011147887,
      "deployment": 0.121085869402727,
      "security": 0.11672442927095975,
      "behaves": 0.11108151329839708
    }
  },
  "testing_pytest.py": {
    "title": "Testing_Pytest.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "testing_pytest.py",
      "pytest",
      "ml_tools"
    ],
    "inlinks": [
      "debugging",
      "testing_pytest.py"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_Pytest.py In [[ML_Tools]] see: [[Testing_Pytest.py]] The pytest example script demonstrates several key features of the [[Pytest]] testing framework: Fixtures: The script uses a fixture named...",
    "TFIDF_Score": {
      "test": 0.43091331405818084,
      "pytest": 0.4114186854201322,
      "marker": 0.2833217476353745,
      "script": 0.21864032976354972,
      "function": 0.2075667113088608,
      "testing_pytest": 0.18888116509024966,
      "fixture": 0.18062596544858742,
      "mark": 0.16899091503813096,
      "testing": 0.16137076064855527,
      "mock": 0.16073571539646872
    }
  },
  "testing_unittest.py": {
    "title": "Testing_unittest.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pytest"
    ],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_unittest.py To explore testing in Python, let's focus on some key concepts and provide a simple example using the unittest framework, which is a built-in...",
    "TFIDF_Score": {
      "test": 0.5142599519817326,
      "behavior": 0.20151056198422299,
      "code": 0.18851121196659854,
      "testing": 0.18054643383675653,
      "unittest": 0.1754330465888351,
      "unit": 0.17511704753172116,
      "development": 0.14841247948780611,
      "run": 0.14015200731217617,
      "bdd": 0.13460612377209802,
      "test_math_operations": 0.13460612377209802
    }
  },
  "text2cypher": {
    "title": "Text2Cypher",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "graphrag",
      "interpretability",
      "cypher",
      "neo4j"
    ],
    "inlinks": [
      "graphrag",
      "how_to_search_within_a_graph"
    ],
    "summary": "Text2Cypher is a concept that allows users to convert natural language queries into Cypher queries, which are used to interact with [[GraphRAG|graph database]] like [[Neo4j]]....",
    "TFIDF_Score": {
      "query": 0.43347872846101065,
      "cypher": 0.33933405786554793,
      "text2cypher": 0.3245031964000493,
      "graph": 0.26964433294235063,
      "user": 0.2370714914127939,
      "ask": 0.2120230094708118,
      "language": 0.20867370159676404,
      "natural": 0.208442447466372,
      "vague": 0.16225159820002466,
      "traversal": 0.1518001355902829
    }
  },
  "tf-idf": {
    "title": "TF-IDF",
    "tags": [
      "NLP",
      "preprocessing",
      "code_snippet"
    ],
    "aliases": [
      "TFIDF"
    ],
    "outlinks": [
      "recommender_systems",
      "nltk",
      "clustering",
      "statistics",
      "search",
      "tokenisation",
      "bag_of_words"
    ],
    "inlinks": [
      "nlp",
      "cosine_similarity",
      "bag_of_words"
    ],
    "summary": "TF-IDF is a statistical technique used in text analysis to determine the importance of a word in a document relative to a collection of documents...",
    "TFIDF_Score": {
      "idf": 0.49365347845033647,
      "term": 0.41379309606057285,
      "document": 0.36282967255573695,
      "docs_tokens": 0.22160121455098722,
      "token": 0.20276046206517587,
      "doc": 0.1854179881327588,
      "corpus": 0.16753852691857132,
      "score": 0.13448354485811478,
      "tokenize": 0.13340065407471333,
      "doc_id": 0.12662926545770697
    }
  },
  "thinking_systems": {
    "title": "Thinking Systems",
    "tags": [
      "career",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "knowledge_work",
      "scientific_method"
    ],
    "inlinks": [],
    "summary": "A thinking system is a point of view that helps solve a problem. Part of [[Knowledge Work]]. We view problems through the view of our...",
    "TFIDF_Score": {
      "thinking": 0.566814190232385,
      "system": 0.25309689372025257,
      "design": 0.21035836167295097,
      "view": 0.171231190724884,
      "user": 0.163548706372276,
      "scientific": 0.15297196518151998,
      "whole": 0.1494101941064399,
      "traffic": 0.136459923305033,
      "problem": 0.13249101376092332,
      "human": 0.11901731051518785
    }
  },
  "time_series_forecasting": {
    "title": "Time Series Forecasting",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "time_series",
      "xgboost",
      "lightgbm",
      "forecasting_baseline.py"
    ],
    "inlinks": [
      "time_series",
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "use_cases_for_a_simple_neural_network_like"
    ],
    "summary": "With [[Time Series]] dataset we often want to predict future terms. These are methods to do so. Resources: TimeSeries Forecasting Statistical Methods [[Forecasting_Baseline.py]] [[Forecasting_Exponential_Smoothing.py]] [[Forecasting_AutoArima.py]]...",
    "TFIDF_Score": {
      "method": 0.3900743541361492,
      "forecasting_baseline": 0.30670236420916847,
      "forecasting_exponential_smoothing": 0.30670236420916847,
      "forecasting_autoarima": 0.29329769653938403,
      "lightgbm": 0.2744048785769642,
      "timeseries": 0.26722216254413916,
      "xgboost": 0.2461617248052613,
      "forecasting": 0.22069528997170318,
      "future": 0.2026271912797306,
      "series": 0.19941243929332816
    }
  },
  "time_series_identify_trends_and_patterns": {
    "title": "Time Series Identify Trends and Patterns",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series"
    ],
    "summary": "Analyze long-term trends, seasonal patterns, and cyclical behaviors.",
    "TFIDF_Score": {
      "cyclical": 0.5307867481124721,
      "seasonal": 0.4356267857969643,
      "trend": 0.33032376960004395,
      "behavior": 0.31784329838370584,
      "long": 0.3155756652242248,
      "analyze": 0.3051538584808872,
      "pattern": 0.25348249391221345,
      "term": 0.24778281596336674
    }
  },
  "time_series": {
    "title": "Time Series",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "anomaly_detection",
      "time_series_identify_trends_and_patterns",
      "time_series_forecasting",
      "ml_tools"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "anomaly_detection_in_time_series",
      "cross_validation",
      "time_series_forecasting",
      "datasets"
    ],
    "summary": "Time series data is a sequence of data points collected or recorded at successive points in time, typically at uniform intervals. It captures the temporal...",
    "TFIDF_Score": {
      "series": 0.5304758149556148,
      "time": 0.42985497125146155,
      "timeseries": 0.20310367075272415,
      "day": 0.1904719896608013,
      "data": 0.19020883375070888,
      "forecasting": 0.16774066598493997,
      "trend": 0.15400790844370882,
      "precipitation": 0.12373520229170433,
      "recorded": 0.12373520229170433,
      "successive": 0.12373520229170433
    }
  },
  "tokenisation": {
    "title": "Tokenisation",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "nlp"
    ],
    "inlinks": [
      "tf-idf",
      "generative_ai_from_theory_to_practice",
      "normalisation_of_text"
    ],
    "summary": "Tokenisation is a fundamental process in natural language processing ([[NLP]]) that involves breaking down text into smaller units called tokens. These tokens can be words,...",
    "TFIDF_Score": {
      "temp": 0.5775546120796372,
      "token": 0.29565444438089405,
      "tokenisation": 0.2887773060398186,
      "text_original": 0.249269415737736,
      "word": 0.2290305164270113,
      "word_tokenize": 0.22454308127651315,
      "sent_tokenize": 0.166179610491824,
      "text_sentence_tokens_nltk": 0.166179610491824,
      "text_word_tokens_nltk": 0.166179610491824,
      "tokens_no_stopwords": 0.166179610491824
    }
  },
  "toml": {
    "title": "TOML",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "tool.ruff",
      "tool.bandit",
      "tool.uv",
      "pytest"
    ],
    "inlinks": [
      "tool.ruff",
      "software_development_portal",
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "A .toml file is a configuration file format that stands for \"Tom's Obvious, Minimal Language.\" It is designed to be easy to read due to...",
    "TFIDF_Score": {
      "toml": 0.6387422088438794,
      "file": 0.35643681502616553,
      "configuration": 0.295312287807139,
      "setting": 0.15211662859663588,
      "tool": 0.13544773447399103,
      "environment": 0.13315184840468614,
      "8001": 0.1297119120342756,
      "tom": 0.1297119120342756,
      "project": 0.11408747144747691,
      "parse": 0.10931851397295393
    }
  },
  "tool.bandit": {
    "title": "tool.bandit",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bandit_example_nonfixed.py",
      "ml_tools",
      "common_security_vulnerabilities_in_software_development",
      "bandit_example_output"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development",
      "toml"
    ],
    "summary": "Bandit: A Security Linter for Python Resources Bandit Documentation How to Use Bandit Installation To install Bandit, use pip by running the following command in...",
    "TFIDF_Score": {
      "bandit": 0.7161194521457029,
      "subprocess": 0.23921689700985577,
      "script": 0.1861471617233934,
      "security": 0.17918253980370724,
      "command": 0.16139689000412094,
      "injection": 0.14340876343461748,
      "example": 0.14016384073681837,
      "bash": 0.13887399080678411,
      "call": 0.13498944173951627,
      "user_input": 0.12789011564906846
    }
  },
  "tool.ruff": {
    "title": "tool.ruff",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "python",
      "toml"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Ruff is a fast [[Python]] linter and code formatter. It is designed to enforce coding style and catch potential errors in Python code. Ruff aims...",
    "TFIDF_Score": {
      "style": 0.4279377879168115,
      "black": 0.3287120207745569,
      "ruff": 0.2783071268179101,
      "magic": 0.21353926348271446,
      "trailing": 0.21353926348271446,
      "indent": 0.20114854441279276,
      "code": 0.19936955929155398,
      "comma": 0.19235718932471954,
      "ending": 0.19235718932471954,
      "quote": 0.19235718932471954
    }
  },
  "tool.uv": {
    "title": "tool.uv",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "virtual_environments",
      "toml"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Appears in [[TOML]] file Link: https://github.com/astral-sh/uv uv is a tool for managing Python development [[Virtual environments]], projects, and dependencies. It offers a range of features...",
    "TFIDF_Score": {
      "project": 0.37008716386055074,
      "dependency": 0.3263363844221521,
      "python": 0.29041135029326764,
      "managing": 0.2625802734702237,
      "environment": 0.2313910169429887,
      "tool": 0.18830464254498572,
      "installing": 0.16986671844361173,
      "version": 0.16697882948506146,
      "command": 0.16439719143609258,
      "development": 0.1491200392418047
    }
  },
  "train-dev-test_sets": {
    "title": "Train-Dev-Test Sets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "handling_different_distributions",
      "training_data",
      "model_parameters",
      "evaluation_metrics",
      "validation_data",
      "cross_validation",
      "model_building",
      "distributions",
      "datasets",
      "model_evaluation"
    ],
    "inlinks": [
      "model_building"
    ],
    "summary": "In [[Model Building]] train the model using the prepared data to learn patterns and make predictions. The model is trained on your dataset, which is...",
    "TFIDF_Score": {
      "set": 0.3304685554850238,
      "model": 0.30669318037522775,
      "dev": 0.2968145765099372,
      "training": 0.2624895803456355,
      "test": 0.2354655099708738,
      "validation": 0.20247441886647968,
      "data": 0.20108704167546848,
      "development": 0.15532342586761072,
      "evaluation": 0.15532342586761072,
      "allocated": 0.13269987446204679
    }
  },
  "transaction": {
    "title": "Transaction",
    "tags": null,
    "aliases": [
      "Transactions"
    ],
    "outlinks": [
      "acid_transaction",
      "sqlite",
      "queries",
      "transaction",
      "concurrency",
      "data_integrity",
      "database_management_system_(dbms)",
      "granularity"
    ],
    "inlinks": [
      "transaction",
      "query_optimisation",
      "acid_transaction"
    ],
    "summary": "Transactions are used for maintaining [[Data Integrity]] and should adhere to the [[ACID Transaction]]. Transaction Operations Commit: Saves all changes made during the transaction. Rollback:...",
    "TFIDF_Score": {
      "transaction": 0.5891165722605343,
      "lock": 0.49077769591112125,
      "exclusive": 0.25104233805020465,
      "concurrency": 0.2012379382911288,
      "access": 0.16496995346005425,
      "database": 0.13812237890133394,
      "operation": 0.13468866826221212,
      "shared": 0.13061829094255697,
      "timestamping": 0.12552116902510232,
      "race": 0.10906171020247138
    }
  },
  "transfer_learning": {
    "title": "Transfer Learning",
    "tags": [
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "keras",
      "hugging_face",
      "performance_drift",
      "pytorch",
      "transfer_learning.py",
      "supervised_learning",
      "ml_tools"
    ],
    "inlinks": [
      "deep_learning",
      "bert",
      "transfer_learning.py",
      "llm",
      "imbalanced_datasets"
    ],
    "summary": "Transfer learning is a technique in machine learning that ==leverages knowledge gained from one setting (source domain) to improve performance on a different but related...",
    "TFIDF_Score": {
      "transfer": 0.3589461787838449,
      "learning": 0.2978725977589604,
      "labelled": 0.24896416635604043,
      "domain": 0.2428123885972945,
      "model": 0.20858880998670945,
      "transferred": 0.20827411691454312,
      "task": 0.20139558253653475,
      "target": 0.18387972724984325,
      "dataset": 0.17614458360636456,
      "fine": 0.14824579129013407
    }
  },
  "transfer_learning.py": {
    "title": "transfer_learning.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "transfer_learning"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Neural_Network/transfer_learning.py For deep learning, to do [[Transfer Learning]] we take out and replace a few end layers of the network. We can then train just...",
    "TFIDF_Score": {
      "layer": 0.4881428381134804,
      "network": 0.30512808081250703,
      "higher": 0.23327946332430868,
      "neural_network": 0.21625914823628203,
      "recognise": 0.21625914823628203,
      "transfer_learning": 0.20371060653770476,
      "pretrained": 0.18790131367293061,
      "last": 0.1733554007888572,
      "able": 0.1634997317211691,
      "transfer": 0.1560361249506598
    }
  },
  "transformed_target_regressor": {
    "title": "Transformed Target Regressor",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sklearn"
    ],
    "inlinks": [],
    "summary": "[[Sklearn]] The TransformedTargetRegressor is a utility class in scikit-learn that applies a transformation to the target values in a regression problem. This can be useful...",
    "TFIDF_Score": {
      "transformation": 0.45777822340272056,
      "target": 0.4326854420583298,
      "applies": 0.2574286649080923,
      "variable": 0.24861717181538528,
      "transformedtargetregressor": 0.22297583306823573,
      "inverse": 0.1879194157315322,
      "non": 0.18761265588911744,
      "applying": 0.15662142281420216,
      "regression": 0.1506899414951,
      "help": 0.13077059499596552
    }
  },
  "transformer": {
    "title": "Transformer",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "aliases": [
      "Transformers"
    ],
    "outlinks": [
      "multi-head_attention",
      "recurrent_neural_networks",
      "unsupervised_learning",
      "attention_mechanism",
      "transformers_vs_rnns",
      "bert",
      "standardised/attention_is_all_you_need",
      "feed_forward_neural_network",
      "nlp"
    ],
    "inlinks": [
      "word2vec.py",
      "ds_&_ml_portal",
      "rag",
      "recurrent_neural_networks",
      "attention_mechanism",
      "hugging_face",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "bert",
      "lstm",
      "mathematical_reasoning_in_transformers",
      "llm"
    ],
    "summary": "A transformer in machine learning (ML) refers to a deep learning model architecture designed to process sequential data, such as natural language processing ([[NLP]]). It...",
    "TFIDF_Score": {
      "attention": 0.4348700077046764,
      "transformer": 0.38234536630175914,
      "mechanism": 0.26299648549676297,
      "decoder": 0.2619348700755884,
      "encoder": 0.22758762616680886,
      "word": 0.20216080598708083,
      "network": 0.17246770645034887,
      "neural": 0.160585278262294,
      "feed": 0.12898495971681115,
      "sequence": 0.12735538004612484
    }
  },
  "transformers_vs_rnns": {
    "title": "Transformers vs RNNs",
    "tags": [
      "deep_learning"
    ],
    "aliases": null,
    "outlinks": [
      "reformer",
      "recurrent_neural_networks",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "time-series_data",
      "bert",
      "vanishing_and_exploding_gradients_problem",
      "lstm",
      "gated_recurrent_units",
      "longformer"
    ],
    "inlinks": [
      "transformer",
      "transformers_vs_rnns"
    ],
    "summary": "[[Transformer|Transformers]] and Recurrent Neural Networks ([[Recurrent Neural Networks]]) are both deep learning architectures ==used for processing sequential data==, but they differ significantly in structure, operation,...",
    "TFIDF_Score": {
      "transformer": 0.4990880741381835,
      "rnns": 0.42739655875025284,
      "sequence": 0.3399419348747082,
      "long": 0.2866792081093091,
      "attention": 0.18473824012836795,
      "dependency": 0.17451755153973061,
      "token": 0.13725855392659173,
      "mechanism": 0.10893110436239217,
      "bert": 0.10039240858090294,
      "sequential": 0.10039240858090294
    }
  },
  "ts_anomaly_detection": {
    "title": "TS_Anomaly_Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "ts_anomaly_detection.py": {
    "title": "TS_Anomaly_Detection.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_in_time_series"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/TS_Anomaly_Detection.py",
    "TFIDF_Score": {
      "ts_anomaly_detection": 0.4988882823510892,
      "timeseries": 0.4346689860756016,
      "build": 0.2826786215847396,
      "rhyslwells": 0.2657658893523426,
      "blob": 0.26234607356285,
      "github": 0.25292516945477617,
      "exploration": 0.2509816512119365,
      "ml_tools": 0.24194798895367606,
      "com": 0.23700321026496368,
      "main": 0.22589168457334746
    }
  },
  "turning_a_flat_file_into_a_database": {
    "title": "Turning a flat file into a database",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "'order_id',_'order_date',_'customer_id',_'amount'",
      "foreign_key",
      "'customer_id',_'customer_name',_'contact_name',_'country'"
    ],
    "inlinks": [
      "melt",
      "data_engineering_portal",
      "database"
    ],
    "summary": "Summary: Read and Clean the Data: Load the data from the Excel sheet and clean it. Split the Data: Separate the data into two DataFrames,...",
    "TFIDF_Score": {
      "customer_id": 0.3192991888754956,
      "conn": 0.2815552697000025,
      "cursor": 0.26124479089813274,
      "foreign": 0.2410463003068433,
      "customer": 0.2346579527749961,
      "2024": 0.23221759190945135,
      "doe": 0.2142410445610716,
      "table": 0.20199790240090537,
      "order": 0.16303698668351557,
      "data": 0.16140309292815433
    }
  },
  "types_of_computational_bugs": {
    "title": "Types of Computational Bugs",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "debugging"
    ],
    "inlinks": [
      "debugging",
      "testing"
    ],
    "summary": "Each of these types of bugs can have significant impacts on software functionality and performance, and understanding them is crucial for effective [[Debugging]] and software...",
    "TFIDF_Score": {
      "bug": 0.2539106749762205,
      "error": 0.2538827142818426,
      "leading": 0.20941162908885158,
      "occurs": 0.20304119935660747,
      "program": 0.1842061726402354,
      "overflow": 0.18286809035750012,
      "buffer": 0.17487569686887788,
      "thread": 0.17487569686887788,
      "example": 0.15938928452460505,
      "happens": 0.14941927542174935
    }
  },
  "types_of_database_schema": {
    "title": "Types of Database Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "snowflake_schema",
      "er_diagrams",
      "normalised_schema",
      "star_schema",
      "columnar_storage"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "There are several types of database schemas commonly used in data warehousing and database design. [[Star Schema]] [[Snowflake Schema]] Galaxy Schema (or Fact Constellation Schema):...",
    "TFIDF_Score": {
      "schema": 0.7166717588844139,
      "table": 0.2109437474485152,
      "denormalized": 0.18141489548113998,
      "galaxy": 0.18141489548113998,
      "database": 0.15970187101734723,
      "data": 0.15935742528225808,
      "columnar": 0.1423660309142103,
      "warehousing": 0.13963066070291502,
      "fact": 0.12585448751161246,
      "entity": 0.11384385342273722
    }
  },
  "types_of_neural_networks": {
    "title": "Types of Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "convolutional_neural_networks",
      "recurrent_neural_networks",
      "neural_network",
      "generative_adversarial_networks",
      "transformer",
      "feed_forward_neural_network"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "Types of [[Neural network]]: [[Feed Forward Neural Network]] [[Convolutional Neural Networks]] [[Recurrent Neural Networks]] [[Generative Adversarial Networks]] [[Transformer]]",
    "TFIDF_Score": {
      "network": 0.6242002887849173,
      "neural": 0.5424487122084394,
      "convolutional": 0.25003838209342666,
      "adversarial": 0.23911025825814006,
      "feed": 0.2178522404101046,
      "recurrent": 0.19737748347932565,
      "generative": 0.18644935964403905,
      "forward": 0.18197515800756175,
      "transformer": 0.1761194656312902,
      "type": 0.10543542253874984
    }
  },
  "typescript": {
    "title": "TypeScript",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "debugging",
      "data_validation"
    ],
    "summary": "Superset of JavaScript adding static typing and object-oriented features for building large-scale applications.",
    "TFIDF_Score": {
      "superset": 0.425554833349475,
      "typing": 0.39252895853231484,
      "javascript": 0.3415536494041502,
      "oriented": 0.33075327637840624,
      "static": 0.29974793957160484,
      "adding": 0.2811487314075952,
      "object": 0.2534133271800041,
      "building": 0.23868665092042993,
      "scale": 0.2329615007338125,
      "large": 0.19152209190476951
    }
  },
  "typical_output_formats_in_neural_networks": {
    "title": "Typical Output Formats in Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "generative_ai",
      "loss_function",
      "binary_classification",
      "neural_network",
      "classification",
      "activation_function",
      "regression"
    ],
    "inlinks": [],
    "summary": "The output format of a [[Neural network]] is largely determined by the specific task it is designed to perform. Classification [[Binary Classification]] Single Output Node:...",
    "TFIDF_Score": {
      "output": 0.5430907272080813,
      "representing": 0.24549051076947065,
      "node": 0.2443947429362526,
      "format": 0.17543513201808586,
      "single": 0.1628519651485042,
      "probability": 0.1538921220740061,
      "image": 0.15233227499384933,
      "value": 0.14973267226146636,
      "example": 0.14869203032561873,
      "classification": 0.13028157211880337
    }
  },
  "ubuntu": {
    "title": "Ubuntu",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux"
    ],
    "summary": "Ubuntu is a popular open-source operating system based on the [[Linux]] kernel. It is designed to be user-friendly: Desktop Environment: Ubuntu provides a graphical user...",
    "TFIDF_Score": {
      "ubuntu": 0.6445343784948986,
      "user": 0.22514793731248017,
      "desktop": 0.20568427450566878,
      "development": 0.18860471451672614,
      "operating": 0.18023186250367348,
      "source": 0.15629130028213792,
      "server": 0.15045444935552885,
      "open": 0.14392998434307097,
      "software": 0.1430858076573782,
      "community": 0.13423951557301295
    }
  },
  "uml": {
    "title": "UML",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.drawio.com/ https://www.reddit.com/r/SoftwareEngineering/comments/133iw7n/is_there_any_free_handy_tool_to_create_uml/ https://plantuml.com/",
    "TFIDF_Score": {
      "com": 0.38722556723762275,
      "www": 0.3867843659091626,
      "http": 0.36573904838608434,
      "133iw7n": 0.2884381431359235,
      "drawio": 0.2884381431359235,
      "is_there_any_free_handy_tool_to_create_uml": 0.2884381431359235,
      "plantuml": 0.2884381431359235,
      "reddit": 0.2884381431359235,
      "softwareengineering": 0.2884381431359235,
      "comment": 0.2598264581546551
    }
  },
  "unittest": {
    "title": "unittest",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "testing"
    ],
    "summary": "@patch (from unittest.mock) Explanation @patch is used to replace objects/functions with mock versions during tests. It is part of Python’s unittest.mock module. Example & Usage...",
    "TFIDF_Score": {
      "fetch_data": 0.426441764882106,
      "mocked": 0.426441764882106,
      "patch": 0.41929567237713145,
      "mock": 0.3418397957596434,
      "unittest": 0.18526145247460887,
      "__main__": 0.142147254960702,
      "return_value": 0.142147254960702,
      "test": 0.13576767249124677,
      "return": 0.1267690311698844,
      "data": 0.12486417116632574
    }
  },
  "univariate_vs_multivariate": {
    "title": "univariate vs multivariate",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Single feature versus multiple features",
    "TFIDF_Score": {
      "versus": 0.6252114579791568,
      "feature": 0.5709712866783401,
      "single": 0.3969372056018354,
      "multiple": 0.35432086787185435
    }
  },
  "unstructured_data": {
    "title": "unstructured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "sql"
    ],
    "inlinks": [
      "data_lake",
      "vector_database",
      "data_science"
    ],
    "summary": "[!Important] Unstructured data is data that does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used...",
    "TFIDF_Score": {
      "unstructured": 0.5323526109449738,
      "data": 0.33287055824884376,
      "structured": 0.24588180435634552,
      "example": 0.20000960029057996,
      "easily": 0.1989314780044201,
      "formatted": 0.1950601960928712,
      "defined": 0.19045661792872487,
      "record": 0.1382708920534102,
      "schema": 0.131604871589344,
      "content": 0.1269710786191499
    }
  },
  "unsupervised_learning": {
    "title": "Unsupervised Learning",
    "tags": [
      "#clustering",
      "field"
    ],
    "aliases": [
      "unsupervised"
    ],
    "outlinks": [
      "support_vector_machines",
      "isolated_forest",
      "standardised/outliers",
      "principal_component_analysis",
      "dimensionality_reduction",
      "k-means",
      "dbscan",
      "k-nearest_neighbours",
      "clustering"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "transformer",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "k-means",
      "clustering",
      "learning_styles"
    ],
    "summary": "Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or predefined outputs. Unsupervised learning involves discovering...",
    "TFIDF_Score": {
      "data": 0.2762972796643497,
      "algorithm": 0.2636339446634121,
      "unsupervised": 0.24563698176175672,
      "pattern": 0.24033948714222295,
      "clustering": 0.1967310028831515,
      "isolated": 0.19024348405855906,
      "learning": 0.18080080820374825,
      "description": 0.15533461475348645,
      "input": 0.15116149597053463,
      "point": 0.14767705972071
    }
  },
  "untitled": {
    "title": "Untitled",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "use_cases_for_a_simple_neural_network_like": {
    "title": "Use Cases for a Simple Neural Network Like",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "binary_classification",
      "neural_network",
      "time_series_forecasting",
      "regression"
    ],
    "inlinks": [
      "pytorch"
    ],
    "summary": "Scenarios where a simple [[Neural network|Neural Network]] work like this might be useful: [[Regression]] with Multiple Features If you have multiple input features and you...",
    "TFIDF_Score": {
      "network": 0.33928869530800976,
      "feature": 0.2449460164038471,
      "multiple": 0.2171474846931887,
      "predicting": 0.20956621252113017,
      "like": 0.19251525804080089,
      "weight": 0.18650104493176034,
      "based": 0.18305624734890813,
      "economic": 0.17908954746287795,
      "gradient": 0.16587856042241853,
      "classification": 0.14595908442302927
    }
  },
  "use_of_rnns_in_energy_sector": {
    "title": "Use of RNNs in energy sector",
    "tags": [
      "time_series",
      "deep_learning",
      "energy",
      "anomaly_detection"
    ],
    "aliases": [],
    "outlinks": [
      "gradient_boosting",
      "demand_forecasting",
      "shapley_additive_explanations",
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "recurrent_neural_networks"
    ],
    "summary": "For energy data problems, many interpretable machine learning algorithms can be applied in place of or alongside RNNs. These models offer transparency, making it easier...",
    "TFIDF_Score": {
      "energy": 0.5620350315991229,
      "interpretable": 0.2715490780661366,
      "factor": 0.22684247235884422,
      "clear": 0.19061629153511248,
      "demand": 0.18371977917433877,
      "consumption": 0.17480676348236363,
      "algorithm": 0.1545746749063283,
      "linear": 0.14831073049776974,
      "time": 0.14069892819831237,
      "usage": 0.13472860166943745
    }
  },
  "utilities": {
    "title": "Utilities",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "vacuum": {
    "title": "Vacuum",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "vanishing_and_exploding_gradients_problem": {
    "title": "vanishing and exploding gradients problem",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "vanishing_and_exploding_gradients_problem",
      "recurrent_neural_networks"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "recurrent_neural_networks",
      "batch_normalisation",
      "transformers_vs_rnns",
      "backpropagation",
      "vanishing_and_exploding_gradients_problem",
      "lstm",
      "forward_propagation"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]] [[vanishing and exploding gradients problem]] In standard RNNs, the difficulty lies in retaining useful information over long sequences due to the exponential...",
    "TFIDF_Score": {
      "long": 0.3427283043531082,
      "gradient": 0.30927913730189394,
      "rnn": 0.24291280002396795,
      "difficulty": 0.23655440846635864,
      "exponential": 0.23655440846635864,
      "rnns": 0.23655440846635864,
      "exploding": 0.2218423166683062,
      "vanishing": 0.21791097206897553,
      "lie": 0.21432193564379204,
      "recurrent": 0.21432193564379204
    }
  },
  "variance": {
    "title": "Variance",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "distributions",
      "boxplot"
    ],
    "inlinks": [
      "feature_selection",
      "regression_metrics",
      "standard_deviation",
      "covariance_structures",
      "principal_component_analysis",
      "t-test",
      "data_reduction"
    ],
    "summary": "Variance in a dataset is a statistical measure that represents the degree of spread or dispersion of the data points around the mean (average) of...",
    "TFIDF_Score": {
      "variance": 0.5361727180062642,
      "mean": 0.3568275060262187,
      "spread": 0.32524821167370366,
      "point": 0.24323568206911822,
      "average": 0.16792842523139206,
      "much": 0.16201568395566565,
      "indicates": 0.16093228151464212,
      "data": 0.15169441888965965,
      "mu_x": 0.13815297077846925,
      "measure": 0.13572138017664817
    }
  },
  "vector_database": {
    "title": "Vector Database",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "langchain",
      "semantic_search",
      "vector_embedding",
      "cosine_similarity",
      "llm",
      "standardised/vector_embedding",
      "unstructured_data"
    ],
    "inlinks": [
      "relationships_in_memory"
    ],
    "summary": "Overview Vector databases are specialized systems designed to handle and manage [[Vector Embedding]]. As most real-world data is unstructured, such as text, images, and audio,...",
    "TFIDF_Score": {
      "vector": 0.6355671492948235,
      "search": 0.4014341385858308,
      "database": 0.2028026279183563,
      "semantic": 0.18952318307594762,
      "similarity": 0.18342460637781155,
      "embedding": 0.16394689028725726,
      "embeddings": 0.13933783562371055,
      "application": 0.11317124615301774,
      "capability": 0.11117358321497678,
      "data": 0.09106435002202046
    }
  },
  "vector_embedding": {
    "title": "Vector Embedding",
    "tags": [
      "math",
      "language_models",
      "drafting"
    ],
    "aliases": [
      "embedding",
      "word embedding"
    ],
    "outlinks": [
      "how_to_search_within_a_graph",
      "pasted_image_20241015211844.png",
      "attention_mechanism",
      "semantic_relationships",
      "pytorch",
      "pasted_image_20241015211934.png",
      "bert",
      "dimensionality_reduction",
      "vector_embedding.py",
      "language_models",
      "t-sne",
      "how_would_you_decide_between_using_tf-idf_and_word2vec_for_text_vectorization",
      "ml_tools"
    ],
    "inlinks": [
      "bert",
      "how_llms_store_facts",
      "vector_database"
    ],
    "summary": "Vector Embedding is a technique used in machine learning and natural language processing to represent data in a continuous vector space. This representation captures the...",
    "TFIDF_Score": {
      "embeddings": 0.45749728214243623,
      "word": 0.3335965642452026,
      "vector": 0.309155606635073,
      "semantic": 0.2419955249232011,
      "space": 0.20802833096249612,
      "similarity": 0.16729176114430616,
      "language": 0.16358073912959925,
      "sentence": 0.15822363765313605,
      "dimensional": 0.15323878923730722,
      "close": 0.1299788617124053
    }
  },
  "vectorisation": {
    "title": "Vectorisation",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "pasted_image_20241217204829.png",
      "numpy",
      "gpu"
    ],
    "inlinks": [],
    "summary": "Link ![[Pasted image 20241217204829.png|500]] Numpy dot is better than for loop and summing. Why does it run faster? A: Designed to run in parallel Sequentially...",
    "TFIDF_Score": {
      "numpy": 0.4043762498255368,
      "parallel": 0.38317187287779636,
      "run": 0.28754271405139026,
      "20241217204829": 0.2761645080912358,
      "gpu": 0.24877030903487227,
      "summing": 0.24877030903487227,
      "dot": 0.21672115066434514,
      "sequentially": 0.20879034876477126,
      "versus": 0.2053515307932451,
      "simultaneously": 0.19653256848006478
    }
  },
  "vectorized_engine": {
    "title": "Vectorized Engine",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "columnar_storage",
      "duckdb"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Vectorized Engine A modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of...",
    "TFIDF_Score": {
      "vectorized": 0.45589835245626603,
      "engine": 0.44239701136559817,
      "simd": 0.32265441340276335,
      "instruction": 0.2906486379747006,
      "modern": 0.22383777106321634,
      "locality": 0.16132720670138168,
      "processing": 0.14065355455817663,
      "duckdb": 0.14017253983287578,
      "cache": 0.1359632297713906,
      "overhead": 0.1359632297713906
    }
  },
  "vector_embedding.py": {
    "title": "Vector_Embedding.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "vector_embedding"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Vector_Embedding.py Explanation of the Script Vocabulary and Embedding Layer: Terms are mapped to indices using a dictionary. The embedding layer learns continuous vector representations for...",
    "TFIDF_Score": {
      "embedding": 0.42072246174785044,
      "term": 0.33117793302244536,
      "similarity": 0.3138042724494073,
      "cosine": 0.2843434124408804,
      "semantic": 0.24317832440216386,
      "visualization": 0.19554913078677996,
      "sne": 0.18956227496058692,
      "space": 0.18291447061565042,
      "relationship": 0.15981424701348004,
      "layer": 0.1334447419541495
    }
  },
  "vercel": {
    "title": "Vercel",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "view_use_case": {
    "title": "View Use Case",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "views"
    ],
    "summary": "View Use Case Scenario A company wants to generate monthly performance reports for its employees. The performance data is spread across multiple tables, including employees,...",
    "TFIDF_Score": {
      "view": 0.45561368933726415,
      "employee_id": 0.3433447305463345,
      "employee": 0.2932681889703958,
      "department": 0.25530567448656744,
      "department_id": 0.22889648703088972,
      "report": 0.18611558985931656,
      "performance": 0.17784580423937976,
      "department_name": 0.17167236527316726,
      "employee_performance": 0.17167236527316726,
      "performance_reviews": 0.17167236527316726
    }
  },
  "views": {
    "title": "Views",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "sqlite",
      "querying",
      "common_table_expression",
      "view_use_case",
      "soft_deletion",
      "database_schema"
    ],
    "inlinks": [
      "common_table_expression"
    ],
    "summary": "Views are virtual tables defined by SQL [[Querying|Query]] that ==simplify complex data representation.== They can remove unnecessary columns, aggregate results, partition data, and secure sensitive...",
    "TFIDF_Score": {
      "view": 0.6267345982871241,
      "data": 0.176568575681877,
      "table": 0.16998283504694267,
      "access": 0.15370543264954842,
      "query": 0.1407281035317157,
      "temporary": 0.1316867369175484,
      "user": 0.1282747349041631,
      "simplification": 0.12701841305475675,
      "expression": 0.11997917144488432,
      "abstraction": 0.11718559850518016
    }
  },
  "violin_plot": {
    "title": "Violin plot",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "boxplot"
    ],
    "inlinks": [
      "distributions"
    ],
    "summary": "An extension of a [[Boxplot]] showing the data distribution. Useful when comparing distributions, skewness. python data = [...] # Your data sns.violinplot(data=data, color=\"purple\", fill=\"lightblue\", scale=\"area\")...",
    "TFIDF_Score": {
      "data": 0.3137286126060898,
      "lightblue": 0.2857227059966232,
      "purple": 0.2857227059966232,
      "violinplot": 0.2857227059966232,
      "distribution": 0.2697737636044955,
      "fill": 0.26914350775382057,
      "skewness": 0.24825618819274894,
      "boxplot": 0.24080118115619523,
      "sn": 0.23449806251784094,
      "extension": 0.22903805280137257
    }
  },
  "virtual_environments": {
    "title": "Virtual environments",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "poetry"
    ],
    "inlinks": [
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "Setting up virtual env For windows (need to not be in a venv before del) cmd rmdir /s /q venv python -m venv venv venv\\Scripts\\activate...",
    "TFIDF_Score": {
      "venv": 0.7656675850681823,
      "_vejzukmn4s": 0.1625665178590772,
      "activate": 0.1625665178590772,
      "gitignore": 0.1625665178590772,
      "del": 0.15313351701363645,
      "rmdir": 0.15313351701363645,
      "env": 0.14644069640246846,
      "freeze": 0.14644069640246846,
      "interpreter": 0.14644069640246846,
      "poetry": 0.14644069640246846
    }
  },
  "wcss_and_elbow_method": {
    "title": "WCSS and elbow method",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters",
      "k-means"
    ],
    "summary": "USE: WCSS (within-cluster sum of squares) WCSS is a measure developed within the ANOVA framework. It gives a very good idea about the different distance...",
    "TFIDF_Score": {
      "wcss": 0.4054702249957755,
      "cluster": 0.39464968044168897,
      "elbow": 0.36304076345834735,
      "kmeans": 0.2586907837898073,
      "elbow_num": 0.2512802338186372,
      "plt": 0.2486627011915915,
      "var2": 0.22635443581608142,
      "plot": 0.21495783279540556,
      "var1": 0.20142863781352563,
      "number": 0.17213376767186486
    }
  },
  "weak_learners": {
    "title": "Weak Learners",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "learning_rate",
      "decision_tree",
      "model_ensemble",
      "hyperparameter"
    ],
    "inlinks": [
      "gradient_boosting",
      "boosting"
    ],
    "summary": "Weak learners are simple models that perform slightly better than random guessing. They are often used as the building blocks in [[Model Ensemble]] methods to...",
    "TFIDF_Score": {
      "learner": 0.5899034592182086,
      "weak": 0.4923963208214871,
      "model": 0.2612300006324807,
      "ensemble": 0.2601732538842531,
      "rate": 0.16445024543125727,
      "combined": 0.14641542127288618,
      "smaller": 0.11217914289917644,
      "learning": 0.10776889664482732,
      "perform": 0.10461921719485055,
      "performance": 0.10358867486060014
    }
  },
  "web_feature_server_(wfs)": {
    "title": "Web Feature Server (WFS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Feature Server (WFS) Purpose: WFS is designed to serve raw geographic features (vector data) over the web. Functionality: - Feature-Based: It delivers geographic...",
    "TFIDF_Score": {
      "feature": 0.30977333418983727,
      "wfs": 0.2739466739834386,
      "geographic": 0.25630035522933103,
      "gi": 0.25630035522933103,
      "attribute": 0.20177863628646717,
      "web": 0.18080847560149302,
      "access": 0.15987597928671163,
      "delivers": 0.1520566275721862,
      "gml": 0.1520566275721862,
      "query": 0.1463776717417209
    }
  },
  "web_map_tile_service_(wmts)": {
    "title": "Web Map Tile Service (WMTS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Map Tile Service (WMTS) Purpose: WMTS is designed to serve pre-rendered, cached image tiles of maps. Functionality: - Tile-Based: It serves map images...",
    "TFIDF_Score": {
      "tile": 0.6758024588794267,
      "map": 0.34857373694458754,
      "wmts": 0.28962962523404,
      "cached": 0.19308641682269334,
      "image": 0.14198294668592953,
      "web": 0.12743962238131148,
      "consortium": 0.10717439618862154,
      "efficient": 0.10617724682676175,
      "geospatial": 0.10095555000079064,
      "jpeg": 0.10095555000079064
    }
  },
  "what_algorithms_or_models_are_used_within_the_energy_sector": {
    "title": "What algorithms or models are used within the energy sector",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "what_algorithms_or_models_are_used_within_the_telecommunication_sector": {
    "title": "What algorithms or models are used within the telecommunication sector",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts": {
    "title": "What are the best practices for evaluating the effectiveness of different prompts",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "prompt_engineering"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "what_can_abm_solve_within_the_energy_sector": {
    "title": "What can ABM solve within the energy sector",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "agent-based_modelling"
    ],
    "inlinks": [],
    "summary": "[[Agent-Based Modelling]] energy systems analysis",
    "TFIDF_Score": {
      "modelling": 0.5135305475136283,
      "agent": 0.5065033021454601,
      "energy": 0.5065033021454601,
      "analysis": 0.28812894489425206,
      "system": 0.27613219401366823,
      "based": 0.25283969060486844
    }
  },
  "what_is_the_difference_between_odds_and_probability": {
    "title": "What is the difference between odds and probability",
    "tags": [
      "#question",
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression_does_not_predict_probabilities"
    ],
    "summary": "",
    "TFIDF_Score": {}
  },
  "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models.": {
    "title": "What is the role of gradient-based optimization in training deep learning models.",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "",
    "TFIDF_Score": {}
  },
  "when_and_why_not_to_us_regularisation": {
    "title": "When and why not to us regularisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "validation_data",
      "training_data"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "While regularization is tool to combat overfitting, it is not a always useful. It is crucial to consider the model's - complexity, - the quality...",
    "TFIDF_Score": {
      "regularization": 0.6169732780917532,
      "validation": 0.3202348751005465,
      "model": 0.22232281273611687,
      "may": 0.21046423129984215,
      "data": 0.1681945428905345,
      "underfitting": 0.16077145808092713,
      "lambda": 0.15792237133794113,
      "parameter": 0.1314815446741438,
      "performance": 0.12021864487332301,
      "representative": 0.11736097252285686
    }
  },
  "why_and_when_is_feature_scaling_necessary": {
    "title": "Why and when is feature scaling necessary",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "support_vector_machines",
      "decision_tree",
      "random_forests",
      "feature_scaling",
      "k-means"
    ],
    "inlinks": [],
    "summary": "[[Feature Scaling]] is useful for models that use distances like [[Support Vector Machines|SVM]] and [[K-means]] When Scaling Is Unnecessary Tree-based Algorithms: Algorithms like [[Decision Tree]],...",
    "TFIDF_Score": {
      "scaling": 0.48892251559780386,
      "tree": 0.31685348273046254,
      "feature": 0.2645396769046047,
      "distance": 0.2558783835309009,
      "split": 0.22396089357891072,
      "invariant": 0.1947795695569732,
      "boosted": 0.18347739080268483,
      "algorithm": 0.17491647046136538,
      "percentage": 0.16415618860535627,
      "uniform": 0.1598592996655613
    }
  },
  "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy": {
    "title": "Why does increasing the number of models in a ensemble not necessarily improve the accuracy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": [],
    "summary": "Increasing the number of models in an ensemble ([[Model Ensemble]]) does not always lead to improved accuracy due to several limiting factors: Convergence of Predictions:...",
    "TFIDF_Score": {
      "model": 0.3266811670574706,
      "ensemble": 0.3137396648652692,
      "lead": 0.24097712537598592,
      "noise": 0.2112052734749976,
      "limited": 0.20915977657684615,
      "overfitting": 0.17342694758689514,
      "irreducible": 0.1607735350206118,
      "pattern": 0.15355800331123481,
      "diminishing": 0.1514445728717849,
      "accuracy": 0.14689344745319335
    }
  },
  "why_does_label_encoding_give_different_predictions_from_one-hot_encoding": {
    "title": "Why does label encoding give different predictions from one-hot encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Label Encoding and One-Hot Encoding give different predictions because they represent categorical variables in fundamentally different ways. Label Encoding might cause issues by implying an...",
    "TFIDF_Score": {
      "encoding": 0.44013146954066995,
      "category": 0.3057144003136683,
      "princeton": 0.2705107878442726,
      "west": 0.2705107878442726,
      "windsor": 0.2705107878442726,
      "robbinsville": 0.24045403363935347,
      "hot": 0.22707514101773632,
      "ordinal": 0.2103972794344343,
      "label": 0.19241540442447522,
      "relationship": 0.18688803098526727
    }
  },
  "why_does_the_adam_optimizer_converge": {
    "title": "Why does the Adam Optimizer converge",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "adam_optimizer"
    ],
    "summary": "Why the Adam Optimizer Converges The Adam optimizer is able to efficiently handle sparse gradients and adaptively adjust learning rates. The convergence of Adam, often...",
    "TFIDF_Score": {
      "adam": 0.33825320795945985,
      "cost": 0.2914002745295871,
      "rate": 0.2570013960968515,
      "optimizer": 0.22550213863963992,
      "convergence": 0.21952974178848783,
      "regularization": 0.19781465821755662,
      "learning": 0.18713364790335513,
      "stability": 0.18294145149040653,
      "minimum": 0.17280590686830544,
      "gradient": 0.16768108854044506
    }
  },
  "why_is_named_entity_recognition_(ner)_a_challenging_task": {
    "title": "Why is named entity recognition (NER) a challenging task",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Named Entity Recognition (NER) is considered a challenging task for several reasons: Ambiguity: Entities can be ambiguous, meaning the same word or phrase can refer...",
    "TFIDF_Score": {
      "entity": 0.5453054692916075,
      "ner": 0.3994621476282523,
      "domain": 0.19829289792422092,
      "different": 0.18776925742379236,
      "language": 0.18304165686730414,
      "nested": 0.1824062137573743,
      "multilingual": 0.148826296049609,
      "annotated": 0.14232172591297512,
      "may": 0.13644917901653983,
      "refer": 0.11567201491340214
    }
  },
  "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes": {
    "title": "Why is the Central Limit Theorem important when working with small sample sizes",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hypothesis_testing",
      "distributions",
      "central_limit_theorem",
      "assumption_of_normality"
    ],
    "inlinks": [
      "central_limit_theorem"
    ],
    "summary": "The [[Central Limit Theorem]] (CLT) is particularly important for data scientists working with small sample sizes. It enables the use of various statistical methods, and...",
    "TFIDF_Score": {
      "sample": 0.4323290307423787,
      "clt": 0.41295542500467025,
      "scientist": 0.3166876233295229,
      "population": 0.26394327568779946,
      "size": 0.232792555015127,
      "normality": 0.16324600618261073,
      "mean": 0.16175737047133554,
      "distribution": 0.14782944519894126,
      "statistical": 0.13657674315643598,
      "small": 0.130415881711336
    }
  },
  "why_json_is_better_than_pickle_for_untrusted_data": {
    "title": "Why JSON is Better than Pickle for Untrusted Data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pickle"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "JSON vs. [[Pickle]]: Security: JSON: JSON is a text-based data format that is inherently safer for handling untrusted data. It ==only== supports basic data types...",
    "TFIDF_Score": {
      "pickle": 0.5778317185009355,
      "json": 0.44957242318711194,
      "untrusted": 0.22940002943680057,
      "readable": 0.15293335295786706,
      "arbitrary": 0.14834084217002136,
      "debug": 0.14834084217002136,
      "readability": 0.14109439778660865,
      "interoperability": 0.13547361884708456,
      "data": 0.13528644863100867,
      "format": 0.1342723735442988
    }
  },
  "why_type_1_and_type_2_matter": {
    "title": "Why Type 1 and Type 2 matter",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20250312064809.png"
    ],
    "inlinks": [
      "evaluation_metrics"
    ],
    "summary": "Type I and Type II errors are used in evaluating the performance of classification models, and understanding their differences is essential for interpreting model results...",
    "TFIDF_Score": {
      "error": 0.4466120256578912,
      "type": 0.4069455720467377,
      "positive": 0.3485933369421566,
      "spam": 0.25818952131991857,
      "model": 0.19826111725603868,
      "consequence": 0.1752316890556012,
      "negative": 0.1742966684710783,
      "predicts": 0.15264796593126723,
      "email": 0.15105831283925994,
      "precision": 0.11003075230330737
    }
  },
  "why_use_er_diagrams": {
    "title": "Why use ER diagrams",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "er_diagrams",
      "data_quality",
      "normalised_schema",
      "why_use_er_diagrams"
    ],
    "inlinks": [
      "why_use_er_diagrams",
      "er_diagrams"
    ],
    "summary": "[[Why use ER diagrams]] Cleaning a dataset before creating an [[ER Diagrams]] is crucial for ensuring accuracy and reliability in your database design [[Data Quality]]:...",
    "TFIDF_Score": {
      "diagram": 0.4394242885146675,
      "entity": 0.3874842337262862,
      "cleaning": 0.29263696823240276,
      "dataset": 0.24093822064978124,
      "identification": 0.23838080861459524,
      "accurately": 0.21213068516642392,
      "relationship": 0.1987122157869246,
      "attribute": 0.17558218093944167,
      "data": 0.15497038385123765,
      "creating": 0.14507241245056365
    }
  },
  "wikipedia_api.py": {
    "title": "Wikipedia_API.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "graphrag",
      "api"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Wikipedia_API.py",
    "TFIDF_Score": {
      "wikipedia_api": 0.5143854735883211,
      "utility": 0.4180669099567267,
      "rhyslwells": 0.28654517674298946,
      "blob": 0.28285797774910554,
      "github": 0.2727004867358482,
      "exploration": 0.2706050117305536,
      "ml_tools": 0.26086503962676755,
      "com": 0.25553364632131,
      "main": 0.2435533500502288,
      "http": 0.2413544986269743
    }
  },
  "windows_subsystem_for_linux": {
    "title": "Windows Subsystem for Linux",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "linux",
      "ubuntu",
      "windows_subsystem_for_linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux",
      "powershell_vs_bash"
    ],
    "summary": "[[Windows Subsystem for Linux]] (WSL) is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows 11. It allows users to...",
    "TFIDF_Score": {
      "linux": 0.587723141389031,
      "window": 0.4813756626518511,
      "wsl": 0.3640982302839614,
      "environment": 0.14879146523428358,
      "user": 0.12718633522541753,
      "command": 0.11745821864335111,
      "line": 0.11182715745649442,
      "directly": 0.10715090968990607,
      "run": 0.10061296026089865,
      "debian": 0.09663165616878823
    }
  },
  "word2vec": {
    "title": "Word2vec",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "word2vec.py",
      "neural_network",
      "semantic_relationships",
      "syntactic_relationships",
      "standardised/vector_embedding",
      "bag_of_words",
      "ml_tools"
    ],
    "inlinks": [
      "word2vec.py"
    ],
    "summary": "Word2Vec is a technique for generating vector representations of words. Developed by researchers at Google, it uses a shallow [[neural network]] to produce [[standardised/Vector Embedding|word...",
    "TFIDF_Score": {
      "word": 0.5062705621415151,
      "vector": 0.4105313111517352,
      "word2vec": 0.3869821481521077,
      "embedding": 0.24507838096990084,
      "predicts": 0.15393355585336851,
      "representation": 0.13034833815037378,
      "given": 0.1287834793686522,
      "target": 0.12728977562009058,
      "continuous": 0.12516930934644085,
      "cbow": 0.11479344288461524
    }
  },
  "word2vec.py": {
    "title": "Word2Vec.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bert",
      "transformer",
      "word2vec",
      "cosine_similarity"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Word2Vec.py The script can benefit from Word2Vec embeddings by replacing the randomly initialized embeddings with pretrained or trained embeddings generated using Word2Vec. These embeddings provide...",
    "TFIDF_Score": {
      "embeddings": 0.48914767969660594,
      "word2vec": 0.42409859178588216,
      "king": 0.2423420524490755,
      "semantic": 0.19713307765625662,
      "queen": 0.1817565393368066,
      "cosine": 0.17287773944354365,
      "gigaword": 0.14377551362094718,
      "similarity": 0.14309221932491267,
      "glove": 0.12492238179843096,
      "reflect": 0.12117102622453775
    }
  },
  "wrapper_methods": {
    "title": "Wrapper Methods",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "filter_method",
      "model_evaluation"
    ],
    "inlinks": [
      "feature_selection",
      "embedded_methods"
    ],
    "summary": "Used in [[Feature Selection]]. Wrapper methods are powerful because they directly optimize the performance of the machine learning model by selecting the most informative subset...",
    "TFIDF_Score": {
      "feature": 0.4507820111331739,
      "subset": 0.4165678990924032,
      "wrapper": 0.34606345080297907,
      "method": 0.21207394528096143,
      "performance": 0.1833838645414855,
      "selection": 0.1809547295420414,
      "model": 0.17342158945448985,
      "selecting": 0.16806126182856765,
      "criterion": 0.141721936077043,
      "iteratively": 0.13815486179843062
    }
  },
  "xgboost": {
    "title": "XGBoost",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "XGM"
    ],
    "outlinks": [
      "gradient_boosting",
      "regularisation",
      "model_ensemble",
      "loss_function",
      "handling_missing_data",
      "interpretability",
      "learning_rate",
      "gradient_descent",
      "decision_tree",
      "hyperparameter_tuning"
    ],
    "inlinks": [
      "gradient_boosting",
      "ds_&_ml_portal",
      "lightgbm_vs_xgboost_vs_catboost",
      "optuna",
      "boosting",
      "time_series_forecasting",
      "feature_importance"
    ],
    "summary": "XGBoost (eXtreme Gradient Boosting) is a highly efficient and flexible implementation of [[Gradient Boosting]] that is widely used for its accuracy and performance in machine...",
    "TFIDF_Score": {
      "xgboost": 0.47160401466701346,
      "tree": 0.41782953849554416,
      "model": 0.22367467143318528,
      "gradient": 0.17717914970385218,
      "dmatrix": 0.14677294457998027,
      "xgb": 0.14677294457998027,
      "loss": 0.13503051491018658,
      "function": 0.12154698746736285,
      "python": 0.11818448155240437,
      "accuracy": 0.11733887571741831
    }
  },
  "yaml": {
    "title": "yaml",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "json"
    ],
    "inlinks": [
      "json_to_yaml"
    ],
    "summary": "Stands for YAML ain't markup language and is a superset of JSON lists begin with a hyphen dependent on whitespace / indentation better suited for...",
    "TFIDF_Score": {
      "yaml": 0.616346855617598,
      "markup": 0.41333457066314744,
      "language": 0.27112572321045214,
      "stand": 0.20817951106971402,
      "configuration": 0.19028516464696477,
      "json": 0.18679451009599232,
      "acronym": 0.1462653053228509,
      "hyphen": 0.1462653053228509,
      "indentation": 0.13777819022104915,
      "superset": 0.13777819022104915
    }
  },
  "z-normalisation": {
    "title": "Z-Normalisation",
    "tags": null,
    "aliases": [
      "Z-Score"
    ],
    "outlinks": [
      "pasted_image_20241224091157.png",
      "learning_rate",
      "machine_learning_algorithms",
      "gradient_descent",
      "pasted_image_20241224091151.png",
      "pasted_image_20241224091007.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "z-normalisationz-score",
      "normalisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_z_score.py Z-normalisation, also known as z-score normalization, is a technique used to standardize the range of independent variables or features of data. This process is...",
    "TFIDF_Score": {
      "mu_j": 0.30779063197221396,
      "feature": 0.28739255730698904,
      "normalization": 0.27759783748307504,
      "around": 0.216196553882224,
      "centered": 0.20794445445666299,
      "range": 0.195665355079136,
      "deviation": 0.16427948292585445,
      "score": 0.1634407935327065,
      "sigma_j": 0.15389531598610698,
      "distance": 0.15162711152304925
    }
  },
  "z-normalisationz-score": {
    "title": "Z-NormalisationZ-Score",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "z-normalisation"
    ],
    "inlinks": [],
    "summary": "[[Z-Normalisation|Z-Score]] - Formula: $Z = \\frac{(X - \\mu)}{\\sigma}$ - $X$: Data point - $\\mu$: Mean of the dataset - $\\sigma$: Standard deviation of the dataset...",
    "TFIDF_Score": {
      "mad": 0.35725534470556464,
      "median": 0.293206260473357,
      "procedure": 0.293206260473357,
      "sigma": 0.2656493856738441,
      "deviation": 0.2542408906325488,
      "point": 0.23587236385320998,
      "threshold": 0.2323872771408559,
      "anomaly": 0.22233021565169003,
      "frac": 0.21240374493388597,
      "formula": 0.20538916483408867
    }
  },
  "z-test": {
    "title": "Z-Test",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard_deviation",
      "central_limit_theorem"
    ],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "The Z-test is a statistical method used to determine if there is a ==significant difference between the means of two groups or to compare a...",
    "TFIDF_Score": {
      "sample": 0.54717733757419,
      "mean": 0.3696482350946717,
      "population": 0.36189819013059743,
      "test": 0.3554035684787207,
      "deviation": 0.20369820537659183,
      "standard": 0.1667145408551416,
      "known": 0.153569377604685,
      "size": 0.15199370488171945,
      "distribution": 0.13512804604473108,
      "normal": 0.11394928893009941
    }
  }
}