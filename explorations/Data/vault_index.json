{
  "1-on-1_template": {
    "title": "1-on-1 Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Decisions [Your name] add decisions that need to be made [Other person's name] add decisions that need to be made Action items [Your name] add..."
  },
  "ab_testing": {
    "title": "AB testing",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "A/B testing is a method of performance testing two versions of a product like an app."
  },
  "accessing_gen_ai_generated_content": {
    "title": "Accessing Gen AI generated content",
    "tags": [
      "GenAI",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "bertscore",
      "rag",
      "generative_ai",
      "interpretability",
      "knowledge_graph"
    ],
    "inlinks": [],
    "summary": "To assess whether the content generated by a [[Generative AI]] is truthful and faithful, several methods and frameworks can be employed. Truthfulness refers to whether..."
  },
  "accuracy": {
    "title": "Accuracy",
    "tags": [
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "confusion_matrix",
      "classification",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "handling_different_distributions",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "evaluation_metrics",
      "precision",
      "model_observability",
      "class_separability",
      "confusion_matrix",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Definition Accuracy Score is the proportion of correct predictions out of all predictions made. In other words, it is the percentage of correct predictions. Accuracy..."
  },
  "acid_transaction": {
    "title": "ACID Transaction",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "transaction"
    ],
    "inlinks": [
      "transaction",
      "data_lakehouse",
      "schema_evolution"
    ],
    "summary": "An ACID [[Transaction]] ensures that either all changes are successfully committed or rolled back, preventing the database from ending up in an inconsistent state. This..."
  },
  "activation_atlases": {
    "title": "Activation atlases",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "feature_extraction"
    ],
    "summary": "is a viewing method for high dimensional space that AI system use for predictions. Example AlexNet (cofounder of OpenAI)"
  },
  "activation_function": {
    "title": "Activation Function",
    "tags": [
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "how_do_we_choose_the_right_activation_function",
      "interpretability",
      "binary_classification",
      "neural_network",
      "backpropagation",
      "data_transformation"
    ],
    "inlinks": [
      "neural_network",
      "forward_propagation",
      "typical_output_formats_in_neural_networks"
    ],
    "summary": "Activation functions play a role in [[Neural network]] by introducing non-linearity, allowing models to learn from complex patterns and relationships in the data. [[How do..."
  },
  "active_learning": {
    "title": "Active Learning",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "supervised_learning"
    ],
    "inlinks": [],
    "summary": "Think captchas for training. To help the [[Supervised Learning]] models when they are less confident. Reducing labelling time or need for it."
  },
  "ada_boosting": {
    "title": "Ada boosting",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "boosting",
      "decision_tree",
      "random_forests"
    ],
    "inlinks": [
      "boosting"
    ],
    "summary": "Resources: LINK Overview: Ada Boosting short for ==Adaptive Boosting==, is a specific type of [[Boosting]] algorithm that focuses on improving the accuracy of predictions by..."
  },
  "adam_optimizer": {
    "title": "Adam Optimizer",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "deep_learning",
      "momentum",
      "learning_rate",
      "gradient_descent",
      "why_does_the_adam_optimizer_converge",
      "hyperparameter"
    ],
    "inlinks": [
      "orthogonalization",
      "learning_rate",
      "backpropagation",
      "adaptive_learning_rates",
      "optimisation_techniques"
    ],
    "summary": "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both [[Momentum]] and adaptive learning rates. It is widely used due..."
  },
  "adaptive_learning_rates": {
    "title": "Adaptive Learning Rates",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "learning_rate",
      "momentum",
      "adam_optimizer"
    ],
    "inlinks": [
      "optimisation_techniques"
    ],
    "summary": "[[Adam Optimizer]] Adaptive [[learning rate]] adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients...."
  },
  "adding_a_database_to_postgresql": {
    "title": "Adding a database to PostgreSQL",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "postgresql"
    ],
    "summary": "How to Add a Database to PostgreSQL Using pgAdmin (GUI) Open pgAdmin and log in. In the Object Explorer, right-click Databases \u2192 Create \u2192 Database...."
  },
  "addressing_multicollinearity": {
    "title": "Addressing Multicollinearity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "addressing_multicollinearity.py",
      "principal_component_analysis",
      "dimensionality_reduction",
      "multicollinearity",
      "'var1',_'var2',_'var3'",
      "ml_tools"
    ],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "In [[ML_Tools]] see: [[Addressing_Multicollinearity.py]] Multicollinearity can impact the performance and [[interpretability]] of regression models by causing instability in coefficient estimates and complicating the analysis of..."
  },
  "addressing_multicollinearity.py": {
    "title": "Addressing_Multicollinearity.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "addressing_multicollinearity"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Regression/Addressing_Multicollinearity.py"
  },
  "adjusted_r_squared": {
    "title": "Adjusted R squared",
    "tags": [
      "statistics",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "r_squared",
      "regression_metrics",
      "parsimonious"
    ],
    "inlinks": [
      "r_squared",
      "regression_metrics",
      "linear_regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression"
    ],
    "summary": "Adjusted R-squared is a [[Regression Metrics]]or assessing the quality of a regression model, ==especially when multiple predictors== are involved. It helps ensure that the model..."
  },
  "agent-based_modelling": {
    "title": "Agent-Based Modelling",
    "tags": null,
    "aliases": [
      "ABM"
    ],
    "outlinks": [
      "agent-based_modelling",
      "policy",
      "prompt_engineering",
      "chain_of_thought",
      "llm",
      "emergent_behavior"
    ],
    "inlinks": [
      "agent-based_modelling",
      "energy_abm",
      "agentic_solutions",
      "what_can_abm_solve_within_the_energy_sector",
      "energy"
    ],
    "summary": "(ABM) is a computational approach that simulates the interactions of individual agents within a defined environment to observe complex phenomena and [[emergent behavior]] at a..."
  },
  "agentic_solutions": {
    "title": "Agentic Solutions",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "graphrag",
      "agent-based_modelling",
      "small_language_models"
    ],
    "inlinks": [
      "scaling_agentic_systems",
      "langchain"
    ],
    "summary": "Agentic solutions leverage multiple autonomous agents (usually [[Small Language Models|SLM]]) to achieve goals collaboratively. These systems distribute tasks across agents that operate individually or collectively..."
  },
  "aggregation": {
    "title": "Aggregation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pandas_pivot_table",
      "groupby",
      "de_tools"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ],
    "summary": "Aggregation Summarizing data for analysis ([[Pandas Pivot Table]] and [[Groupby]]). In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/group_by.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb"
  },
  "ai_engineer": {
    "title": "AI Engineer",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "attention_mechanism",
      "neural_network",
      "prompting",
      "lstm"
    ],
    "inlinks": [],
    "summary": "They know what - [[LSTM]] means - [[Attention mechanism]] - [[Prompting]] optimisation - [[Neural network]]"
  },
  "ai_governance": {
    "title": "AI governance",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_governance"
    ],
    "inlinks": [],
    "summary": "AI Governance [[Data Governance]] Used in regulated sectors. Constraints to using ai: - legal, - transparency, - security, - historical bias AI acts and standards:..."
  },
  "algorithms": {
    "title": "Algorithms",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "recursive_algorithm",
      "k-means",
      "memoization"
    ],
    "inlinks": [
      "computer_science",
      "checksum",
      "machine_learning_algorithms"
    ],
    "summary": "[[Recursive Algorithm]] Backtracking Backtracking is a method for solving problems incrementally, by trying partial solutions and then abandoning them if they are not valid. Example:..."
  },
  "alternatives_to_batch_processing": {
    "title": "Alternatives to Batch Processing",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event-driven_architecture",
      "batch_processing",
      "apache_spark_streaming",
      "apache_kafka",
      "data_streaming",
      "apache_flink",
      "data_analysis"
    ],
    "inlinks": [
      "data_streaming"
    ],
    "summary": "If you\u2019re working with a streaming dataset ([[Data Streaming]]), why might [[batch processing]] not be suitable, and what alternatives would you consider? Latency: Batch processing..."
  },
  "amazon_s3": {
    "title": "Amazon S3",
    "tags": [],
    "aliases": [
      "S3"
    ],
    "outlinks": [],
    "inlinks": [
      "data_storage",
      "data_engineering_tools",
      "distributed_computing"
    ],
    "summary": "Amazon S3 Amazon S3 buckets (onedrive essentially) Amazon Simple Storage Service (S3) is a versatile ==object storage solution== known for its scalability, data availability, security,..."
  },
  "anomaly_detection_in_time_series": {
    "title": "Anomaly Detection in Time Series",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ts_anomaly_detection.py",
      "isolated_forest",
      "time_series",
      "lstm",
      "ml_tools"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "In [[Time Series]] In [[ML_Tools]] see: - [[TS_Anomaly_Detection.py]] To perform anomaly detection specifically for time series data, you can utilize various techniques that account for..."
  },
  "anomaly_detection_with_clustering": {
    "title": "Anomaly Detection with Clustering",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "dbscan",
      "isolated_forest"
    ],
    "inlinks": [
      "anomaly_detection",
      "isolated_forest"
    ],
    "summary": "[[Clustering]] - Description: Outliers often form small clusters or are isolated from main clusters. 8. [[DBSCAN]] (Density-Based Spatial Clustering of Applications with Noise) Purpose: Finds..."
  },
  "anomaly_detection_with_statistical_methods": {
    "title": "Anomaly Detection with Statistical Methods",
    "tags": [
      "anomaly_detection",
      "statistics",
      "ml"
    ],
    "aliases": [],
    "outlinks": [
      "z-normalisation",
      "interquartile_range_(iqr)_detection",
      "isolated_forest",
      "multivariate_data",
      "percentile_detection",
      "gaussian_model"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "Basic: - [[Z-Normalisation|Z-Score]] - [[Interquartile Range (IQR) Detection]] - [[Percentile Detection]] Advanced: - [[Gaussian Model]] - [[Isolated Forest]] Grubbs' Test Context: Grubbs' test is a..."
  },
  "anomaly_detection": {
    "title": "Anomaly Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "anomaly_detection_with_statistical_methods",
      "data_cleansing",
      "pycaret_anomaly.ipynb",
      "anomaly_detection_with_clustering",
      "pca-based_anomaly_detection",
      "standardised/outliers",
      "anomaly_detection_in_time_series",
      "standardisation",
      "data_integrity",
      "normalisation",
      "boxplot",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "outliers",
      "gaussian_mixture_models",
      "time_series",
      "clustering",
      "imbalanced_datasets"
    ],
    "summary": "Anomaly detection involves identifying [[standardised/Outliers|Outliers]]. Detecting these anomalies is crucial for maintaining [[data integrity]] and improving model performance. Methods for Detecting Anomalies In [[ML_Tools]] see:..."
  },
  "apache_airflow": {
    "title": "Apache Airflow",
    "tags": [
      "data_orchestration",
      "software",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "directed_acyclic_graph_(dag)"
    ],
    "inlinks": [
      "directed_acyclic_graph_(dag)",
      "data_engineering",
      "etl",
      "data_management"
    ],
    "summary": "Schedular think CROM jobs with python. Apache Nifi may be better. Airflow is a data orchestrator and the first that made task scheduling popular with..."
  },
  "apache_kafka": {
    "title": "Apache Kafka",
    "tags": [
      "software",
      "data_orchestration"
    ],
    "aliases": [
      "Kafka"
    ],
    "outlinks": [
      "publish_and_subscribe",
      "scalability",
      "data_integrity",
      "data_integration",
      "data_storage"
    ],
    "inlinks": [
      "data_streaming",
      "data_engineering_tools",
      "publish_and_subscribe",
      "alternatives_to_batch_processing"
    ],
    "summary": "Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and data streaming applications. It is designed to handle high-throughput,..."
  },
  "apache_spark": {
    "title": "Apache Spark",
    "tags": [
      "software"
    ],
    "aliases": [
      "Spark"
    ],
    "outlinks": [
      "data_engineer"
    ],
    "inlinks": [
      "map_reduce",
      "ds_&_ml_portal",
      "batch_processing",
      "distributed_computing",
      "parquet",
      "pyspark",
      "databricks",
      "scala",
      "big_data",
      "hadoop"
    ],
    "summary": "Apache Spark is an open-source multi-language engine for executing Data Engineer and Machine Learning on single-node machines or clusters. It's optimized for large-scale data processing...."
  },
  "api_driven_microservices": {
    "title": "API Driven Microservices",
    "tags": [
      "software",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "microservices",
      "software_architecture",
      "api"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "API-driven microservices refer to a [[software architecture]] approach where [[microservices]] communicate with each other and with external systems primarily through well-defined [[API]] (Application Programming Interfaces)...."
  },
  "api": {
    "title": "API",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "rest_api",
      "wikipedia_api.py",
      "fastapi",
      "ml_tools"
    ],
    "inlinks": [
      "normalisation_of_data",
      "model_deployment",
      "data_ingestion",
      "api_driven_microservices",
      "data_contract"
    ],
    "summary": "An API (Application Programming Interfaces) allows one system (client) to ==request specific actions from another system== (server). Using a predefined set of rules and ==protocols==...."
  },
  "attention_is_all_you_need": {
    "title": "Attention Is All You Need",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "attention_mechanism": {
    "title": "Attention mechanism",
    "tags": [
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "multi-head_attention",
      "recurrent_neural_networks",
      "ml",
      "transformer",
      "bert",
      "standardised/attention_is_all_you_need",
      "lstm",
      "llm",
      "nlp"
    ],
    "inlinks": [
      "ai_engineer",
      "ds_&_ml_portal",
      "multi-head_attention",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding",
      "language_model_output_optimisation",
      "lstm",
      "feature_extraction",
      "llm"
    ],
    "summary": "[!intuition] Think of attention like human reading behavior: when reading a complex sentence, we don't process all the words equally at every moment. Instead, we..."
  },
  "auc": {
    "title": "AUC",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "inlinks": [
      "precision-recall_curve"
    ],
    "summary": "AUC (Area Under the Curve) is a metric for binary classification problems, representing the area under the [[ROC (Receiver Operating Characteristic)]] Key Concepts Represents the..."
  },
  "automated_feature_creation": {
    "title": "Automated Feature Creation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "automated_feature_creation",
      "feature_engineering"
    ],
    "inlinks": [
      "automated_feature_creation"
    ],
    "summary": "Question: Can autodetect meaningful features. LINK Rough notes [[Feature Engineering]] was an ad-hoc manual process that depended on domain knowledge, intuition, data exploration and creativity...."
  },
  "aws_lambda": {
    "title": "AWS Lambda",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "s3_bucket",
      "event_driven_events"
    ],
    "inlinks": [],
    "summary": "AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that allows you to run code without provisioning or managing servers. AWS..."
  },
  "azure": {
    "title": "Azure",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_engineering_tools"
    ],
    "summary": "Public cloud computing platform from Microsoft offering various services like infrastructure, data storage, and machine learning."
  },
  "b-tree": {
    "title": "B-tree",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_index"
    ],
    "summary": ""
  },
  "backpropagation": {
    "title": "Backpropagation",
    "tags": [
      "deep_learning",
      "ml_optimisation",
      "statistics"
    ],
    "aliases": [
      "Backprop",
      "BP",
      "backward propagation"
    ],
    "outlinks": [
      "supervised_learning",
      "regularisation",
      "gradient_descent",
      "vanishing_and_exploding_gradients_problem",
      "sympy",
      "adam_optimizer"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "deep_learning",
      "recurrent_neural_networks",
      "activation_function",
      "named_entity_recognition",
      "forward_propagation"
    ],
    "summary": "[!Summary] Backpropagation is an essential algorithm in the training of neural networks and iteratively correcting its mistakes. It involves a process of calculating the gradient..."
  },
  "bag_of_words": {
    "title": "Bag of words",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "tf-idf",
      "bag_of_words.py",
      "ml_tools"
    ],
    "inlinks": [
      "tf-idf",
      "naive_bayes",
      "nlp",
      "word2vec"
    ],
    "summary": "In [[ML_Tools]] see: [[Bag_of_Words.py]] In the context of natural language processing (NLP), the Bag of Words (BoW) model is a simple and commonly used ==method..."
  },
  "bagging": {
    "title": "Bagging",
    "tags": [
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "overfitting",
      "model_ensemble",
      "random_forests"
    ],
    "inlinks": [
      "bias_and_variance",
      "random_forests",
      "imbalanced_datasets",
      "model_ensemble"
    ],
    "summary": "Overview: Bagging, short for Bootstrap Aggregating, is an [[Model Ensemble]] technique designed to improve the stability and accuracy of machine learning algorithms. It works by..."
  },
  "bag_of_words.py": {
    "title": "Bag_of_Words.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bag_of_words"
    ],
    "summary": "Summary of What the Script Does: It takes a dataset of text (movie reviews in this case) and processes it to remove HTML tags, non-alphabetic..."
  },
  "bandit_example_output": {
    "title": "Bandit example output",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bandit_example_fixed.py",
      "bandit_example_nonfixed.py",
      "ml_tools"
    ],
    "inlinks": [
      "tool.bandit"
    ],
    "summary": "Complicated example output of bandit Running bandit on [[ML_Tools]] file [[Bandit_Example_Nonfixed.py]] gives. Fixing this gives [[Bandit_Example_Fixed.py]] ```bandit [main] INFO profile include tests: None [main] INFO..."
  },
  "bandit_example_fixed.py": {
    "title": "Bandit_Example_Fixed.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bandit_example_output"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Fixed.py"
  },
  "bandit_example_nonfixed.py": {
    "title": "Bandit_Example_Nonfixed.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tool.bandit",
      "bandit_example_output"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Nonfixed.py"
  },
  "bash": {
    "title": "Bash",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bash_folder",
      "ml_tools"
    ],
    "inlinks": [
      "command_prompt",
      "command_line",
      "powershell_vs_bash"
    ],
    "summary": "Automation Scripts In [[ML_Tools]], see: [[Bash_folder]] Basic Commands Show Current Directory: bash pwd Display Contents of a Text File: bash cat filename.txt Search for a..."
  },
  "batch_normalisation": {
    "title": "Batch Normalisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "overfitting",
      "neural_network",
      "pasted_image_20241219071904.png",
      "vanishing_and_exploding_gradients_problem",
      "normalisation_vs_standardisation"
    ],
    "inlinks": [
      "normalisation"
    ],
    "summary": "Links: - Batch normalization | What it is and how to implement it Can be used to handle [[vanishing and exploding gradients problem]] and [[Overfitting]]..."
  },
  "batch_processing": {
    "title": "Batch Processing",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "batch_processing",
      "distributed_computing",
      "databricks",
      "apache_spark"
    ],
    "inlinks": [
      "map_reduce",
      "publish_and_subscribe",
      "ds_&_ml_portal",
      "batch_processing",
      "lambda_architecture",
      "data_streaming",
      "hadoop",
      "alternatives_to_batch_processing"
    ],
    "summary": "Batch Processing is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them..."
  },
  "bellman_equations": {
    "title": "Bellman Equations",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "what_are_the_bellman_equations_that_are_used_in_rl?"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "[[What are the Bellman equations that are used in RL?]] Equations here may not be accurate. In reinforcement learning, Bellman's equations are fundamental to understanding..."
  },
  "benefits_of_data_transformation": {
    "title": "Benefits of Data Transformation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_transformation",
      "interoperability",
      "data_quality"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Benefits of [[Data Transformation]] Efficiency: Faster query performance. [[Interoperability]]: Converting data into the required format for target systems. Enrichment: Adding contextual data for better insights...."
  },
  "bernoulli": {
    "title": "Bernoulli",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "distributions"
    ],
    "summary": ""
  },
  "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding": {
    "title": "BERT Pretraining of Deep Bidirectional Transformers for Language Understanding",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": ""
  },
  "bert": {
    "title": "BERT",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": null,
    "outlinks": [
      "google",
      "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding",
      "summarisation",
      "vector_embedding",
      "transformer",
      "sentence_similarity",
      "positional_encoding",
      "named_entity_recognition",
      "nlp",
      "transfer_learning"
    ],
    "inlinks": [
      "word2vec.py",
      "ds_&_ml_portal",
      "rag",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "vector_embedding",
      "lstm",
      "small_language_models"
    ],
    "summary": "BERT (==Bidirectional Encoder Representations from [[Transformer]]==) is used in [[NLP]]processing, developed by [[Google]]. Introduced in the paper \"[[BERT Pretraining of Deep Bidirectional Transformers for Language..."
  },
  "bertscore": {
    "title": "BERTScore",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "accessing_gen_ai_generated_content"
    ],
    "summary": ""
  },
  "bias_and_variance": {
    "title": "Bias and variance",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": null,
    "outlinks": [
      "boosting",
      "regularisation",
      "overfitting",
      "bagging"
    ],
    "inlinks": [
      "cross_validation",
      "overfitting",
      "machine_learning_algorithms"
    ],
    "summary": "Related to [[Overfitting]] Ways to Reduce Bias and Variance: - [[Regularisation]] - [[Boosting]] - [[Bagging]] What is Bias in Machine Learning? Bias occurs when a..."
  },
  "big_data": {
    "title": "Big Data",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "hadoop",
      "apache_spark",
      "databricks",
      "scala",
      "big_data",
      "data_storage"
    ],
    "inlinks": [
      "parquet",
      "databricks",
      "scala",
      "big_data",
      "hadoop"
    ],
    "summary": "The concept of Big Data revolves around datasets that are too large or complex to be managed using traditional data processing techniques. It\u2019s characterized by..."
  },
  "big_o_notation": {
    "title": "Big O Notation",
    "tags": [
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "mathematics"
    ],
    "summary": "Big-O Notation is an analysis of the algorithm using Big \u2013 O asymptotic notation. Mostly related to computing rather than storage Doing things not exponentially,..."
  },
  "bigquery": {
    "title": "BigQuery",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "google",
      "data_warehouse"
    ],
    "inlinks": [
      "elt",
      "google_cloud_platform"
    ],
    "summary": "cloud-based [[Data Warehouse]] BigQuery is a fully managed, serverless data warehouse offered by [[Google]] Cloud Platform (GCP). It is designed to handle large-scale data analytics..."
  },
  "binary_classification": {
    "title": "Binary Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "classification"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "logistic_regression",
      "precision-recall_curve",
      "use_cases_for_a_simple_neural_network_like",
      "determining_threshold_values",
      "typical_output_formats_in_neural_networks",
      "cosine_similarity",
      "activation_function"
    ],
    "summary": "Binary classification is a type of [[Classification]] task that involves predicting one of two possible classes or outcomes. It is used in scenarios where the..."
  },
  "binder": {
    "title": "Binder",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://mybinder.org/"
  },
  "boosting": {
    "title": "Boosting",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "gradient_boosting",
      "ada_boosting",
      "model_ensemble",
      "weak_learners",
      "interpretability",
      "xgboost"
    ],
    "inlinks": [
      "gradient_boosting",
      "ada_boosting",
      "model_ensemble",
      "bias_and_variance",
      "gradient_boosting_regressor"
    ],
    "summary": "Boosting is a type of [[Model Ensemble]] in machine learning that focuses on improving the accuracy of predictions by building a ==sequence of models==. Each..."
  },
  "bootstrap": {
    "title": "Bootstrap",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "sampling with replacement from an original dataset."
  },
  "boxplot": {
    "title": "Boxplot",
    "tags": [
      "#statistics",
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "standardised/outliers",
      "distributions"
    ],
    "inlinks": [
      "variance",
      "distributions",
      "violin_plot",
      "anomaly_detection"
    ],
    "summary": "A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first..."
  },
  "business_intelligence": {
    "title": "business intelligence",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "rollup",
      "single_source_of_truth"
    ],
    "inlinks": [
      "data_scientist",
      "granularity",
      "data_ingestion"
    ],
    "summary": "Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization\u2019s business decisions. The new term is Data Engineer...."
  },
  "business_observability": {
    "title": "Business observability",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "data_visualisation",
      "model_observability",
      "data_collection"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "Business [[Model Observability|observability]] refers to the ability to gain insights into the internal state and performance of a business through the continuous monitoring and analysis..."
  },
  "career_interest": {
    "title": "Career Interest",
    "tags": [],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "This is a portal to notes that I find relevant to my career:"
  },
  "casual_inference": {
    "title": "Casual Inference",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "missing data problem"
  },
  "catboost": {
    "title": "CatBoost",
    "tags": null,
    "aliases": [
      "CAT"
    ],
    "outlinks": [
      "gradient_boosting",
      "categorical",
      "hyperparameter"
    ],
    "inlinks": [
      "lightgbm_vs_xgboost_vs_catboost",
      "gradient_boosting",
      "optuna"
    ],
    "summary": "CatBoost is a [[Gradient Boosting]] library developed by Yandex, designed to handle [[categorical]] features efficiently and provide robust performance with minimal [[Hyperparameter|Hyperparameter tuning]] It is..."
  },
  "central_limit_theorem": {
    "title": "Central Limit Theorem",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "central_limit_theorem",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes"
    ],
    "inlinks": [
      "statistics",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "central_limit_theorem",
      "z-test"
    ],
    "summary": "The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a..."
  },
  "chain_of_thought": {
    "title": "Chain of thought",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling",
      "deepseek",
      "llm"
    ],
    "summary": "Chain of Thought (CoT) reasoning in AI systems is a cognitive-inspired framework that improves the performance of large language models (LLMs) by explicitly guiding the..."
  },
  "change_management": {
    "title": "Change Management",
    "tags": [
      "business"
    ],
    "aliases": [
      "Change Program"
    ],
    "outlinks": [],
    "inlinks": [
      "prevention_is_better_than_the_cure",
      "digital_transformation",
      "data_quality"
    ],
    "summary": "Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a desired future state. It involves - preparing,..."
  },
  "checksum": {
    "title": "Checksum",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "security",
      "data_storage",
      "algorithms"
    ],
    "inlinks": [
      "data_integrity"
    ],
    "summary": "A checksum is a value calculated from a data set that is used to verify the integrity of that data. It acts as a fingerprint..."
  },
  "chi-squared_test": {
    "title": "Chi-Squared Test",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Chi-Squared Test The Chi-squared test is used to determine if there is a significant association between categorical variables. It assesses whether the observed frequencies in..."
  },
  "choosing_a_threshold": {
    "title": "Choosing a Threshold",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cost-sensitive_analysis",
      "precision-recall_curve",
      "roc_(receiver_operating_characteristic)"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal threshold depends on the specific problem and the desired trade-off between different types of errors: Manual Selection: Based on domain expertise or prior..."
  },
  "choosing_the_number_of_clusters": {
    "title": "Choosing the Number of Clusters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "silhouette_analysis",
      "granularity",
      "wcss_and_elbow_method"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal number of clusters ([[clustering]]) depends on the data and the desired level of [[granularity]]. Here are some common approaches: Elbow Method: [[WCSS and..."
  },
  "ci-cd": {
    "title": "CI-CD",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "continuous_delivery/deployment",
      "docker",
      "gitlab",
      "software_development_life_cycle",
      "continuous_integration"
    ],
    "inlinks": [
      "devops",
      "gitlab"
    ],
    "summary": "CI/CD stands for [[Continuous Integration]] and [[Continuous Delivery/Deployment]]. It is a set of practices aimed at streamlining and accelerating the [[Software Development Life Cycle]]. The..."
  },
  "class_separability": {
    "title": "Class Separability",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "classification",
      "feature_engineering",
      "imbalanced_datasets",
      "accuracy"
    ],
    "inlinks": [],
    "summary": "If you have a perfectly balanced dataset (unlike [[Imbalanced Datasets]]) but still experience poor [[classification]] [[accuracy]], class separability might be an issue due to the..."
  },
  "classification_report": {
    "title": "Classification Report",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "evaluation_metrics.py",
      "pasted_image_20240404163858.png",
      "f1_score",
      "precision",
      "recall",
      "ml_tools"
    ],
    "inlinks": [
      "precision"
    ],
    "summary": "The classification_report function in sklearn.metrics is used to evaluate the performance of a classification model. It provides a summary of key metrics for each class,..."
  },
  "classification": {
    "title": "Classification",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "support_vector_machines",
      "model_ensemble",
      "interpretability",
      "neural_network",
      "naive_bayes",
      "decision_tree",
      "k-nearest_neighbours",
      "supervised_learning",
      "regression"
    ],
    "inlinks": [
      "precision",
      "recall",
      "neural_network_classification",
      "k-nearest_neighbours",
      "supervised_learning",
      "class_separability",
      "precision_or_recall",
      "confusion_matrix",
      "gini_impurity",
      "learning_styles",
      "support_vector_machines",
      "machine_learning_algorithms",
      "decision_tree",
      "typical_output_formats_in_neural_networks",
      "cross_entropy",
      "imbalanced_datasets",
      "ds_&_ml_portal",
      "accuracy",
      "loss_function",
      "binary_classification"
    ],
    "summary": "Classification is a type of [[Supervised Learning]] in machine learning, where the algorithm learns from labeled data to predict which category or class a new,..."
  },
  "claude": {
    "title": "Claude",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Claude is better for code and uses Artifact for tracking code changes. Claude is crazy see: https://youtu.be/RudrWy9uPZE?t=473"
  },
  "cleaning_terminal_path": {
    "title": "cleaning terminal path",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.youtube.com/watch?v=18hUejOK0qk cmd prompt $g powershell ```powershell $profile microsfot_Powershell_profile have function prompt{ $p = -path \"$p> \" } ``` getting the script working https://stackoverflow.com/questions/41117421/ps1-cannot-be-loaded-because-running-scripts-is-disabled-on-this-system"
  },
  "click_implementation.py": {
    "title": "Click_Implementation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "python_click"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Click_Implementation.py This script implements a command-line interface (CLI) tool using Python's click library. The CLI allows users to interact with a JSON file, enabling them..."
  },
  "cloud_providers": {
    "title": "Cloud Providers",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "load_balancing",
      "memory_caching",
      "databricks",
      "scaling_server",
      "data_lakehouse",
      "data_warehouse"
    ],
    "inlinks": [
      "storage_layer_object_store",
      "parquet",
      "data_storage"
    ],
    "summary": "Among the biggest cloud providers are AWS, Microsoft Azure, Google Cloud. Whereas [[Databricks]] ( Databrick) and Snowflake provide dedicated [[Data Warehouse]]and [[Data Lakehouse|Lakehouse]] solutions Features..."
  },
  "clustering": {
    "title": "Clustering",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "hierarchical_clustering",
      "anomaly_detection",
      "unsupervised_learning",
      "gaussian_mixture_models",
      "interpretability",
      "standardised/outliers",
      "correlation",
      "feature_scaling",
      "k-means",
      "dbscan",
      "dendrograms"
    ],
    "inlinks": [
      "feature_selection",
      "gaussian_mixture_models",
      "tf-idf",
      "neural_network_classification",
      "problem_definition",
      "covariance_structures",
      "model_parameters",
      "anomaly_detection_with_clustering",
      "unsupervised_learning",
      "choosing_the_number_of_clusters",
      "wcss_and_elbow_method",
      "multicollinearity",
      "learning_styles",
      "machine_learning_algorithms",
      "dbscan",
      "ds_&_ml_portal",
      "correlation",
      "silhouette_analysis",
      "data_mining_-_crisp"
    ],
    "summary": "Clustering involves grouping a set of data points into subsets or clusters based on inherent patterns or similarities. It is an [[Unsupervised Learning]]technique used for..."
  },
  "clustering_dashboard.py": {
    "title": "Clustering_Dashboard.py",
    "tags": [
      "code_snippet"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "dash"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Clustering_Dashboard.py"
  },
  "clustermap": {
    "title": "Clustermap",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "dendrograms"
    ],
    "inlinks": [],
    "summary": "Clustermap Related to: [[Preprocessing|Preprocess]] Purpose: Identify which features are most similar using [[Dendrograms]]. Visualization: Regions of color show clustering, similar to a heatmap. Functionality: Performs..."
  },
  "code_diagrams": {
    "title": "Code Diagrams",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "architecture_diagram",
      "documentation_&_meetings",
      "mermaid",
      "classes",
      "sequence_diagram"
    ],
    "inlinks": [],
    "summary": "[[Documentation & Meetings]] There are class diagrams showing the hierarchy of classes [[Classes]] (Object orientated). Done in [[Mermaid]]. Overall [[Architecture Diagram]]: showing how software components..."
  },
  "columnar_storage": {
    "title": "Columnar Storage",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying"
    ],
    "inlinks": [
      "vectorized_engine",
      "row-based_storage",
      "duckdb_vs_sqlite",
      "database_storage",
      "duckdb",
      "types_of_database_schema"
    ],
    "summary": "A database storage technique that stores ==data by columns== rather than rows, Useful for read-heavy operations and ==large-scale data analytics==, as it enables the retrieval..."
  },
  "command_line": {
    "title": "Command Line",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "command_prompt",
      "bash",
      "powershell_vs_bash",
      "powershell"
    ],
    "inlinks": [],
    "summary": "The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts,..."
  },
  "command_prompt": {
    "title": "Command Prompt",
    "tags": [
      "software"
    ],
    "aliases": [
      "cmd"
    ],
    "outlinks": [
      "bash",
      "powershell"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_line",
      "powershell"
    ],
    "summary": "Command Prompt (cmd) is a command-line interpreter on Windows systems that allows users to execute commands to perform various basic tasks. Below are some common..."
  },
  "common_security_vulnerabilities_in_software_development": {
    "title": "Common Security Vulnerabilities in Software Development",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "security",
      "input_is_not_properly_sanitized",
      "sql_injection",
      "software_development_portal",
      "tool.bandit",
      "why_json_is_better_than_pickle_for_untrusted_data"
    ],
    "inlinks": [
      "testing",
      "security",
      "tool.bandit"
    ],
    "summary": "[[Security]] vulnerabilities can be encountered and mitigated in [[Software Development Portal]]. In this not describe potential security risks in their applications. Useful Tools - [[tool.bandit]]..."
  },
  "common_table_expression": {
    "title": "Common Table Expression",
    "tags": [
      "database",
      "querying"
    ],
    "aliases": [
      "CTE"
    ],
    "outlinks": [
      "querying",
      "views",
      "de_tools",
      "recursive_algorithm"
    ],
    "inlinks": [
      "views"
    ],
    "summary": "A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE..."
  },
  "communication_principles": {
    "title": "Communication principles",
    "tags": [
      "communication"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240916075439.png",
      "pasted_image_20240916075433.png"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20240916075433.png]] ![[Pasted image 20240916075439.png]]"
  },
  "communication_techniques": {
    "title": "Communication Techniques",
    "tags": [
      "communication"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Overview Using these structured communication bridges can enhance clarity and engagement, especially in spontaneous or high-stakes discussions. Tips for Using Communication Bridges 1. Start Small:..."
  },
  "comparing_llm": {
    "title": "Comparing LLM",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "llm"
    ],
    "inlinks": [],
    "summary": "Use lmarena.ai as a bench marking tool. [[LLM]] web dev arena text to image leader board"
  },
  "comparing_ensembles.py": {
    "title": "Comparing_Ensembles.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": [
      "model_ensemble"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Comparing_Ensembles.py This script explores various [[Model Ensemble]] techniques in machine learning, demonstrating their effectiveness in improving prediction accuracy through programmatic examples [!Important Notes] - {{Note..."
  },
  "components_of_the_database": {
    "title": "Components of the database",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "dimension_table",
      "fact_table",
      "obsidian_csp0fnavd1.png"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "[[Fact Table]] in main table that [[Dimension Table]] connect to them. ![[Obsidian_CSP0FnAVD1.png]]"
  },
  "computer_science": {
    "title": "Computer Science",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "algorithms"
    ],
    "inlinks": [],
    "summary": "[[Algorithms]]"
  },
  "concatenate": {
    "title": "Concatenate",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": ""
  },
  "conceptual_data_model": {
    "title": "conceptual data model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_schema"
    ],
    "summary": ""
  },
  "conceptual_model": {
    "title": "Conceptual Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "er_diagrams"
    ],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Conceptual Model - Entities: Customer, Order, Book - Relationships: Customers place Orders, Orders include Books Conceptual Model - Focuses on high-level business requirements. - Defines..."
  },
  "concurrency": {
    "title": "Concurrency",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "transaction",
      "database_techniques",
      "sqlite"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb"
  },
  "confidence_interval": {
    "title": "Confidence Interval",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "statistics",
      "data_analysis",
      "model_interpretability"
    ],
    "summary": "A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with..."
  },
  "confusion_matrix": {
    "title": "Confusion Matrix",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "accuracy",
      "f1_score",
      "pasted_image_20240120215414.png",
      "pasted_image_20240116210541.png",
      "classification",
      "precision",
      "recall",
      "pasted_image_20240116205937.png",
      "specificity"
    ],
    "inlinks": [
      "logistic_regression",
      "evaluation_metrics",
      "accuracy"
    ],
    "summary": "A Confusion Matrix is a table used to evaluate the performance of a [[Classification]] model. It provides a detailed breakdown of the model's predictions across..."
  },
  "continuous_delivery_-_deployment": {
    "title": "Continuous Delivery - Deployment",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_deployment"
    ],
    "inlinks": [],
    "summary": "Continuous Delivery - Ensures that code changes are automatically prepared for a release to production. - Builds, tests, and releases are automated, but the deployment..."
  },
  "continuous_integration": {
    "title": "Continuous Integration",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "ci-cd",
      "data_engineer"
    ],
    "summary": "Developers frequently integrate code into a shared repository. Automated builds and tests are run to detect issues early. Encourages smaller, more manageable code changes."
  },
  "converting_categorical_variables_to_a_dummy_indicators": {
    "title": "Converting categorical variables to a dummy indicators",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "convolutional_neural_networks": {
    "title": "Convolutional Neural Networks",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "feature_extraction",
      "pasted_image_20241006124735.png",
      "deep_learning",
      "pasted_image_20241006124829.png"
    ],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Convolutional networks, or CNNs, are specialized [[Deep Learning]] architectures designed for processing data with grid-like structures, such as images. They use convolutional layers with learnable..."
  },
  "correlation_vs_causation": {
    "title": "Correlation vs Causation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "correlation"
    ],
    "inlinks": [
      "correlation",
      "statistics"
    ],
    "summary": "What is the meaning of [[Correlation]] does not imply causation? Correlation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. Correlation:..."
  },
  "correlation": {
    "title": "Correlation",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "standardised/outliers",
      "heatmap",
      "multicollinearity",
      "clustering",
      "covariance",
      "correlation_vs_causation",
      "'var1',_'target'"
    ],
    "inlinks": [
      "eda",
      "feature_selection",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "pca_principal_components",
      "filter_methods",
      "heatmap",
      "multicollinearity",
      "clustering",
      "covariance",
      "correlation_vs_causation"
    ],
    "summary": "Use in understanding relationships between variables in data analysis. While it helps identify associations, it's important to remember that ==correlation does not imply causation.== Visualization..."
  },
  "cosine_similarity": {
    "title": "Cosine Similarity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "tf-idf",
      "metric",
      "binary_classification"
    ],
    "inlinks": [
      "word2vec.py",
      "vector_database"
    ],
    "summary": "Cosine similarity is a [[Metric]] used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from..."
  },
  "cost_function": {
    "title": "Cost Function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241216202917.png",
      "model_optimisation",
      "model_parameters",
      "loss_versus_cost_function",
      "loss_function",
      "reward_function",
      "gradient_descent",
      "reinforcement_learning",
      "pasted_image_20241216202825.png"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "momentum",
      "loss_versus_cost_function",
      "loss_function",
      "optimising_a_logistic_regression_model",
      "gradient_descent",
      "model_parameters_tuning",
      "lbfgs",
      "optimisation_techniques"
    ],
    "summary": "The concept of a Cost Function is central to [[Model Optimisation]], particularly in training models. A cost function, also known as a loss function or..."
  },
  "cost-sensitive_analysis": {
    "title": "Cost-Sensitive Analysis",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "model_parameters"
    ],
    "inlinks": [
      "determining_threshold_values",
      "choosing_a_threshold",
      "imbalanced_datasets"
    ],
    "summary": "Cost-sensitive analysis in machine learning refers to the practice of incorporating the costs associated with different types of errors into the model training and evaluation..."
  },
  "covariance_structures": {
    "title": "Covariance Structures",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gaussian_mixture_models",
      "variance",
      "principal_component_analysis",
      "distributions",
      "clustering",
      "statistics",
      "covariance",
      "data_analysis"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "kmeans_vs_gmm"
    ],
    "summary": "A covariance structure in general refers to the way variability and relationships between variables (or dimensions) are modeled and described in a dataset. It specifies..."
  },
  "covariance_vs_correlation": {
    "title": "Covariance vs Correlation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "covariance"
    ],
    "inlinks": [],
    "summary": "[[Covariance]] provides a basic measure of how two variables move together, correlation offers a more [[interpretability|interpretable]] and standardized way to understand their relationship. Definition: -..."
  },
  "covariance": {
    "title": "Covariance",
    "tags": [
      "statistics",
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "gaussian_mixture_models",
      "correlation"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "correlation",
      "covariance_structures",
      "covariance_vs_correlation"
    ],
    "summary": "In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between..."
  },
  "covering_index": {
    "title": "Covering Index",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "database_index"
    ],
    "summary": "Like an [[Database Index|Index]] but for partial indexes?"
  },
  "cron_jobs": {
    "title": "Cron jobs",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "cross_entropy": {
    "title": "Cross Entropy",
    "tags": [
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "categorical_data",
      "loss_function",
      "cross_entropy_net.py",
      "cross_entropy_single.py",
      "classification",
      "cross_entropy.py",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "gini_impurity_vs_cross_entropy",
      "loss_function",
      "decision_tree",
      "language_model_output_optimisation",
      "cross_entropy.py",
      "neural_scaling_laws"
    ],
    "summary": "Cross entropy is a [[Loss function]] used in [[Classification]] tasks, particularly for [[categorical data]]. The cross entropy loss function is particularly effective for multi-class classification..."
  },
  "cross_validation": {
    "title": "Cross Validation",
    "tags": [
      "#evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "kfold_cross_validation.py",
      "ml_tools",
      "model_optimisation",
      "overfitting",
      "bias_and_variance",
      "time_series",
      "data_leakage",
      "model_validation",
      "model_evaluation",
      "hyperparameter"
    ],
    "inlinks": [
      "model_optimisation",
      "overfitting",
      "decision_tree",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "model_selection",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "kaggle_abalone_regression_example",
      "train-dev-test_sets",
      "hyperparameter_tuning",
      "model_evaluation"
    ],
    "summary": "Cross-validation is a statistical technique used in machine learning to ==assess how well a model will generalize== to an independent dataset. It is a crucial..."
  },
  "crosstab": {
    "title": "Crosstab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "groupby_vs_crosstab"
    ],
    "summary": "Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables. Here's an example: ```python Sample DataFrame..."
  },
  "cross_entropy.py": {
    "title": "Cross_Entropy.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "mean_squared_error",
      "cross_entropy"
    ],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py Generalized Script Description: Dataset: Uses the Iris dataset from sklearn to classify flower species. Preprocessing: One-hot encodes the target labels and splits the data..."
  },
  "cross_entropy_single.py": {
    "title": "Cross_Entropy_Single.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py Example Let's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class..."
  },
  "crud": {
    "title": "CRUD",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "rest_api",
      "row-based_storage",
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": "Create,Read,Update,Delete."
  },
  "cryptography": {
    "title": "Cryptography",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "javascript",
      "node.js",
      "security",
      "hash"
    ],
    "inlinks": [],
    "summary": "Cryptography is the foundation of digital [[Security]], enabling privacy and secure communication over the internet. Examples are implemented in [[Node.JS]] (using crypto module) and are..."
  },
  "current_challenges_within_the_energy_sector": {
    "title": "Current challenges within the energy sector",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "current_challenges_within_the_energy_sector"
    ],
    "inlinks": [
      "current_challenges_within_the_energy_sector"
    ],
    "summary": "[[Current challenges within the energy sector]] related to reinforcement learning and that can be progressed with recent technological advances"
  },
  "dagster": {
    "title": "dagster",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "declarative",
      "data_lifecycle_management"
    ],
    "inlinks": [
      "directed_acyclic_graph_(dag)",
      "etl",
      "data_management"
    ],
    "summary": "Dagster is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [[Data Lifecycle Management]] lifecycle, with integrated lineage and observability, a [[declarative]]..."
  },
  "dash": {
    "title": "Dash",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "plotly",
      "clustering_dashboard.py",
      "ml_tools"
    ],
    "inlinks": [
      "dashboarding"
    ],
    "summary": "Dash is an open-source framework for building interactive web applications using Python. It is particularly well-suited for data visualization and dashboard creation. Dash integrates with..."
  },
  "dashboarding": {
    "title": "Dashboarding",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "streamlit.io",
      "dash"
    ],
    "inlinks": [
      "react"
    ],
    "summary": "[[Dash]] [[Streamlit.io]]"
  },
  "data_ai_education_at_work": {
    "title": "Data AI Education at Work",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Introduction Organizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are..."
  },
  "data_analysis_portal": {
    "title": "Data Analysis Portal",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "data_analysis": {
    "title": "Data Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "eda",
      "confidence_interval",
      "data_visualisation",
      "hypothesis_testing",
      "statistics",
      "data_analyst"
    ],
    "inlinks": [
      "eda",
      "covariance_structures",
      "ds_&_ml_portal",
      "statistical_assumptions",
      "data_lifecycle_management",
      "alternatives_to_batch_processing",
      "duckdb",
      "data_roles",
      "fact_table",
      "data_analyst"
    ],
    "summary": "What is it? Usually done with a [[Data Analyst]].After processing, data is analyzed to extract meaningful insights and derive value from the data. Types of..."
  },
  "data_analyst": {
    "title": "Data Analyst",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "eda",
      "data_collection",
      "documentation_&_meetings",
      "data_visualisation",
      "distributions",
      "hypothesis_testing",
      "statistics",
      "data_analysis"
    ],
    "inlinks": [
      "data_roles",
      "data_analysis"
    ],
    "summary": "Summary: - Gathers and processes data to generate reports. - Communicates insights and findings to management - Conducts [[Data Analysis]]. Key responsibilities of a data..."
  },
  "data_architect": {
    "title": "Data Architect",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Architect - Designs and manages the data infrastructure. - Ensures data is stored, organized, and accessible for analysis."
  },
  "data_archive_graph_analysis": {
    "title": "Data Archive Graph Analysis",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "graph_analysis_plugin",
      "dataview",
      "graph_view"
    ],
    "inlinks": [],
    "summary": "Use the following to [[Dataview]] [[Graph View]] Check out [[Graph Analysis Plugin]] Convert Dataview to CSV"
  },
  "data_asset": {
    "title": "data asset",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_product"
    ],
    "summary": ""
  },
  "data_cleansing": {
    "title": "Data Cleansing",
    "tags": [
      "data_transformation",
      "data_cleaning",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "deleting_rows_or_filling_them_with_the_mean_is_not_always_best_",
      "handling_different_distributions",
      "data_selection",
      "de_tools",
      "data_quality",
      "handling_missing_data",
      "standardised/outliers"
    ],
    "inlinks": [
      "anomaly_detection",
      "preprocessing",
      "why_use_er_diagrams",
      "pandas_stack",
      "fuzzywuzzy",
      "boxplot",
      "data_transformation",
      "data_analyst"
    ],
    "summary": "Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [[Data Quality]] for analysis. Involves: [[standardised/Outliers|Handling Outliers]] [[Handling..."
  },
  "data_collection": {
    "title": "Data Collection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "imbalanced_datasets",
      "data_quality"
    ],
    "inlinks": [
      "preprocessing",
      "business_observability",
      "data_analyst"
    ],
    "summary": "Determine the [[Data Quality]] and quantity of data required and get it. [[Imbalanced Datasets]]"
  },
  "data_contract": {
    "title": "Data Contract",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_quality",
      "api",
      "dbt",
      "data_contract",
      "pasted_image_20250312163351.png"
    ],
    "inlinks": [
      "data_contract",
      "data_quality"
    ],
    "summary": "[[Data Contract]] pattern to handle schema changes Pattern to apply to organisation using tools they have. Tooling: - [[dbt]] Data contracts help prevent preventable data..."
  },
  "data_distribution": {
    "title": "Data Distribution",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_management",
      "data_lifecycle_management"
    ],
    "summary": "Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. This can involve supplying data to -..."
  },
  "data_drift": {
    "title": "Data Drift",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "performance_drift"
    ],
    "inlinks": [
      "model_observability",
      "performance_drift"
    ],
    "summary": "Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead..."
  },
  "data_engineer": {
    "title": "Data Engineer",
    "tags": [
      "career",
      "field"
    ],
    "aliases": [
      "Data Engineering"
    ],
    "outlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "data_management",
      "documentation_&_meetings",
      "data_engineering_portal",
      "data_roles",
      "continuous_integration"
    ],
    "inlinks": [
      "data_engineering",
      "data_roles",
      "databricks",
      "apache_spark"
    ],
    "summary": "The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on - automating..."
  },
  "data_engineering_portal": {
    "title": "Data Engineering Portal",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "spreadsheets_vs_databases",
      "structured_data",
      "mysql",
      "components_of_the_database",
      "database_management_system_(dbms)",
      "database_techniques",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "postgresql"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] They facilitate efficient CRUD.md operations and transactional processing (OLTP.md), structured..."
  },
  "data_engineering_tools": {
    "title": "Data Engineering Tools",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "amazon_s3",
      "dbt",
      "sql",
      "data_ingestion",
      "apache_kafka",
      "azure",
      "data_storage",
      "cloud"
    ],
    "inlinks": [
      "data_storage",
      "data_ingestion",
      "data_engineer"
    ],
    "summary": "Snowflake: [[Cloud]]-based data warehousing for scalable storage and processing. Microsoft SQL Server: [[SQL]]-based relational database management. [[Azure]] SQL Database: Managed relational database service on Azure...."
  },
  "data_engineering": {
    "title": "Data Engineering",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "data_management",
      "apache_airflow",
      "data_engineer",
      "prefect"
    ],
    "inlinks": [
      "data_storage",
      "data_transformation_in_data_engineering",
      "normalisation"
    ],
    "summary": "The definition from the Fundamentals of Data Engineering, as it\u2019s one of the most recent and complete: Data engineering is the development, implementation, and maintenance..."
  },
  "data_governance": {
    "title": "Data Governance",
    "tags": [
      "business",
      "#data_governance"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability"
    ],
    "inlinks": [
      "data_roles",
      "master_data_management",
      "data_principles",
      "data_steward",
      "ai_governance",
      "data_storage"
    ],
    "summary": "Data governance is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization..."
  },
  "data_hierarchy_of_needs": {
    "title": "Data Hierarchy of Needs",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241005170237.png"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20241005170237.png|500]] The Data Hierarchy of Needs is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow\u2019s..."
  },
  "data_ingestion": {
    "title": "Data Ingestion",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "scalability",
      "data_engineering_tools",
      "data_quality",
      "api",
      "data_ingestion",
      "database",
      "data_streaming",
      "latency",
      "business_intelligence",
      "machine_learning"
    ],
    "inlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "data_lifecycle_management",
      "data_ingestion",
      "data_storage",
      "data_warehouse"
    ],
    "summary": "Data ingestion is the process of collecting and importing raw data from various sources ([[Database]], [[API]], [[Data Streaming]] services) into a system for processing and..."
  },
  "data_integration": {
    "title": "Data Integration",
    "tags": [
      "data_storage",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "data_virtualization",
      "single_source_of_truth"
    ],
    "inlinks": [
      "apache_kafka",
      "data_virtualization"
    ],
    "summary": "Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [[Single Source of Truth]]...."
  },
  "data_integrity": {
    "title": "Data Integrity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "checksum",
      "hash",
      "data_integrity",
      "database"
    ],
    "inlinks": [
      "data_pipeline",
      "performance_dimensions",
      "anomaly_detection",
      "transaction",
      "hash",
      "soft_deletion",
      "data_lifecycle_management",
      "data_integrity",
      "apache_kafka",
      "data_principles",
      "relating_tables_together"
    ],
    "summary": "Data integrity refers to the - accuracy, - consistency, and - reliability of data throughout its lifecycle. It ensures that data remains ==unaltered== and ==trustworthy==,..."
  },
  "data_lake": {
    "title": "Data Lake",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "structured_data",
      "unstructured_data"
    ],
    "inlinks": [
      "fabric",
      "data_storage",
      "databricks",
      "event_driven_events",
      "hadoop"
    ],
    "summary": "A Data Lake is a storage system with vast amounts of [[unstructured data]] and [[structured data]], stored as-is, without a specific purpose in mind, that..."
  },
  "data_lakehouse": {
    "title": "Data Lakehouse",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Lakehouse"
    ],
    "outlinks": [
      "database_schema",
      "data_warehouse",
      "acid_transaction",
      "data_management"
    ],
    "inlinks": [
      "cloud_providers",
      "single_source_of_truth"
    ],
    "summary": "A Data Lakehouse open [[Data Management]] architecture that combines the flexibility, cost-efficiency, and scale of Data Lake with the data management and ACID transactions of..."
  },
  "data_leakage": {
    "title": "Data Leakage",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cross_validation",
      "data_selection_in_ml",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data"
    ],
    "summary": "Data Leakage refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly..."
  },
  "data_lifecycle_management": {
    "title": "Data Lifecycle Management",
    "tags": [
      "data_management",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "performance_dimensions",
      "preprocessing",
      "data_distribution",
      "data_ingestion",
      "data_visualisation",
      "data_integrity",
      "data_storage",
      "data_analysis",
      "software_development_life_cycle"
    ],
    "inlinks": [
      "data_principles",
      "data_lineage",
      "dagster"
    ],
    "summary": "This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. Used for maintaining [[data integrity]], optimizing..."
  },
  "data_lineage": {
    "title": "data lineage",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "data_lifecycle_management"
    ],
    "inlinks": [
      "declarative",
      "model_observability",
      "dbt"
    ],
    "summary": "Data lineage uncovers the [[Data Lifecycle Management]] life cycle of data. It aims to show the complete data flow from start to finish. Data lineage..."
  },
  "data_literacy": {
    "title": "data literacy",
    "tags": [
      "#business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Data literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This..."
  },
  "data_management": {
    "title": "Data Management",
    "tags": [
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "data_pipeline",
      "dagster",
      "data_quality",
      "data_management",
      "apache_airflow",
      "data_distribution",
      "master_data_management",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "digital_twin",
      "data_pipeline",
      "data_management",
      "difference_between_snowflake_to_hadoop",
      "spreadsheets_vs_databases",
      "data_engineering",
      "master_data_management",
      "data_storage",
      "data_principles",
      "data_steward",
      "data_engineer",
      "data_lakehouse",
      "data_roles",
      "database_schema"
    ],
    "summary": "Data management involves overseeing processes to maintain data integrity and quality. It includes: Responsibility: Identifying accountable individuals or teams. Issue Resolution: Mechanisms for detecting and..."
  },
  "data_mining_-_crisp": {
    "title": "Data Mining - CRISP",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [],
    "summary": "CRISP-DM stands for Cross-Industry Standard Process for Data Mining, a widely adopted framework that provides a structured approach to planning, organizing, and conducting data mining..."
  },
  "data_modelling": {
    "title": "Data Modelling",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "physical_model",
      "data_modelling",
      "er_diagrams",
      "logical_model",
      "conceptual_model"
    ],
    "inlinks": [
      "data_modelling",
      "database_schema"
    ],
    "summary": "Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. This helps in organizing..."
  },
  "data_observability": {
    "title": "Data Observability",
    "tags": [
      "#data_orchestration",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability",
      "standardised/outliers",
      "dbt",
      "data_quality"
    ],
    "inlinks": [
      "data_governance",
      "declarative",
      "data_quality",
      "data_observability",
      "performance_drift",
      "prevention_is_better_than_the_cure",
      "guardrails",
      "model_observability"
    ],
    "summary": "Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [[Data Quality]], reliability, and availability. It covers various..."
  },
  "data_orchestration": {
    "title": "Data Orchestration",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [
      "#data_orchestration"
    ],
    "outlinks": [
      "data_pipeline_to_data_products"
    ],
    "inlinks": [],
    "summary": "Data orchestration refers to the process of managing and coordinating the flow of data across various systems and environments, particularly in complex and heterogeneous cloud..."
  },
  "data_pipeline_to_data_products": {
    "title": "Data Pipeline to Data Products",
    "tags": [
      "#question",
      "data_orchestration",
      "anomaly_detection",
      "data_pipeline",
      "data_products"
    ],
    "aliases": [],
    "outlinks": [
      "data_product",
      "data_pipeline"
    ],
    "inlinks": [
      "data_pipeline",
      "data_orchestration"
    ],
    "summary": "The journey from [[Data Pipeline]] to [[Data Product]] involves transforming raw data into valuable insights or applications that can be used to drive business decisions...."
  },
  "data_pipeline": {
    "title": "Data Pipeline",
    "tags": [
      "data_pipeline"
    ],
    "aliases": [
      "ETL Pipeline"
    ],
    "outlinks": [
      "data_pipeline",
      "data_management",
      "data_pipeline_to_data_products",
      "preprocessing",
      "data_ingestion",
      "data_integrity",
      "data_transformation",
      "data_storage"
    ],
    "inlinks": [
      "data_pipeline",
      "data_management",
      "data_pipeline_to_data_products",
      "data_engineering",
      "data_ingestion",
      "data_engineer"
    ],
    "summary": "A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can..."
  },
  "data_principles": {
    "title": "Data Principles",
    "tags": [
      "data_quality",
      "data_governance"
    ],
    "aliases": null,
    "outlinks": [
      "security",
      "data_governance",
      "performance_dimensions",
      "data_management",
      "data_quality",
      "documentation_&_meetings",
      "data_lifecycle_management",
      "data_integrity"
    ],
    "inlinks": [],
    "summary": "Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically. [[Data Quality]] Ensure data is accurate, complete, reliable, and..."
  },
  "data_product": {
    "title": "Data Product",
    "tags": [
      "data_management",
      "business",
      "business_intelligence"
    ],
    "aliases": [],
    "outlinks": [
      "data_asset"
    ],
    "inlinks": [
      "data_pipeline_to_data_products"
    ],
    "summary": "A data product is \"a product that facilitates an end goal through data\". Delivering the final output, which could be dashboards, reports, or machine learning..."
  },
  "data_quality": {
    "title": "Data Quality",
    "tags": [
      "#data_quality"
    ],
    "aliases": [],
    "outlinks": [
      "change_management",
      "data_observability",
      "data_contract",
      "prevention_is_better_than_the_cure"
    ],
    "inlinks": [
      "eda",
      "declarative",
      "determining_threshold_values",
      "data_principles",
      "neural_network_classification",
      "data_management",
      "data_collection",
      "data_selection_in_ml",
      "data_observability",
      "master_data_management",
      "prevention_is_better_than_the_cure",
      "data_cleansing",
      "performance_dimensions",
      "benefits_of_data_transformation",
      "machine_learning_algorithms",
      "why_use_er_diagrams",
      "data_ingestion",
      "data_contract",
      "ds_&_ml_portal",
      "data_validation",
      "data_roles"
    ],
    "summary": "Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis. Definition: Data quality refers..."
  },
  "data_reduction": {
    "title": "Data Reduction",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "dimensionality_reduction",
      "variance",
      "sampling"
    ],
    "inlinks": [
      "preprocessing"
    ],
    "summary": "Reducing the volume of data through techniques: [[Dimensionality Reduction]] [[Sampling]]: Use subsets of data for training to speed up the process and address issues like..."
  },
  "data_roles": {
    "title": "Data Roles",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_scientist",
      "data_architect",
      "data_governance",
      "data_management",
      "data_quality",
      "ml_engineer",
      "data_engineer",
      "data_steward",
      "data_analysis",
      "data_analyst"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. The team..."
  },
  "data_science": {
    "title": "Data Science",
    "tags": [
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "statistics",
      "unstructured_data",
      "scientific_method"
    ],
    "inlinks": [],
    "summary": "A field that uses the [[Scientific Method]], algorithms, and systems to ==extract knowledge== and insights from structured and [[unstructured data]]. It combines techniques from [[statistics]],..."
  },
  "data_scientist": {
    "title": "Data Scientist",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "business_intelligence"
    ],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Scientist - Utilizes [[Business Intelligence]] (BI) tools to analyze data. - Works with data lakes to extract insights. - Develops and deploys production Machine..."
  },
  "data_selection_in_ml": {
    "title": "Data Selection in ML",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "model_optimisation",
      "data_quality",
      "correlation",
      "dimensionality_reduction",
      "distributions",
      "data_leakage",
      "multicollinearity",
      "data_transformation",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "data_selection"
    ],
    "summary": "When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[[Model Optimisation]] and the insights you can derive from it...."
  },
  "data_selection": {
    "title": "Data Selection",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "data_selection_in_ml"
    ],
    "inlinks": [
      "data_cleansing",
      "data_transformation",
      "pandas",
      "how_do_you_do_the_data_selection"
    ],
    "summary": "Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame. In [[DE_Tools]] we explore..."
  },
  "data_steward": {
    "title": "Data Steward",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_governance",
      "data_steward",
      "data_management"
    ],
    "inlinks": [
      "data_roles",
      "data_steward"
    ],
    "summary": "A Data Steward is responsible for ensuring the quality, integrity, and governance of an organization's data assets. They act as a bridge between business users,..."
  },
  "data_storage": {
    "title": "Data Storage",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "data_management"
    ],
    "outlinks": [
      "data_governance",
      "cloud_providers",
      "storage_layer_object_store\\",
      "data_management",
      "sqlite",
      "querying",
      "amazon_s3",
      "data_engineering_tools",
      "data_engineering",
      "data_ingestion",
      "nosql",
      "data_lake",
      "database",
      "data_transformation",
      "data_warehouse"
    ],
    "inlinks": [
      "data_pipeline",
      "data_engineering_tools",
      "checksum",
      "data_lifecycle_management",
      "parquet",
      "apache_kafka",
      "big_data",
      "data_warehouse"
    ],
    "summary": "Data storage is a fundamental aspect of [[Data Engineering]], influencing processes such as - (occurring after [[Data Ingestion]]) - [[Data Transformation]] - [[Querying]] - [[data..."
  },
  "data_streaming": {
    "title": "Data Streaming",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "publish_and_subscribe",
      "batch_processing",
      "apache_kafka",
      "data_streaming",
      "alternatives_to_batch_processing"
    ],
    "inlinks": [
      "publish_and_subscribe",
      "distributed_computing",
      "data_ingestion",
      "lambda_architecture",
      "data_streaming",
      "alternatives_to_batch_processing"
    ],
    "summary": "Data Streaming is used for real-time data processing, allowing continuous flow and processing of data as it arrives. This is different from [[batch processing]], which..."
  },
  "data_terms": {
    "title": "Data Terms",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "data_transformation_in_data_engineering": {
    "title": "Data transformation in Data Engineering",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "elt",
      "data_engineering",
      "etl"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Data transformation in [[Data Engineering]] is a key step in data pipelines, often part of: ETL (Extract, Transform, Load) [[ETL]]: Data is transformed before loading..."
  },
  "data_transformation_in_machine_learning": {
    "title": "Data transformation in Machine Learning",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "encoding_categorical_variables",
      "supervised_learning"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Transforming raw data into a meaningful format is necessary for building effective models. [[Supervised Learning]]: Annotating datasets with correct labels (e.g., labeling images of apples..."
  },
  "data_transformation_with_pandas": {
    "title": "Data Transformation with Pandas",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "pandas_join_vs_merge",
      "de_tools",
      "aggregation",
      "pasted_image_20250323081817.png",
      "joining_datasets",
      "merge",
      "pandas",
      "pandas_stack",
      "concatenate",
      "multi-level_index",
      "crosstab"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Using [[pandas]] we can do the following: [[Merge]] [[Concatenate]] [[Joining Datasets]] [[Pandas join vs merge]] [[Multi-level index]] [[Aggregation]] [[Pandas Stack]] [[Crosstab]] A summary of transformations..."
  },
  "data_transformation": {
    "title": "Data Transformation",
    "tags": [
      "data_cleaning",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "data_selection",
      "data_transformation_in_machine_learning",
      "normalised_schema",
      "aggregation",
      "normalisation_of_data",
      "benefits_of_data_transformation",
      "data_transformation_with_pandas",
      "joining_datasets",
      "structuring_and_organizing_data",
      "data_transformation_in_data_engineering"
    ],
    "inlinks": [
      "eda",
      "data_pipeline",
      "data_selection_in_ml",
      "preprocessing",
      "handling_missing_data",
      "benefits_of_data_transformation",
      "melt",
      "etl",
      "dbt",
      "pandas_stack",
      "standardisation",
      "pandas",
      "duckdb",
      "activation_function",
      "data_storage"
    ],
    "summary": "Data transformation is the process of converting data from one format to another. Data transformation may involve: - [[Data Cleansing]] - [[Structuring and organizing data]]..."
  },
  "data_validation": {
    "title": "Data Validation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "typescript",
      "type_checking",
      "pydantic",
      "data_quality"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "excel_&_sheets",
      "pydantic"
    ],
    "summary": "Data Validation: Error Prevention: It ensures data accuracy by preventing incorrect or inappropriate data entries. Consistent Data Entry: Helps maintain consistency across large datasets by..."
  },
  "data_virtualization": {
    "title": "Data Virtualization",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_integration"
    ],
    "inlinks": [
      "semantic_layer",
      "data_integration"
    ],
    "summary": "Organizations may also consider adopting a data virtualization solution to integrate their data. In this type of [[data integration]], data from multiple sources is left..."
  },
  "data_visualisation": {
    "title": "Data Visualisation",
    "tags": [
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "looker_studio",
      "powerbi",
      "tableau"
    ],
    "inlinks": [
      "eda",
      "melt",
      "data_lifecycle_management",
      "grouped_plots",
      "dimensionality_reduction",
      "tableau",
      "business_observability",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Data visualization involves presenting data in a visual format, enabling stakeholders to quickly grasp insights and make informed decisions. Effective visualization tools include dashboards and..."
  },
  "data_warehouse": {
    "title": "Data Warehouse",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "Warehouse",
      "DWH"
    ],
    "outlinks": [
      "querying",
      "documentation_&_meetings",
      "etl",
      "data_ingestion",
      "data_storage"
    ],
    "inlinks": [
      "cloud_providers",
      "single_source_of_truth",
      "fabric",
      "fact_table",
      "databricks",
      "snowflake",
      "bigquery",
      "dimensional_modelling",
      "data_lakehouse",
      "data_storage",
      "databricks_vs_snowflake"
    ],
    "summary": "A Data Warehouse (DWH) is a centralized repository designed for [[Querying]] and analysis, storing large volumes of structured data from various sources within an organization...."
  },
  "database_index": {
    "title": "Database Index",
    "tags": [
      "database_optimisation"
    ],
    "aliases": [
      "Indexing",
      "Index"
    ],
    "outlinks": [
      "covering_index",
      "de_tools",
      "querying",
      "b-tree",
      "database"
    ],
    "inlinks": [
      "query_plan",
      "covering_index",
      "query_optimisation",
      "database_techniques"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Indexing/Indexing.ipynb Related terms: - [[Covering Index]] - Partial Index (Index with where clause) Indexing is a technique used to ==speed up..."
  },
  "database_management_system_(dbms)": {
    "title": "Database Management System (DBMS)",
    "tags": [
      "database",
      "data_management"
    ],
    "aliases": [
      "DBMS"
    ],
    "outlinks": [
      "mongodb",
      "sqlite",
      "mysql",
      "crud",
      "oracle",
      "postgresql"
    ],
    "inlinks": [
      "sqlite",
      "data_management",
      "transaction",
      "data_engineering_portal",
      "database"
    ],
    "summary": "A Database Management System (DBMS) is software that allows you to interact with and manage databases. Easiest to use: - [[SQLite]] - [[PostgreSQL]] Others: -..."
  },
  "database_schema": {
    "title": "Database Schema",
    "tags": [
      "data_modeling",
      "database_structure"
    ],
    "aliases": [
      "schema",
      "Schema"
    ],
    "outlinks": [
      "conceptual_data_model",
      "data_modelling",
      "data_management",
      "database",
      "implementing_database_schema",
      "types_of_database_schema",
      "database_schema"
    ],
    "inlinks": [
      "soft_deletion",
      "structured_data",
      "database",
      "views",
      "data_lakehouse",
      "schema_evolution",
      "database_schema",
      "fact_table"
    ],
    "summary": "A [[Database Schema|schema]] is the structure that defines how data is organized in a [[Database]], used in [[Data Management]]. It specifies the tables, columns, relationships,..."
  },
  "database_storage": {
    "title": "Database Storage",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "row-based_storage",
      "columnar_storage",
      "database",
      "vectorized_engine"
    ],
    "inlinks": [],
    "summary": "Methods and optimizations for storing, retrieving, and processing data in [[database]] systems. [[Columnar Storage]] [[Row-based Storage]] [[Vectorized Engine]]"
  },
  "database_techniques": {
    "title": "Database Techniques",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "race_conditions",
      "database_index",
      "querying",
      "concurrency",
      "vacuum",
      "soft_deletion",
      "stored_procedures",
      "query_plan",
      "sql_joins"
    ],
    "inlinks": [
      "melt",
      "data_engineering_portal",
      "database",
      "sql"
    ],
    "summary": "Techniques: - [[Soft Deletion]] - [[Concurrency]] - [[Race Conditions]] - [[Querying]] - [[SQL Joins]] - [[Stored Procedures]] - Cleaning: Use Levenshtein Distance (if SQLite extension..."
  },
  "database": {
    "title": "Database",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "mongodb",
      "spreadsheets_vs_databases",
      "oltp",
      "mysql",
      "structured_data",
      "components_of_the_database",
      "database_management_system_(dbms)",
      "database_techniques",
      "crud",
      "relating_tables_together",
      "turning_a_flat_file_into_a_database",
      "postgresql",
      "database_schema"
    ],
    "inlinks": [
      "database_index",
      "rollup",
      "single_source_of_truth",
      "structured_data",
      "sql",
      "data_ingestion",
      "data_integrity",
      "database_storage",
      "slowly_changing_dimension",
      "microsoft_access",
      "software_design_patterns",
      "data_storage",
      "database_schema"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] [[MongoDB]] They facilitate efficient [[CRUD]] operations and transactional processing ([[OLTP]])..."
  },
  "databricks_vs_snowflake": {
    "title": "Databricks vs Snowflake",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "data_warehouse",
      "databricks",
      "snowflake"
    ],
    "inlinks": [
      "databricks"
    ],
    "summary": "Comparison between [[Databricks]] and [[Snowflake]]: Databricks is a versatile platform that emphasizes collaborative data science and engineering through interactive notebooks, making it suitable for advanced..."
  },
  "databricks": {
    "title": "Databricks",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "apache_spark",
      "databricks_vs_snowflake",
      "data_lake",
      "data_engineer",
      "big_data",
      "hadoop",
      "data_warehouse"
    ],
    "inlinks": [
      "batch_processing",
      "big_data",
      "cloud_providers",
      "databricks_vs_snowflake"
    ],
    "summary": "Databricks Overview [!Summary] Databricks is a cloud-based platform for [[big data]] processing built on [[Apache Spark]]. It provides an integrated workspace for collaboration among [[data..."
  },
  "datasets": {
    "title": "Datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "time_series"
    ],
    "inlinks": [
      "handling_different_distributions",
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "mnist",
      "train-dev-test_sets"
    ],
    "summary": "This note collects notes on datasets that are good examples for exploring various concepts. Heart Failure Prediction Dataset Link: Heart Failure Prediction Dataset Useful for:..."
  },
  "dbscan": {
    "title": "DBScan",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "clustering",
      "standardised/outliers",
      "k-means"
    ],
    "inlinks": [
      "clustering",
      "unsupervised_learning",
      "isolated_forest",
      "anomaly_detection_with_clustering"
    ],
    "summary": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a [[Clustering]] algorithm that groups together data points ==based on density==. It is particularly useful when..."
  },
  "dbt": {
    "title": "dbt",
    "tags": [
      "software"
    ],
    "aliases": [
      "data build tool"
    ],
    "outlinks": [
      "documentation_&_meetings",
      "dbt",
      "sql",
      "elt",
      "data_transformation",
      "data_lineage"
    ],
    "inlinks": [
      "data_engineering_tools",
      "data_observability",
      "dbt",
      "data_contract",
      "jinja_template",
      "elt"
    ],
    "summary": "Data build tool is an open-source framework designed for [[Data Transformation]] within a modern data stack. It enables analysts and engineers to transform, model, and..."
  },
  "debugging_ipynb": {
    "title": "Debugging ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "debugging jupyter cells https://www.youtube.com/watch?v=CY6uZIoF_kQ Sometimes dissapears: https://stackoverflow.com/questions/72671709/vs-code-debug-cell-disappears-arbitrarily-in-jupyter-notebook-view"
  },
  "debugging": {
    "title": "Debugging",
    "tags": [
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "types_of_computational_bugs",
      "stackbiz",
      "testing_unittest.py",
      "typescript",
      "git",
      "testing_pytest.py",
      "debugging.py",
      "software_development_life_cycle",
      "ml_tools"
    ],
    "inlinks": [
      "pyright",
      "types_of_computational_bugs",
      "pydantic"
    ],
    "summary": "Debugging is the process of identifying, analyzing, and resolving bugs or defects in software while [[Testing]]. It is a critical part of the [[Software Development..."
  },
  "debugging.py": {
    "title": "Debugging.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Debugging.py This script includes examples of logging, using breakpoints, and reproducing a simple bug for practice Key Concepts Demonstrated in the Script Logging in Python:..."
  },
  "decision_tree": {
    "title": "Decision Tree",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "gini_impurity",
      "supervised_learning",
      "gridseachcv",
      "overfitting",
      "gini_impurity_vs_cross_entropy",
      "interpretability",
      "pasted_image_20240404154526.png",
      "decision_tree",
      "cross_validation",
      "pruning",
      "classification",
      "cross_entropy",
      "hyperparameter"
    ],
    "inlinks": [
      "gini_impurity",
      "gradient_boosting",
      "ds_&_ml_portal",
      "ada_boosting",
      "model_parameters",
      "weak_learners",
      "regularisation_of_tree_based_models",
      "machine_learning_algorithms",
      "decision_tree",
      "random_forests",
      "why_and_when_is_feature_scaling_necessary",
      "gradient_boosting_regressor",
      "classification",
      "feature_scaling",
      "xgboost",
      "supervised_learning",
      "embedded_methods"
    ],
    "summary": "A Decision Tree is a type of [[Supervised Learning]] algorithm used to predict a target variable based on input features. It involves splitting data into..."
  },
  "declarative": {
    "title": "declarative",
    "tags": [
      "data_orchestration",
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "data_observability",
      "data_lineage",
      "sql",
      "data_quality"
    ],
    "inlinks": [
      "dagster",
      "pydantic"
    ],
    "summary": "In a declarative data pipeline, the focus is on what needs to be achieved, not how it should be executed. You define the desired outcome..."
  },
  "deep_learning_frameworks": {
    "title": "Deep Learning Frameworks",
    "tags": "deep_learning drafting",
    "aliases": [],
    "outlinks": [
      "sci-kit_learn"
    ],
    "inlinks": [],
    "summary": "Watch Overview Video TensorFlow Focus: TensorFlow is a comprehensive open-source platform for machine learning. It provides a flexible and comprehensive ecosystem of tools, libraries, and..."
  },
  "deep_learning": {
    "title": "Deep Learning",
    "tags": [
      "deep_learning"
    ],
    "aliases": [
      "DL"
    ],
    "outlinks": [
      "stochastic_gradient_descent",
      "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.",
      "neural_network",
      "pytorch",
      "gradient_descent",
      "backpropagation",
      "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models._",
      "llm",
      "tensorflow",
      "optimisation_techniques",
      "transfer_learning"
    ],
    "inlinks": [
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "optimising_neural_networks",
      "model_parameters",
      "ds_&_ml_portal",
      "convolutional_neural_networks",
      "neural_network",
      "pytorch",
      "neural_network_classification",
      "adam_optimizer"
    ],
    "summary": "[!Summary] Deep learning is a subset of machine learning that uses neural networks to process large-scale data for tasks like image and speech recognition, natural..."
  },
  "deep_q-learning": {
    "title": "Deep Q-Learning",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "reinforcement_learning",
      "q-learning",
      "pasted_image_20250220133838.png",
      "neural_network"
    ],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": "Deep [[Q-Learning]] is a type of [[reinforcement learning]] algorithm that combines Q-Learning with [[Neural network]]. Necessary when Q-Table grows too large. Updates the weights in..."
  },
  "deepseek": {
    "title": "DeepSeek",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "security",
      "distillation",
      "edge_machine_learning_models",
      "chain_of_thought",
      "llm",
      "jevon_paradox"
    ],
    "inlinks": [],
    "summary": "[[LLM]] example open source optimising for preformance vs efficency. Deepseek leading in efficency o3 mini [[Chain of thought]] can see it - ui choice [[Distillation]]..."
  },
  "deleting_rows_or_filling_them_with_the_mean_is_not_always_best": {
    "title": "Deleting rows or filling them with the mean is not always best",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "demand_forecasting": {
    "title": "Demand forecasting",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "use_of_rnns_in_energy_sector",
      "energy"
    ],
    "summary": "Overview: Demand response programs encourage consumers to adjust their energy usage during peak periods in response to time-based rates or other incentives. RL can optimize..."
  },
  "dendrograms": {
    "title": "Dendrograms",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240405173403.png",
      "dendrograms"
    ],
    "inlinks": [
      "clustering",
      "clustermap",
      "heatmaps_dendrograms.py",
      "dendrograms"
    ],
    "summary": "Dendrograms show close vectors is the data where taken as a vector. Can tell which ==features are the most similar== with [[Dendrograms]] ![[Pasted image 20240405173403.png]]"
  },
  "dependency_manager": {
    "title": "dependency manager",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "virtual_environments",
      "toml",
      "poetry",
      "requirements.txt"
    ],
    "inlinks": [
      "poetry"
    ],
    "summary": "[[Virtual environments]] [[requirements.txt]] [[TOML]] [[Poetry]]"
  },
  "determining_threshold_values": {
    "title": "Determining Threshold Values",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "precision-recall_curve",
      "data_quality",
      "evaluation_metrics",
      "binary_classification",
      "cost-sensitive_analysis",
      "roc_(receiver_operating_characteristic)",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "In [[Binary Classification]] problems, a threshold value is used to convert predicted probabilities into discrete class labels. The choice of threshold significantly impacts the model's..."
  },
  "devops": {
    "title": "DevOps",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "dataops",
      "ci-cd",
      "software_development_portal"
    ],
    "inlinks": [
      "software_development_portal",
      "machine_learning_operations",
      "software_development_life_cycle"
    ],
    "summary": "DevOps refers to practices for collaboration and automation between [[Software Development Portal]] (Dev) and IT operations (Ops) teams, aiming for faster, more reliable software delivery...."
  },
  "difference_between_databricks_vs._snowflake": {
    "title": "Difference between Databricks vs. Snowflake",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "difference_between_snowflake_to_hadoop": {
    "title": "Difference between snowflake to hadoop",
    "tags": [
      "software_architecture",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "snowflake",
      "hadoop",
      "data_management"
    ],
    "inlinks": [],
    "summary": "Snowflake and Hadoop are both [[Data Management]] systems, but they serve different purposes and have distinct architectures and functionalities. In summary, Snowflake and Hadoop are..."
  },
  "differentation": {
    "title": "Differentation",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Forward Mode Automatic Differentiation uses dual numbers implemented in tensor flow see also Reverse Mode Automatic Differentiation Fast,Flexible,Exact"
  },
  "digital_transformation": {
    "title": "Digital Transformation",
    "tags": [
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "data_audit",
      "change_management",
      "digital_transformation"
    ],
    "inlinks": [
      "digital_transformation"
    ],
    "summary": "==\"Digital transformation starts with data centralisation\"== To digitally transform your department, you'll need to approach the process in a structured and strategic way that addresses..."
  },
  "digital_twin": {
    "title": "Digital twin",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "data_management",
      "energy"
    ],
    "inlinks": [],
    "summary": "[!Summary] A digital twin is a virtual representation of a physical object, system, or process that mirrors its real-world counterpart in real-time. This digital model..."
  },
  "dimension_table": {
    "title": "Dimension Table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "star_schema",
      "facts",
      "dimension_table",
      "fact_table"
    ],
    "inlinks": [
      "components_of_the_database",
      "slowly_changing_dimension",
      "dimension_table",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "A dimension table is a key component of a [[star schema]] or snowflake schema in a data warehouse. It provides descriptive attributes (or dimensions) related..."
  },
  "dimensional_modelling": {
    "title": "Dimensional Modelling",
    "tags": [
      "data_modeling"
    ],
    "aliases": null,
    "outlinks": [
      "facts",
      "performance_dimensions",
      "queries",
      "star_schema",
      "grain",
      "dimension_table",
      "dimensional_modelling",
      "data_warehouse",
      "fact_table"
    ],
    "inlinks": [
      "dimensional_modelling",
      "granularity"
    ],
    "summary": "Dimensional modeling is a design technique used in [[Data Warehouse]]used to structure data for efficient ==retrieval== and analysis. It is particularly well-suited for organizing data..."
  },
  "dimensionality_reduction": {
    "title": "Dimensionality Reduction",
    "tags": [
      "ml_process",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "preprocessing",
      "principal_component_analysis",
      "data_visualisation",
      "linear_discriminant_analysis",
      "explain_the_curse_of_dimensionality",
      "t-sne"
    ],
    "inlinks": [
      "feature_selection",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "preprocessing",
      "unsupervised_learning",
      "vector_embedding",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "addressing_multicollinearity",
      "interview_notepad",
      "explain_the_curse_of_dimensionality",
      "factor_analysis",
      "feature_engineering",
      "t-sne",
      "feature_extraction",
      "learning_styles",
      "manifold_learning",
      "data_reduction"
    ],
    "summary": "Dimensionality reduction is a step in the [[Preprocessing]] phase of machine learning that helps simplify models, enhance interpretability, and improve computational efficiency. Its a technique..."
  },
  "dimensions": {
    "title": "Dimensions",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "facts",
      "olap_(online_analytical_processing)"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "Dimensions are the categorical buckets that can be used to segment, filter, or group\u2014such as sales amount region, city, product, color, and distribution channel. Traditionally..."
  },
  "directed_acyclic_graph_(dag)": {
    "title": "Directed Acyclic Graph (DAG)",
    "tags": [
      "math",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "apache_airflow",
      "prefect",
      "dagster"
    ],
    "inlinks": [
      "apache_airflow",
      "pyspark",
      "mathematics"
    ],
    "summary": "DAG stands for Directed Acyclic Graph. A DAG is a graph where information must travel along with a finite set of nodes connected by vertices...."
  },
  "directory_structure": {
    "title": "Directory Structure",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "To make a file tree \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third..."
  },
  "distillation": {
    "title": "Distillation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "small_language_models",
      "pasted_image_20250130074219.png"
    ],
    "inlinks": [
      "llm",
      "deepseek",
      "small_language_models"
    ],
    "summary": "training smaller models with larger. [[Small Language Models]] ![[Pasted image 20250130074219.png]]"
  },
  "distributed_computing": {
    "title": "Distributed Computing",
    "tags": [
      "data_management",
      "data_processing",
      "cloud_computing"
    ],
    "aliases": null,
    "outlinks": [
      "kubernetes",
      "scalability",
      "apache_spark",
      "amazon_s3",
      "data_streaming",
      "edge_computing",
      "hadoop",
      "latency"
    ],
    "inlinks": [
      "batch_processing",
      "publish_and_subscribe",
      "map_reduce"
    ],
    "summary": "Distributed Computing is essential for managing massive data volumes by distributing tasks across multiple servers or machines. This enables scalability and efficient data processing. [[Hadoop]]..."
  },
  "distributions": {
    "title": "Distributions",
    "tags": [
      "statistics",
      "drafting"
    ],
    "aliases": [
      "Distribution"
    ],
    "outlinks": [
      "violin_plot",
      "binomial",
      "poisson",
      "distribution_analysis.py",
      "bernoulli",
      "pasted_image_20250308191945.png",
      "gaussian_distribution",
      "hypothesis_testing",
      "uniform",
      "boxplot",
      "feature_distribution.py",
      "ml_tools"
    ],
    "inlinks": [
      "eda",
      "feature_selection",
      "standard_deviation",
      "precision-recall_curve",
      "gaussian_mixture_models",
      "variance",
      "t-test",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "covariance_structures",
      "data_selection_in_ml",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "statistics",
      "train-dev-test_sets",
      "data_analyst",
      "fitting_weights_and_biases_of_a_neural_network",
      "gaussian_distribution",
      "handling_different_distributions",
      "kmeans_vs_gmm",
      "ds_&_ml_portal",
      "gini_impurity_vs_cross_entropy",
      "boxplot"
    ],
    "summary": "In [[ML_Tools]] see: - [[Distribution_Analysis.py]] Discrete Distributions These distributions have probabilities concentrated on specific values. [[Uniform]] Distribution: All outcomes are equally likely. Example: Drawing a..."
  },
  "distribution_analysis.py": {
    "title": "Distribution_Analysis.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Distribution_Analysis.py The goodness-of-fit results represent the p-values from the Kolmogorov-Smirnov (KS) test, which assesses how well the data fits each distribution. Here's how to interpret..."
  },
  "docker_image": {
    "title": "Docker Image",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "docker"
    ],
    "summary": "A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries,..."
  },
  "docker": {
    "title": "Docker",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "docker_image"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "Utilizes Docker images [[Docker Image]] to set up containers for consistent development and testing environments. Containers can include necessary dependencies like Python and pip. Tutorial:..."
  },
  "documentation_&_meetings": {
    "title": "Documentation & Meetings",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "one_pager_template",
      "pull_request_template",
      "technical_design_doc_template",
      "feedback_template",
      "meeting_notes_template",
      "pdoc",
      "retrospective_template",
      "data_request_template",
      "postmortem_template",
      "mermaid",
      "experiment_plan_template",
      "1-on-1_template"
    ],
    "inlinks": [
      "pyright",
      "code_diagrams",
      "ipynb",
      "software_development_portal",
      "dbt",
      "fishbone_diagram",
      "data_principles",
      "data_engineer",
      "data_warehouse",
      "data_analyst"
    ],
    "summary": "Tools [[pdoc]] \u2013 Auto-generate Python API documentation [[Mermaid]] \u2013 Create diagrams and flowcharts from text in a Markdown-like syntax Templates Project & Technical Meetings [[Technical..."
  },
  "dropout": {
    "title": "Dropout",
    "tags": [
      "deep_learning",
      "ml_optimisation"
    ],
    "aliases": null,
    "outlinks": [
      "regularisation",
      "overfitting",
      "neural_network"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network"
    ],
    "summary": "Dropout is a [[Regularisation]] technique used in [[Neural network]] training to prevent [[overfitting]]. It works by randomly dropping units (neurons) during training, which helps the..."
  },
  "ds_&_ml_portal": {
    "title": "DS & ML Portal",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "model_ensemble",
      "recurrent_neural_networks",
      "data_quality",
      "attention_mechanism",
      "rag",
      "transformer",
      "principal_component_analysis",
      "bert",
      "precision",
      "recall",
      "distributions",
      "k-nearest_neighbours",
      "supervised_learning",
      "clustering",
      "linear_regression",
      "xgboost",
      "model_evaluation",
      "optimisation_techniques",
      "deep_learning",
      "outliers",
      "evaluation_metrics",
      "unsupervised_learning",
      "learning_rate",
      "sklearn",
      "classification",
      "lstm",
      "multicollinearity",
      "statistics",
      "cost_function",
      "hyperparameter_tuning",
      "regression",
      "hyperparameter",
      "optimisation_function",
      "overfitting",
      "anomaly_detection",
      "support_vector_machines",
      "batch_processing",
      "machine_learning_algorithms",
      "decision_tree",
      "random_forests",
      "gradient_descent",
      "vanishing_and_exploding_gradients_problem",
      "model_selection",
      "cross_entropy",
      "imbalanced_datasets",
      "gradient_boosting",
      "regularisation",
      "model_optimisation",
      "logistic_regression",
      "accuracy",
      "loss_function",
      "apache_spark",
      "correlation",
      "binary_classification",
      "neural_network",
      "interpretability",
      "dimensionality_reduction",
      "reinforcement_learning",
      "feature_engineering",
      "data_analysis",
      "ml_tools"
    ],
    "inlinks": [
      "orthogonalization",
      "machine_learning_operations"
    ],
    "summary": "Machine Learning Fundamentals [[ML_Tools]] [[Supervised Learning]] [[Unsupervised Learning]] [[Reinforcement learning]] [[Deep Learning]] Model Training and Optimisation [[Learning rate]] [[Overfitting]] [[Regularisation]] [[Hyperparameter]] [[Hyperparameter Tuning]] [[Model Optimisation]]..."
  },
  "duckdb_in_python": {
    "title": "DuckDB in python",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "duckdb"
    ],
    "summary": "To use DuckDB in Python, you can follow these steps to install the DuckDB library and perform basic operations such as creating a database, running..."
  },
  "duckdb_vs_sqlite": {
    "title": "DuckDB vs SQLite",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "columnar_storage",
      "python",
      "sqlite",
      "duckdb"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "Choosing between [[DuckDB]] and [[SQLite]] for data processing in [[Python]] depends on your specific use case and requirements. While SQLite is an excellent choice for..."
  },
  "duckdb": {
    "title": "DuckDB",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_transformation",
      "querying",
      "duckdb_vs_sqlite",
      "parquet",
      "duckdb_in_python",
      "columnar_storage",
      "data_analysis"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "vectorized_engine"
    ],
    "summary": "DuckDB is an open-source analytical database management system designed for efficient data processing and analysis. It is optimized for running complex queries on large datasets..."
  },
  "dummy_variable_trap": {
    "title": "Dummy variable trap",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "one-hot_encoding",
      "multicollinearity"
    ],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Key Takeaways: The dummy variable trap occurs due to [[multicollinearity]], where ==one dummy variable can be perfectly predicted from others.== Dropping one dummy variable avoids..."
  },
  "eda": {
    "title": "EDA",
    "tags": [
      "data_exploration",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "standard_deviation",
      "data_quality",
      "correlation",
      "standardised/outliers",
      "data_visualisation",
      "distributions",
      "data_transformation",
      "eda_pandas.py",
      "data_analysis",
      "ml_tools"
    ],
    "inlinks": [
      "preprocessing",
      "factor_analysis",
      "scientific_method",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Exploratory [[Data Analysis]] (EDA) is an approach to analyzing datasets to summarize their main characteristics, often utilizing visual methods. EDA helps users to: Understand the..."
  },
  "eda_pandas.py": {
    "title": "EDA_Pandas.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "eda"
    ],
    "summary": ""
  },
  "edge_machine_learning_models": {
    "title": "Edge Machine Learning Models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pytorch",
      "onnx"
    ],
    "inlinks": [
      "small_language_models",
      "deepseek"
    ],
    "summary": "Edge ML refers to deploying machine learning models directly on edge devices, such as IoT sensors, smartphones, or embedded systems, instead of relying on cloud-based..."
  },
  "education_and_training": {
    "title": "Education and Training",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Adaptive Learning Systems Overview: Adaptive learning systems use technology to tailor educational experiences to individual student needs. RL is instrumental in personalizing these systems. Applications:..."
  },
  "elastic_net": {
    "title": "Elastic Net",
    "tags": [
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "ridge",
      "lasso"
    ],
    "inlinks": [
      "embedded_methods"
    ],
    "summary": "This method combines both L1 ([[Lasso]]) and L2 ([[Ridge]]) regularization by adding both absolute and squared penalties to the loss function. It strikes a balance..."
  },
  "elt": {
    "title": "ELT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "elt",
      "bigquery",
      "dbt"
    ],
    "inlinks": [
      "elt",
      "etl",
      "data_transformation_in_data_engineering",
      "dbt"
    ],
    "summary": "ELT (Extract, Load, Transform) is a data integration approach that involves three main steps: Extract (E): Data is extracted from a source system. Load (L):..."
  },
  "embedded_methods": {
    "title": "Embedded Methods",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "gradient_boosting",
      "regularisation",
      "ridge",
      "filter_method",
      "lasso",
      "loss_function",
      "interpretability",
      "decision_tree",
      "random_forests",
      "elastic_net",
      "wrapper_methods"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Embedded methods for [[Feature Selection]] ==integrate feature selection directly into the model training process.== Embedded methods provide a convenient and efficient approach to feature selection..."
  },
  "emergent_behavior": {
    "title": "emergent behavior",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling"
    ],
    "summary": ""
  },
  "encoding_categorical_variables": {
    "title": "Encoding Categorical Variables",
    "tags": [
      "code_snippet",
      "regressor",
      "data_cleaning"
    ],
    "aliases": null,
    "outlinks": [
      "feature_engineering",
      "regression"
    ],
    "inlinks": [
      "naive_bayes",
      "data_transformation_in_machine_learning"
    ],
    "summary": "Overview Categorical variables need to be converted into numerical representations to be used in models, particularly in [[Regression]] analysis. This process is essential for transforming..."
  },
  "energy_abm": {
    "title": "Energy ABM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agent-based_modelling"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy ABM Complex Systems Understanding: Energy systems involve numerous stakeholders (producers, consumers, regulators) with diverse interests and behaviors. [[Agent-Based Modelling|ABM]] helps capture this complexity, providing..."
  },
  "energy_storage": {
    "title": "Energy Storage",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy Storage Battery farms exist. Stored energy can be traded. Stored energy can be stored using distributed system such as EV cars."
  },
  "energy": {
    "title": "Energy",
    "tags": [
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "demand_forecasting",
      "energy_abm",
      "agent-based_modelling",
      "how_to_model_to_improve_demand_forecasting",
      "differential_equations",
      "network_design",
      "stochastic_modeling",
      "neural_network",
      "smart_grids",
      "energy_storage",
      "regression"
    ],
    "inlinks": [
      "digital_twin",
      "industries_of_interest",
      "interview_notepad"
    ],
    "summary": "Areas of interest: - [[Smart Grids]] - [[Energy Storage]] - [[Demand forecasting]] - [[Network Design]] - [[Energy ABM]] Questions: - [[How to model to improve..."
  },
  "environment_variables": {
    "title": "Environment Variables",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Solution 1: Set Environment Variables Permanently (Recommended) This ensures that environment variables persist across sessions. On Windows (Permanent) Open Control Panel \u2192 System \u2192 Advanced..."
  },
  "epoch": {
    "title": "Epoch",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An epoch in machine learning is a single pass through the entire training dataset. The number of epochs, denoted as $N$, determines how many times..."
  },
  "er_diagrams": {
    "title": "ER Diagrams",
    "tags": [
      "database",
      "database_design",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "why_use_er_diagrams",
      "mermaid"
    ],
    "inlinks": [
      "data_modelling",
      "why_use_er_diagrams",
      "implementing_database_schema",
      "types_of_database_schema",
      "conceptual_model",
      "relating_tables_together"
    ],
    "summary": "ER Diagrams are a visual representation of the database structure. Related: - [[Why use ER diagrams]] - [[Mermaid]] Example Entities are tables in the database...."
  },
  "estimator": {
    "title": "Estimator",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "maximum_likelihood_estimation",
      "statistics",
      "statistical_tests"
    ],
    "summary": "Given a sample an estimator is a formula that approximates a population parameter i.e feature"
  },
  "etl_pipeline_example": {
    "title": "ETL Pipeline Example",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "\"domains\",\"country\",\"web_pages\",\"name\""
    ],
    "inlinks": [
      "etl"
    ],
    "summary": "Link [Github](https://github.com/syalanuj/youtube/blob/main/de_fundamentals_python/etl.py 1. Extract using a API Get data via api or download. 2. Transform Put into a pandas df. 3. Load Save df as..."
  },
  "etl_vs_elt": {
    "title": "ETL vs ELT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl"
    ],
    "inlinks": [
      "elt",
      "etl",
      "data_transformation_in_data_engineering"
    ],
    "summary": "ETL (Extract, Transform, and Load) and ELT (Extract, Load, and Transform) are two paradigms for moving data from one system to another. ==ELT is most..."
  },
  "etl": {
    "title": "ETL",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "etl_vs_elt",
      "dagster",
      "apache_airflow",
      "etl",
      "elt",
      "etl_pipeline_example",
      "data_transformation"
    ],
    "inlinks": [
      "etl_vs_elt",
      "fabric",
      "etl",
      "reverse_etl",
      "slowly_changing_dimension",
      "data_transformation_in_data_engineering",
      "data_warehouse"
    ],
    "summary": "ETL (Extract, Transform, Load) is a data integration process that involves moving data from one system to another. It consists of three main stages: Extract:..."
  },
  "etlt": {
    "title": "EtLT",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "[!important] EtLT refers to Extract, \u201ctweak\u201d, Load, Transform, and can be thought of an extension to the ELT approach to data integration. When compared to..."
  },
  "evaluating_language_models": {
    "title": "Evaluating Language Models",
    "tags": [
      "evaluation",
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "bradley-terry_model",
      "elo_rating_system",
      "prompting",
      "llm",
      "lmsys"
    ],
    "inlinks": [],
    "summary": "The [[LMSYS]] Chatbot Arena is a platform where various large language models ([[LLM]]s), including versions of GPT and other prominent models like LLaMA or Claude,..."
  },
  "evaluation_metrics": {
    "title": "Evaluation Metrics",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "ml_tools",
      "evaluation_metrics.py",
      "pasted_image_20241217073706.png",
      "accuracy",
      "pasted_image_20241222091831.png",
      "f1_score",
      "precision",
      "recall",
      "why_type_1_and_type_2_matter",
      "precision_or_recall",
      "confusion_matrix",
      "specificity"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "test_loss_when_evaluating_models",
      "forecasting_autoarima.py",
      "determining_threshold_values",
      "model_selection",
      "recall",
      "neural_network_classification",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "metric",
      "train-dev-test_sets",
      "imbalanced_datasets",
      "model_evaluation"
    ],
    "summary": "Description [[Confusion Matrix]] [[Accuracy]] [[Precision]] [[Recall]] [[Precision or Recall]] [[F1 Score]] [[Recall]] [[Specificity]] ![[Pasted image 20241222091831.png]] Resources: Link to good website describing these In [[ML_Tools]]..."
  },
  "event_driven_events": {
    "title": "Event Driven Events",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event_driven_microservices",
      "monolith_architecture",
      "data_lake",
      "api_driven_microservices",
      "business_observability"
    ],
    "inlinks": [
      "aws_lambda",
      "event_driven",
      "event_driven_microservices"
    ],
    "summary": "Events can be stored in a [[Data Lake]] and analysed to find patterns/predictions. [[Event Driven Microservices]] allow for [[Business observability]] [[Monolith Architecture]] [[Event Driven Microservices]]..."
  },
  "event_driven_microservices": {
    "title": "Event Driven Microservices",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "microservices",
      "software_architecture",
      "event_driven_events"
    ],
    "inlinks": [
      "event_driven",
      "event_driven_events"
    ],
    "summary": "Event-driven microservices refer to a [[software architecture]] pattern where [[microservices]] communicate and coordinate their actions through the production, detection, consumption, and reaction to [[Event Driven..."
  },
  "event_driven": {
    "title": "Event Driven",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "event_driven_microservices",
      "event-driven_architecture",
      "scalability",
      "event_driven_events"
    ],
    "inlinks": [],
    "summary": "Event-driven refers to a ==programming paradigm== or architectural style where the flow of the program is determined by events\u2014changes in state or conditions that trigger..."
  },
  "event-driven_architecture": {
    "title": "Event-Driven Architecture",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "publish_and_subscribe",
      "event_driven",
      "alternatives_to_batch_processing"
    ],
    "summary": ""
  },
  "everything": {
    "title": "Everything",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Can we search with descriptions ? Tips use \\ to match in paths i.e \\playground can copy file new window crl+ n Use | to..."
  },
  "excel_&_sheets": {
    "title": "Excel & Sheets",
    "tags": [
      "software",
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "standardised/gsheets",
      "data_validation"
    ],
    "inlinks": [],
    "summary": "Links Google sheets example folder see [[standardised/GSheets|GSheets]] Excel Example folder: Desktop/Example_Examples Tools common to Excel and Sheets Vlookup Table | Product ID | Product Name..."
  },
  "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.": {
    "title": "Explain different gradient descent algorithms, their advantages, and limitations.",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "deep_learning"
    ],
    "summary": ""
  },
  "explain_the_curse_of_dimensionality": {
    "title": "Explain the curse of dimensionality",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "regularisation",
      "ngrams",
      "multidimensional_scaling",
      "dimensionality_reduction",
      "language_models",
      "manifold_learning",
      "nlp"
    ],
    "inlinks": [
      "dimensionality_reduction"
    ],
    "summary": "The curse of dimensionality refers to the various phenomena that arise when working with data in high-dimensional spaces. Increased Data ==Sparsity==: As the number of..."
  },
  "exploration_vs._exploitation": {
    "title": "Exploration vs. Exploitation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "One of the major challenges in [[Reinforcement learning]] is balancing exploration (trying new actions) and exploitation (choosing the best-known actions). The ==epsilon-greedy strategy== is commonly..."
  },
  "exploration": {
    "title": "Exploration",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "policy"
    ],
    "summary": ""
  },
  "f1_score": {
    "title": "F1 Score",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "precision",
      "recall",
      "harmonic_mean"
    ],
    "inlinks": [
      "confusion_matrix",
      "classification_report",
      "model_observability",
      "evaluation_metrics"
    ],
    "summary": "Definition The F1 Score is the harmonic mean of precision and recall. It provides a balanced view of both metrics and is particularly useful when:..."
  },
  "fabric": {
    "title": "Fabric",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "t-sql",
      "powerbi",
      "etl",
      "synapse",
      "microsoft",
      "data_lake",
      "pyspark",
      "scala",
      "relational_database",
      "data_factory",
      "data_warehouse"
    ],
    "inlinks": [],
    "summary": "Fabric is a unified analytics platform that operates in the cloud, eliminating the need for local installations. It provides an integrated environment for data analysis,..."
  },
  "fact_table": {
    "title": "Fact Table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "facts",
      "database_schema",
      "fact_table",
      "dimension_table",
      "granularity",
      "data_analysis",
      "data_warehouse"
    ],
    "inlinks": [
      "facts",
      "components_of_the_database",
      "dimension_table",
      "granularity",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "A fact table is a central component of a star [[Database Schema|schema]] or snowflake schema in a [[data warehouse]], it stores [[Facts]]. Essential for [[data..."
  },
  "factor_analysis": {
    "title": "Factor Analysis",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "factor_analysis.py",
      "eda",
      "ml_tools"
    ],
    "inlinks": [],
    "summary": "Factor Analysis (FA) is a statistical method used for: - [[dimensionality reduction]], - [[EDA]] - or latent variable detection It identifies underlying relationships between observed..."
  },
  "factor_analysis.py": {
    "title": "Factor_Analysis.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "factor_analysis"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Factor_Analysis.py 1. Factor Loadings Table This table shows how strongly each feature (e.g., sepal length (cm)) is correlated with the two extracted factors (Factor 1..."
  },
  "facts": {
    "title": "Facts",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fact_table"
    ],
    "inlinks": [
      "dimensions",
      "dimension_table",
      "granularity",
      "dimensional_modelling",
      "fact_table"
    ],
    "summary": "Facts are quantitative data points that are typically stored in the [[Fact Table]]. They represent measurable events or metrics, such as sales revenue or quantities..."
  },
  "fastapi": {
    "title": "FastAPI",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fastapi_example.py",
      "ml_tools",
      "pydantic"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "model_deployment",
      "api",
      "pydantic"
    ],
    "summary": "FastAPI is a modern web framework for building APIs with Python. It is designed to be fast and easy to use, leveraging Python's type hints..."
  },
  "fastapi_example.py": {
    "title": "FastAPI_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fastapi"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/FastAPI_Example.py Explanation of New Features Path and Query Parameter Metadata: Added descriptions and constraints for better validation and autogenerated documentation. Nested Models: Demonstrated hierarchical data..."
  },
  "feature_engineering": {
    "title": "Feature Engineering",
    "tags": [
      "#ml_process",
      "#ml_optimisation",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "c1_w2_lab07_featureenglecture.png"
    ],
    "inlinks": [
      "feature_selection_and_creation",
      "ds_&_ml_portal",
      "model_optimisation",
      "preprocessing",
      "class_separability",
      "automated_feature_creation",
      "encoding_categorical_variables",
      "regression"
    ],
    "summary": "Its the term given to the iterative process of building good features for a better model. Its the process that makes relevant features (using formulas..."
  },
  "feature_evaluation": {
    "title": "Feature Evaluation",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Note Garbage in garbae out. It is the features that What is involved: Want to assess the usefulness of chosen features Measuring feature importance: Quantifying..."
  },
  "feature_extraction": {
    "title": "Feature Extraction",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "attention_mechanism",
      "interpretability",
      "dimensionality_reduction",
      "llm",
      "activation_atlases"
    ],
    "inlinks": [
      "convolutional_neural_networks"
    ],
    "summary": "Summary: In machine learning, Feature extraction is the process of transforming raw data into a set of useful features that can be effectively used by..."
  },
  "feature_importance": {
    "title": "Feature Importance",
    "tags": [
      "ml_process",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "gini_impurity",
      "feature_selection",
      "shapley_additive_explanations",
      "interpretability",
      "random_forests",
      "model_building",
      "xgboost",
      "local_interpretable_model-agnostic_explanations"
    ],
    "inlinks": [
      "shapley_additive_explanations",
      "feature_selection_vs_feature_importance",
      "model_evaluation"
    ],
    "summary": "Feature importance refers to ==techniques that assign scores to input features== (predictors) in a machine learning model to ==indicate their relative impact on the model's..."
  },
  "feature_scaling": {
    "title": "Feature Scaling",
    "tags": [
      "data_cleaning",
      "data_processing"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "naive_bayes",
      "principal_component_analysis",
      "decision_tree",
      "gradient_descent",
      "standardisation",
      "linear_discriminant_analysis",
      "normalisation",
      "pasted_image_20241224083928.png"
    ],
    "inlinks": [
      "clustering",
      "preprocessing",
      "why_and_when_is_feature_scaling_necessary"
    ],
    "summary": "Used in preparing data for machine learning models. Feature Scaling is a [[preprocessing]] step in machine learning that involves adjusting the range and distribution of..."
  },
  "feature_selection_and_creation": {
    "title": "Feature selection and creation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "feature_engineering"
    ],
    "inlinks": [],
    "summary": "[[Feature Selection]] [[Feature Engineering]] After the data is ready. Which features have the best value, which play the biggest role. Combining features to simplify the..."
  },
  "feature_selection_vs_feature_importance": {
    "title": "Feature Selection vs Feature Importance",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "feature_selection",
      "interpretability",
      "feature_importance"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Summary [[Feature Selection]] is about choosing which features to include in the model ==before training==, aiming to improve model performance and efficiency. [[Feature Importance]] is..."
  },
  "feature_selection": {
    "title": "Feature Selection",
    "tags": [
      "ml_process",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "correlation",
      "variance",
      "filter_methods",
      "clustering",
      "heatmap",
      "principal_component_analysis",
      "svd",
      "dimensionality_reduction",
      "anova",
      "distributions",
      "feature_selection_vs_feature_importance",
      "wrapper_methods",
      "embedded_methods",
      "model_evaluation"
    ],
    "inlinks": [
      "feature_selection_and_creation",
      "regularisation",
      "ds_&_ml_portal",
      "ridge",
      "data_selection_in_ml",
      "lasso",
      "pca_principal_components",
      "correlation",
      "preprocessing",
      "filter_methods",
      "explain_the_curse_of_dimensionality",
      "feature_selection_vs_feature_importance",
      "p_values",
      "feature_importance",
      "linear_regression",
      "wrapper_methods",
      "logistic_regression_statsmodel_summary_table",
      "embedded_methods"
    ],
    "summary": "Purpose: The primary goal of feature selection is to identify and retain the most relevant features for model training while ==removing irrelevant or redundant ones==...."
  },
  "feature_distribution.py": {
    "title": "Feature_Distribution.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": ""
  },
  "feed_forward_neural_network": {
    "title": "Feed Forward Neural Network",
    "tags": [
      "deep_learning",
      "classifier"
    ],
    "aliases": [
      "FFNN"
    ],
    "outlinks": [
      "ridge",
      "overfitting",
      "recurrent_neural_networks",
      "loss_function",
      "neural_network",
      "supervised_learning",
      "forward_propagation"
    ],
    "inlinks": [
      "transformer",
      "types_of_neural_networks"
    ],
    "summary": "A Feedforward Neural Network (FFNN) is the simplest type of [[Neural network]]. In this model, connections between neurons do not form a cycle, allowing data..."
  },
  "feedback_template": {
    "title": "Feedback Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Praise: I really appreciate your work on this add here FYI: It's really not a big deal, but I'm letting you know just in case...."
  },
  "filter_method": {
    "title": "Filter method",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "wrapper_methods",
      "embedded_methods"
    ],
    "summary": ""
  },
  "filter_methods": {
    "title": "filter methods",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "categorical",
      "correlation",
      "anova"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "For [[Feature Selection]] Pearson [[Correlation]] Coefficient: Measures the linear correlation between two continuous variables. Features with low correlation with the target variable are considered less..."
  },
  "firebase": {
    "title": "Firebase",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "aws"
    ],
    "inlinks": [],
    "summary": "Googles version of [[AWS]] Setup basics Project idea: Set up a basic emailer app."
  },
  "fishbone_diagram": {
    "title": "Fishbone diagram",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "documentation_&_meetings",
      "pasted_image_20250312162034.png"
    ],
    "inlinks": [],
    "summary": "Fishbone diagram [[Documentation & Meetings]] Root cause analysis: [[Documentation & Meetings]] - 5 Y's - Fishbone diagram: start at issue at head - ![[Pasted image..."
  },
  "fitting_weights_and_biases_of_a_neural_network": {
    "title": "Fitting weights and biases of a neural network",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "regularisation",
      "model_parameters",
      "stochastic_gradient_descent",
      "ridge",
      "loss_function",
      "binary_classification",
      "learning_rate",
      "gradient_descent",
      "backpropagation",
      "distributions",
      "dropout",
      "forward_propagation",
      "optimisation_techniques",
      "ml_tools"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "For a neural network model, fitting weights and biases involves optimizing these [[Model Parameters]] so the model learns to map input features ($X$) to target..."
  },
  "flask": {
    "title": "Flask",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "flask",
      "pasted_image_20240922202938.png"
    ],
    "inlinks": [
      "model_deployment",
      "flask",
      "jinja_template"
    ],
    "summary": "software web app framework for writing web pages uses decorators ![[Pasted image 20240922202938.png]] [[Flask]] Flask app example https://www.youtube.com/watch?v=wBCEDCiQh3Q&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl You can run a flask app in..."
  },
  "folder_tree_diagram": {
    "title": "Folder Tree Diagram",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Links Simple method https://www.digitalcitizen.life/how-export-directory-tree-folder-windows/ More general https://superuser.com/questions/272699/how-do-i-draw-a-tree-file-structure Treeviz Graphviz tree /a /f >output.doc"
  },
  "forecasting_autoarima.py": {
    "title": "Forecasting_AutoArima.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "evaluation_metrics",
      "time_series_forecasting",
      "datasets",
      "ml_tools"
    ],
    "inlinks": [
      "forecasting_exponential_smoothing.py",
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_AutoArima.py ARIMA (AutoRegressive Integrated Moving Average) is a popular [[Time Series Forecasting]]method that models the autocorrelations within the data. It is particularly useful for datasets..."
  },
  "forecasting_baseline.py": {
    "title": "Forecasting_Baseline.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Baseline.py Baseline methods are essential for establishing a performance benchmark. They provide insights into the data's underlying patterns and help in assessing the effectiveness of..."
  },
  "forecasting_exponential_smoothing.py": {
    "title": "Forecasting_Exponential_Smoothing.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "forecasting_autoarima.py",
      "time_series_forecasting",
      "datasets"
    ],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Exponential_Smoothing.py Exponential smoothing models are a set of [[Time Series Forecasting]] techniques that apply weighted averages of past observations, with the weights decaying exponentially over..."
  },
  "foreign_key": {
    "title": "Foreign Key",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together",
      "turning_a_flat_file_into_a_database"
    ],
    "summary": "A foreign key is a field in one table that uniquely identifies a row in another table, linking to the primary key of that table...."
  },
  "forward_propagation": {
    "title": "Forward Propagation",
    "tags": [
      "deep_learning",
      "statistics"
    ],
    "aliases": [
      "feedforward pass",
      "forward pass"
    ],
    "outlinks": [
      "activation_function",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation"
    ],
    "inlinks": [
      "feed_forward_neural_network",
      "fitting_weights_and_biases_of_a_neural_network"
    ],
    "summary": "[!Summary] Forward propagation is the process by which input data moves through a neural network, layer by layer, to produce an output. During this process,..."
  },
  "functional_programming": {
    "title": "functional programming",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pyright"
    ],
    "summary": "Functional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a..."
  },
  "fuzzywuzzy": {
    "title": "Fuzzywuzzy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing"
    ],
    "inlinks": [],
    "summary": "Tool used for correcting spelling with pandas. [[Data Cleansing]]"
  },
  "gaussian_distribution": {
    "title": "Gaussian Distribution",
    "tags": null,
    "aliases": [
      "normally distributed data"
    ],
    "outlinks": [
      "distributions"
    ],
    "inlinks": [
      "distributions",
      "standardisation"
    ],
    "summary": "Common assumption for a [[Distributions]]."
  },
  "gaussian_mixture_models": {
    "title": "Gaussian Mixture Models",
    "tags": [
      "clustering"
    ],
    "aliases": [
      "GMM"
    ],
    "outlinks": [
      "gaussian_mixture_model_implementation.py",
      "kmeans_vs_gmm",
      "covariance_structures",
      "anomaly_detection",
      "pasted_image_20250126135722.png",
      "distributions",
      "k-means",
      "clustering",
      "covariance",
      "ml_tools"
    ],
    "inlinks": [
      "clustering",
      "covariance",
      "covariance_structures",
      "machine_learning_algorithms"
    ],
    "summary": "Gaussian Mixture Models (GMMs) represent data as a mixture of multiple Gaussian [[distributions]], with each cluster corresponding to a different Gaussian component. GMMs are more..."
  },
  "gaussian_model": {
    "title": "Gaussian Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241230202826.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "Gaussian Model (Univariate) Formula: $p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)$ Steps: Estimate $\\mu$ and $\\sigma^2$ from the data. Compute the..."
  },
  "gaussian_mixture_model_implementation.py": {
    "title": "Gaussian_Mixture_Model_Implementation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/Gaussian_Mixture_Model_Implementation.py Follow-Up Questions How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency? What are the implications of choosing different..."
  },
  "general_linear_regression": {
    "title": "General Linear Regression",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "t-test",
      "linear_regression",
      "anova"
    ],
    "inlinks": [],
    "summary": "[[Linear Regression]] [[t-test]] - to compare means between two populations. [[ANOVA]] - tests"
  },
  "generative_adversarial_networks": {
    "title": "Generative Adversarial Networks",
    "tags": null,
    "aliases": [
      "GAN"
    ],
    "outlinks": [],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Composed of two neural networks, a generator, and a discriminator, that compete against each other. GANs are used for tasks like generating realistic images or..."
  },
  "generative_ai_from_theory_to_practice": {
    "title": "Generative AI From Theory to Practice",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "markov_chain",
      "call_summarisation",
      "pasted_image_20240524130607.png",
      "ngrams",
      "rag",
      "gan",
      "pasted_image_20240524131603.png",
      "pasted_image_20240524131311.png",
      "llm",
      "software_development_life_cycle",
      "tokenisation",
      "nlp"
    ],
    "inlinks": [],
    "summary": "Objective: How do LLMs work and operate. Enabling [[LLM]]'s at scale: Explore recent AI and Generative AI language models Steps Math on words: Turn words..."
  },
  "generative_ai": {
    "title": "Generative AI",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "inference_versus_prediction",
      "accessing_gen_ai_generated_content",
      "how_to_reduce_the_need_for_gen_ai_responses",
      "typical_output_formats_in_neural_networks",
      "guardrails"
    ],
    "summary": ""
  },
  "get_data": {
    "title": "Get data",
    "tags": [
      "data_collection"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "What is involved: df = pd.read_csv('Categorical.csv') Gather relevant data from appropriate sources, addressing any quality or privacy concerns. ```python Get textbook data using for example:..."
  },
  "gini_impurity_vs_cross_entropy": {
    "title": "Gini Impurity vs Cross Entropy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gini_impurity",
      "distributions",
      "cross_entropy"
    ],
    "inlinks": [
      "decision_tree"
    ],
    "summary": "When working with decision trees, both [[Gini Impurity]] and [[Cross Entropy]] are metrics used to evaluate the quality of a split. They help determine how..."
  },
  "gini_impurity": {
    "title": "Gini Impurity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "regression_metrics",
      "classification",
      "decision_tree"
    ],
    "inlinks": [
      "gini_impurity_vs_cross_entropy",
      "feature_importance",
      "decision_tree"
    ],
    "summary": "Gini impurity is a metric used in decision trees to measure the degree or probability of misclassification in a dataset. It is associated with the..."
  },
  "gis": {
    "title": "GIS",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)",
      "shapefile",
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)"
    ],
    "inlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)",
      "shapefile"
    ],
    "summary": "Geographic information system. File formats: The Web Map Tile Service (WMTS) and Web Feature Server (WFS) are both specifications used in the field of Geographic..."
  },
  "git": {
    "title": "Git",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_to_do_git_commit_messages_properly"
    ],
    "inlinks": [
      "debugging",
      "powershell_vs_bash",
      "how_to_do_git_commit_messages_properly"
    ],
    "summary": "tags: - software Do git bash here. git status git add . (adds all) git status git commit -m \"\" git push Notes https://www.youtube.com/watch?v=xnR0dlOqNVE Git..."
  },
  "gitlab-ci.yml": {
    "title": "gitlab-ci.yml",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The purpose of a gitlab-ci.yml file is to define and configure the GitLab CI/CD pipeline for automating tasks such as building, testing, and deploying your..."
  },
  "gitlab": {
    "title": "Gitlab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ci-cd"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "GitLab CI CD Tutorial for Beginners Crash Course Provides managed runners to execute [[CI-CD]] pipelines. Integrates with version control systems to automate the CI/CD process."
  },
  "google_cloud_platform": {
    "title": "Google Cloud Platform",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "kubernetes",
      "standardised/firebase",
      "mysql",
      "sql",
      "nosql",
      "bigquery"
    ],
    "inlinks": [],
    "summary": "Google Cloud Platform is a suite of cloud computing services offered by Google. It provides a range of services including computing, storage, and application development..."
  },
  "google_my_maps_data_extraction": {
    "title": "Google My Maps Data Extraction",
    "tags": [],
    "aliases": null,
    "outlinks": [
      "**google_apps_script**"
    ],
    "inlinks": [],
    "summary": "Summary: This guide covers the key workflows and tools for managing and processing location data in Google Sheets and Google My Maps. Suppose we have..."
  },
  "gradient_boosting_regressor": {
    "title": "Gradient Boosting Regressor",
    "tags": [
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "boosting",
      "decision_tree",
      "model_ensemble"
    ],
    "inlinks": [],
    "summary": "https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor [[Boosting]] The GradientBoostingRegressor from the sklearn.ensemble module is a model used for regression tasks. It builds an [[Model Ensemble]] of [[Decision Tree]] in a..."
  },
  "gradient_boosting": {
    "title": "Gradient Boosting",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "heterogeneous_features",
      "overfitting",
      "model_ensemble",
      "weak_learners",
      "loss_function",
      "catboost",
      "learning_rate",
      "machine_learning_algorithms",
      "decision_tree",
      "gradient_descent",
      "boosting",
      "model_building",
      "xgboost",
      "lightgbm"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "lightgbm_vs_xgboost_vs_catboost",
      "catboost",
      "use_of_rnns_in_energy_sector",
      "boosting",
      "xgboost",
      "embedded_methods"
    ],
    "summary": "Gradient Boosting is a technique used for building predictive models [[Model Building]], particularly in tasks like regression and classification. It combines the concepts of [[Boosting]]..."
  },
  "gradient_descent": {
    "title": "Gradient Descent",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "GD"
    ],
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "stochastic_gradient_descent",
      "loss_function",
      "mini-batch_gradient_descent",
      "learning_rate",
      "obsidian_epiqlato5w.png",
      "obsidian_m4mzgsax7d.png",
      "gradient_descent",
      "gradient_descent#",
      "pasted_image_20241224082847.png",
      "obsidian_fegflf5rkq.png",
      "batch_gradient_descent",
      "cost_function",
      "optimisation_techniques"
    ],
    "inlinks": [
      "z-normalisation",
      "stochastic_gradient_descent",
      "optimising_a_logistic_regression_model",
      "backpropagation",
      "linear_regression",
      "xgboost",
      "optimisation_techniques",
      "deep_learning",
      "momentum",
      "learning_rate",
      "standardisation",
      "lightgbm",
      "cost_function",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "pytorch",
      "gradient_descent",
      "feature_scaling",
      "gradient_boosting",
      "logistic_regression_in_sklearn_&_gradient_descent",
      "ds_&_ml_portal",
      "adam_optimizer",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Gradient descent is an [[Optimisation function]] used to minimize errors in a model by adjusting its parameters iteratively. It works by moving in the direction..."
  },
  "gradio": {
    "title": "Gradio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "overview"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "Gradio is an open-source platform that simplifies the process of ==creating user interfaces== for machine learning models. It allows users to quickly build interactive demos..."
  },
  "grain": {
    "title": "Grain",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "granularity"
    ],
    "inlinks": [
      "dimensional_modelling"
    ],
    "summary": "Grain - Definition: The level of detail or [[granularity]] of the data stored in the fact table. - Importance: Defining the grain is crucial as..."
  },
  "grammar_method": {
    "title": "Grammar method",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "can understand the Grammar as a method for acceptable sentences."
  },
  "granularity": {
    "title": "granularity",
    "tags": [
      "database",
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "semantic_layer",
      "facts",
      "dimensions",
      "olap",
      "fact_table",
      "dimensional_modelling",
      "business_intelligence"
    ],
    "inlinks": [
      "transaction",
      "choosing_the_number_of_clusters",
      "grain",
      "rollup",
      "fact_table"
    ],
    "summary": "Definition of Grain in [[Dimensional Modelling]] - The grain of a [[Fact Table]] defines what a single row in the table represents. It is the..."
  },
  "graph_analysis_plugin": {
    "title": "Graph Analysis Plugin",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_archive_graph_analysis"
    ],
    "summary": ""
  },
  "graph_neural_network": {
    "title": "Graph Neural Network",
    "tags": [],
    "aliases": [
      "GNN"
    ],
    "outlinks": [
      "recommender_systems"
    ],
    "inlinks": [
      "graphrag"
    ],
    "summary": "Resources: - How Graph Neural Networks Are Transforming Industries Use cases: - [[Recommender systems]] i.e. Uber, Pinterest (PinSage) - Traffic Prediction - Deepmind in google..."
  },
  "graphrag": {
    "title": "GraphRAG",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "graph database"
    ],
    "outlinks": [
      "how_to_search_within_a_graph",
      "rag",
      "graph_neural_network",
      "interpretability",
      "knowledge_graph",
      "neo4j",
      "text2cypher",
      "wikipedia_api.py",
      "graphrag",
      "named_entity_recognition",
      "ml_tools"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "text2cypher",
      "agentic_solutions",
      "relationships_in_memory",
      "graphrag"
    ],
    "summary": "[[GraphRAG]] is a [[RAG]] framework that utilizes [[Knowledge Graph]]s to enhance information retrieval and processing. A significant aspect of this framework is the use of..."
  },
  "grep": {
    "title": "Grep",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "grep.png"
    ],
    "inlinks": [],
    "summary": "![[grep.png]]"
  },
  "gridseachcv": {
    "title": "GridSeachCv",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gridseachcv",
      "pasted_image_20240128194244.png",
      "hyperparameter"
    ],
    "inlinks": [
      "gridseachcv",
      "model_selection",
      "hyperparameter_tuning",
      "decision_tree"
    ],
    "summary": "Used [[GridSeachCv]] to search through the [[Hyperparameter]] space ```python rf_regressor = RandomForestRegressor(random_state=42) grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error') grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ Model Training..."
  },
  "groupby_vs_crosstab": {
    "title": "Groupby vs Crosstab",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "groupby",
      "de_tools",
      "crosstab"
    ],
    "inlinks": [
      "groupby"
    ],
    "summary": "In pandas, [[Groupby]] and [[Crosstab]] serve related but distinct purposes for data ==aggregation== and summarization. groupby is more flexible for aggregation and transformations, whereas crosstab..."
  },
  "groupby": {
    "title": "Groupby",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20250323081619.png",
      "de_tools",
      "groupby_vs_crosstab"
    ],
    "inlinks": [
      "aggregation",
      "melt",
      "handling_missing_data",
      "pd.grouper",
      "pandas_stack",
      "multi-level_index",
      "groupby_vs_crosstab"
    ],
    "summary": "Groupby is a versatile method in pandas used to group data based on one or more columns, and then perform aggregate functions on the grouped..."
  },
  "grouped_plots": {
    "title": "Grouped plots",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_visualisation",
      "pasted_image_20250402212849.png"
    ],
    "inlinks": [
      "melt"
    ],
    "summary": "Related: - [[Data Visualisation]] - pairplots ```python import seaborn as sns import matplotlib.pyplot as plt Load example dataset tips = sns.load_dataset(\"tips\") Facet Grid Example g..."
  },
  "gru": {
    "title": "GRU",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "lstm",
      "recurrent_neural_networks"
    ],
    "summary": ""
  },
  "gsheets": {
    "title": "GSheets",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "query_gsheets"
    ],
    "inlinks": [],
    "summary": "Useful functions: - [[QUERY GSheets]] - ARRAYFORMULA - Indirect Accessing google sheets from a script: https://www.youtube.com/watch?v=zCEJurLGFRk"
  },
  "guardrails": {
    "title": "Guardrails",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "prompting",
      "data_observability",
      "guardrails",
      "generative_ai"
    ],
    "inlinks": [
      "guardrails"
    ],
    "summary": "Controlling a [[Generative AI]] in business through the use of [[Guardrails]] ensures that the AI remains aligned with specific business goals and avoids unintended or..."
  },
  "hadoop": {
    "title": "Hadoop",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "data_lake",
      "batch_processing",
      "big_data",
      "apache_spark"
    ],
    "inlinks": [
      "map_reduce",
      "difference_between_snowflake_to_hadoop",
      "distributed_computing",
      "parquet",
      "databricks",
      "big_data"
    ],
    "summary": "Hadoop provides the backbone for distributed storage and computation. It uses HDFS (Hadoop Distributed File System) to split large datasets across clusters of servers, while..."
  },
  "handling_different_distributions": {
    "title": "Handling Different Distributions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "accuracy",
      "preprocessing",
      "distributions",
      "model_selection",
      "model_robustness",
      "datasets",
      "ml_tools"
    ],
    "inlinks": [
      "data_cleansing",
      "train-dev-test_sets"
    ],
    "summary": "Handling different [[distributions]] is needed for developing robust, fair, and accurate machine learning models that can adapt to a wide range of data environments. Importance..."
  },
  "handling_missing_data": {
    "title": "Handling Missing Data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "handling_missing_data_basic.ipynb",
      "groupby",
      "'var1',_'var2'",
      "data_transformation",
      "handling_missing_data.ipynb"
    ],
    "inlinks": [
      "data_cleansing",
      "pandas",
      "outliers",
      "xgboost"
    ],
    "summary": "Missing data can provide insights into the data collection process. It's important to determine whether the missing data is randomly distributed or specific to certain..."
  },
  "handling_missing_data.ipynb": {
    "title": "Handling_Missing_Data.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data.ipynb"
  },
  "handling_missing_data_basic.ipynb": {
    "title": "Handling_Missing_Data_Basic.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data_Basic.ipynb"
  },
  "handwritten_digit_classification": {
    "title": "Handwritten Digit Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241006124356.png"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "![[Pasted image 20241006124356.png|800]]"
  },
  "hash": {
    "title": "Hash",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_integrity"
    ],
    "inlinks": [
      "cryptography",
      "data_integrity"
    ],
    "summary": "A hash is a fixed-size string of characters that is generated from input data of any size using a hash function. Hashes are used to..."
  },
  "heatmap": {
    "title": "Heatmap",
    "tags": [
      "code_snippet",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "correlation",
      "ml_tools",
      "heatmaps_dendrograms.py",
      "multicollinearity"
    ],
    "inlinks": [
      "feature_selection",
      "heatmaps_dendrograms.py",
      "pca_principal_components",
      "correlation",
      "multicollinearity"
    ],
    "summary": "Description A heatmap is a two-dimensional graphical representation of data where individual values are represented by colors. It is particularly useful for visualizing numerical data..."
  },
  "heatmaps_dendrograms.py": {
    "title": "Heatmaps_Dendrograms.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "heatmap",
      "dendrograms"
    ],
    "inlinks": [
      "heatmap"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations\\Preprocess\\Correlation\\Heatmaps_Dendrograms.py See: - [[Heatmap]] - [[Dendrograms]]"
  },
  "heterogeneous_features": {
    "title": "heterogeneous features",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing"
    ],
    "inlinks": [
      "gradient_boosting"
    ],
    "summary": "Description In machine learning, heterogeneous features refer to a situation where the input data contains a variety of different types of features. Let's break it..."
  },
  "hierarchical_clustering": {
    "title": "Hierarchical Clustering",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "clustering"
    ],
    "summary": "Hierarchical clustering builds a treelike structure of clusters, with similar clusters merged together at higher levels. Hierarchical clustering builds a tree-like structure of clusters, with..."
  },
  "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data": {
    "title": "High cross validation accuracy is not directly proportional to performance on unseen test data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "evaluation_metrics",
      "preprocessing",
      "cross_validation",
      "distributions",
      "data_leakage",
      "hyperparameter_tuning",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "Reasons a Model with High [[Cross Validation]] Accuracy May Perform Poorly on Unseen Test Data [[Data Leakage]]: - Information from test folds leaks into training,..."
  },
  "how_businesses_use_gen_ai": {
    "title": "How businesses use Gen AI",
    "tags": [
      "business",
      "GenAI",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "transactional_journeys",
      "model_performance"
    ],
    "inlinks": [],
    "summary": "Businesses leverage generative AI to transform various operations, using models like OpenAI, Gemini (Google Cloud), Anthropic, and Meta models. These models provide services through cloud..."
  },
  "how_do_we_evaluate_of_llm_outputs": {
    "title": "How do we evaluate of LLM Outputs",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm_evaluation_metrics",
      "llm",
      "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts",
      "prompt_engineering"
    ],
    "inlinks": [
      "llm"
    ],
    "summary": "Methods for assessing the quality and relevance of LLM-generated outputs, critical for improving model performance. The evaluation of [[LLM]] outputs involves various methodologies to assess..."
  },
  "how_do_you_do_the_data_selection": {
    "title": "how do you do the data selection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_do_you_do_the_data_selection",
      "data_selection"
    ],
    "inlinks": [
      "how_do_you_do_the_data_selection"
    ],
    "summary": "When you sample a dataset, [[how do you do the data selection]]? [[Data Selection]] A: By randomly sampling, by time period (use a feature).."
  },
  "how_is_reinforcement_learning_being_combined_with_deep_learning": {
    "title": "How is reinforcement learning being combined with deep learning",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "reinforcement_learning",
      "deep_learning"
    ],
    "inlinks": [],
    "summary": "The sources touch upon reinforcement learning as an area beyond the scope of their discussion. However, the combination of [[Reinforcement learning]] with [[Deep Learning]] has..."
  },
  "how_is_schema_evolution_done_in_practice_with_sql": {
    "title": "How is schema evolution done in practice with SQL",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "how_llms_store_facts": {
    "title": "How LLMs store facts",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "multilayer_perceptrons",
      "johnson\u2013lindenstrauss_lemma",
      "interpretability",
      "vector_embedding",
      "anthropic",
      "llm"
    ],
    "inlinks": [],
    "summary": "How might LLMs store facts Not solved How do [[Multilayer Perceptrons]] store facts? Different directions encode information in [[Vector Embedding]] space. MLP's are blocks of..."
  },
  "how_to_do_git_commit_messages_properly": {
    "title": "How to do git commit messages properly",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "git"
    ],
    "inlinks": [
      "git"
    ],
    "summary": "Structure of a goof [[Git]] Commit Message Subject Line Keep it short (50 characters or less). Use the imperative mood (e.g., \"Fix bug\" instead of..."
  },
  "how_to_model_to_improve_demand_forecasting": {
    "title": "How to model to improve demand forecasting",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": ""
  },
  "how_to_normalise_a_merged_table": {
    "title": "How to normalise a merged table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "normalised_schema"
    ],
    "inlinks": [
      "normalised_schema",
      "powerquery",
      "normalisation"
    ],
    "summary": "See: [[Normalised Schema]] Splitting out tables. Resource: Database Normalization for Beginners | How to Normalize Data w/ Power Query (full tutorial!)"
  },
  "how_to_reduce_the_need_for_gen_ai_responses": {
    "title": "How to reduce the need for Gen AI responses",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "transactional_journeys",
      "caching",
      "generative_ai"
    ],
    "inlinks": [],
    "summary": "Reducing the need for frequent [[Generative AI]] (Gen AI) responses can be done by leveraging techniques such as [[caching]] and setting up predefined [[transactional journeys]]...."
  },
  "how_to_search_within_a_graph": {
    "title": "How to search within a graph",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how_to_search_within_a_graph",
      "knowledge_graph",
      "text2cypher",
      "pasted_image_20241004074458.png",
      "graphrag",
      "standardised/vector_embedding"
    ],
    "inlinks": [
      "graphrag",
      "vector_embedding",
      "how_to_search_within_a_graph"
    ],
    "summary": "[[How to search within a graph]] Vector Search with Graph Context [[standardised/Vector Embedding]] plays a crucial role in enhancing search capabilities: Comparison of Vector-Only vs...."
  },
  "how_to_use_sklearn_pipeline": {
    "title": "How to use Sklearn Pipeline",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "hugging_face": {
    "title": "Hugging Face",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "transformer"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "Hugging Face is open-source platform known for its contributions to natural language processing (NLP) and machine learning. It provides a comprehensive library called [[Transformer]], which..."
  },
  "hyperparameter_tuning": {
    "title": "Hyperparameter Tuning",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "gridseachcv",
      "regularisation",
      "standardised/optuna",
      "hyperparameter_tuning_gridsearchcv.py",
      "random_forests",
      "cross_validation",
      "interpretable_decision_trees",
      "ml_tools"
    ],
    "inlinks": [
      "pycaret",
      "ds_&_ml_portal",
      "optuna",
      "xgboost",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "test_loss_when_evaluating_models",
      "hyperparameter"
    ],
    "summary": "Objective: - Tune the model\u2019s hyperparameters to improve performance. For example, in regularized linear regression, the main hyperparameter to tune is the regularization strength (e.g.,..."
  },
  "hyperparameter": {
    "title": "Hyperparameter",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "neural_network",
      "k-nearest_neighbours",
      "model_parameters_vs_hyperparameters",
      "hyperparameter_tuning"
    ],
    "inlinks": [
      "weak_learners",
      "cross_validation",
      "k-means",
      "isolated_forest",
      "momentum",
      "learning_rate",
      "gridseachcv",
      "fitting_weights_and_biases_of_a_neural_network",
      "pycaret",
      "optuna",
      "catboost",
      "decision_tree",
      "random_forests",
      "ds_&_ml_portal",
      "model_optimisation",
      "neural_network",
      "adam_optimizer",
      "model_parameters_vs_hyperparameters",
      "kaggle_abalone_regression_example"
    ],
    "summary": "Hyperparameters are parameters set before training that control the learning process, such as: - the number of nodes in a [[Neural network]] - or k..."
  },
  "hypothesis_testing": {
    "title": "Hypothesis testing",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "statistics",
      "p_values",
      "inference"
    ],
    "inlinks": [
      "testing",
      "statistical_assumptions",
      "t-test",
      "distributions",
      "statistical_tests",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "statistics",
      "data_analysis",
      "data_analyst"
    ],
    "summary": "Used to draw inferences about population parameters based on sample data. The process involves the formulation of two competing hypotheses: the null hypothesis ($H_0$) and..."
  },
  "imbalanced_datasets": {
    "title": "Imbalanced Datasets",
    "tags": [
      "data_quality",
      "data_cleaning",
      "data_exploration"
    ],
    "aliases": [
      "Class Imbalance"
    ],
    "outlinks": [
      "imbalanced_datasets_smote.py",
      "anomaly_detection",
      "evaluation_metrics",
      "imbalanced_datasets",
      "bagging",
      "random_forests",
      "classification",
      "cost-sensitive_analysis",
      "transfer_learning",
      "smote_(synthetic_minority_over-sampling_technique)",
      "model_evaluation",
      "regression",
      "ml_tools"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "data_selection_in_ml",
      "data_collection",
      "accuracy",
      "precision-recall_curve",
      "determining_threshold_values",
      "neural_network_classification",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "class_separability",
      "imbalanced_datasets"
    ],
    "summary": "Handling imbalanced datasets to ensure robustness of models is a common challenge in machine learning, particularly in classification tasks where one class significantly outnumbers the..."
  },
  "imbalanced_datasets_smote.py": {
    "title": "Imbalanced_Datasets_SMOTE.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "support_vector_machines",
      "accuracy",
      "random_forests",
      "recall",
      "smote_(synthetic_minority_over-sampling_technique)",
      "imbalanced_datasets"
    ],
    "inlinks": [
      "imbalanced_datasets"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Imbalanced_Datasets_SMOTE.py Demonstrating the Value of Resampling in Imbalanced Classification This example highlights the effectiveness of resampling techniques, such as [[SMOTE (Synthetic Minority Over-sampling Technique)|SMOTE]], in..."
  },
  "immutable_vs_mutable": {
    "title": "Immutable vs mutable",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "python"
    ],
    "inlinks": [
      "python"
    ],
    "summary": "[[Python]] list being mutable Side effect ```python def get_largest_numbers(numbers, n): numbers. sort() return numbers[-n:] nums [2, 3, 4, 1,34, 123, 321, 1] print(nums) largest =..."
  },
  "impact_of_multicollinearity_on_model_parameters": {
    "title": "Impact of multicollinearity on model parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "See https://youtu.be/StSAJIZuqws?t=655 ```R ) Monte Carlo Simulation: Multicollinearity & Harm results = expand_grid( rho = seq(0, 0.95, 0.05), rep = 1:1000 ) %>% mutate( sim..."
  },
  "imperative": {
    "title": "imperative",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An imperative pipeline tells ==how to proceed== at each step in a procedural manner. In contrast, a declarative data pipeline does not tell the order..."
  },
  "implementing_database_schema": {
    "title": "Implementing Database Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "many-to-many_relationships",
      "er_diagrams"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "To manage and create a database schema in SQLite, you can use the following commands: To view all commands used to create a database, execute:..."
  },
  "in_ner_how_would_you_handle_ambiguous_entities": {
    "title": "In NER how would you handle ambiguous entities",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Handling ambiguous entities in Named Entity Recognition (NER) can be quite challenging. Here are some strategies that can be employed: Contextual Analysis: Utilize the surrounding..."
  },
  "in-memory_format": {
    "title": "in-memory format",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The term \"in-memory format\" refers to the way data is stored and managed directly in a ==computer's RAM== (Random Access Memory) rather than on disk..."
  },
  "incremental_synchronization": {
    "title": "incremental synchronization",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "soft_deletion"
    ],
    "summary": ""
  },
  "industries_of_interest": {
    "title": "Industries of interest",
    "tags": [
      "career"
    ],
    "aliases": [],
    "outlinks": [
      "markov_decision_processes",
      "education_and_training",
      "reinforcement_learning",
      "telecommunications",
      "energy",
      "what_algorithms_or_models_are_used_within_the_telecommunication_sector",
      "what_algorithms_or_models_are_used_within_the_energy_sector"
    ],
    "inlinks": [],
    "summary": "Industries to investigate related to my background & interests: - [[Energy]] - [[Telecommunications]] - [[Education and Training]] Both Reinforcement Learning and Explainable AI offer exciting..."
  },
  "inference_versus_prediction": {
    "title": "inference versus prediction",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "prediction",
      "llm",
      "generative_ai",
      "inference"
    ],
    "inlinks": [
      "inference"
    ],
    "summary": "[[inference]] is similar to prediction, ==but in the context of [[Generative AI]],== it is more specific to the application of a pre-trained model to ==produce..."
  },
  "inference": {
    "title": "inference",
    "tags": null,
    "aliases": [
      "inferencing"
    ],
    "outlinks": [
      "inference_versus_prediction"
    ],
    "inlinks": [
      "inference_versus_prediction",
      "small_language_models",
      "hypothesis_testing"
    ],
    "summary": "Inferencing involves prediction, but the output is more generative and creative in nature. [[inference versus prediction]]"
  },
  "information_theory": {
    "title": "information theory",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "language_model_output_optimisation",
      "mathematics"
    ],
    "summary": "Information theory is a mathematical framework for quantifying the transmission, processing, and storage of information. Information theory has profound implications and applications across various domains,..."
  },
  "input_is_not_properly_sanitized": {
    "title": "Input is Not Properly Sanitized",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Input is Not Properly Sanitized When we say that ==\"input is not properly sanitized,\"== it means that the input data from users or external sources..."
  },
  "interoperable": {
    "title": "interoperable",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multi-level_index"
    ],
    "summary": ""
  },
  "interpretability": {
    "title": "interpretability",
    "tags": [
      "drafting",
      "model_explainability"
    ],
    "aliases": [
      "explainability",
      "interpretable"
    ],
    "outlinks": [],
    "inlinks": [
      "standard_deviation",
      "covariance_vs_correlation",
      "model_ensemble",
      "statistical_assumptions",
      "principal_component_analysis",
      "addressing_multicollinearity",
      "boosting",
      "xgboost",
      "clustering",
      "activation_function",
      "pdp_and_ice",
      "feature_importance",
      "regression_metrics",
      "pca_explained_variance_ratio",
      "text2cypher",
      "classification",
      "feature_selection_vs_feature_importance",
      "accessing_gen_ai_generated_content",
      "machine_learning_algorithms",
      "decision_tree",
      "model_observability",
      "how_llms_store_facts",
      "graphrag",
      "small_language_models",
      "model_interpretability",
      "regularisation",
      "ds_&_ml_portal",
      "feature_extraction",
      "embedded_methods"
    ],
    "summary": "Links Interpretability Importance https://christophm.github.io/interpretable-ml-book/index.html Interpretability Interpretability in machine learning (ML) is about understanding the reasoning behind a model's predictions. It involves making the model's decision-making..."
  },
  "interpreting_logistic_regression_model_parameters": {
    "title": "Interpreting logistic regression model parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "How do this in terms of odds, probabilities ,odds ratio. [[Logistic Regression]]"
  },
  "interquartile_range_(iqr)_detection": {
    "title": "Interquartile Range (IQR) Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "univariate_data"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_IQR.py Context: The IQR method is a robust and widely used statistical technique for identifying outliers, especially in [[univariate data]]. It is based on the..."
  },
  "interview_notepad": {
    "title": "interview notepad",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "normalised_schema",
      "preprocessing",
      "dimensionality_reduction",
      "model_selection",
      "linear_regression",
      "energy"
    ],
    "inlinks": [],
    "summary": "Tell about a recent project of yours;; Collaborating in the image matching kaggle competition. Obviously S2DS project too. What are some areas in this business..."
  },
  "ipynb": {
    "title": "ipynb",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "documentation_&_meetings",
      "nbconvert"
    ],
    "inlinks": [],
    "summary": "Printing without code : [[Documentation & Meetings]]/ [[nbconvert]] https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html jupyter nbconvert stock_analysis.ipynb --no-input --to pdf jupyter nbconvert --to html --no-input --no-prompt phi_analysis.ipynb --clear -output jupyter..."
  },
  "isolated_forest": {
    "title": "Isolated Forest",
    "tags": [
      "anomaly_detection",
      "data_quality"
    ],
    "aliases": [
      "iForest",
      "anomaly isolation"
    ],
    "outlinks": [
      "model_ensemble",
      "anomaly_detection_with_clustering",
      "support_vector_machines",
      "standardised/outliers",
      "random_forests",
      "dbscan",
      "hyperparameter"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "model_ensemble",
      "anomaly_detection_with_clustering",
      "unsupervised_learning",
      "anomaly_detection_in_time_series",
      "model_observability"
    ],
    "summary": "Isolation Forest (iForest) is an [[Model Ensemble]]-based method used for anomaly detection. It operates by isolating data points using a series of random binary splits...."
  },
  "java_vs_javascript": {
    "title": "Java vs JavaScript",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "java",
      "javascript"
    ],
    "inlinks": [],
    "summary": "Difference Between [[Java]] and [[JavaScript]] Although their names are similar, Java and JavaScript are fundamentally different languages designed for different purposes. Below is a comparison..."
  },
  "javascript": {
    "title": "JavaScript",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "java_vs_javascript",
      "quartz",
      "cryptography",
      "react",
      "strongly_vs_weakly_typed_language"
    ],
    "summary": ""
  },
  "jinja_template": {
    "title": "jinja template",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "dbt",
      "quartz",
      "pasted_image_20240922201606.png",
      "flask",
      "pasted_image_20240922202345.png"
    ],
    "inlinks": [
      "quartz"
    ],
    "summary": "Resources LINK Practical jinja2 works with python 3. ![[Pasted image 20240922201606.png]] Renders templates with variable substitutions You can use tags too. ![[Pasted image 20240922202345.png]] Get..."
  },
  "johnson\u2013lindenstrauss_lemma": {
    "title": "Johnson\u2013Lindenstrauss lemma",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm"
    ],
    "inlinks": [
      "how_llms_store_facts",
      "mathematics"
    ],
    "summary": "Johnson\u2013Lindenstrauss lemma math https://youtu.be/9-Jl0dxWQs8?list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&t=1125 THe number of vectors that can be fit into a spaces grows exponentially. Useful for [[LLM]] in storing ideas. Plotting M>N..."
  },
  "joining_datasets": {
    "title": "Joining Datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sql_joins",
      "de_tools"
    ],
    "inlinks": [
      "data_transformation",
      "data_transformation_with_pandas"
    ],
    "summary": "Joining Datasets In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ```python Merge df1 = pd.DataFrame({'key': ['A', 'B'], 'value': [1, 2]}) df2 = pd.DataFrame({'key': ['A', 'B'], 'value': [3, 4]})..."
  },
  "json_to_yaml": {
    "title": "Json to Yaml",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "yaml",
      "json"
    ],
    "inlinks": [],
    "summary": "[[Json]] [[yaml]] JSON { \"json\": [ \"rigid\", \"better for data interchange\" ], \"yaml\": [ \"slim and flexible\", \"better for configuration\" ], \"object\": { \"key\": \"value\",..."
  },
  "json": {
    "title": "Json",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "json_to_yaml",
      "yaml",
      "normalisation_of_data",
      "software_development_portal",
      "structured_data",
      "pydantic",
      "rest_api",
      "semi-structured_data",
      "multi-level_index"
    ],
    "summary": "Stands for javascript object notation records separated by commas keys & strings wrapped by double quotes good choice for data transport JSON data embedded inside..."
  },
  "junction_tables": {
    "title": "Junction Tables",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": ""
  },
  "justfile": {
    "title": "Justfile",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "Justfile is a command runner designed to streamline workflows by allowing users to define simple, reusable commands for common tasks. This approach minimizes the cognitive..."
  },
  "k-means": {
    "title": "K-means",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "ml_tools",
      "unsupervised_learning",
      "k_means.py",
      "pasted_image_20241230200255.png",
      "wcss_and_elbow_method",
      "hyperparameter"
    ],
    "inlinks": [
      "kmeans_vs_gmm",
      "model_parameters",
      "gaussian_mixture_models",
      "unsupervised_learning",
      "machine_learning_algorithms",
      "algorithms",
      "why_and_when_is_feature_scaling_necessary",
      "dbscan",
      "clustering"
    ],
    "summary": "K-means clustering is an [[Unsupervised Learning]] algorithm that partitions data into (k) clusters. Each data point is assigned to the cluster with the nearest centroid...."
  },
  "k-nearest_neighbours": {
    "title": "K-nearest neighbours",
    "tags": [
      "classifier"
    ],
    "aliases": [
      "KNN"
    ],
    "outlinks": [
      "parametric_vs_non-parametric_models",
      "classification",
      "recommender_systems",
      "k-nearest_neighbours",
      "regression"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "unsupervised_learning",
      "standardisation",
      "classification",
      "k-nearest_neighbours",
      "supervised_learning",
      "hyperparameter"
    ],
    "summary": "K-nearest Neighbors is a non-parametric method used for both [[classification]] and [[regression]] tasks. It classifies a sample by a majority vote of its neighbors, assigning..."
  },
  "kaggle_abalone_regression_example": {
    "title": "Kaggle Abalone regression example",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cross_validation",
      "hyperparameter"
    ],
    "inlinks": [],
    "summary": "Task: For each model as we tune the hyperparameters what happens to the (RMSLE) metric (scatter metric against hyperparameter). Using Root Mean Squared Logarithmic Error..."
  },
  "kernelling": {
    "title": "Kernelling",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "support_vector_machines",
      "kernelling"
    ],
    "inlinks": [
      "support_vector_machines",
      "kernelling"
    ],
    "summary": "[[Kernelling]] is a technique where the [[Support Vector Machines|SVM]] uses a kernel function to map the dataset into a higher-dimensional space, making it easier to..."
  },
  "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)": {
    "title": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS) Data Type: [[Web Map Tile Service (WMTS)]]: Serves image tiles (raster data). [[Web..."
  },
  "kmeans_vs_gmm": {
    "title": "Kmeans vs GMM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "distributions",
      "k-means",
      "covariance_structures"
    ],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "Key Differences Between [[k-Means]] and GMM Cluster Shape k-Means: Assumes clusters are spherical and equidistant from their centroids. GMM: Models clusters as Gaussian distributions, allowing..."
  },
  "knowledge_graph_vs_rag_setup": {
    "title": "Knowledge graph vs RAG setup",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "semantic_search",
      "generative",
      "knowledge_graph",
      "rag"
    ],
    "inlinks": [],
    "summary": "Comparison: Knowledge Graph vs. RAG Setup ==Knowledge Graphs are structured representations of entities and their relationships, designed primarily for querying, reasoning, and storing factual information.==..."
  },
  "knowledge_graph": {
    "title": "Knowledge Graph",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "explainability",
      "pasted_image_20240921154214.png"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "accessing_gen_ai_generated_content",
      "knowledge_graphs_with_obsidian",
      "knowledge_graph_vs_rag_setup",
      "graphrag"
    ],
    "summary": "[!Summary] Knowledge graphs (KGs) enable large language models (LLMs) to generate more accurate, trustworthy AI outputs. Neo4j is leader in this space and make use..."
  },
  "knowledge_graphs_with_obsidian": {
    "title": "Knowledge Graphs with Obsidian",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "llm",
      "knowledge_graph",
      "rag"
    ],
    "inlinks": [],
    "summary": "[!Summary] Llama Index is a python package that can be used to create Knowledge graphs (KGs). There exists a method to integrate with Obsidian. This..."
  },
  "knowledge_work": {
    "title": "Knowledge Work",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "scientific_method"
    ],
    "inlinks": [
      "thinking_systems"
    ],
    "summary": "Knowledge work refers to tasks that primarily involve handling or using information and require cognitive skills rather than manual labor. It is characterized by problem-solving,..."
  },
  "kubernetes": {
    "title": "kubernetes",
    "tags": [
      "data_orchestration",
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "google_cloud_platform"
    ],
    "summary": "It\u2019s a platform that allows you to run and orchestrate container workloads. Kubernetes has become the de-facto standard for your cloud-native apps to (auto-) scale-out..."
  },
  "k_means.py": {
    "title": "K_Means.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "k-means"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/KMeans/K_Means.py Key Concepts Used in the Script Data Loading: The script reads data from a CSV file (penguins.csv) and uses a sample dataset with random..."
  },
  "label_encoding": {
    "title": "Label encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": ""
  },
  "labelling_data": {
    "title": "Labelling data",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Possible missing labelling or bias in the data, or under-represented data. Construction of the data set comes from the group collecting it. Examples: - ImageNet"
  },
  "lambda_architecture": {
    "title": "lambda architecture",
    "tags": [
      "data_modeling",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "batch_processing",
      "data_streaming",
      "design_pattern"
    ],
    "inlinks": [],
    "summary": "Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. [[Data Streaming]] This..."
  },
  "langchain": {
    "title": "Langchain",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agentic_solutions",
      "python",
      "llm",
      "pandas_dataframe_agent"
    ],
    "inlinks": [
      "vector_database"
    ],
    "summary": "[[Python]] framework For building apps with [[LLM]] and interaction with them and combining models. Its end to end, through composability Example: [[Pandas Dataframe Agent]] Modules..."
  },
  "language_model_output_optimisation": {
    "title": "Language Model Output Optimisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "attention_mechanism",
      "language_models",
      "information_theory",
      "cross_entropy"
    ],
    "inlinks": [],
    "summary": "What techniques from [[information theory]] can be used to measure and optimize the amount of information conveyed by an language model? In information theory, several..."
  },
  "language_models_large_(llms)_vs_small_(slms)": {
    "title": "Language Models Large (LLMs) vs Small (SLMs)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm",
      "slm"
    ],
    "inlinks": [],
    "summary": "Overview Language models can be categorized into large language models ([[LLM]]) and small language models ([[SLM]]). While LLMs boast extensive general-purpose knowledge and capabilities, SLMs..."
  },
  "language_models": {
    "title": "Language Models",
    "tags": [
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "llm",
      "small_language_models"
    ],
    "inlinks": [
      "small_language_models",
      "vector_embedding",
      "language_model_output_optimisation",
      "explain_the_curse_of_dimensionality",
      "memory",
      "llm"
    ],
    "summary": "A language model is a machine learning model that is designed to understand, generate, and predict human language. It does this by analyzing large amounts..."
  },
  "lasso": {
    "title": "Lasso",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection"
    ],
    "inlinks": [
      "regularisation",
      "elastic_net",
      "embedded_methods",
      "regression"
    ],
    "summary": "L1 Regularization (Lasso Regression): In L1 regularization, a penalty proportional to the absolute value of the coefficients is added to the loss function. The L1..."
  },
  "latency": {
    "title": "Latency",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "performance_dimensions",
      "data_ingestion"
    ],
    "summary": ""
  },
  "lbfgs": {
    "title": "LBFGS",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "logistic_regression",
      "model_parameters",
      "sklearn",
      "cost_function"
    ],
    "inlinks": [],
    "summary": "LBFGS stands for Limited-memory Broyden-Fletcher-Goldfarb-Shanno, which is an [[Optimisation function]]optimization algorithm used to find the minimum of a function. In the context of [[logistic regression]],..."
  },
  "learning_rate": {
    "title": "learning rate",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241216204925.png",
      "loss_function",
      "standardised/optuna",
      "gradient_descent",
      "adam_optimizer",
      "hyperparameter"
    ],
    "inlinks": [
      "gradient_boosting",
      "z-normalisation",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "momentum",
      "adam_optimizer",
      "weak_learners",
      "gradient_descent",
      "linear_regression",
      "adaptive_learning_rates",
      "xgboost",
      "model_parameters_vs_hyperparameters",
      "optimisation_techniques"
    ],
    "summary": "Description The learning rate is a [[Hyperparameter]] in machine learning that ==determines the step size at which a model's parameters are updated during training==. It..."
  },
  "learning_styles": {
    "title": "Learning Styles",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "continuous",
      "unsupervised_learning",
      "dimensionality_reduction",
      "classification",
      "categorical",
      "_pasted_image_20240112101344.png",
      "clustering",
      "regression"
    ],
    "inlinks": [],
    "summary": "What does the data look like [[continuous]] or [[categorical]]? ![[ Pasted image 20240112101344.png|500]] [[Unsupervised Learning]] [[Regression]] [[Classification]] [[Unsupervised Learning]] [[Dimensionality Reduction]] [[Clustering]]"
  },
  "lemmatization": {
    "title": "lemmatization",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "nlp",
      "normalisation_of_text"
    ],
    "summary": "Lemmatization is the process of ==reducing a word to its base or root== form, known as the \"lemma.\" Unlike stemming, which simply cuts off word..."
  },
  "lightgbm_vs_xgboost_vs_catboost": {
    "title": "LightGBM vs XGBoost vs CatBoost",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "gradient_boosting",
      "regularisation",
      "catboost",
      "xgboost",
      "lightgbm"
    ],
    "inlinks": [],
    "summary": "This table summarizes the key differences and strengths of each [[Gradient Boosting]] framework. | Feature/Aspect | [[LightGBM]] (LGBM) | [[XGBoost]] | [[CatBoost]] | | ---------------------------------..."
  },
  "lightgbm": {
    "title": "LightGBM",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "LGBM"
    ],
    "outlinks": [
      "gradient_descent"
    ],
    "inlinks": [
      "lightgbm_vs_xgboost_vs_catboost",
      "gradient_boosting",
      "optuna",
      "time_series_forecasting"
    ],
    "summary": "LightGBM is a gradient boosting framework that is designed for speed and efficiency. It is particularly well-suited for handling large datasets and high-dimensional data. Tree..."
  },
  "linear_discriminant_analysis": {
    "title": "Linear Discriminant Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dimensionality_reduction",
      "feature_scaling"
    ],
    "summary": ""
  },
  "linear_regression": {
    "title": "Linear Regression",
    "tags": [
      "regressor"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "p-values_in_linear_regression_in_sklearn",
      "linearity",
      "loss_function",
      "learning_rate",
      "gradient_descent",
      "ordinary_least_squares",
      "pasted_image_20240124135607.png",
      "adjusted_r_squared",
      "r_squared",
      "pasted_image_20240117145455_1.png",
      "f-statistic",
      "model_evaluation"
    ],
    "inlinks": [
      "logistic_regression",
      "ds_&_ml_portal",
      "model_parameters",
      "outliers",
      "p-values_in_linear_regression_in_sklearn",
      "ridge",
      "machine_learning_algorithms",
      "pytorch",
      "general_linear_regression",
      "interview_notepad",
      "maximum_likelihood_estimation",
      "supervised_learning",
      "regression"
    ],
    "summary": "Description Linear regression assumes [[linearity]] between the input features and the target variable. Assumes that the relationship between the independent variable(s) and the dependent variable..."
  },
  "linked_list": {
    "title": "Linked List",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A linked list is a linear data structure in which elements (called nodes) are linked together using pointers. Unlike arrays, linked lists do not store..."
  },
  "llm_evaluation_metrics": {
    "title": "LLM Evaluation Metrics",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "llm_evaluation_metrics",
      "llm"
    ],
    "inlinks": [
      "llm_evaluation_metrics",
      "how_do_we_evaluate_of_llm_outputs"
    ],
    "summary": "[[LLM Evaluation Metrics]] - BLEU, - ROUGE, - perplexity which quantify the similarity between generated text and reference outputs. [[LLM]]"
  },
  "llm": {
    "title": "LLM",
    "tags": [
      "language_models"
    ],
    "aliases": [
      "LLMs",
      "Large Language Models"
    ],
    "outlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "attention_mechanism",
      "distillation",
      "transformer",
      "chain_of_thought",
      "reinforcement_learning",
      "language_models",
      "relationships_in_memory",
      "memory",
      "mixture_of_experts",
      "standardised/vector_embedding",
      "transfer_learning"
    ],
    "inlinks": [
      "knowledge_graphs_with_obsidian",
      "rag",
      "attention_mechanism",
      "langchain",
      "how_do_we_evaluate_of_llm_outputs",
      "deep_learning",
      "johnson\u2013lindenstrauss_lemma",
      "inference_versus_prediction",
      "agent-based_modelling",
      "language_models_large_(llms)_vs_small_(slms)",
      "scaling_agentic_systems",
      "comparing_llm",
      "generative_ai_from_theory_to_practice",
      "language_models",
      "how_llms_store_facts",
      "small_language_models",
      "llm_evaluation_metrics",
      "evaluating_language_models",
      "vector_database",
      "deepseek",
      "feature_extraction",
      "neural_scaling_laws"
    ],
    "summary": "A Large Language Model (LLM) is a type of language model designed for language understanding and generation. They can perform a variety of tasks, including:..."
  },
  "load_balancing": {
    "title": "Load Balancing",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Load balancing is a technique used to distribute incoming network traffic across multiple servers. This helps ensure both reliability and performance by preventing any single..."
  },
  "local_interpretable_model-agnostic_explanations": {
    "title": "Local Interpretable Model-agnostic Explanations",
    "tags": [],
    "aliases": [
      "LIME"
    ],
    "outlinks": [],
    "inlinks": [
      "feature_importance",
      "model_interpretability"
    ],
    "summary": "LIME explains individual predictions ==by approximating the model locally== with an interpretable model and calculating the feature importance based on the surrogate model. Key Points..."
  },
  "logical_model": {
    "title": "Logical Model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Logical Model - Customer: CustomerID, Name, Email - Order: OrderID, OrderDate, CustomerID - Book: BookID, Title, Author - Order-Book Relationship: OrderID, BookID Logical Model -..."
  },
  "logistic_regression_does_not_predict_probabilities": {
    "title": "Logistic Regression does not predict probabilities",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "what_is_the_difference_between_odds_and_probability"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "In logistic regression, the model predicts the ==odds of an event happening rather than directly predicting probabilities.== The odds are defined as: $$ \\text{Odds} =..."
  },
  "logistic_regression_in_sklearn_&_gradient_descent": {
    "title": "Logistic regression in sklearn & Gradient Descent",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "gradient_descent"
    ],
    "inlinks": [],
    "summary": "Logistic regression in sklearn & Gradient Descent sklearn's Logistic Regression implementation does not use [[Gradient Descent]] by default. Instead, it uses more sophisticated optimization techniques..."
  },
  "logistic_regression_statsmodel_summary_table": {
    "title": "Logistic Regression Statsmodel Summary table",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "mle",
      "sklearn",
      "pasted_image_20240124095916.png"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "Statsmodel has this summary table unlike [[Sklearn]] Explanation of summary The dependent variable is 'duration'. The model used is a Logit regression (logistic in common..."
  },
  "logistic_regression": {
    "title": "Logistic Regression",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "ml_tools",
      "model_parameters",
      "support_vector_machines",
      "interpreting_logistic_regression_model_parameters",
      "binary_classification",
      "regression_logistic_metrics.ipynb",
      "linear_regression",
      "confusion_matrix",
      "logistic_regression_does_not_predict_probabilities",
      "logistic_regression_statsmodel_summary_table",
      "regression",
      "model_evaluation"
    ],
    "inlinks": [
      "ridge",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "model_parameters",
      "interpreting_logistic_regression_model_parameters",
      "statistical_assumptions",
      "optimising_a_logistic_regression_model",
      "machine_learning_algorithms",
      "lbfgs",
      "statistics",
      "roc_(receiver_operating_characteristic)",
      "regression"
    ],
    "summary": "==Logistic regression models the log-odds of the probability as a linear function of the input features.== It models the probability of an input belonging to..."
  },
  "looker_studio": {
    "title": "Looker Studio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/gsheets",
      "google",
      "powerbi",
      "postgresql"
    ],
    "inlinks": [
      "data_visualisation"
    ],
    "summary": "Looker studio is [[Google]] version of [[PowerBI]], but its free. Connectors to data Can connect to data sources i.e: - [[standardised/GSheets|GSheets]] - [[PostgreSQL]] Data Modelling..."
  },
  "loss_function": {
    "title": "Loss function",
    "tags": [
      "deep_learning",
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "loss_versus_cost_function",
      "classification",
      "cross_entropy",
      "mean_squared_error",
      "cost_function",
      "regression",
      "model_evaluation"
    ],
    "inlinks": [
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "optimising_a_logistic_regression_model",
      "linear_regression",
      "xgboost",
      "learning_rate",
      "feed_forward_neural_network",
      "cost_function",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "ridge",
      "pytorch",
      "gradient_descent",
      "typical_output_formats_in_neural_networks",
      "cross_entropy",
      "gradient_boosting",
      "regularisation",
      "ds_&_ml_portal",
      "loss_versus_cost_function",
      "model_parameters_vs_hyperparameters",
      "neural_scaling_laws",
      "embedded_methods",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Loss functions are used in training machine learning models. Also known as a [[cost function]], error function, or objective function. Serves as a metric for..."
  },
  "loss_versus_cost_function": {
    "title": "Loss versus Cost function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "loss_function",
      "model_parameters_vs_hyperparameters",
      "cost_function",
      "model_optimisation"
    ],
    "inlinks": [
      "loss_function",
      "cost_function"
    ],
    "summary": "In machine learning, the terms \"loss function\" and \"cost function\" are often used interchangeably, but they can have slightly different meanings depending on the context:..."
  },
  "lstm": {
    "title": "LSTM",
    "tags": [
      "deep_learning",
      "time_series",
      "code_snippet",
      "drafting"
    ],
    "aliases": [
      "LSTM vs. Transformer",
      "RNN vs. Transformer"
    ],
    "outlinks": [
      "rnn",
      "recurrent_neural_networks",
      "keras",
      "attention_mechanism",
      "transformer",
      "pytorch",
      "bert",
      "vanishing_and_exploding_gradients_problem",
      "gru",
      "lstm",
      "pasted_image_20241015211424.png",
      "nlp"
    ],
    "inlinks": [
      "ai_engineer",
      "ds_&_ml_portal",
      "recurrent_neural_networks",
      "attention_mechanism",
      "anomaly_detection_in_time_series",
      "transformers_vs_rnns",
      "lstm",
      "named_entity_recognition"
    ],
    "summary": "What is LSTM LSTM (Long Short-Term Memory) networks are a specialized type of Recurrent Neural Network (RNN) designed to overcome the [[vanishing and exploding gradients..."
  },
  "machine_learning_algorithms": {
    "title": "Machine Learning Algorithms",
    "tags": [
      "ml_process",
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "data_quality",
      "gaussian_mixture_models",
      "principal_component_analysis",
      "k-means",
      "linear_regression",
      "supervised_learning",
      "clustering",
      "scalability",
      "unsupervised_learning",
      "bias_and_variance",
      "classification",
      "manifold_learning",
      "regression",
      "overfitting",
      "support_vector_machines",
      "naive_bayes",
      "support_vector_regression",
      "decision_tree",
      "random_forests",
      "logistic_regression",
      "interpretability",
      "algorithms",
      "dimensionality_reduction",
      "random_forest_regression"
    ],
    "inlinks": [
      "gradient_boosting",
      "z-normalisation",
      "ds_&_ml_portal",
      "use_of_rnns_in_energy_sector",
      "model_building",
      "supervised_learning",
      "pandas_stack",
      "machine_learning"
    ],
    "summary": "Machine learning [[Algorithms]] are used to automate tasks, extract insights, and make more informed decisions. Choosing the right algorithm for a specific problem involves understanding..."
  },
  "machine_learning_operations": {
    "title": "Machine Learning Operations",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "MLOPs"
    ],
    "outlinks": [
      "devops",
      "model_observability",
      "ds_&_ml_portal"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Machine Learning Operations (MLOps) is a set of practices and tools designed to streamline the entire lifecycle of machine learning models, from development to deployment..."
  },
  "machine_learning": {
    "title": "Machine Learning",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "tensorflow",
      "data_ingestion"
    ],
    "summary": "Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed..."
  },
  "maintainable_code": {
    "title": "Maintainable Code",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "testing",
      "pyright",
      "pydantic"
    ],
    "inlinks": [
      "pyright"
    ],
    "summary": "[[Pydantic]] : runtine analysis [[Pyright]]: static analysis [[Testing]] Want robust and reliable Python applications."
  },
  "makefile": {
    "title": "Makefile",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "A Makefile is a special file used by the make build automation tool to manage the build process of a project. It defines a set..."
  },
  "manifold_learning": {
    "title": "Manifold Learning",
    "tags": [
      "deleted",
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "pasted_image_20240127124620.png"
    ],
    "inlinks": [
      "explain_the_curse_of_dimensionality",
      "machine_learning_algorithms",
      "principal_component_analysis"
    ],
    "summary": "Manifold learning is a powerful approach for high-dimensional data exploration, focusing on uncovering the lower-dimensional manifold that the data resides on. These algorithms aim to..."
  },
  "many-to-many_relationships": {
    "title": "Many-to-Many Relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "many-to-many_relationships"
    ],
    "inlinks": [
      "many-to-many_relationships",
      "relating_tables_together",
      "implementing_database_schema"
    ],
    "summary": "Many-to-Many Relationships Occurs when multiple records in one table are associated with multiple records in another table. Need to use a junction table (also known..."
  },
  "map_reduce": {
    "title": "map reduce",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "real-time_processing",
      "apache_spark",
      "batch_processing",
      "distributed_computing",
      "hadoop"
    ],
    "inlinks": [],
    "summary": "MapReduce is a programming model and processing technique used for processing and generating large data sets with a parallel, distributed algorithm on a cluster. [[Distributed..."
  },
  "markov_chain": {
    "title": "Markov chain",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "Is a stochastic model that describes a sequence of events in which the probability of each event depends only on the state attained in the..."
  },
  "markov_decision_processes": {
    "title": "Markov Decision Processes",
    "tags": [
      "model_algorithm"
    ],
    "aliases": [
      "MDP"
    ],
    "outlinks": [
      "markov_decision_processes"
    ],
    "inlinks": [
      "industries_of_interest",
      "reinforcement_learning",
      "markov_decision_processes"
    ],
    "summary": "Markov Decision Process ([[Markov Decision Processes|MDP]]) is a formal framework for decision-making where outcomes depend solely on the current state (Markov property). \\ architecture [[Markov..."
  },
  "master_data_management": {
    "title": "master data management",
    "tags": [
      "data_storage",
      "data_governance",
      "data_management"
    ],
    "aliases": [
      "mdm"
    ],
    "outlinks": [
      "data_governance",
      "source_of_truth",
      "data_management",
      "data_quality"
    ],
    "inlinks": [
      "data_management",
      "single_source_of_truth"
    ],
    "summary": "Master data management is a method to ==centralize== master data. It's the bridge between the business that maintain the data and know them best and..."
  },
  "master_observability_datadog": {
    "title": "Master Observability Datadog",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "lambdas"
    ],
    "inlinks": [
      "model_observability"
    ],
    "summary": "what happens in prod, pre prod. monitoring web frontend. how is infrastructure working in prod Datadog agents tagging profile how it was working versus other..."
  },
  "mathematical_reasoning_in_transformers": {
    "title": "Mathematical Reasoning in Transformers",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "grok",
      "transformer",
      "gpt-f"
    ],
    "inlinks": [
      "symbolic_computation",
      "reasoning_tokens"
    ],
    "summary": "transformer-based models that address mathematical reasoning either through pretraining, hybrid systems, or fine-tuning on specific mathematical tasks Challenges: General-purpose transformers [[Transformer|Transformer]] are trained primarily on..."
  },
  "mathematics": {
    "title": "Mathematics",
    "tags": [
      "portal",
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "directed_acyclic_graph_(dag)",
      "information_theory",
      "big_o_notation",
      "johnson\u2013lindenstrauss_lemma"
    ],
    "inlinks": [],
    "summary": "[[Johnson\u2013Lindenstrauss lemma]] [[Big O Notation]] [[Directed Acyclic Graph (DAG)]] [[information theory]]"
  },
  "maximum_likelihood_estimation": {
    "title": "Maximum Likelihood Estimation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "parametric_vs_non-parametric_models",
      "estimator",
      "model_parameters",
      "linear_regression"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resource: - https://www.youtube.com/watch?v=YevSE6bRhTo Used to infer [[Model Parameters]] from collected data for example in [[Linear Regression]] ($\\beta_0,\\beta_1$). Definition: Likelihood Why is it a good tool..."
  },
  "mean_absolute_error": {
    "title": "mean absolute error",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "mean_squared_error": {
    "title": "Mean Squared Error",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "loss_function",
      "cross_entropy.py"
    ],
    "summary": "==Measures numerical proximity.=="
  },
  "melt": {
    "title": "Melt",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "de_tools",
      "grouped_plots",
      "data_visualisation",
      "groupby",
      "database_techniques",
      "normalisation",
      "data_transformation",
      "turning_a_flat_file_into_a_database",
      "multi-level_index"
    ],
    "inlinks": [],
    "summary": "In pandas, the melt function is used to ==transform ([[Data Transformation]]) a DataFrame from a wide format to a long format==. This is especially useful..."
  },
  "memory_caching": {
    "title": "Memory Caching",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Memory Caching - Use in-memory caches to store frequently accessed data closer to the user, reducing latency."
  },
  "memory": {
    "title": "Memory",
    "tags": null,
    "aliases": [
      "What is LLM memory",
      "context"
    ],
    "outlinks": [
      "memory",
      "language_models",
      "semantic_relationships"
    ],
    "inlinks": [
      "memory",
      "llm"
    ],
    "summary": "Memory in large [[language models]] (LLMs) involves managing context windows to enhance reasoning capabilities without the high costs associated with traditional training methods. The goal..."
  },
  "merge": {
    "title": "Merge",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_join_vs_merge",
      "data_transformation_with_pandas"
    ],
    "summary": ""
  },
  "mermaid": {
    "title": "Mermaid",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings",
      "er_diagrams",
      "code_diagrams"
    ],
    "summary": "Step 1. Ask Chatgpt to make er diagram with mermaidcode. Step 2. Use https://mermaid.js.org/ Use obsidians built in feature: ```mermaid erDiagram SAMPLING_POINT { string notation..."
  },
  "metadata_handling": {
    "title": "Metadata Handling",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parquet"
    ],
    "summary": ""
  },
  "methods_for_handling_outliers": {
    "title": "Methods for Handling Outliers",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "outliers"
    ],
    "summary": "Trimming Description: Removing data points identified as outliers based on criteria such as being beyond a certain number of standard deviations from the mean or..."
  },
  "metric": {
    "title": "Metric",
    "tags": [
      "business"
    ],
    "aliases": [
      "Measure",
      "KPI"
    ],
    "outlinks": [
      "regression_metrics",
      "evaluation_metrics"
    ],
    "inlinks": [
      "semantic_layer",
      "cosine_similarity"
    ],
    "summary": "Metrics in Machine Learning [[Evaluation Metrics]] [[Regression Metrics]] Metrics in business A metric, also called KPI or (calculated) measure, are terms that serve as the..."
  },
  "microsoft_access": {
    "title": "Microsoft Access",
    "tags": [
      "software",
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "database"
    ],
    "inlinks": [],
    "summary": "Tasks [ ] How to update (or more so insert) into multiple related tables. How to insert existing data into a [[Database]]. SQL triggers? [..."
  },
  "mini-batch_gradient_descent": {
    "title": "Mini-batch gradient descent",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gradient_descent"
    ],
    "summary": ""
  },
  "mixture_of_experts": {
    "title": "Mixture of Experts",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "llm"
    ],
    "summary": "Different parts of the network focusing on parts of the questions Routing, distribution activating"
  },
  "ml_engineer": {
    "title": "ML Engineer",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "ML Engineer - Configures and optimizes production ML models. - Monitors the performance and accuracy of ML models in production environments."
  },
  "mnist": {
    "title": "MNIST",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "datasets"
    ],
    "inlinks": [
      "neural_scaling_laws"
    ],
    "summary": "[[Datasets]]"
  },
  "model_building": {
    "title": "Model Building",
    "tags": "ml_optimisation evaluation",
    "aliases": "Build",
    "outlinks": [
      "model_parameters",
      "preprocessing",
      "machine_learning_algorithms",
      "model_selection",
      "parametric_vs_nonparametric_models",
      "train-dev-test_sets"
    ],
    "inlinks": [
      "gradient_boosting",
      "train-dev-test_sets",
      "feature_importance"
    ],
    "summary": "The Model Building phase follows the [[Preprocessing]] phase, where data is organized and prepared for analysis. This phase focuses on selecting and setting up the..."
  },
  "model_cascading": {
    "title": "Model Cascading",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "small_language_models"
    ],
    "summary": ""
  },
  "model_deployment": {
    "title": "Model Deployment",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": [
      "Deployment"
    ],
    "outlinks": [
      "streamlit.io",
      "scalability",
      "pycaret",
      "preprocessing",
      "api",
      "gradio",
      "fastapi",
      "model_observability",
      "flask",
      "sklearn_pipeline"
    ],
    "inlinks": [
      "testing",
      "pycaret",
      "continuous_delivery_-_deployment"
    ],
    "summary": "Deploying a machine learning model involves moving it from a development environment to a production environment where it can make predictions on new data. Steps..."
  },
  "model_ensemble": {
    "title": "Model Ensemble",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "stacking",
      "isolated_forest",
      "interpretability",
      "comparing_ensembles.py",
      "bagging",
      "boosting",
      "ml_tools"
    ],
    "inlinks": [
      "gradient_boosting",
      "stacking",
      "ds_&_ml_portal",
      "model_optimisation",
      "isolated_forest",
      "weak_learners",
      "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy",
      "regularisation_of_tree_based_models",
      "bagging",
      "random_forests",
      "gradient_boosting_regressor",
      "boosting",
      "classification",
      "xgboost",
      "comparing_ensembles.py"
    ],
    "summary": "Ensemble models in machine learning are techniques that ==combine the predictions of multiple individual models== to improve overall performance. Ensemble methods can achieve better accuracy..."
  },
  "model_evaluation_vs_model_optimisation": {
    "title": "Model Evaluation vs Model Optimisation",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "model_optimisation",
      "model_evaluation"
    ],
    "inlinks": [],
    "summary": "[[Model Evaluation]]focuses on assessing a model's performance, while [[Model Optimisation]]aims to improve that performance through various techniques. Iterative Process: Model evaluation and optimization are often..."
  },
  "model_evaluation": {
    "title": "Model Evaluation",
    "tags": [
      "evaluation",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "regression_metrics",
      "overfitting",
      "evaluation_metrics",
      "cross_validation",
      "feature_importance"
    ],
    "inlinks": [
      "feature_selection",
      "logistic_regression",
      "ds_&_ml_portal",
      "model_optimisation",
      "test_loss_when_evaluating_models",
      "wrapper_methods",
      "loss_function",
      "cross_validation",
      "neural_network_in_practice",
      "model_selection",
      "linear_regression",
      "model_evaluation_vs_model_optimisation",
      "train-dev-test_sets",
      "imbalanced_datasets"
    ],
    "summary": "Assess the model's performance using various metrics to ensure it meets the desired accuracy and reliability. Appropriate evaluation metrics are used based on the problem..."
  },
  "model_interpretability": {
    "title": "Model Interpretability",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "confidence_interval",
      "shapley_additive_explanations",
      "interpretability",
      "local_interpretable_model-agnostic_explanations",
      "p_values"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Model [[interpretability]] tools are crucial in ensuring that machine learning models are transparent, explainable, and understandable to stakeholders, particularly in industries where decisions need to..."
  },
  "model_observability": {
    "title": "Model Observability",
    "tags": [
      "deleted",
      "#model_explainability"
    ],
    "aliases": [
      "Observability"
    ],
    "outlinks": [
      "data_drift",
      "isolated_forest",
      "accuracy",
      "data_observability",
      "interpretability",
      "f1_score",
      "performance_drift",
      "roc_(receiver_operating_characteristic)",
      "precision",
      "recall",
      "validation",
      "model_validation",
      "data_lineage",
      "master_observability_datadog"
    ],
    "inlinks": [
      "model_validation",
      "model_deployment",
      "machine_learning_operations",
      "business_observability"
    ],
    "summary": "Monitor the model's performance over time (in production). Similar to [[Model Validation]]. In the context of machine learning (ML), Observability refers to the ability to..."
  },
  "model_optimisation": {
    "title": "Model Optimisation",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "Optimisation"
    ],
    "outlinks": [
      "hyperparameter",
      "model_parameters",
      "model_ensemble",
      "cross_validation",
      "feature_engineering",
      "model_evaluation"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "loss_versus_cost_function",
      "momentum",
      "cross_validation",
      "model_evaluation_vs_model_optimisation",
      "cost_function"
    ],
    "summary": "Model optimization is a step in the machine learning workflow aimed at enhancing a model's performance by fine-tuning its parameters and hyperparameters. The goal is..."
  },
  "model_parameters_tuning": {
    "title": "Model Parameters Tuning",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "pasted_image_20241231142918.png",
      "cost_function",
      "optimisation_techniques"
    ],
    "inlinks": [
      "regularisation",
      "model_parameters"
    ],
    "summary": "To find optimal [[Model Parameters]]. Finding Optimal Model Parameters Parameter Space Exploration: It's useful to visualize slices of the parameter space by selecting two parameters..."
  },
  "model_parameters_vs_hyperparameters": {
    "title": "Model parameters vs hyperparameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_parameters",
      "loss_function",
      "neural_network",
      "learning_rate",
      "gradient_descent",
      "hyperparameter"
    ],
    "inlinks": [
      "loss_versus_cost_function",
      "hyperparameter"
    ],
    "summary": "Model parameters and hyperparameters serve different roles: [[Model Parameters]] - These are the internal variables of the model that are learned from the training data...."
  },
  "model_parameters": {
    "title": "Model Parameters",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "deep_learning",
      "support_vector_machines",
      "model_parameters_tuning",
      "decision_tree",
      "k-means",
      "linear_regression",
      "clustering",
      "optimisation_techniques"
    ],
    "inlinks": [
      "optimising_a_logistic_regression_model",
      "lbfgs",
      "model_parameters_tuning",
      "model_validation",
      "cost_function",
      "train-dev-test_sets",
      "hyperparameter",
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "forecasting_autoarima.py",
      "gradient_descent",
      "maximum_likelihood_estimation",
      "cost-sensitive_analysis",
      "logistic_regression",
      "model_optimisation",
      "loss_function",
      "neural_network",
      "model_building",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Model parameters are also called weights and biases. These parameters are adjusted during the training process to optimize the model's performance on the given task...."
  },
  "model_preparation": {
    "title": "Model preparation",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "python model = model.fit(X_train, y_train) print(model) y_pred = model.predict(X_test) print(accuracy_score(y_expect, y_pred))"
  },
  "model_selection": {
    "title": "Model Selection",
    "tags": [
      "ml_process",
      "deleted",
      "evaluation"
    ],
    "aliases": [
      "Selection"
    ],
    "outlinks": [
      "gridseachcv",
      "evaluation_metrics",
      "random_search",
      "cross_validation",
      "machine_learning_operations",
      "model_interpretability",
      "model_evaluation"
    ],
    "inlinks": [
      "handling_different_distributions",
      "regularisation",
      "ds_&_ml_portal",
      "pycaret",
      "parsimonious",
      "interview_notepad",
      "model_building",
      "test_loss_when_evaluating_models"
    ],
    "summary": "Model selection is an integral part of building a [[Machine Learning Operations]] to ensure that the best performing model is chosen for a given task,..."
  },
  "model_validation": {
    "title": "Model Validation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_observability",
      "model_parameters",
      "performance_drift"
    ],
    "inlinks": [
      "cross_validation",
      "model_observability"
    ],
    "summary": "Model Validation refers to the process of evaluating a machine learning model's performance on a separate dataset (often called the validation set) to ensure it..."
  },
  "momentum": {
    "title": "Momentum",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "model_optimisation",
      "learning_rate",
      "gradient_descent",
      "cost_function",
      "momentum.py",
      "ml_tools"
    ],
    "inlinks": [
      "adaptive_learning_rates",
      "adam_optimizer"
    ],
    "summary": "Momentum is an [[Model Optimisation|Optimisation]] technique used to accelerate the [[Gradient Descent]] algorithm by incorporating the concept of inertia. It helps in reducing oscillations and..."
  },
  "momentum.py": {
    "title": "Momentum.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "momentum"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Momentum.py"
  },
  "mongodb": {
    "title": "MongoDB",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": ""
  },
  "monolith_architecture": {
    "title": "Monolith Architecture",
    "tags": [
      "software_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "microservices",
      "software_architecture"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "A monolith, in the context of [[software architecture]], refers to a ==single, unified application where all components and functionalities are interconnected and interdependent==. In a..."
  },
  "monte_carlo_simulation": {
    "title": "Monte Carlo Simulation",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resources: - https://www.youtube.com/watch?v=r7cn3WS5x9c Algorithms that use repeated random sampling. Monte Carlo: random How does the randomness in data generation impact the randomness of the paramaeter..."
  },
  "multi-agent_reinforcement_learning": {
    "title": "Multi-Agent Reinforcement Learning",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": ""
  },
  "multi-head_attention": {
    "title": "Multi-head attention",
    "tags": [
      "deleted",
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "attention_mechanism"
    ],
    "inlinks": [
      "attention_mechanism",
      "transformer"
    ],
    "summary": "Summary ==Aggregates different perspectives== This approach allows the model to attend to different parts of the input sequence simultaneously, capturing various aspects of the context..."
  },
  "multi-level_index": {
    "title": "Multi-level index",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "interoperable",
      "groupby",
      "json",
      "pandas_stack"
    ],
    "inlinks": [
      "melt",
      "pd.grouper",
      "data_transformation_with_pandas",
      "pandas_stack",
      "structuring_and_organizing_data"
    ],
    "summary": "Multi-level indexing in pandas\u2014also called hierarchical indexing\u2014enables you to work with higher-dimensional data in a 2D DataFrame. It's particularly useful for working with grouped or..."
  },
  "multicollinearity": {
    "title": "Multicollinearity",
    "tags": [
      "code_snippet",
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "correlation",
      "impact_of_multicollinearity_on_model_parameters",
      "heatmap",
      "addressing_multicollinearity",
      "clustering"
    ],
    "inlinks": [
      "ridge",
      "ds_&_ml_portal",
      "data_selection_in_ml",
      "correlation",
      "dummy_variable_trap",
      "heatmap",
      "addressing_multicollinearity",
      "statistics",
      "regression"
    ],
    "summary": "When two or more regressors are in [[Correlation]] Multicollinearity refers to the ==instability== of a model due to ==highly correlated independent variables.== It occurs when..."
  },
  "multinomial_naive_bayes": {
    "title": "Multinomial Naive bayes",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "naive_bayes"
    ],
    "summary": ""
  },
  "mysql": {
    "title": "MySql",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_engineering_portal",
      "database",
      "database_management_system_(dbms)",
      "google_cloud_platform"
    ],
    "summary": "MySQL has more ==granularity== with types than SQLite. For example, an integer could be TINYINT, SMALLINT, MEDIUMINT, INT or BIGINT based on the size of..."
  },
  "naive_bayes": {
    "title": "Naive Bayes",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "multinomial_naive_bayes",
      "pasted_image_20240118111554.png",
      "naive_bayes",
      "pasted_image_20240116184108.png",
      "encoding_categorical_variables",
      "gaussian_naive_bayes",
      "bag_of_words"
    ],
    "inlinks": [
      "naive_bayes",
      "machine_learning_algorithms",
      "feature_scaling",
      "classification",
      "supervised_learning"
    ],
    "summary": "Can values for X,y be categroical ? [[Encoding Categorical Variables]] BernoulliNB() Why Naive Bayes?;;Order doesn't matter, features are independent. Treated it as a [[Bag of..."
  },
  "named_entity_recognition": {
    "title": "Named Entity Recognition",
    "tags": [
      "NLP",
      "model_algorithm"
    ],
    "aliases": [
      "NER",
      "Entity Recognition"
    ],
    "outlinks": [
      "in_ner_how_would_you_handle_ambiguous_entities",
      "what_are_the_challenges_of_ner_in_multilingual_contexts",
      "structured_data",
      "backpropagation",
      "lstm",
      "why_is_named_entity_recognition_(ner)_a_challenging_task",
      "how_does_the_choice_of_training_data_affect_the_performance_of_ner_models",
      "nlp"
    ],
    "inlinks": [
      "bert",
      "graphrag"
    ],
    "summary": "Named Entity Recognition (NER) is a subtask of [[NLP|Natural Language Processing]] (NLP) that involves identifying and classifying key entities in text into predefined categories such..."
  },
  "nbconvert": {
    "title": "nbconvert",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "ipynb"
    ],
    "summary": ""
  },
  "neo4j": {
    "title": "neo4j",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "graphrag",
      "text2cypher"
    ],
    "summary": ""
  },
  "network_design": {
    "title": "Network Design",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Mixed-Integer Programming: Handles problems where some variables must be integers, commonly used in optimizing network design and capacity planning. How to systems interact."
  },
  "neural_network_classification": {
    "title": "Neural Network Classification",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "deep_learning",
      "data_quality",
      "evaluation_metrics",
      "neural_network",
      "choosing_the_number_of_clusters",
      "classification",
      "clustering",
      "choosing_a_threshold",
      "imbalanced_datasets"
    ],
    "inlinks": [],
    "summary": "Choosing Thresholds/Clusters in [[Neural network]] [[Classification]] When working with [[Deep Learning|neural networks]], the output is often a probability distribution across different classes. To make a..."
  },
  "neural_network_in_practice": {
    "title": "Neural network in Practice",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "model_evaluation",
      "neural_network",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "ml_tools"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "This guide provides practical insights into building and using [[Neural network]]. Refer to [[ML_Tools]] for more details: Neural_Net_Build.py Softmax Placement at the End Numerical stability..."
  },
  "neural_network": {
    "title": "Neural network",
    "tags": [
      "#deep_learning",
      "drafting"
    ],
    "aliases": [
      "Neural Network"
    ],
    "outlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "model_parameters",
      "deep_learning",
      "neural_network",
      "types_of_neural_networks",
      "neural_network_in_practice",
      "normalisation",
      "activation_function",
      "optimisation_techniques",
      "hyperparameter"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "batch_normalisation",
      "neural_network_classification",
      "energy",
      "activation_function",
      "deep_learning",
      "use_cases_for_a_simple_neural_network_like",
      "classification",
      "feed_forward_neural_network",
      "dropout",
      "hyperparameter",
      "ai_engineer",
      "ridge",
      "pytorch",
      "typical_output_formats_in_neural_networks",
      "deep_q-learning",
      "neural_network_in_practice",
      "optimising_neural_networks",
      "ds_&_ml_portal",
      "regularisation",
      "over_parameterised_models",
      "neural_network",
      "word2vec",
      "types_of_neural_networks",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "A [[Neural network|Neural Network]] is a computational model inspired by biological neural networks in the human brain. It consists of layers of interconnected nodes (neurons)..."
  },
  "neural_scaling_laws": {
    "title": "Neural Scaling Laws",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "entropy_of_natural_language",
      "loss_function",
      "pasted_image_20241017072233.png",
      "mnist",
      "pasted_image_20241017072732.png",
      "resolution_limited_scaling",
      "cross_entropy",
      "validation_loss",
      "llm",
      "pasted_image_20241017074030.png",
      "pasted_image_20241017073743.png",
      "cross_entropy_loss",
      "compute_efficent_frontier",
      "intrinsic_dimension_of_natural_language"
    ],
    "inlinks": [],
    "summary": "Even scaled model cannot cross tthe [[compute efficent frontier]] ![[Pasted image 20241017072233.png|500]] [[validation loss]] Neural scaling laws. That is error rates scale with compute,model size..."
  },
  "ngrams": {
    "title": "Ngrams",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "text_classification",
      "nlp"
    ],
    "inlinks": [
      "nlp",
      "explain_the_curse_of_dimensionality",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "N-grams are used in NLP that allow for the analysis of text data by breaking it down into smaller, manageable sequences. An N-gram is a..."
  },
  "nlp": {
    "title": "NLP",
    "tags": [
      "NLP"
    ],
    "aliases": [
      "Natural Language Processing"
    ],
    "outlinks": [
      "summarisation",
      "ngrams",
      "preprocessing",
      "part_of_speech_tagging",
      "tf-idf",
      "one_hot_encoding",
      "grammar_method",
      "nltk",
      "standardised/vector_embedding",
      "normalisation_of_text",
      "bag_of_words",
      "lemmatization"
    ],
    "inlinks": [
      "ngrams",
      "rag",
      "recurrent_neural_networks",
      "attention_mechanism",
      "transformer",
      "generative_ai_from_theory_to_practice",
      "bert",
      "sentence_similarity",
      "explain_the_curse_of_dimensionality",
      "lstm",
      "normalisation",
      "named_entity_recognition",
      "tokenisation"
    ],
    "summary": "Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It encompasses various techniques and models to process and analyze large..."
  },
  "nltk": {
    "title": "nltk",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf",
      "nlp"
    ],
    "summary": ""
  },
  "node.js": {
    "title": "Node.JS",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cryptography"
    ],
    "summary": ""
  },
  "non-parametric_tests": {
    "title": "Non-parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "normalisation_of_data": {
    "title": "Normalisation of data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "json",
      "postgresql",
      "api"
    ],
    "inlinks": [
      "normalised_schema",
      "data_transformation",
      "normalisation"
    ],
    "summary": "Normalization is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from..."
  },
  "normalisation_of_text": {
    "title": "Normalisation of Text",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "stemming",
      "tokenisation",
      "lemmatization"
    ],
    "inlinks": [
      "normalisation",
      "nlp"
    ],
    "summary": "[[Preprocessing]] in NLP tasks is called Normalization involves reducing words to their base or root form, converting them to lowercase, and removing stop words. Processes..."
  },
  "normalisation_vs_standardisation": {
    "title": "Normalisation vs Standardisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241219071120.png",
      "normalisation",
      "standardisation"
    ],
    "inlinks": [
      "batch_normalisation",
      "normalisation"
    ],
    "summary": "Key Differences: [[Normalisation]] changes the range of the data, while standardisation changes the data distribution. Normalisation is preferred when the data does not follow a..."
  },
  "normalisation": {
    "title": "Normalisation",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "z-normalisation",
      "normalised_schema",
      "batch_normalisation",
      "data_engineering",
      "normalisation_of_data",
      "standardisation",
      "normalisation_vs_standardisation",
      "how_to_normalise_a_merged_table",
      "nlp",
      "normalisation_of_text"
    ],
    "inlinks": [
      "anomaly_detection",
      "melt",
      "neural_network",
      "normalisation_vs_standardisation",
      "feature_scaling"
    ],
    "summary": "Standardizing data distributions for consistency. In ML: - [[Z-Normalisation]] - [[Standardisation]] - [[Normalisation vs Standardisation]] - [[Batch Normalisation]] In [[Data Engineering]]: - [[Normalisation of data]]..."
  },
  "normalised_schema": {
    "title": "Normalised Schema",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "normalisation_of_data",
      "how_to_normalise_a_merged_table"
    ],
    "inlinks": [
      "snowflake_schema",
      "why_use_er_diagrams",
      "interview_notepad",
      "normalisation",
      "types_of_database_schema",
      "data_transformation",
      "how_to_normalise_a_merged_table"
    ],
    "summary": "In a normalized schema, data is organized into multiple related tables to minimize redundancy and dependency, and improve data integrity. This approach is often used..."
  },
  "nosql": {
    "title": "NoSQL",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "sql_vs_nosql",
      "data_storage",
      "google_cloud_platform"
    ],
    "summary": "(Not Only SQL):** ==Non-relational== database management systems offering flexibility and scalability for unstructured or document-based data. NoSQL Databases: Accommodate unstructured data and can be represented..."
  },
  "notebooklm": {
    "title": "NotebookLM",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "notebooklm",
      "data_archive"
    ],
    "inlinks": [
      "notebooklm"
    ],
    "summary": "https://www.youtube.com/watch?v=EOmgC3-hznM key topics chat interface takes into account resources. save to note- to dave. how to select and folders - from obsidian [[Data Archive]] for..."
  },
  "npy_files_a_numpy_array_storage": {
    "title": "npy Files A NumPy Array storage",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A .npy file is a binary file format specifically designed to store a single NumPy array. NumPy, or Numerical Python, is a powerful library in..."
  },
  "olap_(online_analytical_processing)": {
    "title": "OLAP (online analytical processing)",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [
      "OLAP"
    ],
    "outlinks": [
      "excel_pivot_table"
    ],
    "inlinks": [
      "dimensions"
    ],
    "summary": "OLAP, or Online Analytical Processing, is a category of database technology. OLAP systems allow organizations to gain insights by examining data across various dimensions, such..."
  },
  "oltp_(online_transactional_processing)": {
    "title": "oltp (online transactional processing)",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "oltp": {
    "title": "OLTP",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "row-based_storage",
      "database"
    ],
    "summary": "In online transaction processing (OLTP), information systems typically facilitate and manage transaction-oriented applications. It's the opposite of OLAP (Online Analytical Processing)."
  },
  "one_pager_template": {
    "title": "One Pager Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Proposal: [Project name] About this doc Metadata about this document. Describe the scope and current status. This doc is a proposal for [feature or change]...."
  },
  "one-hot_encoding": {
    "title": "One-hot encoding",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "one_hot_encoding.py",
      "dummy_variable_trap",
      "why_does_label_encoding_give_different_predictions_from_one-hot_encoding",
      "label_encoding",
      "ml_tools"
    ],
    "inlinks": [
      "dummy_variable_trap",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ],
    "summary": "Related terms: - Why do we need to drop one of the dummy columns? [[Dummy variable trap]]: ==Dummy variables & One-hot encoding are fundamentally different..."
  },
  "one_hot_encoding.py": {
    "title": "One_hot_encoding.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Explorations\\Preprocess\\One_hot_encoding\\One_hot_encoding.py This script demonstrates how to preprocess categorical variables and apply linear regression for house price prediction. Key steps include: Data Loading: It loads a..."
  },
  "optimisation_function": {
    "title": "Optimisation function",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "optimisation_function",
      "model_parameters",
      "loss_function",
      "gradient_descent",
      "optimisation_techniques"
    ],
    "inlinks": [
      "logistic_regression_in_sklearn_&_gradient_descent",
      "optimisation_function",
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "model_parameters_tuning",
      "gradient_descent",
      "lbfgs"
    ],
    "summary": "Optimization functions adjust the [[Model Parameters]] to minimize the [[Loss function]], which measures how well the model performs. This is a fundamental step in training..."
  },
  "optimisation_techniques": {
    "title": "Optimisation techniques",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "stochastic_gradient_descent",
      "standardised/optuna",
      "learning_rate",
      "sklearn",
      "gradient_descent",
      "adaptive_learning_rates",
      "adam_optimizer",
      "cost_function"
    ],
    "inlinks": [
      "optimisation_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "deep_learning",
      "model_parameters",
      "neural_network",
      "model_parameters_tuning",
      "gradient_descent"
    ],
    "summary": "Optimisation techniques - [[Adam Optimizer]] - RMSprop - [[Stochastic Gradient Descent]] - [[standardised/Optuna]] [[Gradient Descent]] - Iteratively updates parameters using the gradient of the [[Cost..."
  },
  "optimising_a_logistic_regression_model": {
    "title": "Optimising a Logistic Regression Model",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "optimisation_function",
      "logistic_regression",
      "model_parameters",
      "ridge",
      "loss_function",
      "sklearn",
      "gradient_descent",
      "cost_function"
    ],
    "inlinks": [],
    "summary": "Optimising a [[Logistic Regression]] Model In sklearn, the logistic regression model uses an optimization algorithm to find the best parameters (intercept and coefficients) that minimize..."
  },
  "optimising_neural_networks": {
    "title": "Optimising Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "neural_network",
      "deep_learning"
    ],
    "inlinks": [
      "orthogonalization"
    ],
    "summary": "[[Deep Learning]] Ways to improve in using a [[Neural network]] more data, bigger network, diverse training set, try dropout, change network architechure. ==Need strategies that..."
  },
  "optuna": {
    "title": "Optuna",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "catboost",
      "xgboost",
      "lightgbm",
      "hyperparameter_tuning",
      "hyperparameter"
    ],
    "inlinks": [],
    "summary": "Optuna is a [[hyperparameter]] optimization framework used to automatically tune hyperparameters for machine learning models. Optuna automates the process of tuning hyperparameters by defining an..."
  },
  "ordinary_least_squares": {
    "title": "Ordinary Least Squares",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pytorch",
      "linear_regression"
    ],
    "summary": "Derivation of Coefficients: - OLS derives the coefficients by setting the partial derivatives of the SSE with respect to each coefficient to zero. This results..."
  },
  "orthogonalization": {
    "title": "Orthogonalization",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "regularisation",
      "ds_&_ml_portal",
      "optimising_neural_networks",
      "adam_optimizer"
    ],
    "inlinks": [],
    "summary": "link When training ML model need Orthogonalization in order to Determine what to tune, and observe the effect it has. Each button does one thing...."
  },
  "outliers": {
    "title": "Outliers",
    "tags": [
      "statistics",
      "anomaly_detection",
      "data_cleaning"
    ],
    "aliases": [
      "anomalies",
      "Handling Outliers"
    ],
    "outlinks": [
      "handling_missing_data",
      "methods_for_handling_outliers",
      "anomaly_detection",
      "linear_regression"
    ],
    "inlinks": [
      "ds_&_ml_portal"
    ],
    "summary": "Outliers are data points that differ significantly from other observations in the dataset. They can skew and mislead the training of machine learning models, especially..."
  },
  "over_parameterised_models": {
    "title": "Over parameterised models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "neural_network"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Neural network]] Universal approximation theory"
  },
  "overfitting": {
    "title": "Overfitting",
    "tags": [
      "model_architecture"
    ],
    "aliases": [
      "model overfitting",
      "high variance models"
    ],
    "outlinks": [
      "cross_validation",
      "bias_and_variance",
      "regularisation"
    ],
    "inlinks": [
      "gradient_boosting",
      "ridge",
      "ds_&_ml_portal",
      "parsimonious",
      "batch_normalisation",
      "machine_learning_algorithms",
      "decision_tree",
      "bagging",
      "bias_and_variance",
      "cross_validation",
      "feed_forward_neural_network",
      "dropout",
      "model_evaluation"
    ],
    "summary": "[!Summary] Overfitting in machine learning occurs when a model captures not only the underlying patterns in the training data ==but also the noise==, leading to..."
  },
  "p_values": {
    "title": "p values",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "feature_selection"
    ],
    "inlinks": [
      "statistics",
      "p-values_in_linear_regression_in_sklearn",
      "hypothesis_testing",
      "model_interpretability"
    ],
    "summary": "A p-value is a measure of the evidence against a null hypothesis. p-values indicate whether an effect exists Used in [[Feature Selection]]"
  },
  "p-values_in_linear_regression_in_sklearn": {
    "title": "p-values in linear regression in sklearn",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "p_values",
      "linear_regression"
    ],
    "inlinks": [
      "sklearn",
      "linear_regression"
    ],
    "summary": "Question How to include [[p values]] in sklearn for a [[Linear Regression]]? import scipy.stats as stat. You can modify the class of LinearRegression() from sklearn..."
  },
  "pandas_dataframe_agent": {
    "title": "Pandas Dataframe Agent",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "langchain"
    ],
    "summary": "Example: https://github.com/AssemblyAI/youtube-tutorials/tree/main/pandas-dataframe-agent Follow: https://www.youtube.com/watch?v=ZIfzpmO8MdA&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=7 Can as pandas questions to a dataframe. Types of questions: - what is the max value of \"col1\""
  },
  "pandas_join_vs_merge": {
    "title": "Pandas join vs merge",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "merge"
    ],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": "In pandas, both .join() and pd.merge() are used to combine DataFrames, but they differ in syntax, defaults, and use cases. [[Merge]] is better than Join...."
  },
  "pandas_pivot_table": {
    "title": "Pandas Pivot Table",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "de_tools"
    ],
    "inlinks": [
      "aggregation",
      "pandas_stack"
    ],
    "summary": "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html Pivot Table: Summarize Data python df = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': ['one', 'two', 'one'], 'C': [1, 2, 3]}) pivot_table = df.pivot_table(values='C', index='A', columns='B',..."
  },
  "pandas_stack": {
    "title": "Pandas Stack",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "de_tools",
      "pandas_stack.py",
      "pandas_pivot_table",
      "pandas_common.py",
      "machine_learning_algorithms",
      "groupby",
      "data_transformation",
      "multi-level_index"
    ],
    "inlinks": [
      "multi-level_index",
      "data_transformation_with_pandas"
    ],
    "summary": "Tool for reshaping data, particularly when you need to pivot a DataFrame ([[Pandas Pivot Table]]) from a wide format to a long format. See: -..."
  },
  "pandas": {
    "title": "Pandas",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "data_selection",
      "handling_missing_data",
      "pandas_common.py",
      "data_transformation",
      "ml_tools"
    ],
    "inlinks": [
      "pyspark",
      "data_transformation_with_pandas"
    ],
    "summary": "In [[ML_Tools]] see: - [[Pandas_Common.py]] Areas: - [[Handling Missing Data]] - [[Data Selection]] - [[Data Transformation]]"
  },
  "pandas_common.py": {
    "title": "Pandas_Common.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack",
      "pandas"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Common.py"
  },
  "pandas_stack.py": {
    "title": "Pandas_Stack.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Stack.py"
  },
  "parametric_tests": {
    "title": "Parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "parametric_vs_non-parametric_models": {
    "title": "parametric vs non-parametric models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "statistics",
      "bernoulli",
      "support_vector_machines",
      "regression"
    ],
    "inlinks": [
      "maximum_likelihood_estimation",
      "k-nearest_neighbours"
    ],
    "summary": "Parametric Models In [[Statistics]] Definition: Models that summarize data with a ==set of parameters of fixed size, regardless of the number of data points.== Characteristics:..."
  },
  "parametric_vs_non-parametric_tests": {
    "title": "parametric vs non-parametric tests",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "statistical_assumptions",
      "non-parametric_tests_",
      "parametric_tests_"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Parametric tests ]] are statistical tests that make ==assumptions about the distribution== of the data. For example, a t-test assumes that the data is normally..."
  },
  "parquet": {
    "title": "Parquet",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "hadoop",
      "cloud_providers",
      "apache_spark",
      "data_storage",
      "metadata_handling",
      "big_data",
      "schema_evolution"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "A Parquet file is a columnar storage file format specifically designed for storing large amounts of data efficiently. It is commonly used in [[Big Data]]..."
  },
  "parsimonious": {
    "title": "parsimonious",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_selection",
      "overfitting"
    ],
    "inlinks": [
      "adjusted_r_squared"
    ],
    "summary": "Parsimonious refers to a principle in [[Model Selection]] and statistical modeling that emphasizes ==simplicity==. In the context of regression and other statistical models, a parsimonious..."
  },
  "part_of_speech_tagging": {
    "title": "Part of speech tagging",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "Part of speech tagging : assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text Part-of-speech tagging involves..."
  },
  "pca_explained_variance_ratio": {
    "title": "PCA Explained Variance Ratio",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "pca_explained_variance_ratio"
    ],
    "inlinks": [
      "pca_explained_variance_ratio",
      "principal_component_analysis"
    ],
    "summary": "[[PCA Explained Variance Ratio]] - The variance explained by each principal component is printed using pca.explained_variance_ratio_. - The sum of the explained variances is calculated..."
  },
  "pca_principal_components": {
    "title": "PCA Principal Components",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "correlation",
      "pca_analysis.ipynb",
      "heatmap",
      "pasted_image_20250317093551.png"
    ],
    "inlinks": [
      "principal_component_analysis"
    ],
    "summary": "The principal components (or the new axes that explain the most variance) are stored in pca.components_ and displayed as a DataFrame for easier reading Interpretating..."
  },
  "pca-based_anomaly_detection": {
    "title": "PCA-Based Anomaly Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pca_based_anomaly_detection.py",
      "ml_tools"
    ],
    "inlinks": [
      "anomaly_detection",
      "principal_component_analysis"
    ],
    "summary": "For implementation, see: [[ML_Tools]]: - [[PCA_Based_Anomaly_Detection.py]]"
  },
  "pca_analysis.ipynb": {
    "title": "PCA_Analysis.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "principal_component_analysis"
    ],
    "inlinks": [
      "principal_component_analysis",
      "pca_principal_components"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Analysis.ipynb This script performs Principal Component Analysis (PCA) on the Iris dataset to reduce its dimensionality while preserving key variance. See also [[Principal Component Analysis|PCA]]..."
  },
  "pca_based_anomaly_detection.py": {
    "title": "PCA_Based_Anomaly_Detection.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pca-based_anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Based_Anomaly_Detection.py"
  },
  "pd.grouper": {
    "title": "pd.Grouper",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "groupby",
      "multi-level_index"
    ],
    "inlinks": [],
    "summary": "pd.Grouper is a utility in pandas used with .groupby() to flexibly group data by a specific column, often useful for time-based grouping, multi-index grouping, or..."
  },
  "pdoc": {
    "title": "pdoc",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pdoc"
    ],
    "inlinks": [
      "documentation_&_meetings",
      "pdoc"
    ],
    "summary": "PDOC is a documentation generator specifically designed for Python projects. Here are some key features and details: Automatic Documentation: It scans your Python code and..."
  },
  "pdp_and_ice": {
    "title": "PDP and ICE",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20241204203413.png",
      "interpretability",
      "pasted_image_20241204203338.png"
    ],
    "inlinks": [],
    "summary": "link: https://scikit-learn.org/1.5/modules/partial_dependence.html#h2009 [[interpretability|interpretable]] Regression example: ![[Pasted image 20241204203338.png]] Categorical Example ![[Pasted image 20241204203413.png]]"
  },
  "percentile_detection": {
    "title": "Percentile Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_percentile.py"
  },
  "performance_dimensions": {
    "title": "Performance Dimensions",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "availability",
      "scalability",
      "accessibility",
      "data_quality",
      "speed",
      "usability",
      "simplicity",
      "latency",
      "reusability",
      "data_integrity",
      "performance",
      "interoperability",
      "data_compatibility",
      "flexibility",
      "cost-efficiency"
    ],
    "inlinks": [
      "dimensional_modelling",
      "data_principles",
      "data_lifecycle_management"
    ],
    "summary": "Efficiency & Performance - [[Cost-efficiency]]: Ensuring that the solutions used are cost-effective and provide value for money. - [[Speed]]: The ability to process and analyze..."
  },
  "performance_drift": {
    "title": "Performance Drift",
    "tags": [
      "deleted",
      "data_quality",
      "deleted",
      "model_explainability"
    ],
    "aliases": [
      "model degradation",
      "accuracy drift",
      "concept drift"
    ],
    "outlinks": [
      "data_drift",
      "data_observability",
      "pasted_image_20250113072251.png"
    ],
    "inlinks": [
      "data_drift",
      "model_validation",
      "model_observability",
      "transfer_learning"
    ],
    "summary": "Not [[Data Drift]] TL;DR. Data drift is a change in the input data. Concept drift is a change in input-output relationships. Both often happen simultaneously...."
  },
  "physical_model": {
    "title": "Physical Model",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Physical Model (for a SQL database): ```sql CREATE TABLE Customer ( CustomerID INT PRIMARY KEY, Name VARCHAR(100), Email VARCHAR(100) ); CREATE TABLE Order ( OrderID..."
  },
  "poetry": {
    "title": "Poetry",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dependency_manager"
    ],
    "inlinks": [
      "virtual_environments",
      "dependency_manager"
    ],
    "summary": "Modern version of setting up dependencies instead of requirements.txt ([[dependency manager]]) Primary Purpose: Poetry is a [[dependency manager]] and packaging tool for Python projects. Main..."
  },
  "policy": {
    "title": "Policy",
    "tags": [
      "question",
      "#question"
    ],
    "aliases": [
      "policies"
    ],
    "outlinks": [
      "sarsa",
      "exploration",
      "q-learning",
      "exploitation",
      "reinforcement_learning"
    ],
    "inlinks": [
      "agent-based_modelling",
      "q-learning",
      "reinforcement_learning"
    ],
    "summary": "In [[reinforcement learning]] (RL), a policy is a strategy or a rule that defines the actions an agent takes in a given state to achieve..."
  },
  "positional_encoding": {
    "title": "Positional Encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": ""
  },
  "postgresql": {
    "title": "PostgreSQL",
    "tags": [
      "relational_database",
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "adding_a_database_to_postgresql",
      "pasted_image_20250329081752.png",
      "tableau"
    ],
    "inlinks": [
      "normalisation_of_data",
      "data_engineering_portal",
      "tableau",
      "looker_studio",
      "database_management_system_(dbms)",
      "database"
    ],
    "summary": "Installation How to set up a Postgres database on your Windows 10 PC In [[Tableau]] can connect to a database here. There are plugins. Spatial..."
  },
  "powerbi": {
    "title": "PowerBI",
    "tags": [
      "software",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_visualisation",
      "looker_studio",
      "fabric",
      "semantic_layer"
    ],
    "summary": "tutorial Business analytics tool for data visualization and reporting."
  },
  "powerquery": {
    "title": "Powerquery",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "how_to_normalise_a_merged_table"
    ],
    "inlinks": [],
    "summary": "[[How to normalise a merged table]]"
  },
  "powershell_versus_cmd": {
    "title": "Powershell versus cmd",
    "tags": [
      "#software"
    ],
    "aliases": null,
    "outlinks": [
      "command_prompt",
      "powershell"
    ],
    "inlinks": [],
    "summary": "PowerShell and Command Prompt (cmd) are both command-line interfaces available on Windows systems, but they differ significantly in their capabilities, syntax, and scripting abilities. Here..."
  },
  "powershell_vs_bash": {
    "title": "Powershell vs Bash",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "bash",
      "windows_subsystem_for_linux",
      "powershell",
      "git"
    ],
    "inlinks": [
      "command_line"
    ],
    "summary": "The choice between [[PowerShell]] and [[Bash]] largely depends on the user's needs and the environment in which they are working. Here are some considerations for..."
  },
  "powershell": {
    "title": "PowerShell",
    "tags": [
      "software"
    ],
    "aliases": [
      "Why is Powershell better than cmd"
    ],
    "outlinks": [
      ".net",
      "command_prompt",
      "batch_files"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_prompt",
      "command_line",
      "powershell_vs_bash"
    ],
    "summary": "Why is Powershell better than [[Command Prompt|cmd]]? PowerShell is often considered better than Command Prompt (cmd) for several reasons: Object-Oriented: PowerShell is built on the..."
  },
  "precision_or_recall": {
    "title": "Precision or Recall",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "pasted_image_20240116211130.png",
      "classification",
      "precision",
      "recall"
    ],
    "inlinks": [
      "precision",
      "precision-recall_curve",
      "evaluation_metrics"
    ],
    "summary": "[[Precision]] and [[Recall]] are two fundamental metrics used to evaluate the performance of a [[Classification]] model, particularly in binary classification tasks. They are related through..."
  },
  "precision-recall_curve": {
    "title": "Precision-Recall Curve",
    "tags": [],
    "aliases": [
      "PR Curve"
    ],
    "outlinks": [
      "auc",
      "binary_classification",
      "roc_pr_example.py",
      "precision",
      "recall",
      "distributions",
      "precision_or_recall",
      "pasted_image_20241231172749.png",
      "roc_(receiver_operating_characteristic)",
      "imbalanced_datasets",
      "ml_tools"
    ],
    "inlinks": [
      "determining_threshold_values",
      "choosing_a_threshold"
    ],
    "summary": "A [[precision]]-[[recall]] curve is a graphical representation used to evaluate the performance of a [[Binary Classification]] model, particularly in scenarios where the classes are imbalanced...."
  },
  "precision": {
    "title": "Precision",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "classification_report",
      "accuracy",
      "pasted_image_20241222091831.png",
      "classification",
      "precision_or_recall"
    ],
    "inlinks": [
      "classification_report",
      "ds_&_ml_portal",
      "precision-recall_curve",
      "evaluation_metrics",
      "f1_score",
      "model_observability",
      "precision_or_recall",
      "confusion_matrix"
    ],
    "summary": "Precision Score is a metric used to evaluate the [[Accuracy]] of a [[Classification]] model, specifically focusing on the positive class. ==How many retrieved items are..."
  },
  "preprocessing": {
    "title": "Preprocessing",
    "tags": [
      "#ml_optimisation",
      "data_transformation",
      "data_cleaning",
      "data_collection",
      "portal"
    ],
    "aliases": [
      "Data Preprocessing",
      "Feature Preprocessing",
      "Preprocess"
    ],
    "outlinks": [
      "data_cleansing",
      "eda",
      "feature_selection",
      "data_collection",
      "dimensionality_reduction",
      "feature_scaling",
      "feature_engineering",
      "data_transformation",
      "data_reduction"
    ],
    "inlinks": [
      "data_pipeline",
      "handling_different_distributions",
      "heterogeneous_features",
      "pycaret",
      "model_deployment",
      "data_lifecycle_management",
      "standardisation",
      "dimensionality_reduction",
      "feature_scaling",
      "interview_notepad",
      "model_building",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "clustermap",
      "nlp",
      "normalisation_of_text"
    ],
    "summary": "Data Preprocessing Data Preprocessing refers to the overall process of cleaning and transforming raw data into a format that is suitable for analysis and modelling...."
  },
  "prevention_is_better_than_the_cure": {
    "title": "Prevention Is Better Than The Cure",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "change_management",
      "data_observability",
      "data_quality"
    ],
    "inlinks": [
      "data_quality"
    ],
    "summary": "To ensure data products are effective essential to prioritize prevention over remediation of [[Data Quality]] Prevention Preventing data quality issues is the most effective strategy...."
  },
  "primary_key": {
    "title": "Primary Key",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": "A primary key (PK) is a unique identifier for each record in a database table. Uniqueness: No two records can have the same primary key..."
  },
  "principal_component_analysis": {
    "title": "Principal Component Analysis",
    "tags": [
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [
      "PCA"
    ],
    "outlinks": [
      "pca_explained_variance_ratio",
      "pca-based_anomaly_detection",
      "unsupervised_learning",
      "pca_principal_components",
      "variance",
      "interpretability",
      "pca_analysis.ipynb",
      "dimensionality_reduction",
      "t-sne",
      "manifold_learning",
      "ml_tools"
    ],
    "inlinks": [
      "feature_selection",
      "covariance_structures",
      "ds_&_ml_portal",
      "unsupervised_learning",
      "pca_analysis.ipynb",
      "machine_learning_algorithms",
      "standardisation",
      "addressing_multicollinearity",
      "dimensionality_reduction",
      "feature_scaling",
      "t-sne"
    ],
    "summary": "PCA is a tool for [[Dimensionality Reduction]] in [[Unsupervised Learning]] to reduce the dimensionality of data. It transforms the original data into a new coordinate..."
  },
  "probability_in_other_fields": {
    "title": "Probability in other fields",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "problem_definition": {
    "title": "Problem Definition",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "scientific_method"
    ],
    "summary": "What is involved: Clearly articulate the problem you're trying to solve and the outcomes you expect. Follow up questions What assumption can we make based..."
  },
  "programming_languages": {
    "title": "programming languages",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "prompt_engineering": {
    "title": "Prompt Engineering",
    "tags": [
      "language_models",
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "how_can_prompt_engineering_be_integrated_into_existing_nlp_workflows_to_enhance_performance",
      "prompt_retrievers",
      "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts"
    ],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "agent-based_modelling"
    ],
    "summary": "Prompt engineering is a technique in the field of natural language processing (NLP), particularly when working with large language models (LLMs). It involves designing and..."
  },
  "prompt_extracting_information_from_blog_posts": {
    "title": "Prompt Extracting information from blog posts",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Prompt Extracting information from blog posts You are an expert in summarizing and analyzing online content. Your assignment involves producing a 200-word summary of the..."
  },
  "prompting": {
    "title": "Prompting",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "pasted_image_20240910072458.png"
    ],
    "inlinks": [
      "evaluating_language_models",
      "ai_engineer",
      "guardrails",
      "rag"
    ],
    "summary": "Pre set prompts are most useful when they are easily accessible. In obsidian copilot can select prompts with \"/\". We tag prompts with #prompt How..."
  },
  "proportion_test": {
    "title": "Proportion Test",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Proportion Test The proportion test is used to compare proportions between groups. It can be categorized into: - One-Sample Proportion Test: Compares the proportion of..."
  },
  "publish_and_subscribe": {
    "title": "Publish and Subscribe",
    "tags": [
      "event_driven",
      "data_streaming"
    ],
    "aliases": null,
    "outlinks": [
      "event-driven_architecture",
      "scalability",
      "batch_processing",
      "distributed_computing",
      "apache_kafka",
      "data_streaming"
    ],
    "inlinks": [
      "apache_kafka",
      "data_streaming"
    ],
    "summary": "The Publish-Subscribe (Pub-Sub) model is a messaging pattern that enables real-time data distribution by decoupling message producers from consumers. This architecture is widely used in..."
  },
  "pull_request_template": {
    "title": "Pull Request Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Tl;dr 1-liner if the context of the change is long Context A few sentences on the high level context for the change. Link to relevant..."
  },
  "push-down": {
    "title": "Push-Down",
    "tags": [
      "database"
    ],
    "aliases": [
      "Query Pushdown"
    ],
    "outlinks": [
      "querying"
    ],
    "inlinks": [],
    "summary": "Query pushdown aims to execute as much work as possible in the source databases. Push-downs or query pushdowns push transformation logic to the source database...."
  },
  "pycaret": {
    "title": "PyCaret",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "preprocessing",
      "model_deployment",
      "model_selection",
      "pycaret_example.py",
      "hyperparameter_tuning",
      "ml_tools"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "PyCaret is an open-source, low-code Python library designed to simplify machine learning workflows. It allows users to build, evaluate, and deploy machine learning models with..."
  },
  "pycaret_anomaly.ipynb": {
    "title": "Pycaret_Anomaly.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/Pycaret_Anomaly.ipynb"
  },
  "pycaret_example.py": {
    "title": "Pycaret_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pycaret"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Pycaret/Pycaret_Example.py"
  },
  "pydantic": {
    "title": "Pydantic",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "declarative",
      "debugging",
      "json",
      "pydantic.py",
      "pydantic_more.py",
      "data_validation",
      "fastapi",
      "object-oriented_programming",
      "type_checking",
      "ml_tools"
    ],
    "inlinks": [
      "fastapi",
      "pyright_vs_pydantic",
      "maintainable_code",
      "data_validation"
    ],
    "summary": "Pydantic is a Python library used for [[data validation]] and settings management using Python type annotations. It provides a way to define data models with..."
  },
  "pydantic.py": {
    "title": "Pydantic.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic.py Explanation: BaseModel: This is the base class for creating data models in Pydantic. You define your model by subclassing BaseModel and specifying fields with..."
  },
  "pydantic_more.py": {
    "title": "Pydantic_More.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic_More.py Key Features Demonstrated in the Script: Nested Models: Use of the Friend model inside the User model. Custom Validators: Validating age and email fields..."
  },
  "pyright_vs_pydantic": {
    "title": "Pyright vs Pydantic",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "fastapi",
      "pyright",
      "data_validation",
      "pydantic"
    ],
    "inlinks": [],
    "summary": "While [[Pyright]] and [[Pydantic]] serve different roles in Python development, they complement each other well. Pyright helps ensure that the code adheres to ==type constraints..."
  },
  "pyright": {
    "title": "Pyright",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [
      "functional_programming",
      "documentation_&_meetings",
      "maintainable_code",
      "debugging",
      "type_checking"
    ],
    "inlinks": [
      "pyright_vs_pydantic",
      "maintainable_code"
    ],
    "summary": "Pyright is a ==static type checker== for Python that enhances code reliability by enforcing type constraints ==at compile-time.== It utilizes type hints to identify potential..."
  },
  "pyspark": {
    "title": "PySpark",
    "tags": [
      "#data_orchestration",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "directed_acyclic_graph_(dag)",
      "pandas",
      "sql",
      "apache_spark"
    ],
    "inlinks": [
      "fabric"
    ],
    "summary": "Python API for [[Apache Spark]], a ==distributed processing framework== for big data analysis and machine learning on clusters. Part of [[Apache Spark]] [[Directed Acyclic Graph..."
  },
  "pytest": {
    "title": "Pytest",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "testing_unittest.py",
      "testing_pytest.py",
      "toml"
    ],
    "summary": "@pytest.fixture Explanation @pytest.fixture is a decorator in pytest used to define reusable test setup functions. It allows tests to use shared resources without redundant code...."
  },
  "python_click": {
    "title": "Python Click",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "click_implementation.py",
      "ml_tools"
    ],
    "inlinks": [],
    "summary": "Python Click, or \"Command Line Interface Creation Kit,\" is a library for building command-line interfaces (CLIs). It supports arbitrary nesting of commands, automatic help page..."
  },
  "python": {
    "title": "Python",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "immutable_vs_mutable"
    ],
    "inlinks": [
      "testing",
      "langchain",
      "tool.ruff",
      "duckdb_vs_sqlite",
      "immutable_vs_mutable"
    ],
    "summary": "dynamic language lower learning, support object orientated [[Immutable vs mutable]]"
  },
  "pytorch_vs_tensorflow": {
    "title": "Pytorch vs Tensorflow",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "tensorflow",
      "pytorch",
      "keras"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "[[Tensorflow]] is widely adopted but pytorch picking up Dynamic vs static graph Tensorboard is better than [[pytorch]] visualization Plain tensorflow looks pretty much like a..."
  },
  "pytorch": {
    "title": "PyTorch",
    "tags": [
      "software"
    ],
    "aliases": [
      "PyTorch"
    ],
    "outlinks": [
      "parallelism",
      "deep_learning",
      "stochastic_gradient_descent",
      "use_cases_for_a_simple_neural_network_like",
      "loss_function",
      "neural_network",
      "pytorch",
      "gradient_descent",
      "numpy",
      "1.0,_2.0",
      "ordinary_least_squares",
      "linear_regression",
      "tensorflow"
    ],
    "inlinks": [
      "pytorch_vs_tensorflow",
      "deep_learning",
      "recurrent_neural_networks",
      "vector_embedding",
      "pytorch",
      "edge_machine_learning_models",
      "lstm",
      "transfer_learning"
    ],
    "summary": "Text Generation With LSTM Recurrent Neural Networks in Python with Keras want for [[PyTorch]] Open-source [[Deep Learning]] framework with dynamic computational graphs, emphasizing flexibility and..."
  },
  "q-learning": {
    "title": "Q-Learning",
    "tags": [
      "regressor",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "exploration_vs._exploitation",
      "bellman_equations",
      "policy",
      "pasted_image_20250220133556.png",
      "reinforcement_learning"
    ],
    "inlinks": [
      "deep_q-learning",
      "reinforcement_learning",
      "policy"
    ],
    "summary": "Q-learning is a value-based, model-free [[Reinforcement learning]] algorithm where the agent learns the optimal [[policy]] by updating Q-values based on the rewards received. It is..."
  },
  "quartz": {
    "title": "Quartz",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "javascript",
      "jinja_template",
      "vim"
    ],
    "inlinks": [
      "jinja_template"
    ],
    "summary": "[[Vim]]: telescope? Search preview feature? https://www.youtube.com/watch?v=v5LGaczJaf0 How does quartz work of a software level: - Transforming text. Think [[jinja template]]. - Manipulating markdown notes -..."
  },
  "query_gsheets": {
    "title": "QUERY GSheets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/gsheets"
    ],
    "inlinks": [
      "gsheets"
    ],
    "summary": "In [[standardised/GSheets]] I want to use query, but I also want to remove certain rows based on a range of keys , can I do..."
  },
  "query_optimisation": {
    "title": "Query Optimisation",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "database_index",
      "queries",
      "querying",
      "transaction",
      "query_optimisation"
    ],
    "inlinks": [
      "query_optimisation"
    ],
    "summary": "[[Querying]] can be optimised for time, ==space efficiency==, and concurrency of queries. Optimizing SQL [[Querying]]: - Timing queries - [[Database Index|Indexing]] - Managing [[Transaction]] -..."
  },
  "query_plan": {
    "title": "Query Plan",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "database_index"
    ],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "What is expected to happen to the query plan if there is [[Database Index|Indexing]]?"
  },
  "querying": {
    "title": "Querying",
    "tags": [
      "database",
      "data_analysis",
      "data_exploration"
    ],
    "aliases": [
      "Queries",
      "Query"
    ],
    "outlinks": [
      "sql_injection",
      "sql_joins",
      "de_tools"
    ],
    "inlinks": [
      "sql_groupby",
      "database_index",
      "sqlite",
      "common_table_expression",
      "soft_deletion",
      "sql",
      "query_optimisation",
      "database_techniques",
      "sql_window_functions",
      "duckdb",
      "views",
      "columnar_storage",
      "data_storage",
      "data_warehouse",
      "push-down"
    ],
    "summary": "Querying is the process of asking questions of data. Querying makes use of keys primary and foreign within tables. Useful Links - CS50 SQL Course..."
  },
  "quicksort": {
    "title": "QuickSort",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "recursive_algorithm"
    ],
    "inlinks": [],
    "summary": "[[Recursive Algorithm]] Quicksort Algorithm in Five Lines of Code! - Computerphile Fast algorithm (compared to say Insertion Sort) 1) Pick pivot value 2) Divide remaining..."
  },
  "r_squared": {
    "title": "R squared",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "adjusted_r_squared",
      "regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression"
    ],
    "inlinks": [
      "regression_metrics",
      "linear_regression",
      "adjusted_r_squared"
    ],
    "summary": "R\u00b2, or the coefficient of determination, ==measures the proportion of variance in the dependent variable that is explained by the independent variables== in a [[regression]]..."
  },
  "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression": {
    "title": "R-squared metric not always a good indicator of model performance in regression",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cross_validation",
      "adjusted_r_squared"
    ],
    "inlinks": [
      "r_squared"
    ],
    "summary": "R-squared (R\u00b2) is a commonly used metric for assessing the performance of regression models, but it is not always a reliable indicator of model quality...."
  },
  "r": {
    "title": "R",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": ""
  },
  "race_conditions": {
    "title": "Race Conditions",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": ""
  },
  "rag": {
    "title": "RAG",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "llm",
      "rag",
      "transformer",
      "bert",
      "prompting",
      "pasted_image_20240928194559.png",
      "pasted_image_20241017165540.png",
      "generative",
      "nlp",
      "hallucinating"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "accessing_gen_ai_generated_content",
      "knowledge_graphs_with_obsidian",
      "rag",
      "generative_ai_from_theory_to_practice",
      "knowledge_graph_vs_rag_setup",
      "relationships_in_memory",
      "graphrag",
      "small_language_models"
    ],
    "summary": "Rag is a framework the help [[LLM]] be more up to date. RAG grounds the Gen AI in external data. [!Summary] Given a question sometimes..."
  },
  "random_forest_regression": {
    "title": "Random Forest Regression",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Random Forest Regression : Like random forests for classification, random forest regression combines multiple regression trees to improve prediction accuracy."
  },
  "random_forests": {
    "title": "Random Forests",
    "tags": [
      "classifier",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "pasted_image_20240128194716.png",
      "model_ensemble",
      "decision_tree",
      "random_forests",
      "bagging",
      "pasted_image_20240118145117.png",
      "hyperparameter"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "ada_boosting",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "regularisation_of_tree_based_models",
      "machine_learning_algorithms",
      "why_and_when_is_feature_scaling_necessary",
      "bagging",
      "random_forests",
      "feature_importance",
      "hyperparameter_tuning",
      "embedded_methods",
      "imbalanced_datasets"
    ],
    "summary": "A random forest is an [[Model Ensemble]] of [[Decision Tree]]s. Take many decision trees decisions to get better result. What is the Random Forest method;;..."
  },
  "react": {
    "title": "React",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "javascript",
      "dashboarding"
    ],
    "inlinks": [],
    "summary": "React is a [[JavaScript]] library developed by Meta for building user interfaces, particularly in web development. Related to: - [[Dashboarding]] Core Concepts React's component-based architecture..."
  },
  "reasoning_tokens": {
    "title": "Reasoning tokens",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "mathematical_reasoning_in_transformers"
    ],
    "inlinks": [],
    "summary": "Transformers rely on pattern recognition and language-based reasoning. Thus, reasoning tokens serve as a mechanism for token-based logical progression, allowing models like ChatGPT to simulate..."
  },
  "recall": {
    "title": "Recall",
    "tags": [
      "evaluation"
    ],
    "aliases": [
      "sensitivity"
    ],
    "outlinks": [
      "classification",
      "pasted_image_20241222091831.png",
      "evaluation_metrics"
    ],
    "inlinks": [
      "classification_report",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "precision-recall_curve",
      "evaluation_metrics",
      "f1_score",
      "model_observability",
      "precision_or_recall",
      "confusion_matrix",
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "Recall Score is a [[Evaluation Metrics]] used to evaluate the performance of a [[Classification]] model, focusing on the model's ability to identify all relevant instances..."
  },
  "recommender_systems": {
    "title": "Recommender systems",
    "tags": [
      "evaluation",
      "model_algorithm"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf",
      "graph_neural_network",
      "k-nearest_neighbours"
    ],
    "summary": "Crab on Python A recommender system, or recommendation system, is a type of information filtering system that aims to predict the preferences or interests of..."
  },
  "recurrent_neural_networks": {
    "title": "Recurrent Neural Networks",
    "tags": [
      "deep_learning",
      "time_series"
    ],
    "aliases": [
      "RNN",
      "RNNs"
    ],
    "outlinks": [
      "neural_network",
      "use_of_rnns_in_energy_sector",
      "pytorch",
      "transformer",
      "gated_recurrent_unit",
      "pasted_image_20241219073017.png",
      "time_series",
      "vanishing_and_exploding_gradients_problem",
      "backpropagation",
      "gru",
      "lstm",
      "pasted_image_20241219073440.png",
      "nlp"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "reward_function",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "vanishing_and_exploding_gradients_problem",
      "feed_forward_neural_network",
      "lstm"
    ],
    "summary": "Recurrent Neural Networks (RNNs) are a type of [[neural network]] designed to process sequential data by maintaining a memory of previous inputs through hidden states...."
  },
  "recursive_algorithm": {
    "title": "Recursive Algorithm",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "common_table_expression",
      "quicksort",
      "algorithms"
    ],
    "summary": ""
  },
  "regression_metrics": {
    "title": "Regression Metrics",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "interpretability",
      "standardised/outliers",
      "variance",
      "adjusted_r_squared",
      "r_squared"
    ],
    "inlinks": [
      "gini_impurity",
      "metric",
      "adjusted_r_squared",
      "model_evaluation"
    ],
    "summary": "These metrics provide various ways to evaluate the performance of regression models. Evaluating Regression Models These metric provide: Comprehensive Evaluation: Each metric provides a different..."
  },
  "regression": {
    "title": "Regression",
    "tags": [
      "statistics",
      "regressor"
    ],
    "aliases": [
      "regressive models",
      "predictive regression",
      "linear models"
    ],
    "outlinks": [
      "logistic_regression",
      "ridge",
      "linearity",
      "lasso",
      "multicollinearity",
      "polynomial_regression",
      "feature_engineering",
      "linear_regression",
      "supervised_learning",
      "regression"
    ],
    "inlinks": [
      "supervised_learning",
      "parametric_vs_non-parametric_models",
      "logistic_regression",
      "ds_&_ml_portal",
      "use_cases_for_a_simple_neural_network_like",
      "loss_function",
      "machine_learning_algorithms",
      "typical_output_formats_in_neural_networks",
      "classification",
      "regression",
      "k-nearest_neighbours",
      "energy",
      "r_squared",
      "learning_styles",
      "encoding_categorical_variables",
      "imbalanced_datasets"
    ],
    "summary": "[!Summary] [[Regression]] analysis is a statistical method used to ==predict== a continuous variable based on one or more predictor variables. The most common form, [[Linear..."
  },
  "regression_logistic_metrics.ipynb": {
    "title": "Regression_Logistic_Metrics.ipynb",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Regression_Logistic_Metrics.ipynb"
  },
  "regularisation_of_tree_based_models": {
    "title": "Regularisation of Tree based models",
    "tags": [
      "ml_process",
      "ml_optimisation",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "regularisation",
      "model_ensemble",
      "decision_tree",
      "random_forests",
      "hyperparameters"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "Tree models, such as Random Forests and Gradient Boosting, can also be regularized, although they don\u2019t use L1 or L2 regularization directly. Instead, they are..."
  },
  "regularisation": {
    "title": "Regularisation",
    "tags": [
      "deleted",
      "ml_process",
      "data_visualization",
      "statistics",
      "ml_optimisation",
      "model_explainability"
    ],
    "aliases": [
      "Regulation in ML",
      "Regularisation techniques"
    ],
    "outlinks": [
      "feature_selection",
      "ridge",
      "when_and_why_not_to_us_regularisation",
      "lasso",
      "loss_function",
      "interpretability",
      "neural_network",
      "regularisation_of_tree_based_models",
      "model_parameters_tuning",
      "model_selection",
      "regularisation.py",
      "ml_tools"
    ],
    "inlinks": [
      "orthogonalization",
      "fitting_weights_and_biases_of_a_neural_network",
      "ds_&_ml_portal",
      "overfitting",
      "lightgbm_vs_xgboost_vs_catboost",
      "regularisation_of_tree_based_models",
      "backpropagation",
      "bias_and_variance",
      "explain_the_curse_of_dimensionality",
      "xgboost",
      "dropout",
      "hyperparameter_tuning",
      "embedded_methods"
    ],
    "summary": "Regularization is a technique in machine learning that reduces the risk of overfitting by adding a penalty to the [[Loss function]] during model training. This..."
  },
  "regularisation.py": {
    "title": "Regularisation.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "regularisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Regularisation/Regularisation.py"
  },
  "reinforcement_learning": {
    "title": "Reinforcement learning",
    "tags": [
      "field",
      "reinforcement_learning"
    ],
    "aliases": null,
    "outlinks": [
      "exploration_vs._exploitation",
      "markov_decision_processes",
      "sarsa",
      "bellman_equations",
      "q-learning",
      "policy",
      "deep_q-learning",
      "reinforcement_learning",
      "multi-agent_reinforcement_learning"
    ],
    "inlinks": [
      "industries_of_interest",
      "demand_forecasting",
      "exploration_vs._exploitation",
      "ds_&_ml_portal",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "llm",
      "sarsa",
      "q-learning",
      "policy",
      "deep_q-learning",
      "reinforcement_learning",
      "cost_function"
    ],
    "summary": "Reinforcement Learning ( [[Reinforcement learning|RL]]) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent..."
  },
  "relating_tables_together": {
    "title": "Relating Tables Together",
    "tags": [
      "data_integrity",
      "database_design"
    ],
    "aliases": [],
    "outlinks": [
      "primary_key",
      "er_diagrams",
      "junction_tables",
      "data_integrity",
      "many-to-many_relationships",
      "foreign_key"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "Implementing these concepts, database tables can be effectively related, ensuring [[Data Integrity]], efficient retrieval, and easy maintenance. Resources: - LINK Notes on Relating Database Tables..."
  },
  "relational_database": {
    "title": "Relational Database",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fabric"
    ],
    "summary": ""
  },
  "relationships_in_memory": {
    "title": "Relationships in memory",
    "tags": [
      "memory_management",
      "language_models"
    ],
    "aliases": [
      "Managing LLM memory"
    ],
    "outlinks": [
      "graphrag",
      "vector_database",
      "semantic_relationships",
      "rag"
    ],
    "inlinks": [
      "llm"
    ],
    "summary": "In managing the memory of a large language model (LLM), several key concepts and techniques play a crucial role in forming and maintaining relationships between..."
  },
  "requirements.txt": {
    "title": "requirements.txt",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dependency_manager"
    ],
    "summary": ""
  },
  "rest_api": {
    "title": "REST API",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "crud",
      "json"
    ],
    "inlinks": [
      "api"
    ],
    "summary": "REST API REST stands for Representational State Transfer. It is a ==standardized== software architecture style used for API communication between a client and a server...."
  },
  "reverse_etl": {
    "title": "reverse etl",
    "tags": [
      "data_transformation"
    ],
    "aliases": [
      "Data Activation",
      "Operational Analytics"
    ],
    "outlinks": [
      "etl"
    ],
    "inlinks": [],
    "summary": "Reverse [[ETL]] is the flip side of the ETL/ELT. With Reverse ETL, the data warehouse becomes the source rather than the destination. Data is taken..."
  },
  "reward_function": {
    "title": "Reward Function",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "recurrent_neural_networks"
    ],
    "inlinks": [
      "cost_function"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]]"
  },
  "ridge": {
    "title": "Ridge",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "L2"
    ],
    "outlinks": [
      "feature_selection",
      "logistic_regression",
      "overfitting",
      "ridge",
      "loss_function",
      "neural_network",
      "linear_regression",
      "multicollinearity"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "regularisation",
      "ridge",
      "optimising_a_logistic_regression_model",
      "feed_forward_neural_network",
      "elastic_net",
      "embedded_methods",
      "regression"
    ],
    "summary": "L2 Regularization, also known as Ridge Regularization, adds a penalty term proportional to the square of the weights to the [[loss function]]. This technique enhances..."
  },
  "roc_(receiver_operating_characteristic)": {
    "title": "ROC (Receiver Operating Characteristic)",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "logistic_regression",
      "ml_tools",
      "recall",
      "roc_curve.py",
      "specificity"
    ],
    "inlinks": [
      "precision-recall_curve",
      "auc",
      "determining_threshold_values",
      "model_observability",
      "choosing_a_threshold"
    ],
    "summary": "ROC (Receiver Operating Characteristic) is a graphical representation of a classifier's performance across different thresholds, showing the trade-off between sensitivity (true positive rate) and specificity..."
  },
  "roc_curve.py": {
    "title": "ROC_Curve.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/ROC_Curve.py Overview This script demonstrates how to compute and interpret Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC) scores using Random..."
  },
  "rollup": {
    "title": "rollup",
    "tags": [
      "database"
    ],
    "aliases": null,
    "outlinks": [
      "database",
      "granularity"
    ],
    "inlinks": [
      "business_intelligence"
    ],
    "summary": "Rollup refers to aggregating data to a higher level of [[granularity]], such as summarizing hourly data into daily totals. [[Database]]"
  },
  "row-based_storage": {
    "title": "Row-based Storage",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "crud",
      "columnar_storage",
      "oltp"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Data is stored in consecutive rows, allows [[CRUD]] Row-based storage is well-suited for transactional systems ([[OLTP]]) Less efficient than [[Columnar Storage]] in largedatasets. Row-based Storage..."
  },
  "sarsa": {
    "title": "Sarsa",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "reinforcement_learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "policy"
    ],
    "summary": "SARSA stands for State-Action-Reward-State-Action SARSA is another value-based [[Reinforcement learning]] algorithm, differing from Q-learning in that it updates the Q-values based on the action actually..."
  },
  "scala": {
    "title": "Scala",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "java",
      "big_data",
      "lambdas",
      "apache_spark"
    ],
    "inlinks": [
      "fabric",
      "big_data"
    ],
    "summary": "[!Summary] Scala is a functional programming language primarily used for [[big data]] processing, particularly with frameworks like [[Apache Spark]]. It is known for its concise..."
  },
  "scalability": {
    "title": "Scalability",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "performance_dimensions",
      "publish_and_subscribe",
      "spreadsheets_vs_databases",
      "distributed_computing",
      "model_deployment",
      "machine_learning_algorithms",
      "event_driven",
      "data_ingestion",
      "apache_kafka"
    ],
    "summary": "Scalability refers to the capability of a system, network, or process to handle a growing amount of work or its potential to accommodate growth. Key..."
  },
  "scaling_agentic_systems": {
    "title": "Scaling Agentic Systems",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "agentic_solutions",
      "llm",
      "small_language_models"
    ],
    "inlinks": [],
    "summary": "[[Agentic solutions]] propose an improvement over traditional Large Language Model ([[LLM]]) usage by employing networks of Small Language Models (SLMs). These systems aim to strike..."
  },
  "scaling_server": {
    "title": "Scaling Server",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Scaling Server - Horizontal Scaling: Adding more servers, preferred for scalability. - Vertical Scaling: Adding more resources (memory, CPU) to existing servers."
  },
  "scheduled_tasks": {
    "title": "Scheduled Tasks",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "cron_jobs_",
      "unix"
    ],
    "inlinks": [],
    "summary": "Similar to [[Cron jobs ]]in [[Unix]] Using schtasks (Command-Line) Windows provides schtasks, a command-line tool to schedule tasks. Example Commands Run a Python script every..."
  },
  "schema_evolution": {
    "title": "Schema Evolution",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "database_schema",
      "acid_transaction"
    ],
    "inlinks": [
      "parquet"
    ],
    "summary": "[[Database Schema|Schema]] Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might..."
  },
  "scientific_method": {
    "title": "Scientific Method",
    "tags": [
      "field",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "eda",
      "problem_definition"
    ],
    "inlinks": [
      "knowledge_work",
      "thinking_systems",
      "data_science"
    ],
    "summary": "Step 1: Start with Data Collect Data: Gather all relevant data sources that might be useful for your analysis. Understand Data: Familiarize yourself with the..."
  },
  "search": {
    "title": "Search",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf"
    ],
    "summary": ""
  },
  "security": {
    "title": "Security",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "inlinks": [
      "checksum",
      "cryptography",
      "data_principles",
      "deepseek",
      "software_design_patterns",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "[[Common Security Vulnerabilities in Software Development]]"
  },
  "semantic_layer": {
    "title": "semantic layer",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "metric",
      "powerbi",
      "data_virtualization",
      "tableau"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "A Semantic Layer is much more flexible and makes the most sense on top of transformed data in a Data Warehouse. A semantic layer in..."
  },
  "semantic_relationships": {
    "title": "Semantic Relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "vector_embedding",
      "word2vec",
      "relationships_in_memory",
      "memory",
      "syntactic_relationships"
    ],
    "summary": ""
  },
  "semi-structured_data": {
    "title": "semi-structured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "json",
      "xml",
      "structured_data"
    ],
    "inlinks": [],
    "summary": "Semi-structured data is data that lacks a rigid structure and that does not conform directly to a data model, but that has tags, metadata, or..."
  },
  "sentence_similarity": {
    "title": "Sentence Similarity",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "nlp"
    ],
    "inlinks": [
      "bert"
    ],
    "summary": "Sentence similarity refers to the degree to which two sentences are alike in meaning. It is a crucial concept in natural language processing ([[NLP]]) tasks..."
  },
  "shapefile": {
    "title": "shapefile",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "A shapefile is a popular geospatial vector data format for geographic information system (GIS) software. It is widely used for storing the location, shape, and..."
  },
  "shapley_additive_explanations": {
    "title": "SHapley Additive exPlanations",
    "tags": [],
    "aliases": [
      "SHAP"
    ],
    "outlinks": [
      "feature_importance"
    ],
    "inlinks": [
      "use_of_rnns_in_energy_sector",
      "feature_importance",
      "model_interpretability"
    ],
    "summary": "SHAP provides a unified approach to measure [[Feature Importance]] by computing the contribution of each feature to each prediction, based on game theory. Key Points..."
  },
  "sharepoint": {
    "title": "Sharepoint",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "microsoft"
    ],
    "inlinks": [],
    "summary": "SharePoint is a web-based collaboration platform developed by [[Microsoft]]. It is primarily used for creating intranet sites, document management, and team collaboration, providing a centralized..."
  },
  "silhouette_analysis": {
    "title": "Silhouette Analysis",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "pasted_image_20241231172459.png",
      "pasted_image_20241231172403.png"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters"
    ],
    "summary": "Sklearn link Silhouette analysis is a technique used to evaluate the quality of clustering results. It provides a measure of how similar an object is..."
  },
  "single_source_of_truth": {
    "title": "Single Source of Truth",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "single_source_of_truth",
      "master_data_management",
      "database",
      "data_lakehouse",
      "data_warehouse"
    ],
    "inlinks": [
      "data_integration",
      "business_intelligence",
      "single_source_of_truth"
    ],
    "summary": "Sending data from across an enterprise into a centralized system such as a: [[Database]] [[Data Warehouse]] [[Data Lakehouse]] [[Data Lakehouse]] [[master data management]] results in..."
  },
  "sklearn_datasets": {
    "title": "sklearn datasets",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "make a dataframe by ```python ds = datasets.load_dataset() df = pd.DataFrame(ds.data,columns=ds.feature_names) df.head() add target column df['target'] = ds.target"
  },
  "sklearn_pipiline": {
    "title": "Sklearn Pipiline",
    "tags": [
      "code_snippet",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "```python Naivebayesfor email prediction from sklearn.pipeline import Pipeline clf = Pipeline([ ('vectorizer', CountVectorizer()), ('nb', MultinomialNB()) ]) clf.fit(X_train, y_train) clf.score(X_test,y_test) clf.predict(user_input) ```"
  },
  "sklearn": {
    "title": "Sklearn",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "p-values_in_linear_regression_in_sklearn",
      "sklearn"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "transformed_target_regressor",
      "sklearn",
      "lbfgs",
      "logistic_regression_statsmodel_summary_table",
      "optimisation_techniques"
    ],
    "summary": "Terms of interest: Also called Scikit-learn. train_test_split X and y are separate things (y is the target variable/column) and X is multiple is columns used..."
  },
  "slowly_changing_dimension": {
    "title": "Slowly Changing Dimension",
    "tags": [
      "database"
    ],
    "aliases": [
      "SCD"
    ],
    "outlinks": [
      "etl",
      "database",
      "dimension_table"
    ],
    "inlinks": [],
    "summary": "A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a Data Warehouse. It is..."
  },
  "small_language_models": {
    "title": "Small Language Models",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": [
      "SLM"
    ],
    "outlinks": [
      "rag",
      "contrastive_decoding",
      "interpretability",
      "distillation",
      "data_synthesis:",
      "model_cascading",
      "edge_machine_learning_models",
      "inference",
      "bert",
      "language_models",
      "llm"
    ],
    "inlinks": [
      "agentic_solutions",
      "scaling_agentic_systems",
      "distillation",
      "language_models"
    ],
    "summary": "[[LLM|LLMs]] dominate many general-purpose NLP tasks, small [[Language Models]] have their own place in specialized tasks, where they excel due to computational efficiency, [[interpretability]], and..."
  },
  "smart_grids": {
    "title": "Smart Grids",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [
      "rl"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Smart Grids Want adaptive grid that can handle the volatility of energy coming on or off. This occurs more often due to the variety of..."
  },
  "smote_(synthetic_minority_over-sampling_technique)": {
    "title": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "tags": [],
    "aliases": [
      "SMOTE"
    ],
    "outlinks": [],
    "inlinks": [
      "imbalanced_datasets_smote.py",
      "imbalanced_datasets"
    ],
    "summary": "SMOTE (Synthetic Minority Over-sampling Technique) Generate synthetic samples for the minority class by interpolating between existing samples. SMOTE: This technique generates synthetic samples for the..."
  },
  "smss": {
    "title": "SMSS",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "microsoft sql server management."
  },
  "snowflake_schema": {
    "title": "Snowflake Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "normalised_schema",
      "star_schema"
    ],
    "inlinks": [
      "types_of_database_schema"
    ],
    "summary": "Snowflake Schema - Description: A more [[Normalised Schema]] normalized form of a star schema where dimension tables are further broken down into additional tables. -..."
  },
  "snowflake": {
    "title": "Snowflake",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_warehouse",
      "cloud"
    ],
    "inlinks": [
      "databricks_vs_snowflake",
      "difference_between_snowflake_to_hadoop"
    ],
    "summary": "Snowflake Architecture: Cloud-Native: Snowflake is a fully managed, cloud-native data warehousing service. It operates entirely on cloud platforms like AWS, Azure, and Google [[Cloud]]. Separation..."
  },
  "soft_deletion": {
    "title": "Soft Deletion",
    "tags": [
      "data_integrity",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "querying",
      "data_integrity",
      "incremental_synchronization",
      "database_schema"
    ],
    "inlinks": [
      "views",
      "database_techniques"
    ],
    "summary": "Soft deletion is a technique used in databases to ==mark records as deleted without physically removing them from the database==. This approach is particularly useful..."
  },
  "software_design_patterns": {
    "title": "Software Design Patterns",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "security",
      "database"
    ],
    "inlinks": [],
    "summary": "10 Design Patterns Explained in 10 Minutes Software Design Patterns What Are Software Design Patterns? Software design patterns provide reusable solutions to common software design..."
  },
  "software_development_life_cycle": {
    "title": "Software Development Life Cycle",
    "tags": [
      "#data_orchestration"
    ],
    "aliases": [
      "sdlc"
    ],
    "outlinks": [
      "devops"
    ],
    "inlinks": [
      "debugging",
      "ci-cd",
      "generative_ai_from_theory_to_practice",
      "data_lifecycle_management"
    ],
    "summary": "A structured approach like the Software Development Life Cycle (SDLC) ensures ==systematic progression through various stages of development==. SDLC remains relevant today by outlining the..."
  },
  "software_development_portal": {
    "title": "Software Development Portal",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "testing",
      "tool.ruff",
      "makefile",
      "documentation_&_meetings",
      "devops",
      "json",
      "tool.uv",
      "toml",
      "justfile"
    ],
    "inlinks": [
      "devops",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Tools: - [[tool.uv]] - [[tool.ruff]] File types: - [[Justfile]] - [[TOML]] - [[Makefile]] - [[Json]] Practices: - [[Testing]] - [[Documentation & Meetings]] Related to: -..."
  },
  "sparsecategorialcrossentropy_or_categoricalcrossentropy": {
    "title": "SparseCategorialCrossentropy or CategoricalCrossEntropy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "loss_function",
      "one-hot_encoding",
      "tensorflow",
      "cross_entropy"
    ],
    "inlinks": [
      "neural_network_in_practice"
    ],
    "summary": "To understand the differences and use cases for SparseCategoricalCrossentropy and CategoricalCrossentropy in [[TensorFlow]], let's break down each one: CategoricalCrossentropy Use Case: This [[loss function]] is..."
  },
  "specificity": {
    "title": "Specificity",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "confusion_matrix",
      "roc_(receiver_operating_characteristic)",
      "evaluation_metrics"
    ],
    "summary": "Specificity, also known as the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. It indicates how well..."
  },
  "spreadsheets_vs_databases": {
    "title": "Spreadsheets vs Databases",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "scalability",
      "data_management"
    ],
    "inlinks": [
      "data_engineering_portal",
      "database"
    ],
    "summary": "Compared to spreadsheets, databases offer: [[Scalability]]: Databases are designed to handle large volumes of data, making them suitable for applications with millions or even billions..."
  },
  "sql_groupby": {
    "title": "SQL Groupby",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying",
      "sql_groupby",
      "sql"
    ],
    "inlinks": [
      "sql_groupby"
    ],
    "summary": "The [[SQL]] GROUP BY clause is used to group rows that have the same values in specified columns into summary rows, like \"total sales per..."
  },
  "sql_injection": {
    "title": "SQL Injection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "querying",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "SQL injection is a code injection technique that targets applications using SQL databases. It occurs when a malicious user injects harmful SQL code into a..."
  },
  "sql_joins": {
    "title": "SQL Joins",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "pasted_image_20250323083319.png"
    ],
    "inlinks": [
      "querying",
      "joining_datasets",
      "database_techniques"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ![[Pasted image 20250323083319.png|800]]"
  },
  "sql_vs_nosql": {
    "title": "SQL vs NoSQL",
    "tags": [
      "#question",
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "nosql"
    ],
    "inlinks": [],
    "summary": "[[NoSQL]]"
  },
  "sql_window_functions": {
    "title": "SQL Window functions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "querying",
      "sql_window_functions"
    ],
    "inlinks": [
      "sql_window_functions"
    ],
    "summary": "SQL Window Functions are a feature in SQL that allow you to perform calculations across a set of table rows that are somehow related to..."
  },
  "sql": {
    "title": "SQL",
    "tags": [
      "software",
      "database",
      "query_language"
    ],
    "aliases": [],
    "outlinks": [
      "querying",
      "database_techniques",
      "database"
    ],
    "inlinks": [
      "sql_groupby",
      "data_engineering_tools",
      "declarative",
      "sqlite",
      "dbt",
      "pyspark",
      "sqlalchemy",
      "unstructured_data",
      "google_cloud_platform"
    ],
    "summary": "Structured Query Language (SQL) is the standard language for interacting with relational databases, enabling efficient data [[Querying]] and manipulation. It serves as a common interface..."
  },
  "sqlalchemy_vs._sqlite3": {
    "title": "SQLAlchemy vs. sqlite3",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sqlite",
      "sqlalchemy"
    ],
    "inlinks": [
      "sqlalchemy"
    ],
    "summary": "SQLAlchemy vs. sqlite3: Which One Should You Use? The choice between [[SQLAlchemy]] and [[SQLite]] depends on your specific needs. Here\u2019s a comparison based on key..."
  },
  "sqlalchemy": {
    "title": "SQLAlchemy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sqlalchemy_vs._sqlite3",
      "sql"
    ],
    "inlinks": [
      "sqlalchemy_vs._sqlite3"
    ],
    "summary": "What is SQLAlchemy? SQLAlchemy is a Python SQL toolkit and ==Object Relational Mapper== (ORM) that provides tools to interact with databases in a more Pythonic..."
  },
  "sqlite_studio": {
    "title": "SQLite Studio",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "sqlite"
    ],
    "summary": ""
  },
  "sqlite": {
    "title": "SQLite",
    "tags": [
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "sqlite_studio",
      "querying",
      "concurrency",
      "sql",
      "database_management_system_(dbms)"
    ],
    "inlinks": [
      "transaction",
      "duckdb_vs_sqlite",
      "database_management_system_(dbms)",
      "views",
      "data_storage",
      "sqlalchemy_vs._sqlite3"
    ],
    "summary": "Lightweight [[Database Management System (DBMS)|DBMS]] used in various applications (phone apps, desktop apps, websites). Note [[SQLite Studio]] exists To get in terminal enter: sqlite3 database.db..."
  },
  "stacking": {
    "title": "Stacking",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "stacking",
      "model_ensemble"
    ],
    "inlinks": [
      "stacking",
      "model_ensemble"
    ],
    "summary": "What is [[Stacking]]?;; is an [[Model Ensemble]] combines predictions of multiple base models ==by training a meta-model== on the outputs of the base models."
  },
  "standard_deviation": {
    "title": "Standard deviation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/outliers",
      "variance",
      "distributions",
      "interpretability"
    ],
    "inlinks": [
      "eda",
      "z-test",
      "t-test"
    ],
    "summary": "Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a set of data values. It indicates how much individual..."
  },
  "standardisation": {
    "title": "Standardisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "principal_component_analysis",
      "gradient_descent",
      "gaussian_distribution",
      "k-nearest_neighbours",
      "data_transformation"
    ],
    "inlinks": [
      "anomaly_detection",
      "normalisation_vs_standardisation",
      "feature_scaling",
      "statistical_tests",
      "normalisation"
    ],
    "summary": "Standardisation is a [[Preprocessing|data preprocessing]] technique used to [[Data Transformation]] features so that they have a mean of 0 and a standard deviation of 1...."
  },
  "star_schema": {
    "title": "Star Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "star_schema"
    ],
    "inlinks": [
      "snowflake_schema",
      "star_schema",
      "dimension_table",
      "types_of_database_schema",
      "dimensional_modelling"
    ],
    "summary": "Star Schema - This schema consists of a central fact table surrounded by dimension tables. The fact table contains quantitative data, while the dimension tables..."
  },
  "statistical_assumptions": {
    "title": "Statistical Assumptions",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "logistic_regression",
      "interpretability",
      "assumption_of_normality",
      "statistical_tests",
      "hypothesis_testing",
      "data_analysis"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_tests",
      "statistical_tests",
      "statistics"
    ],
    "summary": "Statistical assumptions are essential conditions that must be met for various statistical methods and models to produce valid results. Necessary for robustness and reliability of..."
  },
  "statistical_tests": {
    "title": "Statistical Tests",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "estimator",
      "z-test",
      "statistical_assumptions",
      "t-test",
      "standardisation",
      "chi-squared_test",
      "hypothesis_testing",
      "proportion_test"
    ],
    "inlinks": [
      "statistical_assumptions",
      "statistics"
    ],
    "summary": "Statistical tests are methods used to determine if there is a significant difference between groups or if a relationship exists between variables. Each test has..."
  },
  "statistics": {
    "title": "Statistics",
    "tags": [
      "statistics",
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "confidence_interval",
      "statistical_assumptions",
      "type_1_error_and_power",
      "t-test",
      "adaptive_decision_analysis",
      "distributions",
      "tidyverse",
      "parametric_vs_non-parametric_tests",
      "expectation_maximisation_algorithm",
      "r",
      "bootstrap",
      "statistical_tests",
      "hypothesis_testing",
      "multicollinearity",
      "central_limit_theorem",
      "maximum_likelihood_estimation",
      "likelihood_ratio",
      "correlation_vs_causation",
      "proportional_hazard_model",
      "markov_chain",
      "monte_carlo_simulation",
      "logistic_regression",
      "estimator",
      "over_parameterised_models",
      "casual_inference",
      "univariate_vs_multivariate",
      "p_values"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "covariance_structures",
      "ds_&_ml_portal",
      "data_analyst",
      "tf-idf",
      "hypothesis_testing",
      "data_analysis",
      "data_science"
    ],
    "summary": "Statistics want to understand the world. The world is made of probabilities, we model probabilities with functions, and we model functions with parameters. \"Observe data..."
  },
  "stemming": {
    "title": "Stemming",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "normalisation_of_text"
    ],
    "summary": "Shorting words to the key term."
  },
  "stochastic_gradient_descent": {
    "title": "Stochastic Gradient Descent",
    "tags": [],
    "aliases": [
      "SGD"
    ],
    "outlinks": [
      "gradient_descent"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "deep_learning",
      "pytorch",
      "gradient_descent",
      "optimisation_techniques"
    ],
    "summary": "[[Gradient Descent]]"
  },
  "storage_layer_object_store": {
    "title": "storage layer object store",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Object Store"
    ],
    "outlinks": [
      "cloud_providers",
      "s3_bucket"
    ],
    "inlinks": [],
    "summary": "A storage layer or object storage are services from the three big [[Cloud Providers]], AWS S3,[[S3 bucket]] Azure Blob Storage, and Google Cloud Storage. The..."
  },
  "stored_procedures": {
    "title": "Stored Procedures",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "Stored procedures are a way to automate SQL statements, allowing them to be executed repeatedly without rewriting the code. Demonstration with the Boston MFA Database..."
  },
  "strongly_vs_weakly_typed_language": {
    "title": "Strongly vs Weakly typed language",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "java",
      "javascript"
    ],
    "inlinks": [],
    "summary": "A strongly typed programming language is one where ==types== are strictly enforced. This means that once a variable is assigned a type, it cannot be..."
  },
  "structured_data": {
    "title": "structured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "xml",
      "json",
      "database",
      "database_schema"
    ],
    "inlinks": [
      "data_engineering_portal",
      "data_lake",
      "database",
      "semi-structured_data",
      "named_entity_recognition"
    ],
    "summary": "Structured data refers to data that has been formatted into a well-defined schema ([[Database Schema]]). An example would be data that is stored with precisely..."
  },
  "structuring_and_organizing_data": {
    "title": "Structuring and organizing data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "multi-level_index"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Structuring and organizing data. In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/multi_index.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Related terms: - [[Multi-level index]]"
  },
  "summarisation": {
    "title": "Summarisation",
    "tags": [
      "NLP"
    ],
    "aliases": [],
    "outlinks": [
      "extraction",
      "abstraction"
    ],
    "inlinks": [
      "bert",
      "nlp"
    ],
    "summary": "Summarization in NLP Summarization in natural language processing (NLP) is the process of condensing a text document into a shorter version while retaining its main..."
  },
  "supervised_learning": {
    "title": "Supervised Learning",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "support_vector_machines",
      "naive_bayes",
      "machine_learning_algorithms",
      "decision_tree",
      "classification",
      "pasted_image_20241012152141.png",
      "linear_regression",
      "k-nearest_neighbours",
      "regression"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "data_transformation_in_machine_learning",
      "machine_learning_algorithms",
      "decision_tree",
      "backpropagation",
      "classification",
      "feed_forward_neural_network",
      "active_learning",
      "regression",
      "transfer_learning"
    ],
    "summary": "Supervised learning is a type of machine learning where an algorithm learns from ==labeled data== to make predictions or decisions. In supervised learning, the training..."
  },
  "support_vector_classifier_(svc)": {
    "title": "Support Vector Classifier (SVC)",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "support_vector_machines"
    ],
    "inlinks": [],
    "summary": "Overview Support Vector Classifiers (SVCs) are a fundamental component of [[Support Vector Machines|SVM]]s, designed to find the optimal hyperplane that separates data into distinct classes...."
  },
  "support_vector_machines": {
    "title": "Support Vector Machines",
    "tags": [
      "classifier",
      "clustering"
    ],
    "aliases": [
      "SVM"
    ],
    "outlinks": [
      "pasted_image_20240128193838.png",
      "kernelling",
      "classification",
      "pasted_image_20240128193726.png",
      "svm_example.py",
      "ml_tools"
    ],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "logistic_regression",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py",
      "isolated_forest",
      "model_parameters",
      "unsupervised_learning",
      "machine_learning_algorithms",
      "support_vector_regression",
      "kernelling",
      "why_and_when_is_feature_scaling_necessary",
      "classification",
      "supervised_learning",
      "support_vector_classifier_(svc)"
    ],
    "summary": "Support Vector Machines (SVM) are a type of supervised learning algorithm primarily used for [[classification]] tasks, though they can also be adapted for regression. The..."
  },
  "support_vector_regression": {
    "title": "Support Vector Regression",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "support_vector_machines"
    ],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Support Vector Regression use similar principles to [[Support Vector Machines|SVM]]s but for predicting continuous variables."
  },
  "svm_example.py": {
    "title": "SVM_Example.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "support_vector_machines"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main\\Explorations/Build/Classifiers/SVM/SVM_Example.py Overview Objective: To classify Iris flowers using SVM and explore various hyperparameters like kernel type, regularization (C), and gamma. Dataset: The Iris dataset contains..."
  },
  "symbolic_computation": {
    "title": "Symbolic computation",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "mathematical_reasoning_in_transformers",
      "mathematica"
    ],
    "inlinks": [],
    "summary": "[[Mathematical Reasoning in Transformers]] Summary of Wolfram Alpha\u2019s Approach: Uses symbolic computation with precise algorithms. Leverages predefined mathematical rules for various domains. Provides step-by-step solutions..."
  },
  "sympy": {
    "title": "Sympy",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "backpropagation"
    ],
    "summary": ""
  },
  "syntactic_relationships": {
    "title": "syntactic relationships",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "semantic_relationships"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "Syntactic relationships refer to the structural connections between words or phrases in a sentence, focusing on grammar and the arrangement of words. They determine how..."
  },
  "t-sne": {
    "title": "t-SNE",
    "tags": [
      "data_visualization",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "dimensionality_reduction",
      "t-sne",
      "principal_component_analysis",
      "pasted_image_20241015211844.png"
    ],
    "inlinks": [
      "dimensionality_reduction",
      "vector_embedding",
      "t-sne",
      "principal_component_analysis"
    ],
    "summary": "t-SNE (t-distributed Stochastic Neighbor Embedding) is a [[Dimensionality Reduction]] technique used primarily for visualizing high-dimensional data. Unlike methods such as [[Principal Component Analysis|PCA]] (Principal Component..."
  },
  "t-test": {
    "title": "T-test",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard_deviation",
      "variance",
      "distributions",
      "hypothesis_testing"
    ],
    "inlinks": [
      "general_linear_regression",
      "statistics",
      "statistical_tests"
    ],
    "summary": "The T-test is a statistical method ==used to determine if there is a significant difference between the means of two groups, especially when the population..."
  },
  "tableau": {
    "title": "Tableau",
    "tags": [
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "data_visualisation",
      "postgresql"
    ],
    "inlinks": [
      "data_visualisation",
      "semantic_layer",
      "postgresql"
    ],
    "summary": "Next Steps Load a [[PostgreSQL]] database and perform analytics as an example. Resources Tableau How-To Videos Tutorial Link Example Usage Video Features Can publish to..."
  },
  "technical_debt": {
    "title": "Technical Debt",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Technical debt refers to the concept in software development where developers take shortcuts or make suboptimal decisions to spped up the delivery of a project...."
  },
  "technical_design_doc_template": {
    "title": "Technical Design Doc Template",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "[Project name] Design Doc About this doc Metadata about this document. Describe the scope and current status. This doc describes the technical approach, milestones, and..."
  },
  "telecommunications": {
    "title": "Telecommunications",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Network Optimization Overview: In telecommunications, RL is used to enhance network performance, optimize resource allocation, and manage traffic efficiently. Applications: Traffic Management: RL algorithms can..."
  },
  "tensorflow": {
    "title": "Tensorflow",
    "tags": [
      "deep_learning",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "pytorch_vs_tensorflow",
      "handwritten_digit_classification",
      "machine_learning"
    ],
    "inlinks": [
      "pytorch",
      "pytorch_vs_tensorflow",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "deep_learning"
    ],
    "summary": "Open sourced by Google Based on a dataflow graph Text summarization with TensorFlow Open-source library for numerical computation and large-scale [[Machine Learning]], focusing on static..."
  },
  "terminal_commands": {
    "title": "Terminal commands",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "jupyter nbconvert K-Means_VideoGames_Raw.ipynb --to python --no-prompt"
  },
  "test_loss_when_evaluating_models": {
    "title": "Test Loss When Evaluating Models",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "accuracy",
      "loss_function",
      "evaluation_metrics",
      "standardised/outliers",
      "model_selection",
      "hyperparameter_tuning",
      "model_evaluation"
    ],
    "inlinks": [],
    "summary": "Test loss is used for [[Model Evaluation]] to assess how well a model generalizes to unseen data, which is essential for evaluating its performance in..."
  },
  "testing": {
    "title": "Testing",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "types_of_computational_bugs",
      "unittest",
      "model_deployment",
      "python",
      "hypothesis_testing",
      "pytest",
      "common_security_vulnerabilities_in_software_development",
      "continuous_integration"
    ],
    "inlinks": [
      "debugging",
      "software_development_portal",
      "maintainable_code",
      "hypothesis_testing"
    ],
    "summary": "Testing in coding projects refers to the systematic process of evaluating software to ensure it meets specified requirements and functions correctly. It enhances software robustness,..."
  },
  "testing_pytest.py": {
    "title": "Testing_Pytest.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "testing_pytest.py",
      "pytest",
      "ml_tools"
    ],
    "inlinks": [
      "debugging",
      "testing_pytest.py"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_Pytest.py In [[ML_Tools]] see: [[Testing_Pytest.py]] The pytest example script demonstrates several key features of the [[Pytest]] testing framework: Fixtures: The script uses a fixture named..."
  },
  "testing_unittest.py": {
    "title": "Testing_unittest.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pytest"
    ],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_unittest.py To explore testing in Python, let's focus on some key concepts and provide a simple example using the unittest framework, which is a built-in..."
  },
  "text2cypher": {
    "title": "Text2Cypher",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "graphrag",
      "interpretability",
      "cypher",
      "neo4j"
    ],
    "inlinks": [
      "graphrag",
      "how_to_search_within_a_graph"
    ],
    "summary": "Text2Cypher is a concept that allows users to convert natural language queries into Cypher queries, which are used to interact with [[GraphRAG|graph database]] like [[Neo4j]]...."
  },
  "tf-idf": {
    "title": "TF-IDF",
    "tags": [
      "NLP",
      "preprocessing",
      "code_snippet"
    ],
    "aliases": [
      "TFIDF"
    ],
    "outlinks": [
      "recommender_systems",
      "nltk",
      "clustering",
      "statistics",
      "search",
      "tokenisation",
      "bag_of_words"
    ],
    "inlinks": [
      "nlp",
      "cosine_similarity",
      "bag_of_words"
    ],
    "summary": "TF-IDF is a statistical technique used in text analysis to determine the importance of a word in a document relative to a collection of documents..."
  },
  "thinking_systems": {
    "title": "Thinking Systems",
    "tags": [
      "career",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "knowledge_work",
      "scientific_method"
    ],
    "inlinks": [],
    "summary": "A thinking system is a point of view that helps solve a problem. Part of [[Knowledge Work]]. We view problems through the view of our..."
  },
  "time_series_forecasting": {
    "title": "Time Series Forecasting",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "time_series",
      "xgboost",
      "lightgbm",
      "forecasting_baseline.py"
    ],
    "inlinks": [
      "time_series",
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "use_cases_for_a_simple_neural_network_like"
    ],
    "summary": "With [[Time Series]] dataset we often want to predict future terms. These are methods to do so. Resources: TimeSeries Forecasting Statistical Methods [[Forecasting_Baseline.py]] [[Forecasting_Exponential_Smoothing.py]] [[Forecasting_AutoArima.py]]..."
  },
  "time_series_identify_trends_and_patterns": {
    "title": "Time Series Identify Trends and Patterns",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series"
    ],
    "summary": "Analyze long-term trends, seasonal patterns, and cyclical behaviors."
  },
  "time_series": {
    "title": "Time Series",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "anomaly_detection",
      "time_series_identify_trends_and_patterns",
      "time_series_forecasting",
      "ml_tools"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "anomaly_detection_in_time_series",
      "cross_validation",
      "time_series_forecasting",
      "datasets"
    ],
    "summary": "Time series data is a sequence of data points collected or recorded at successive points in time, typically at uniform intervals. It captures the temporal..."
  },
  "tokenisation": {
    "title": "Tokenisation",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "nlp"
    ],
    "inlinks": [
      "tf-idf",
      "generative_ai_from_theory_to_practice",
      "normalisation_of_text"
    ],
    "summary": "Tokenisation is a fundamental process in natural language processing ([[NLP]]) that involves breaking down text into smaller units called tokens. These tokens can be words,..."
  },
  "toml": {
    "title": "TOML",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "tool.ruff",
      "tool.bandit",
      "tool.uv",
      "pytest"
    ],
    "inlinks": [
      "tool.ruff",
      "software_development_portal",
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "A .toml file is a configuration file format that stands for \"Tom's Obvious, Minimal Language.\" It is designed to be easy to read due to..."
  },
  "tool.bandit": {
    "title": "tool.bandit",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bandit_example_nonfixed.py",
      "ml_tools",
      "common_security_vulnerabilities_in_software_development",
      "bandit_example_output"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development",
      "toml"
    ],
    "summary": "Bandit: A Security Linter for Python Resources Bandit Documentation How to Use Bandit Installation To install Bandit, use pip by running the following command in..."
  },
  "tool.ruff": {
    "title": "tool.ruff",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "python",
      "toml"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Ruff is a fast [[Python]] linter and code formatter. It is designed to enforce coding style and catch potential errors in Python code. Ruff aims..."
  },
  "tool.uv": {
    "title": "tool.uv",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "virtual_environments",
      "toml"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Appears in [[TOML]] file Link: https://github.com/astral-sh/uv uv is a tool for managing Python development [[Virtual environments]], projects, and dependencies. It offers a range of features..."
  },
  "train-dev-test_sets": {
    "title": "Train-Dev-Test Sets",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "handling_different_distributions",
      "training_data",
      "model_parameters",
      "evaluation_metrics",
      "validation_data",
      "cross_validation",
      "model_building",
      "distributions",
      "datasets",
      "model_evaluation"
    ],
    "inlinks": [
      "model_building"
    ],
    "summary": "In [[Model Building]] train the model using the prepared data to learn patterns and make predictions. The model is trained on your dataset, which is..."
  },
  "transaction": {
    "title": "Transaction",
    "tags": null,
    "aliases": [
      "Transactions"
    ],
    "outlinks": [
      "acid_transaction",
      "sqlite",
      "queries",
      "transaction",
      "concurrency",
      "data_integrity",
      "database_management_system_(dbms)",
      "granularity"
    ],
    "inlinks": [
      "transaction",
      "query_optimisation",
      "acid_transaction"
    ],
    "summary": "Transactions are used for maintaining [[Data Integrity]] and should adhere to the [[ACID Transaction]]. Transaction Operations Commit: Saves all changes made during the transaction. Rollback:..."
  },
  "transfer_learning": {
    "title": "Transfer Learning",
    "tags": [
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "keras",
      "hugging_face",
      "performance_drift",
      "pytorch",
      "transfer_learning.py",
      "supervised_learning",
      "ml_tools"
    ],
    "inlinks": [
      "deep_learning",
      "bert",
      "transfer_learning.py",
      "llm",
      "imbalanced_datasets"
    ],
    "summary": "Transfer learning is a technique in machine learning that ==leverages knowledge gained from one setting (source domain) to improve performance on a different but related..."
  },
  "transfer_learning.py": {
    "title": "transfer_learning.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "transfer_learning"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Neural_Network/transfer_learning.py For deep learning, to do [[Transfer Learning]] we take out and replace a few end layers of the network. We can then train just..."
  },
  "transformed_target_regressor": {
    "title": "Transformed Target Regressor",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "sklearn"
    ],
    "inlinks": [],
    "summary": "[[Sklearn]] The TransformedTargetRegressor is a utility class in scikit-learn that applies a transformation to the target values in a regression problem. This can be useful..."
  },
  "transformer": {
    "title": "Transformer",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "aliases": [
      "Transformers"
    ],
    "outlinks": [
      "multi-head_attention",
      "recurrent_neural_networks",
      "unsupervised_learning",
      "attention_mechanism",
      "transformers_vs_rnns",
      "bert",
      "standardised/attention_is_all_you_need",
      "feed_forward_neural_network",
      "nlp"
    ],
    "inlinks": [
      "word2vec.py",
      "ds_&_ml_portal",
      "rag",
      "recurrent_neural_networks",
      "attention_mechanism",
      "hugging_face",
      "transformers_vs_rnns",
      "types_of_neural_networks",
      "bert",
      "lstm",
      "mathematical_reasoning_in_transformers",
      "llm"
    ],
    "summary": "A transformer in machine learning (ML) refers to a deep learning model architecture designed to process sequential data, such as natural language processing ([[NLP]]). It..."
  },
  "transformers_vs_rnns": {
    "title": "Transformers vs RNNs",
    "tags": [
      "deep_learning"
    ],
    "aliases": null,
    "outlinks": [
      "reformer",
      "recurrent_neural_networks",
      "attention_mechanism",
      "transformer",
      "transformers_vs_rnns",
      "time-series_data",
      "bert",
      "vanishing_and_exploding_gradients_problem",
      "lstm",
      "gated_recurrent_units",
      "longformer"
    ],
    "inlinks": [
      "transformer",
      "transformers_vs_rnns"
    ],
    "summary": "[[Transformer|Transformers]] and Recurrent Neural Networks ([[Recurrent Neural Networks]]) are both deep learning architectures ==used for processing sequential data==, but they differ significantly in structure, operation,..."
  },
  "ts_anomaly_detection": {
    "title": "TS_Anomaly_Detection",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "ts_anomaly_detection.py": {
    "title": "TS_Anomaly_Detection.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_in_time_series"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/TS_Anomaly_Detection.py"
  },
  "turning_a_flat_file_into_a_database": {
    "title": "Turning a flat file into a database",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "'order_id',_'order_date',_'customer_id',_'amount'",
      "foreign_key",
      "'customer_id',_'customer_name',_'contact_name',_'country'"
    ],
    "inlinks": [
      "melt",
      "data_engineering_portal",
      "database"
    ],
    "summary": "Summary: Read and Clean the Data: Load the data from the Excel sheet and clean it. Split the Data: Separate the data into two DataFrames,..."
  },
  "types_of_computational_bugs": {
    "title": "Types of Computational Bugs",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "debugging"
    ],
    "inlinks": [
      "debugging",
      "testing"
    ],
    "summary": "Each of these types of bugs can have significant impacts on software functionality and performance, and understanding them is crucial for effective [[Debugging]] and software..."
  },
  "types_of_database_schema": {
    "title": "Types of Database Schema",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "snowflake_schema",
      "er_diagrams",
      "normalised_schema",
      "star_schema",
      "columnar_storage"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "There are several types of database schemas commonly used in data warehousing and database design. [[Star Schema]] [[Snowflake Schema]] Galaxy Schema (or Fact Constellation Schema):..."
  },
  "types_of_neural_networks": {
    "title": "Types of Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "convolutional_neural_networks",
      "recurrent_neural_networks",
      "neural_network",
      "generative_adversarial_networks",
      "transformer",
      "feed_forward_neural_network"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "Types of [[Neural network]]: [[Feed Forward Neural Network]] [[Convolutional Neural Networks]] [[Recurrent Neural Networks]] [[Generative Adversarial Networks]] [[Transformer]]"
  },
  "typescript": {
    "title": "TypeScript",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "debugging",
      "data_validation"
    ],
    "summary": "Superset of JavaScript adding static typing and object-oriented features for building large-scale applications."
  },
  "typical_output_formats_in_neural_networks": {
    "title": "Typical Output Formats in Neural Networks",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "generative_ai",
      "loss_function",
      "binary_classification",
      "neural_network",
      "classification",
      "activation_function",
      "regression"
    ],
    "inlinks": [],
    "summary": "The output format of a [[Neural network]] is largely determined by the specific task it is designed to perform. Classification [[Binary Classification]] Single Output Node:..."
  },
  "ubuntu": {
    "title": "Ubuntu",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux"
    ],
    "summary": "Ubuntu is a popular open-source operating system based on the [[Linux]] kernel. It is designed to be user-friendly: Desktop Environment: Ubuntu provides a graphical user..."
  },
  "uml": {
    "title": "UML",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.drawio.com/ https://www.reddit.com/r/SoftwareEngineering/comments/133iw7n/is_there_any_free_handy_tool_to_create_uml/ https://plantuml.com/"
  },
  "unittest": {
    "title": "unittest",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "testing"
    ],
    "summary": "@patch (from unittest.mock) Explanation @patch is used to replace objects/functions with mock versions during tests. It is part of Python\u2019s unittest.mock module. Example & Usage..."
  },
  "univariate_vs_multivariate": {
    "title": "univariate vs multivariate",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Single feature versus multiple features"
  },
  "unstructured_data": {
    "title": "unstructured data",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "sql"
    ],
    "inlinks": [
      "data_lake",
      "vector_database",
      "data_science"
    ],
    "summary": "[!Important] Unstructured data is data that does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used..."
  },
  "unsupervised_learning": {
    "title": "Unsupervised Learning",
    "tags": [
      "#clustering",
      "field"
    ],
    "aliases": [
      "unsupervised"
    ],
    "outlinks": [
      "support_vector_machines",
      "isolated_forest",
      "standardised/outliers",
      "principal_component_analysis",
      "dimensionality_reduction",
      "k-means",
      "dbscan",
      "k-nearest_neighbours",
      "clustering"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "transformer",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "k-means",
      "clustering",
      "learning_styles"
    ],
    "summary": "Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or predefined outputs. Unsupervised learning involves discovering..."
  },
  "untitled": {
    "title": "Untitled",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "use_cases_for_a_simple_neural_network_like": {
    "title": "Use Cases for a Simple Neural Network Like",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "binary_classification",
      "neural_network",
      "time_series_forecasting",
      "regression"
    ],
    "inlinks": [
      "pytorch"
    ],
    "summary": "Scenarios where a simple [[Neural network|Neural Network]] work like this might be useful: [[Regression]] with Multiple Features If you have multiple input features and you..."
  },
  "use_of_rnns_in_energy_sector": {
    "title": "Use of RNNs in energy sector",
    "tags": [
      "time_series",
      "deep_learning",
      "energy",
      "anomaly_detection"
    ],
    "aliases": [],
    "outlinks": [
      "gradient_boosting",
      "demand_forecasting",
      "shapley_additive_explanations",
      "machine_learning_algorithms"
    ],
    "inlinks": [
      "recurrent_neural_networks"
    ],
    "summary": "For energy data problems, many interpretable machine learning algorithms can be applied in place of or alongside RNNs. These models offer transparency, making it easier..."
  },
  "utilities": {
    "title": "Utilities",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "vacuum": {
    "title": "Vacuum",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": ""
  },
  "vanishing_and_exploding_gradients_problem": {
    "title": "vanishing and exploding gradients problem",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "vanishing_and_exploding_gradients_problem",
      "recurrent_neural_networks"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "recurrent_neural_networks",
      "batch_normalisation",
      "transformers_vs_rnns",
      "backpropagation",
      "vanishing_and_exploding_gradients_problem",
      "lstm",
      "forward_propagation"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]] [[vanishing and exploding gradients problem]] In standard RNNs, the difficulty lies in retaining useful information over long sequences due to the exponential..."
  },
  "variance": {
    "title": "Variance",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "distributions",
      "boxplot"
    ],
    "inlinks": [
      "feature_selection",
      "regression_metrics",
      "standard_deviation",
      "covariance_structures",
      "principal_component_analysis",
      "t-test",
      "data_reduction"
    ],
    "summary": "Variance in a dataset is a statistical measure that represents the degree of spread or dispersion of the data points around the mean (average) of..."
  },
  "vector_database": {
    "title": "Vector Database",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "langchain",
      "semantic_search",
      "vector_embedding",
      "cosine_similarity",
      "llm",
      "standardised/vector_embedding",
      "unstructured_data"
    ],
    "inlinks": [
      "relationships_in_memory"
    ],
    "summary": "Overview Vector databases are specialized systems designed to handle and manage [[Vector Embedding]]. As most real-world data is unstructured, such as text, images, and audio,..."
  },
  "vector_embedding": {
    "title": "Vector Embedding",
    "tags": [
      "math",
      "language_models",
      "drafting"
    ],
    "aliases": [
      "embedding",
      "word embedding"
    ],
    "outlinks": [
      "how_to_search_within_a_graph",
      "pasted_image_20241015211844.png",
      "attention_mechanism",
      "semantic_relationships",
      "pytorch",
      "pasted_image_20241015211934.png",
      "bert",
      "dimensionality_reduction",
      "vector_embedding.py",
      "language_models",
      "t-sne",
      "how_would_you_decide_between_using_tf-idf_and_word2vec_for_text_vectorization",
      "ml_tools"
    ],
    "inlinks": [
      "bert",
      "how_llms_store_facts",
      "vector_database"
    ],
    "summary": "Vector Embedding is a technique used in machine learning and natural language processing to represent data in a continuous vector space. This representation captures the..."
  },
  "vectorisation": {
    "title": "Vectorisation",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "pasted_image_20241217204829.png",
      "numpy",
      "gpu"
    ],
    "inlinks": [],
    "summary": "Link ![[Pasted image 20241217204829.png|500]] Numpy dot is better than for loop and summing. Why does it run faster? A: Designed to run in parallel Sequentially..."
  },
  "vectorized_engine": {
    "title": "Vectorized Engine",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "columnar_storage",
      "duckdb"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Vectorized Engine A modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of..."
  },
  "vector_embedding.py": {
    "title": "Vector_Embedding.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "vector_embedding"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Vector_Embedding.py Explanation of the Script Vocabulary and Embedding Layer: Terms are mapped to indices using a dictionary. The embedding layer learns continuous vector representations for..."
  },
  "vercel": {
    "title": "Vercel",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "view_use_case": {
    "title": "View Use Case",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "views"
    ],
    "summary": "View Use Case Scenario A company wants to generate monthly performance reports for its employees. The performance data is spread across multiple tables, including employees,..."
  },
  "views": {
    "title": "Views",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "de_tools",
      "sqlite",
      "querying",
      "common_table_expression",
      "view_use_case",
      "soft_deletion",
      "database_schema"
    ],
    "inlinks": [
      "common_table_expression"
    ],
    "summary": "Views are virtual tables defined by SQL [[Querying|Query]] that ==simplify complex data representation.== They can remove unnecessary columns, aggregate results, partition data, and secure sensitive..."
  },
  "violin_plot": {
    "title": "Violin plot",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "boxplot"
    ],
    "inlinks": [
      "distributions"
    ],
    "summary": "An extension of a [[Boxplot]] showing the data distribution. Useful when comparing distributions, skewness. python data = [...] # Your data sns.violinplot(data=data, color=\"purple\", fill=\"lightblue\", scale=\"area\")..."
  },
  "virtual_environments": {
    "title": "Virtual environments",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "poetry"
    ],
    "inlinks": [
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "Setting up virtual env For windows (need to not be in a venv before del) cmd rmdir /s /q venv python -m venv venv venv\\Scripts\\activate..."
  },
  "wcss_and_elbow_method": {
    "title": "WCSS and elbow method",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters",
      "k-means"
    ],
    "summary": "USE: WCSS (within-cluster sum of squares) WCSS is a measure developed within the ANOVA framework. It gives a very good idea about the different distance..."
  },
  "weak_learners": {
    "title": "Weak Learners",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "learning_rate",
      "decision_tree",
      "model_ensemble",
      "hyperparameter"
    ],
    "inlinks": [
      "gradient_boosting",
      "boosting"
    ],
    "summary": "Weak learners are simple models that perform slightly better than random guessing. They are often used as the building blocks in [[Model Ensemble]] methods to..."
  },
  "web_feature_server_(wfs)": {
    "title": "Web Feature Server (WFS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Feature Server (WFS) Purpose: WFS is designed to serve raw geographic features (vector data) over the web. Functionality: - Feature-Based: It delivers geographic..."
  },
  "web_map_tile_service_(wmts)": {
    "title": "Web Map Tile Service (WMTS)",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "gis"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Map Tile Service (WMTS) Purpose: WMTS is designed to serve pre-rendered, cached image tiles of maps. Functionality: - Tile-Based: It serves map images..."
  },
  "what_algorithms_or_models_are_used_within_the_energy_sector": {
    "title": "What algorithms or models are used within the energy sector",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": ""
  },
  "what_algorithms_or_models_are_used_within_the_telecommunication_sector": {
    "title": "What algorithms or models are used within the telecommunication sector",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": ""
  },
  "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts": {
    "title": "What are the best practices for evaluating the effectiveness of different prompts",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "prompt_engineering"
    ],
    "summary": ""
  },
  "what_can_abm_solve_within_the_energy_sector": {
    "title": "What can ABM solve within the energy sector",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "agent-based_modelling"
    ],
    "inlinks": [],
    "summary": "[[Agent-Based Modelling]] energy systems analysis"
  },
  "what_is_the_difference_between_odds_and_probability": {
    "title": "What is the difference between odds and probability",
    "tags": [
      "#question",
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression_does_not_predict_probabilities"
    ],
    "summary": ""
  },
  "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models.": {
    "title": "What is the role of gradient-based optimization in training deep learning models.",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "when_and_why_not_to_us_regularisation": {
    "title": "When and why not to us regularisation",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "validation_data",
      "training_data"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "While regularization is tool to combat overfitting, it is not a always useful. It is crucial to consider the model's - complexity, - the quality..."
  },
  "why_and_when_is_feature_scaling_necessary": {
    "title": "Why and when is feature scaling necessary",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "support_vector_machines",
      "decision_tree",
      "random_forests",
      "feature_scaling",
      "k-means"
    ],
    "inlinks": [],
    "summary": "[[Feature Scaling]] is useful for models that use distances like [[Support Vector Machines|SVM]] and [[K-means]] When Scaling Is Unnecessary Tree-based Algorithms: Algorithms like [[Decision Tree]],..."
  },
  "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy": {
    "title": "Why does increasing the number of models in a ensemble not necessarily improve the accuracy",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model_ensemble"
    ],
    "inlinks": [],
    "summary": "Increasing the number of models in an ensemble ([[Model Ensemble]]) does not always lead to improved accuracy due to several limiting factors: Convergence of Predictions:..."
  },
  "why_does_label_encoding_give_different_predictions_from_one-hot_encoding": {
    "title": "Why does label encoding give different predictions from one-hot encoding",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Label Encoding and One-Hot Encoding give different predictions because they represent categorical variables in fundamentally different ways. Label Encoding might cause issues by implying an..."
  },
  "why_does_the_adam_optimizer_converge": {
    "title": "Why does the Adam Optimizer converge",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "adam_optimizer"
    ],
    "summary": "Why the Adam Optimizer Converges The Adam optimizer is able to efficiently handle sparse gradients and adaptively adjust learning rates. The convergence of Adam, often..."
  },
  "why_is_named_entity_recognition_(ner)_a_challenging_task": {
    "title": "Why is named entity recognition (NER) a challenging task",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Named Entity Recognition (NER) is considered a challenging task for several reasons: Ambiguity: Entities can be ambiguous, meaning the same word or phrase can refer..."
  },
  "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes": {
    "title": "Why is the Central Limit Theorem important when working with small sample sizes",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hypothesis_testing",
      "distributions",
      "central_limit_theorem",
      "assumption_of_normality"
    ],
    "inlinks": [
      "central_limit_theorem"
    ],
    "summary": "The [[Central Limit Theorem]] (CLT) is particularly important for data scientists working with small sample sizes. It enables the use of various statistical methods, and..."
  },
  "why_json_is_better_than_pickle_for_untrusted_data": {
    "title": "Why JSON is Better than Pickle for Untrusted Data",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pickle"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "JSON vs. [[Pickle]]: Security: JSON: JSON is a text-based data format that is inherently safer for handling untrusted data. It ==only== supports basic data types..."
  },
  "why_type_1_and_type_2_matter": {
    "title": "Why Type 1 and Type 2 matter",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pasted_image_20250312064809.png"
    ],
    "inlinks": [
      "evaluation_metrics"
    ],
    "summary": "Type I and Type II errors are used in evaluating the performance of classification models, and understanding their differences is essential for interpreting model results..."
  },
  "why_use_er_diagrams": {
    "title": "Why use ER diagrams",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data_cleansing",
      "er_diagrams",
      "data_quality",
      "normalised_schema",
      "why_use_er_diagrams"
    ],
    "inlinks": [
      "why_use_er_diagrams",
      "er_diagrams"
    ],
    "summary": "[[Why use ER diagrams]] Cleaning a dataset before creating an [[ER Diagrams]] is crucial for ensuring accuracy and reliability in your database design [[Data Quality]]:..."
  },
  "wikipedia_api.py": {
    "title": "Wikipedia_API.py",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "graphrag",
      "api"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Wikipedia_API.py"
  },
  "windows_subsystem_for_linux": {
    "title": "Windows Subsystem for Linux",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "linux",
      "ubuntu",
      "windows_subsystem_for_linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux",
      "powershell_vs_bash"
    ],
    "summary": "[[Windows Subsystem for Linux]] (WSL) is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows 11. It allows users to..."
  },
  "word2vec": {
    "title": "Word2vec",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "word2vec.py",
      "neural_network",
      "semantic_relationships",
      "syntactic_relationships",
      "standardised/vector_embedding",
      "bag_of_words",
      "ml_tools"
    ],
    "inlinks": [
      "word2vec.py"
    ],
    "summary": "Word2Vec is a technique for generating vector representations of words. Developed by researchers at Google, it uses a shallow [[neural network]] to produce [[standardised/Vector Embedding|word..."
  },
  "word2vec.py": {
    "title": "Word2Vec.py",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "bert",
      "transformer",
      "word2vec",
      "cosine_similarity"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Word2Vec.py The script can benefit from Word2Vec embeddings by replacing the randomly initialized embeddings with pretrained or trained embeddings generated using Word2Vec. These embeddings provide..."
  },
  "wrapper_methods": {
    "title": "Wrapper Methods",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "feature_selection",
      "filter_method",
      "model_evaluation"
    ],
    "inlinks": [
      "feature_selection",
      "embedded_methods"
    ],
    "summary": "Used in [[Feature Selection]]. Wrapper methods are powerful because they directly optimize the performance of the machine learning model by selecting the most informative subset..."
  },
  "xgboost": {
    "title": "XGBoost",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "XGM"
    ],
    "outlinks": [
      "gradient_boosting",
      "regularisation",
      "model_ensemble",
      "loss_function",
      "handling_missing_data",
      "interpretability",
      "learning_rate",
      "gradient_descent",
      "decision_tree",
      "hyperparameter_tuning"
    ],
    "inlinks": [
      "gradient_boosting",
      "ds_&_ml_portal",
      "lightgbm_vs_xgboost_vs_catboost",
      "optuna",
      "boosting",
      "time_series_forecasting",
      "feature_importance"
    ],
    "summary": "XGBoost (eXtreme Gradient Boosting) is a highly efficient and flexible implementation of [[Gradient Boosting]] that is widely used for its accuracy and performance in machine..."
  },
  "yaml": {
    "title": "yaml",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "json"
    ],
    "inlinks": [
      "json_to_yaml"
    ],
    "summary": "Stands for YAML ain't markup language and is a superset of JSON lists begin with a hyphen dependent on whitespace / indentation better suited for..."
  },
  "z-normalisation": {
    "title": "Z-Normalisation",
    "tags": null,
    "aliases": [
      "Z-Score"
    ],
    "outlinks": [
      "pasted_image_20241224091157.png",
      "learning_rate",
      "machine_learning_algorithms",
      "gradient_descent",
      "pasted_image_20241224091151.png",
      "pasted_image_20241224091007.png"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods",
      "z-normalisationz-score",
      "normalisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_z_score.py Z-normalisation, also known as z-score normalization, is a technique used to standardize the range of independent variables or features of data. This process is..."
  },
  "z-normalisationz-score": {
    "title": "Z-NormalisationZ-Score",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "z-normalisation"
    ],
    "inlinks": [],
    "summary": "[[Z-Normalisation|Z-Score]] - Formula: $Z = \\frac{(X - \\mu)}{\\sigma}$ - $X$: Data point - $\\mu$: Mean of the dataset - $\\sigma$: Standard deviation of the dataset..."
  },
  "z-test": {
    "title": "Z-Test",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard_deviation",
      "central_limit_theorem"
    ],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "The Z-test is a statistical method used to determine if there is a ==significant difference between the means of two groups or to compare a..."
  }
}