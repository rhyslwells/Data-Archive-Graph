{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e31532",
   "metadata": {},
   "source": [
    "1) Given a single note return TF-IDF score for top words.\n",
    "2) I plan to integrate these top scores into the vault_index.json and produce an enhanced_vault_index.json file.\n",
    "3) Using the scores within the enhanced_vault_index.json possiblely cluster or classify the documents afterward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932e314",
   "metadata": {},
   "source": [
    "## 1) Given a single note return TF-IDF score for top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb2a7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "031a06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    return [\n",
    "        lemmatizer.lemmatize(t)\n",
    "        for t in tokens if t not in stop_words and len(t) > 2\n",
    "    ]\n",
    "\n",
    "def strip_yaml(text):\n",
    "    \"\"\"Remove YAML front matter from a Markdown document.\"\"\"\n",
    "    return re.sub(r\"^---.*?---\\s*\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "def read_markdown_file(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = strip_yaml(text)\n",
    "        html = markdown(text)\n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05f404fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "filename = \"Views.md\"\n",
    "VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "file_path = VAULT_PATH / filename\n",
    "\n",
    "# === TEXT PREPROCESSING ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91e2d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RhysL\\Desktop\\Data-Archive-Graph\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === MAIN TF-IDF PIPELINE ===\n",
    "document = read_markdown_file(file_path)\n",
    "corpus = [document]  # Single-document corpus\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=normalize_document)\n",
    "X_counts = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_counts)\n",
    "\n",
    "# === OUTPUT RESULTS ===\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "scores = X_tfidf[0].T.toarray().flatten()\n",
    "terms_scores = [(feature_names[i], score) for i, score in enumerate(scores) if score > 0]\n",
    "sorted_terms = sorted(terms_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "741a7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF Terms in Views.md:\n",
      "           view: 0.5551\n",
      "           data: 0.4362\n",
      "          table: 0.1983\n",
      "         access: 0.1586\n",
      "          query: 0.1586\n",
      "           user: 0.1586\n",
      "        complex: 0.1190\n",
      "       database: 0.1190\n",
      "    performance: 0.1190\n",
      "         result: 0.1190\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop TF-IDF Terms in {filename}:\")\n",
    "for term, score in sorted_terms[:10]:  # Top 20 terms\n",
    "    print(f\"{term:>15}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70d785",
   "metadata": {},
   "source": [
    "### 2) Integrate these top scores into the vault_index.json and produce an enhanced_vault_index.json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8da60f",
   "metadata": {},
   "source": [
    "Batch Processing with TF-IDF Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "beac3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b54666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "MD_FILES = list(VAULT_PATH.glob(\"*.md\"))\n",
    "JSON_PATH = \"vault_index.json\"\n",
    "OUTPUT_PATH = \"enhanced_vault_index.json\"\n",
    "\n",
    "# === TEXT PREPROCESSING ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20229374",
   "metadata": {},
   "source": [
    "Loads vault_index.json first to identify which .md files to process.\n",
    "\n",
    "Reads only the corresponding Markdown files from disk (not the entire folder).\n",
    "\n",
    "Extracts main content (excluding YAML).\n",
    "\n",
    "Computes top 10 TF-IDF scores per file.\n",
    "\n",
    "Inserts these scores as a new key \"TFIDF_Score\" in the vault_index object.\n",
    "\n",
    "Writes an enhanced version to enhanced_vault_index.json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f444e",
   "metadata": {},
   "source": [
    "The titles in vault_index.json are the titles/aliases \"What is Apache Airflow?\" from metadata and not the title from the file. \n",
    "\n",
    "This is an issue with build_vault_index.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf846e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt:\n",
    "\n",
    "### 2) Integrate the top 10 scores into the vault_index.json and produce an enhanced_vault_index.json file.\n",
    "\n",
    "In the file vault_index.json Which is of the form:\n",
    "\n",
    "{\n",
    "  \"1-on-1_template\": {\n",
    "    \"title\": \"1-on-1 Template\",\n",
    "    \"tags\": [],\n",
    "    \"aliases\": [],\n",
    "    \"outlinks\": [],\n",
    "    \"inlinks\": [\n",
    "      \"documentation_&_meetings\"\n",
    "    ],\n",
    "    \"summary\": \"Decisions [Your name] add decisions that need to be made [Other person's name] add decisions that need to be made Action items [Your name] add...\"\n",
    "  },similar keys...}\n",
    "\n",
    "# Where the key is the filename (just like how we did for View.md) and the value is a dictionary of the title, tags, aliases, outlinks, inlinks and summary.\n",
    "# When we go to add these top score we should add a new key to this dictionary called TFIDF_Score where the value will be the \n",
    "# top 10 scores as a dict. The key will be the term and the value will be the score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcad479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: READ ALL DOCUMENTS ===\n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for md_file in MD_FILES:\n",
    "    text = read_markdown_file(md_file)\n",
    "    corpus.append(text)\n",
    "    filenames.append(md_file.stem)  # filename without \".md\"\n",
    "\n",
    "# === STEP 2: COMPUTE TF-IDF ===\n",
    "vectorizer = CountVectorizer(tokenizer=normalize_document)\n",
    "X_counts = vectorizer.fit_transform(corpus)\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_counts)\n",
    "\n",
    "# === STEP 3: LOAD vault_index.json ===\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    vault_index = json.load(f)\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Normalize title by converting to lowercase and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "# apply normalize_title to all terms in filenames\n",
    "normalized_filenames = [normalize_title(filename) for filename in filenames]\n",
    "\n",
    "normalized_filenames and vault_index_l are the same\n",
    "\n",
    "# === STEP 4: ADD TF-IDF SCORES ===\n",
    "vault_index_l=list(vault_index.keys())\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "skipped = 0\n",
    "for i, filename in enumerate(normalized_filenames):\n",
    "    if filename not in vault_index_l:\n",
    "        # print(f\"Skipping: {filename} not in vault_index.json\")\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    scores = X_tfidf[i].T.toarray().flatten()\n",
    "    terms_scores = [(feature_names[j], float(scores[j])) for j in range(len(scores)) if scores[j] > 0]\n",
    "    top_10_scores = dict(sorted(terms_scores, key=lambda x: -x[1])[:10])\n",
    "\n",
    "    vault_index[filename][\"TFIDF_Score\"] = top_10_scores\n",
    "\n",
    "print(\"number of terms skipped\", skipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff02e64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-on-1 Template',\n",
       " 'AB testing',\n",
       " 'Accessing Gen AI generated content',\n",
       " 'Accuracy',\n",
       " 'ACID Transaction',\n",
       " 'Activation atlases',\n",
       " 'Activation Function',\n",
       " 'Active Learning',\n",
       " 'Ada boosting',\n",
       " 'Adam Optimizer']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2536e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Normalize title by converting to lowercase and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "# apply normalize_title to all terms in filenames\n",
    "normalized_filenames = [normalize_title(filename) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b959340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECK:\n",
    "# normalized_filenames[:10]\n",
    "# # list(vault_index.keys())==normalized_filenames\n",
    "\n",
    "# vault_filenames = list(vault_index.keys())\n",
    "\n",
    "# # show those that are different\n",
    "# l=[]\n",
    "# for i in range(len(vault_filenames)):\n",
    "#     if vault_filenames[i] != normalized_filenames[i]:\n",
    "#         # print(f\"{vault_filenames[i]} != {normalized_filenames[i]}\")\n",
    "#         l.append((vault_filenames[i], normalized_filenames[i]))\n",
    "# len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b023cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-on-1_template',\n",
       " 'ab_testing',\n",
       " 'accessing_gen_ai_generated_content',\n",
       " 'accuracy',\n",
       " 'acid_transaction',\n",
       " 'activation_atlases',\n",
       " 'activation_function',\n",
       " 'active_learning',\n",
       " 'ada_boosting',\n",
       " 'adam_optimizer']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vault_index_l=list(vault_index.keys())\n",
    "len(vault_index_l)\n",
    "vault_index_l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "960b80f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-on-1 Template',\n",
       " 'AB testing',\n",
       " 'Accessing Gen AI generated content',\n",
       " 'Accuracy',\n",
       " 'ACID Transaction',\n",
       " 'Activation atlases',\n",
       " 'Activation Function',\n",
       " 'Active Learning',\n",
       " 'Ada boosting',\n",
       " 'Adam Optimizer']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of terms skipped 770\n"
     ]
    }
   ],
   "source": [
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Normalize title by converting to lowercase and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "# apply normalize_title to all terms in filenames\n",
    "normalized_filenames = [normalize_title(filename) for filename in filenames]\n",
    "\n",
    "normalized_filenames and vault_index_l are the same\n",
    "\n",
    "# === STEP 4: ADD TF-IDF SCORES ===\n",
    "vault_index_l=list(vault_index.keys())\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "skipped = 0\n",
    "for i, filename in enumerate(normalized_filenames):\n",
    "    if filename not in vault_index_l:\n",
    "        # print(f\"Skipping: {filename} not in vault_index.json\")\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    scores = X_tfidf[i].T.toarray().flatten()\n",
    "    terms_scores = [(feature_names[j], float(scores[j])) for j in range(len(scores)) if scores[j] > 0]\n",
    "    top_10_scores = dict(sorted(terms_scores, key=lambda x: -x[1])[:10])\n",
    "\n",
    "    vault_index[filename][\"TFIDF_Score\"] = top_10_scores\n",
    "\n",
    "print(\"number of terms skipped\", skipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Only 27 terms have TFIDF_Score, see dagster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === STEP 5: SAVE TO enhanced_vault_index.json ===\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vault_index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Enhanced vault index written to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d45d1d",
   "metadata": {},
   "source": [
    "### 3) Using the scores within the enhanced_vault_index.json possiblely cluster or classify the documents afterward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dcc49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
