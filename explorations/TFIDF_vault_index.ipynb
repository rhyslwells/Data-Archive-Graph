{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e31532",
   "metadata": {},
   "source": [
    "1) Given a single note return TF-IDF score for top words.\n",
    "2) I plan to integrate these top scores into the vault_index.json and produce an enhanced_vault_index.json file.\n",
    "3) Using the scores within the enhanced_vault_index.json possiblely cluster or classify the documents afterward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932e314",
   "metadata": {},
   "source": [
    "## 1) Given a single note return TF-IDF score for top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2a7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_document(doc):\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    return [\n",
    "        lemmatizer.lemmatize(t)\n",
    "        for t in tokens if t not in stop_words and len(t) > 2\n",
    "    ]\n",
    "\n",
    "def strip_yaml(text):\n",
    "    \"\"\"Remove YAML front matter from a Markdown document.\"\"\"\n",
    "    return re.sub(r\"^---.*?---\\s*\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "def read_markdown_file(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = strip_yaml(text)\n",
    "        html = markdown(text)\n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f404fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "filename = \"Views.md\"\n",
    "VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "file_path = VAULT_PATH / filename\n",
    "\n",
    "# === TEXT PREPROCESSING ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN TF-IDF PIPELINE ===\n",
    "document = read_markdown_file(file_path)\n",
    "corpus = [document]  # Single-document corpus\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=normalize_document)\n",
    "X_counts = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_counts)\n",
    "\n",
    "# === OUTPUT RESULTS ===\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "scores = X_tfidf[0].T.toarray().flatten()\n",
    "terms_scores = [(feature_names[i], score) for i, score in enumerate(scores) if score > 0]\n",
    "sorted_terms = sorted(terms_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741a7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF Terms in Views.md:\n",
      "           view: 0.5551\n",
      "           data: 0.4362\n",
      "          table: 0.1983\n",
      "         access: 0.1586\n",
      "          query: 0.1586\n",
      "           user: 0.1586\n",
      "        complex: 0.1190\n",
      "       database: 0.1190\n",
      "    performance: 0.1190\n",
      "         result: 0.1190\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop TF-IDF Terms in {filename}:\")\n",
    "for term, score in sorted_terms[:10]:  # Top 20 terms\n",
    "    print(f\"{term:>15}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70d785",
   "metadata": {},
   "source": [
    "### 2) Integrate these top scores into the vault_index.json and produce an enhanced_vault_index.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf846e96",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2603132881.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mIn the file vault_index.json\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Prompt:\n",
    "\n",
    "### 2) Integrate the top 10 scores into the vault_index.json and produce an enhanced_vault_index.json file.\n",
    "\n",
    "In the file vault_index.json Which is of the form:\n",
    "\n",
    "{\n",
    "  \"1-on-1_template\": {\n",
    "    \"title\": \"1-on-1 Template\",\n",
    "    \"tags\": [],\n",
    "    \"aliases\": [],\n",
    "    \"outlinks\": [],\n",
    "    \"inlinks\": [\n",
    "      \"documentation_&_meetings\"\n",
    "    ],\n",
    "    \"summary\": \"Decisions [Your name] add decisions that need to be made [Other person's name] add decisions that need to be made Action items [Your name] add...\"\n",
    "  },similar keys...}\n",
    "\n",
    "# Where the key is the filename (just like how we did for View.md) and the value is a dictionary of the title, tags, aliases, outlinks, inlinks and summary.\n",
    "# When we go to add these top score we should add a new key to this dictionary called TFIDF_Score where the value will be the \n",
    "# top 10 scores as a dict. The key will be the term and the value will be the score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8da60f",
   "metadata": {},
   "source": [
    "Batch Processing with TF-IDF Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beac3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "MD_FILES = list(VAULT_PATH.glob(\"*.md\"))\n",
    "JSON_PATH = \"vault_index.json\"\n",
    "OUTPUT_PATH = \"enhanced_vault_index.json\"\n",
    "\n",
    "# === TEXT PREPROCESSING ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20229374",
   "metadata": {},
   "source": [
    "Loads vault_index.json first to identify which .md files to process.\n",
    "\n",
    "Reads only the corresponding Markdown files from disk (not the entire folder).\n",
    "\n",
    "Extracts main content (excluding YAML).\n",
    "\n",
    "Computes top 10 TF-IDF scores per file.\n",
    "\n",
    "Inserts these scores as a new key \"TFIDF_Score\" in the vault_index object.\n",
    "\n",
    "Writes an enhanced version to enhanced_vault_index.json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f444e",
   "metadata": {},
   "source": [
    "The titles in vault_index.json are the titles/aliases \"What is Apache Airflow?\" from metadata and not the title from the file. \n",
    "\n",
    "This is an issue with build_vault_index.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcad479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === STEP 1: READ ALL DOCUMENTS ===\n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for md_file in MD_FILES:\n",
    "    text = read_markdown_file(md_file)\n",
    "    corpus.append(text)\n",
    "    filenames.append(md_file.stem)  # filename without \".md\"\n",
    "\n",
    "# === STEP 2: COMPUTE TF-IDF ===\n",
    "vectorizer = CountVectorizer(tokenizer=normalize_document)\n",
    "X_counts = vectorizer.fit_transform(corpus)\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_counts)\n",
    "\n",
    "# === STEP 3: LOAD vault_index.json ===\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    vault_index = json.load(f)\n",
    "\n",
    "# === STEP 4: ADD TF-IDF SCORES ===\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if filename not in vault_index:\n",
    "        print(f\"Skipping: {filename} not in vault_index.json\")\n",
    "        continue\n",
    "\n",
    "    scores = X_tfidf[i].T.toarray().flatten()\n",
    "    terms_scores = [(feature_names[j], float(scores[j])) for j in range(len(scores)) if scores[j] > 0]\n",
    "    top_10_scores = dict(sorted(terms_scores, key=lambda x: -x[1])[:10])\n",
    "\n",
    "    vault_index[filename][\"TFIDF_Score\"] = top_10_scores\n",
    "\n",
    "# === STEP 5: SAVE TO enhanced_vault_index.json ===\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vault_index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Enhanced vault index written to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d45d1d",
   "metadata": {},
   "source": [
    "### 3) Using the scores within the enhanced_vault_index.json possiblely cluster or classify the documents afterward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dcc49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
