{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3af4bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<!-- ## What You're Doing\n",
    "\n",
    "You are building a structured index (`vault_index.json`) `Data-archive/`, so that an LLM can:\n",
    "- Efficiently retrieve, summarise, and ask questions over a subgraph of relevant notes\n",
    "- Work with metadata like titles, tags, links, aliases, and summaries- Acts as a metadata snapshot of your vault\n",
    "- Enables RAG-style querying without repeatedly parsing the markdown files\n",
    "- Enables LLM reasoning over structure (e.g., graph search, centrality scoring, tag grouping)\n",
    "\n",
    "## `vault_index.json` Format (Example)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"note_id\": {\n",
    "    \"title\": \"Bayesian Uncertainty\",\n",
    "    \"path\": \"notes/probability/bayesian_uncertainty.md\",\n",
    "    \"tags\": [\"uncertainty\", \"bayes\"],\n",
    "    \"aliases\": [\"bayesian confidence\"],\n",
    "    \"outlinks\": [\"confidence_intervals\", \"epistemic_uncertainty\"],\n",
    "    \"inlinks\": [\"decision_making\"],\n",
    "    \"summary\": \"Overview of subjective and objective uncertainty in Bayesian analysis.\"\n",
    "  },\n",
    "}\n",
    "```\n",
    "\n",
    "## Planned Workflow\n",
    "\n",
    "1. Parse a vault folder (e.g. `Data-archive/`)\n",
    "2. For each note:\n",
    "   - Read title from filename or YAML\n",
    "   - Extract frontmatter (tags, aliases)\n",
    "   - Extract links (e.g., `[[note-title]]`)\n",
    "   - Extract summary (e.g., first 100 words of the note)\n",
    "3. Track inlinks/outlinks\n",
    "4. Save as a structured `vault_index.json` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d503eae",
   "metadata": {},
   "source": [
    "<!-- # Code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import re\n",
    "# import yaml\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "# from markdown import markdown\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "# OUTPUT_PATH = \"vault_index.json\"\n",
    "\n",
    "# # === HELPERS ===\n",
    "# def extract_frontmatter(md_text):\n",
    "#     match = re.match(r'^---\\n(.*?)\\n---\\n(.*)', md_text, re.DOTALL)\n",
    "#     if match:\n",
    "#         frontmatter = yaml.safe_load(match.group(1))\n",
    "#         content = match.group(2)\n",
    "#     else:\n",
    "#         frontmatter = {}\n",
    "#         content = md_text\n",
    "#     return frontmatter, content\n",
    "\n",
    "# def extract_links(content):\n",
    "#     return [match.split('|')[0] for match in re.findall(r'\\[\\[([^\\]]+)\\]\\]', content)]\n",
    "\n",
    "# def markdown_to_text(md_content):\n",
    "#     html = markdown(md_content)\n",
    "#     soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "#     return soup.get_text()\n",
    "\n",
    "# def normalize_title(title):\n",
    "#     return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "# def summarise_text(text, word_limit=100):\n",
    "#     words = text.strip().split()\n",
    "#     return \" \".join(words[:word_limit]) + (\"...\" if len(words) > word_limit else \"\")\n",
    "\n",
    "# # === MAIN FUNCTION ===\n",
    "# def index_vault(vault_path):\n",
    "#     vault_index = {}\n",
    "#     outlink_map = defaultdict(list)\n",
    "#     title_to_id = {}\n",
    "\n",
    "#     for note_path in vault_path.rglob(\"*.md\"):\n",
    "#         with open(note_path, 'r', encoding='utf-8') as f:\n",
    "#             raw_md = f.read()\n",
    "#         # even if the note is empty, we still want to index it\n",
    "\n",
    "#         frontmatter, content = extract_frontmatter(raw_md)\n",
    "#         plain_text = markdown_to_text(content)\n",
    "\n",
    "#         raw_title = frontmatter.get(\"title\")\n",
    "#         title = raw_title if raw_title else note_path.stem\n",
    "#         # title = frontmatter.get(\"title\", note_path.stem)\n",
    "#         # if not title:\n",
    "#         #     continue\n",
    "\n",
    "\n",
    "#         note_id = normalize_title(title)\n",
    "#         title_to_id[title] = note_id\n",
    "\n",
    "\n",
    "#         tags = frontmatter.get(\"tags\", [])\n",
    "#         aliases = frontmatter.get(\"aliases\", [])\n",
    "#         outlinks_raw = extract_links(content)\n",
    "#         outlinks = [normalize_title(link) for link in outlinks_raw]\n",
    "#         # there should be no repeats in outlinks or aliases\n",
    "\n",
    "#         vault_index[note_id] = {\n",
    "#             \"title\": title,\n",
    "#             \"note_id\": note_id,\n",
    "#             \"tags\": tags,\n",
    "#             \"aliases\": aliases,\n",
    "#             \"outlinks\": outlinks,\n",
    "#             \"inlinks\": [],\n",
    "#             \"summary\": summarise_text(plain_text, word_limit=25)\n",
    "#         }\n",
    "\n",
    "#         title_to_id[title] = note_id\n",
    "\n",
    "#         for link_id in outlinks:\n",
    "#             outlink_map[link_id].append(note_id)\n",
    "\n",
    "#     for target_id, sources in outlink_map.items():\n",
    "#         if target_id in vault_index:\n",
    "#             vault_index[target_id][\"inlinks\"] = list(set(sources))\n",
    "\n",
    "#     return vault_index, title_to_id\n",
    "\n",
    "# # === EXECUTION ===\n",
    "# vault_index, title_to_id = index_vault(VAULT_PATH)\n",
    "\n",
    "# # There is no need for title_to_id anymore\n",
    "\n",
    "# # Initialize an empty vault_index\n",
    "# new_vault_index = {}\n",
    "\n",
    "# # Iterate through existing vault_index to restructure\n",
    "# for note_id, note_data in vault_index.items():\n",
    "#     # Skip the title_to_id map\n",
    "#     if note_id == \"__title_to_id__\":\n",
    "#         continue\n",
    "    \n",
    "#     # Extract relevant information\n",
    "#     title = note_data[\"title\"]\n",
    "#     tags = note_data[\"tags\"]\n",
    "#     aliases = note_data[\"aliases\"]\n",
    "#     outlinks = note_data[\"outlinks\"]\n",
    "#     inlinks = note_data[\"inlinks\"]\n",
    "#     summary = note_data[\"summary\"]\n",
    "\n",
    "#     # Create a new structure where the note_id is the key\n",
    "#     new_vault_index[note_id] = {\n",
    "#         \"title\": title,\n",
    "#         \"tags\": tags,\n",
    "#         \"aliases\": aliases,\n",
    "#         \"outlinks\": outlinks,\n",
    "#         \"inlinks\": inlinks,\n",
    "#         \"summary\": summary\n",
    "#     }\n",
    "\n",
    "# # Now `new_vault_index` holds the desired structure\n",
    "\n",
    "# with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(new_vault_index, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e992db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "OUTPUT_PATH = \"vault_index.json\"\n",
    "\n",
    "# === HELPERS ===\n",
    "def extract_frontmatter(md_text):\n",
    "    \"\"\"\n",
    "    Extract YAML frontmatter and content from markdown text.\n",
    "    \"\"\"\n",
    "    match = re.match(r'^---\\n(.*?)\\n---\\n(.*)', md_text, re.DOTALL)\n",
    "    if match:\n",
    "        frontmatter = yaml.safe_load(match.group(1))\n",
    "        content = match.group(2)\n",
    "    else:\n",
    "        frontmatter = {}\n",
    "        content = md_text\n",
    "    return frontmatter, content\n",
    "\n",
    "def extract_links(content):\n",
    "    \"\"\"\n",
    "    Extract links in the format [[link|display_name]] from markdown content.\n",
    "    Returns only the link part before the pipe character.\n",
    "    \"\"\"\n",
    "    return [match.split('|')[0] for match in re.findall(r'\\[\\[([^\\]]+)\\]\\]', content)]\n",
    "\n",
    "def markdown_to_text(md_content):\n",
    "    \"\"\"\n",
    "    Convert markdown content to plain text by first converting it to HTML and then extracting the text.\n",
    "    \"\"\"\n",
    "    html = markdown(md_content)\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Normalize title by converting to lowercase and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "def summarise_text(text, word_limit=100):\n",
    "    \"\"\"\n",
    "    Summarize text by limiting it to a specified word count.\n",
    "    \"\"\"\n",
    "    words = text.strip().split()\n",
    "    return \" \".join(words[:word_limit]) + (\"...\" if len(words) > word_limit else \"\")\n",
    "\n",
    "# === MAIN FUNCTION ===\n",
    "def index_vault(vault_path):\n",
    "    \"\"\"\n",
    "    Index all markdown files in the vault, extracting metadata, links, and generating summaries.\n",
    "    This function now also restructures the vault_index.\n",
    "    \"\"\"\n",
    "    vault_index = {}\n",
    "    outlink_map = defaultdict(list)\n",
    "\n",
    "    for note_path in vault_path.rglob(\"*.md\"):\n",
    "        with open(note_path, 'r', encoding='utf-8') as f:\n",
    "            raw_md = f.read()\n",
    "        \n",
    "        # Extract frontmatter and content\n",
    "        frontmatter, content = extract_frontmatter(raw_md)\n",
    "        plain_text = markdown_to_text(content)\n",
    "\n",
    "        # Get title and normalize it\n",
    "        raw_title = frontmatter.get(\"title\")\n",
    "        title = raw_title if raw_title else note_path.stem\n",
    "        note_id = normalize_title(title)\n",
    "\n",
    "        # Extract tags, aliases, and outlinks\n",
    "        tags = frontmatter.get(\"tags\", [])\n",
    "        aliases = frontmatter.get(\"aliases\", [])\n",
    "        outlinks_raw = extract_links(content)\n",
    "        outlinks = [normalize_title(link) for link in outlinks_raw]\n",
    "\n",
    "        # Store note metadata in vault_index\n",
    "        vault_index[note_id] = {\n",
    "            \"title\": title,\n",
    "            \"tags\": tags,\n",
    "            \"aliases\": aliases,\n",
    "            \"outlinks\": outlinks,\n",
    "            \"inlinks\": [],  # Will be filled later\n",
    "            \"summary\": summarise_text(plain_text, word_limit=25)\n",
    "        }\n",
    "\n",
    "        # Add the note's ID to the outlink_map for each link it references\n",
    "        for link_id in outlinks:\n",
    "            outlink_map[link_id].append(note_id)\n",
    "\n",
    "    # Now update inlinks for each target note, ensuring no repeats\n",
    "    for target_id, sources in outlink_map.items():\n",
    "        if target_id in vault_index:\n",
    "            # Remove duplicates by converting to a set, then back to a list\n",
    "            vault_index[target_id][\"inlinks\"] = list(set(sources))\n",
    "\n",
    "    # Restructure vault_index: remove duplicate outlinks and create final structure\n",
    "    new_vault_index = {}\n",
    "    for note_id, note_data in vault_index.items():\n",
    "        # Create a new structure where the note_id is the key, excluding the redundant note_id key\n",
    "        new_vault_index[note_id] = {\n",
    "            \"title\": note_data[\"title\"],\n",
    "            \"tags\": note_data[\"tags\"],\n",
    "            \"aliases\": note_data[\"aliases\"],\n",
    "            \"outlinks\": list(set(note_data[\"outlinks\"])),  # Remove duplicates from outlinks\n",
    "            \"inlinks\": note_data[\"inlinks\"],  # Inlinks are already handled for uniqueness\n",
    "            \"summary\": note_data[\"summary\"]\n",
    "        }\n",
    "\n",
    "    return new_vault_index\n",
    "\n",
    "# === EXECUTION ===\n",
    "vault_index = index_vault(VAULT_PATH)\n",
    "\n",
    "# Write the final structured output to a JSON file\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vault_index, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read json file\n",
    "# with open(OUTPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "#     vault_index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86335944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-on-1_template',\n",
       " 'ab_testing',\n",
       " 'accessing_gen_ai_generated_content',\n",
       " 'accuracy',\n",
       " 'acid_transaction']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vault_index.keys())[:5]  # Show a sample of indexed note IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64a157a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Views',\n",
       " 'tags': ['database'],\n",
       " 'aliases': [],\n",
       " 'outlinks': ['soft_deletion',\n",
       "  'sqlite',\n",
       "  'querying',\n",
       "  'common_table_expression',\n",
       "  'de_tools',\n",
       "  'view_use_case',\n",
       "  'database_schema'],\n",
       " 'inlinks': ['common_table_expression'],\n",
       " 'summary': 'Views are virtual tables defined by SQL [[Querying|Query]] that ==simplify complex data representation.== They can remove unnecessary columns, aggregate results, partition data, and secure sensitive...'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # get detail for a specific note\n",
    "id=\"views\"\n",
    "vault_index[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401715a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['database']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vault_index[id]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ed5f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'common_table_expression'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# get detail for a specific note\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mcommon_table_expression\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mvault_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'common_table_expression'"
     ]
    }
   ],
   "source": [
    "# # get detail for a specific note\n",
    "# id=\"common_table_expression\"\n",
    "# vault_index[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2035d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # count the number of entries\n",
    "# len(vault_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6ac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # count the number of files in:\n",
    "# VAULT_PATH = Path(\"C:/Users/RhysL/Desktop/Data-Archive/content/standardised\")\n",
    "# len(list(VAULT_PATH.rglob(\"*.md\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
