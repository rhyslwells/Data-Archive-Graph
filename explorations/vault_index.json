{
  "1-on-1_template": {
    "title": "1-on-1 Template",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\1-on-1 Template.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Decisions [Your name] add decisions that need to be made [Other person's name] add decisions that need to be made Action items [Your name] add next steps from the discussion [Other person's name] add next steps from the discussion Topics to discuss (bi-directional) [Your name] add topics or questions to discuss together [Other person's name] add topics or questions to discuss together Updates (uni-directional - no action needed) [Your name] add updates with no action needed [Other person's name] add updates with no action needed"
  },
  "ab_testing": {
    "title": "AB testing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\AB testing.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "A/B testing is a method of performance testing two versions of a product like an app."
  },
  "accessing_gen_ai_generated_content": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Accessing Gen AI generated content.md",
    "tags": [
      "GenAI",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "Generative AI",
      "interpretability",
      "Knowledge Graph",
      "RAG",
      "BERTScore"
    ],
    "inlinks": [],
    "summary": "To assess whether the content generated by a [[Generative AI]] is truthful and faithful, several methods and frameworks can be employed. Truthfulness refers to whether the generated content ==is factually correct==, while faithfulness refers to whether it ==accurately== reflects the input data or prompt. [[interpretability]] 1. Frameworks for Truthfulness and Faithfulness Subject Matter Expert (SME) Reviews: One of the most reliable methods for verifying truthfulness and faithfulness is through SME validation. SMEs can manually check the content to ensure it aligns with domain-specific knowledge and is factually accurate. [[Knowledge Graph]] and External Data: Generative AI models can be linked to..."
  },
  "accuracy": {
    "title": "Accuracy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Accuracy.md",
    "tags": [
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "Imbalanced Datasets",
      "Classification",
      "Confusion Matrix"
    ],
    "inlinks": [
      "evaluation_metrics",
      "handling_different_distributions",
      "precision",
      "test_loss_when_evaluating_models",
      "confusion_matrix",
      "model_observability",
      "ds_&_ml_portal",
      "class_separability",
      "imbalanced_datasets_smote.py"
    ],
    "summary": "Definition Accuracy Score is the proportion of correct predictions out of all predictions made. In other words, it is the percentage of correct predictions. Accuracy can have issues with [[Imbalanced Datasets]]where there is more of one class than another. Formula The formula for accuracy is: $$\\text{Accuracy} = \\frac{TN + TP}{\\text{Total}}$$ In the context of [[Classification]] problems, particularly binary classification, TN and TP are components of the confusion matrix: TP (True Positive): The number of instances that are correctly predicted as the positive class. For example, if the model predicts a positive outcome and it is indeed positive, it counts as..."
  },
  "acid_transaction": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ACID Transaction.md",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "Transaction"
    ],
    "inlinks": [
      "transaction",
      "schema_evolution",
      "data_lakehouse"
    ],
    "summary": "An ACID [[Transaction]] ensures that either all changes are successfully committed or rolled back, preventing the database from ending up in an inconsistent state. This guarantees the integrity of the data throughout the transaction process. Key Properties of ACID Transactions Atomicity: This property ensures that transactions are treated as a single, indivisible unit. If any part of the transaction fails, the entire transaction is rolled back, and none of the changes are applied. Users do not see intermediate states of the transaction. Consistency: Transactions must leave the database in a valid state, adhering to all defined constraints. If a transaction..."
  },
  "activation_atlases": {
    "title": "Activation atlases",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Activation atlases.md",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "feature_extraction"
    ],
    "summary": "is a viewing method for high dimensional space that AI system use for predictions. Example AlexNet (cofounder of OpenAI)"
  },
  "activation_function": {
    "title": "Activation Function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Activation Function.md",
    "tags": [
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "Neural network",
      "How do we choose the right Activation Function",
      "Data transformation",
      "Backpropagation",
      "Binary Classification",
      "interpretability|interpretable"
    ],
    "inlinks": [
      "forward_propagation",
      "typical_output_formats_in_neural_networks",
      "neural_network"
    ],
    "summary": "Activation functions play a role in [[Neural network]] by introducing non-linearity, allowing models to learn from complex patterns and relationships in the data. [[How do we choose the right Activation Function]] Key Uses of Activation Functions: Non-linearity: Without activation functions, neural networks would behave as linear models, unable to capture complex, non-linear patterns in the data [[Data transformation]]: Activation functions modify input signals from one layer to another, helping the model focus on important information while ignoring irrelevant data, [[Backpropagation]]: They enable gradient-based optimization by making the network differentiable, essential for efficient learning. Purpose of Typical Activation Functions Linear: Outputs..."
  },
  "active_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Active Learning.md",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "Supervised Learning"
    ],
    "inlinks": [],
    "summary": "Think captchas for training. To help the [[Supervised Learning]] models when they are less confident. Reducing labelling time or need for it."
  },
  "ada_boosting": {
    "title": "Ada boosting",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Ada boosting.md",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "Boosting",
      "Decision Tree",
      "Random Forests",
      "Random Forests",
      "Random Forests",
      "Random Forests"
    ],
    "inlinks": [
      "boosting"
    ],
    "summary": "Resources: LINK Overview: Ada Boosting short for ==Adaptive Boosting==, is a specific type of [[Boosting]] algorithm that focuses on improving the accuracy of predictions by ==combining multiple weak learners== into a strong learner. It is particularly known for its ==simplicity== and effectiveness in classification tasks. How AdaBoost Works: Base Learners: In AdaBoost, the base learners are typically low-depth trees, also known as ==stumps==. These are simple models that perform slightly better than random guessing. Sequential Training: AdaBoost trains these stumps sequentially. Each stump is trained to correct the errors made by the previous stumps. This sequential approach ensures that each..."
  },
  "adam_optimizer": {
    "title": "Adam Optimizer",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Adam Optimizer.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Momentum",
      "deep learning",
      "learning rate",
      "Hyperparameter",
      "Gradient Descent",
      "Why does the Adam Optimizer converge"
    ],
    "inlinks": [
      "adaptive_learning_rates",
      "backpropagation",
      "optimisation_techniques",
      "learning_rate",
      "orthogonalization"
    ],
    "summary": "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both [[Momentum]] and adaptive learning rates. It is widely used due to its efficiency and effectiveness in training [[deep learning]] models. Adam is particularly effective for large datasets and complex models, as it provides robust convergence and requires minimal tuning compared to other optimization algorithms. Its ability to ==dynamically adjust learning rates== makes it a popular choice in the deep learning community. Key Features of Adam: Adaptive Learning Rates: Adam adjusts the [[learning rate]] for each parameter individually, based on the first and second moments of..."
  },
  "adaptive_learning_rates": {
    "title": "Adaptive Learning Rates",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Adaptive Learning Rates.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Adam Optimizer",
      "learning rate",
      "Momentum"
    ],
    "inlinks": [
      "optimisation_techniques"
    ],
    "summary": "[[Adam Optimizer]] Adaptive [[learning rate]] adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients. Adam (short for Adaptive Moment Estimation) combines ideas from [[Momentum]] and adaptive learning rates to help the optimization process."
  },
  "adding_a_database_to_postgresql": {
    "title": "Adding a database to PostgreSQL",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Adding a database to PostgreSQL.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "postgresql"
    ],
    "summary": "How to Add a Database to PostgreSQL Using pgAdmin (GUI) Open pgAdmin and log in. In the Object Explorer, right-click Databases \u2192 Create \u2192 Database. Enter Database Name (e.g., mydatabase). Choose an Owner (optional). Click Save. Using Python (psycopg2) If you're using Python (e.g., in a Jupyter Notebook), install the psycopg2 package if needed: python !pip install psycopg2-binary Then, run this script to create a PostgreSQL database: ```python import psycopg2 Connect to the PostgreSQL server (default 'postgres' database) conn = psycopg2.connect( dbname=\"postgres\", # Default DB to connect before creating a new one user=\"postgres\", password=\"your_password\", host=\"localhost\" ) conn.autocommit = True #..."
  },
  "addressing_multicollinearity": {
    "title": "Addressing Multicollinearity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Addressing Multicollinearity.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Addressing_Multicollinearity.py",
      "interpretability",
      "Principal Component Analysis",
      "dimensionality reduction",
      "multicollinearity",
      "'var1', 'var2', 'var3'"
    ],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "In [[ML_Tools]] see: [[Addressing_Multicollinearity.py]] Multicollinearity can impact the performance and [[interpretability]] of regression models by causing instability in coefficient estimates and complicating the analysis of variable significance. Techniques like PCA can help by transforming correlated variables into uncorrelated principal components, thereby improving model stability and interpretability. [[Principal Component Analysis]] (PCA) is a [[dimensionality reduction]] technique that can help address [[multicollinearity]] in regression models. Combining Correlated Variables: PCA transforms the correlated independent variables into a set of uncorrelated variables called principal components. These components capture the majority of the variance in the data while reducing redundancy. Reducing Dimensionality: By selecting a..."
  },
  "addressing_multicollinearity.py": {
    "title": "Addressing_Multicollinearity.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Addressing_Multicollinearity.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "addressing_multicollinearity"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Regression/Addressing_Multicollinearity.py"
  },
  "adjusted_r_squared": {
    "title": "Adjusted R squared",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Adjusted R squared.md",
    "tags": [
      "statistics",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "Regression Metrics",
      "parsimonious",
      "R squared"
    ],
    "inlinks": [
      "r_squared",
      "regression_metrics",
      "linear_regression",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression"
    ],
    "summary": "Adjusted R-squared is a [[Regression Metrics]]or assessing the quality of a regression model, ==especially when multiple predictors== are involved. It helps ensure that the model remains [[parsimonious]] while still providing a good fit to the data. When evaluating a regression model, if you notice a ==large difference== between [[R squared]] and adjusted R\u00b2, it indicates that the additional predictors may not be improving the model's performance. In such cases, it may be beneficial to drop those extra variables to simplify the model without sacrificing predictive power. Key features: Penalty for Number of Predictors: Adjusted R\u00b2 adjusts the R\u00b2 value by..."
  },
  "agent-based_modelling": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Agent-Based Modelling.md",
    "tags": null,
    "aliases": [
      "ABM"
    ],
    "outlinks": [
      "emergent behavior",
      "Policy|policies",
      "Agent-Based Modelling",
      "LLM|LLMs",
      "chain of thought",
      "prompt engineering"
    ],
    "inlinks": [
      "agent-based_modelling",
      "what_can_abm_solve_within_the_energy_sector",
      "agentic_solutions",
      "energy"
    ],
    "summary": "(ABM) is a computational approach that simulates the interactions of individual agents within a defined environment to observe complex phenomena and [[emergent behavior]] at a system level. Agent-based modeling provides a robust framework for understanding and analyzing complex systems, particularly in the energy sector. By simulating individual agents and their interactions, researchers and practitioners can gain insights into system dynamics, evaluate [[Policy|policies]], and optimize strategies for energy production and consumption. Principles of Agent-Based Modelling Agents: The primary components of ABM, agents can represent individuals, groups, or entities with defined behaviors and attributes. Each agent operates based on its rules and..."
  },
  "agentic_solutions": {
    "title": "Agentic Solutions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Agentic Solutions.md",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "Small Language Models|SLM",
      "GraphRAG",
      "Agent-Based Modelling"
    ],
    "inlinks": [
      "scaling_agentic_systems",
      "langchain"
    ],
    "summary": "Agentic solutions leverage multiple autonomous agents (usually [[Small Language Models|SLM]]) to achieve goals collaboratively. These systems distribute tasks across agents that operate individually or collectively to solve complex problems. Agents can model specific business functions. Role clarity enhances the effectiveness of these systems. Related terms: - [[GraphRAG]] - [[Agent-Based Modelling]] Types of Agentic Solutions Reactive Solutions (Ask Approach): Systems like chatbots and retrieval-augmented generation (RAG) tools respond to user queries. Autonomous Solutions (Do Approach): Agents perform tasks proactively, e.g., drafting documents or scheduling meetings. Business Process Integration Workflow: Identify a business problem. Define personas (agents) required. Develop an agentic workflow...."
  },
  "aggregation": {
    "title": "Aggregation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Aggregation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pandas Pivot Table",
      "Groupby",
      "DE_Tools"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "data_transformation"
    ],
    "summary": "Aggregation Summarizing data for analysis ([[Pandas Pivot Table]] and [[Groupby]]). In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/group_by.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb"
  },
  "ai_engineer": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\AI Engineer.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "LSTM",
      "Attention mechanism",
      "Prompting",
      "Neural network"
    ],
    "inlinks": [],
    "summary": "They know what - [[LSTM]] means - [[Attention mechanism]] - [[Prompting]] optimisation - [[Neural network]]"
  },
  "ai_governance": {
    "title": "AI governance",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\AI governance.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Governance"
    ],
    "inlinks": [],
    "summary": "AI Governance [[Data Governance]] Used in regulated sectors. Constraints to using ai: - legal, - transparency, - security, - historical bias AI acts and standards: - eu and AI acts - NIST standards framework - Security OWASP standards for LLM how will beaurcracy keeps up with ai innovation. Governance can stifle innovation."
  },
  "algorithms": {
    "title": "Algorithms",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Algorithms.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Recursive Algorithm",
      "Memoization",
      "K-means"
    ],
    "inlinks": [
      "machine_learning_algorithms",
      "checksum",
      "computer_science"
    ],
    "summary": "[[Recursive Algorithm]] Backtracking Backtracking is a method for solving problems incrementally, by trying partial solutions and then abandoning them if they are not valid. Example: Graph coloring with the 4-color theorem. Divide and Conquer Divide and conquer is a strategy that involves breaking a problem into smaller subproblems of the same type, solving these subproblems recursively, and then combining their solutions to solve the original problem. Example: Merge sort, where the array is split in half, and each smaller part is sorted. Note: Subproblems do not generally overlap. Dynamic Programming Dynamic programming is used for optimization problems and involves storing..."
  },
  "alternatives_to_batch_processing": {
    "title": "Alternatives to Batch Processing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Alternatives to Batch Processing.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Streaming",
      "batch processing",
      "data analysis",
      "Apache Kafka",
      "Apache Flink",
      "Apache Spark Streaming",
      "event-driven architecture",
      "Apache Spark Streaming"
    ],
    "inlinks": [
      "data_streaming"
    ],
    "summary": "If you\u2019re working with a streaming dataset ([[Data Streaming]]), why might [[batch processing]] not be suitable, and what alternatives would you consider? Latency: Batch processing involves collecting data over a period and processing it in large chunks. This can introduce significant delays, making it unsuitable for applications that require real-time or near-real-time insights. Timeliness: Streaming datasets often require immediate processing to respond to events as they occur. Batch processing cannot meet the demand for timely [[data analysis]]. Data Freshness: In streaming scenarios, data is continuously generated, and waiting for a batch interval can result in outdated information being analyzed. Alternatives..."
  },
  "amazon_s3": {
    "title": "Amazon S3",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Amazon S3.md",
    "tags": [],
    "aliases": [
      "S3"
    ],
    "outlinks": [],
    "inlinks": [
      "data_storage",
      "distributed_computing"
    ],
    "summary": "Amazon S3 Amazon S3 buckets (onedrive essentially) Amazon Simple Storage Service (S3) is a versatile ==object storage solution== known for its scalability, data availability, security, and performance. Stands for Simple Storage Service. What is Amazon S3? It's an object storage service, allowing you to upload and store various types of objects, including images, text, videos, and more. S3's structure resembles that of a typical file system, with folders, subfolders, and files. Notably, it's extremely cost-effective, with storage costs starting at only 0.023 cents per GB, and it offers high durability with data replicated across three availability zones. Why is Amazon..."
  },
  "anomaly_detection_in_time_series": {
    "title": "Anomaly Detection in Time Series",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Anomaly Detection in Time Series.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Time Series",
      "ML_Tools",
      "TS_Anomaly_Detection.py",
      "LSTM",
      "Isolated Forest"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "In [[Time Series]] In [[ML_Tools]] see: - [[TS_Anomaly_Detection.py]] To perform anomaly detection specifically for time series data, you can utilize various techniques that account for the ==temporal nature== of the data. Here are some common methods: Statistical Methods: Moving Average: Calculate a moving average and identify points that deviate significantly from this average. Seasonal Decomposition: Decompose the time series into trend, seasonal, and residual components. Anomalies can be identified in the residuals. Time Series Models: AutoRegressive Integrated Moving Average: Fit an ARIMA model to the time series data and analyze the residuals for anomalies. State Space Model (ETS): Similar to..."
  },
  "anomaly_detection_with_clustering": {
    "title": "Anomaly Detection with Clustering",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Anomaly Detection with Clustering.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Clustering",
      "DBSCAN",
      "Isolated Forest|iForest"
    ],
    "inlinks": [
      "anomaly_detection",
      "isolated_forest"
    ],
    "summary": "[[Clustering]] - Description: Outliers often form small clusters or are isolated from main clusters. 8. [[DBSCAN]] (Density-Based Spatial Clustering of Applications with Noise) Purpose: Finds anomalies based on density rather than explicit statistical assumptions. Steps: Identify points in low-density regions as anomalies. [[Isolated Forest|iForest]] 2. Local Outlier Factor (LOF) LOF is a density-based anomaly detection method that identifies anomalies by comparing the local density of a point with that of its neighbors. Steps: For each point, calculate the local density based on the distance to its k-nearest neighbors. Compute the LOF score, which measures the degree of isolation of a..."
  },
  "anomaly_detection_with_statistical_methods": {
    "title": "Anomaly Detection with Statistical Methods",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Anomaly Detection with Statistical Methods.md",
    "tags": [
      "anomaly_detection",
      "statistics",
      "ml"
    ],
    "aliases": [],
    "outlinks": [
      "Z-Normalisation|Z-Score",
      "Interquartile Range (IQR) Detection",
      "Percentile Detection",
      "Gaussian Model",
      "Isolated Forest",
      "multivariate data"
    ],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "Basic: - [[Z-Normalisation|Z-Score]] - [[Interquartile Range (IQR) Detection]] - [[Percentile Detection]] Advanced: - [[Gaussian Model]] - [[Isolated Forest]] Grubbs' Test Context: Grubbs' test is a hypothesis test designed to detect a single outlier in a normally distributed dataset. It tests the largest deviation from the mean relative to the standard deviation. This test is iterative and removes one outlier at a time. Purpose: To determine whether the most extreme data point (either smallest or largest) is a statistical outlier. Steps: - Compute the test statistic: $G = \\frac{\\max(|X - \\mu|)}{\\sigma}$ where: - $X$: Data points - $\\mu$: Mean of the..."
  },
  "anomaly_detection": {
    "title": "Anomaly Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Anomaly Detection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/Outliers|Outliers",
      "data integrity",
      "ML_Tools",
      "Pycaret_Anomaly.ipynb",
      "Data Cleansing",
      "Normalisation",
      "Standardisation",
      "Anomaly Detection with Clustering",
      "PCA-Based Anomaly Detection",
      "Anomaly Detection in Time Series",
      "Anomaly Detection with Statistical Methods",
      "Boxplot"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "outliers",
      "gaussian_mixture_models",
      "time_series",
      "imbalanced_datasets",
      "clustering"
    ],
    "summary": "Anomaly detection involves identifying [[standardised/Outliers|Outliers]]. Detecting these anomalies is crucial for maintaining [[data integrity]] and improving model performance. Methods for Detecting Anomalies In [[ML_Tools]] see: [[Pycaret_Anomaly.ipynb]] Process of Detection Data Preparation - [[Data Cleansing]]: Handle missing values and remove any irrelevant data points. - [[Normalisation]]/[[Standardisation]]: Scale the data if necessary, especially if using methods sensitive to the scale. Anomaly Detection with a model: Use a chosen method to flag anomalies - [[Anomaly Detection with Clustering]] - [[PCA-Based Anomaly Detection]] - [[Anomaly Detection in Time Series]] - [[Anomaly Detection with Statistical Methods]] Validation - Validate the detected anomalies by comparing them..."
  },
  "apache_airflow": {
    "title": "What is Apache Airflow?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Apache Airflow.md",
    "tags": [
      "data_orchestration",
      "software",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Directed Acyclic Graph (DAG)"
    ],
    "inlinks": [
      "data_engineering",
      "etl",
      "data_management",
      "directed_acyclic_graph_(dag)"
    ],
    "summary": "Schedular think CROM jobs with python. Apache Nifi may be better. Airflow is a data orchestrator and the first that made task scheduling popular with Python. Airflow programmatically author, schedule, and monitor workflows. It follows the imperative paradigm of schedule as how a DAG [[Directed Acyclic Graph (DAG)]] is run has to be defined within the Airflow jobs. Airflow calls its Workflow as code with the main characteristics - Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. - Extensible: The Airflow framework contains operators to connect with numerous technologies. All Airflow components are extensible to..."
  },
  "apache_kafka": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Apache Kafka.md",
    "tags": [
      "software",
      "data_orchestration"
    ],
    "aliases": [
      "Kafka"
    ],
    "outlinks": [
      "Data Integrity",
      "Publish and Subscribe",
      "Scalability",
      "data integration",
      "Data Integration",
      "data storage"
    ],
    "inlinks": [
      "publish_and_subscribe",
      "data_engineering_tools",
      "data_streaming",
      "alternatives_to_batch_processing"
    ],
    "summary": "Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and data streaming applications. It is designed to handle high-throughput, fault-tolerant, and scalable messaging. Features Immutable commit log: Kafka maintains an append-only log of messages, which ensures [[Data Integrity]] and replicability. Kafka allows applications to [[Publish and Subscribe]] to streams of records, similar to a message queue or enterprise messaging system. Durability and Reliability: Kafka stores streams of records in a fault-tolerant way, ensuring data durability and reliability. It replicates data across multiple nodes to prevent data loss. [[Scalability]]: Kafka is designed to scale horizontally..."
  },
  "apache_spark": {
    "title": "What is Apache Spark?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Apache Spark.md",
    "tags": [
      "software"
    ],
    "aliases": [
      "Spark"
    ],
    "outlinks": [
      "Data Engineer"
    ],
    "inlinks": [
      "pyspark",
      "distributed_computing",
      "big_data",
      "parquet",
      "databricks",
      "batch_processing",
      "ds_&_ml_portal",
      "scala",
      "map_reduce"
    ],
    "summary": "Apache Spark is an open-source multi-language engine for executing Data Engineer and Machine Learning on single-node machines or clusters. It's optimized for large-scale data processing. Spark runs well with Kubernetes. Spark is a highly popular framework for large-scale data processing. It allows [[Data Engineer]] to process massive datasets in memory, which makes it faster than traditional disk-based approaches. Spark is versatile, supporting batch processing, real-time data streaming, machine learning, and graph processing."
  },
  "api_driven_microservices": {
    "title": "API Driven Microservices",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\API Driven Microservices.md",
    "tags": [
      "software",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "software architecture",
      "microservices",
      "API"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "API-driven microservices refer to a [[software architecture]] approach where [[microservices]] communicate with each other and with external systems primarily through well-defined [[API]] (Application Programming Interfaces). This architecture is designed to enhance modularity, scalability, and flexibility by breaking down an application into smaller, independent services that can be developed, deployed, and scaled independently. API-driven microservices architecture is particularly beneficial for large, complex applications that require frequent updates and scaling. It allows organizations to innovate faster, improve fault isolation, and better align development efforts with business needs. However, it also introduces complexity in terms of service orchestration, data consistency, and network communication,..."
  },
  "api": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\API.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "REST API",
      "FastAPI",
      "ML_Tools",
      "Wikipedia_API.py"
    ],
    "inlinks": [
      "normalisation_of_data",
      "model_deployment",
      "api_driven_microservices",
      "data_ingestion",
      "data_contract"
    ],
    "summary": "An API (Application Programming Interfaces) allows one system (client) to ==request specific actions from another system== (server). Using a predefined set of rules and ==protocols==. Good API documentation is necessary for developers to integrate and use APIs effectively. Resources: Link [[REST API]] [[FastAPI]] API Principles Controlled Access: APIs provide access to certain parts of a system while keeping the core functionalities secure. System Independence: APIs function independently of changes in the underlying system. Simplicity: APIs are designed to be ==user-friendly== and come with comprehensive documentation to guide developers. Implementation In [[ML_Tools]] see: [[Wikipedia_API.py]] Example: For instance, a weather app querying..."
  },
  "attention_is_all_you_need": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Attention Is All You Need.md",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "attention_mechanism": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Attention mechanism.md",
    "tags": [
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "ML",
      "NLP",
      "LLM",
      "Recurrent Neural Networks|RNN",
      "LSTM",
      "Transformer|Transformer",
      "standardised/Attention Is All You Need",
      "BERT",
      "Multi-head attention"
    ],
    "inlinks": [
      "multi-head_attention",
      "transformers_vs_rnns",
      "vector_embedding",
      "llm",
      "ai_engineer",
      "language_model_output_optimisation",
      "lstm",
      "ds_&_ml_portal",
      "transformer",
      "feature_extraction"
    ],
    "summary": "[!intuition] Think of attention like human reading behavior: when reading a complex sentence, we don't process all the words equally at every moment. Instead, we might \"attend\" more to certain words based on the context of what we\u2019ve read so far and what we're trying to understand. This is similar to what the attention mechanism does in neural networks. [!Summary] The attention mechanism is a key concept in modern [[ML]] particularly in natural language processing ([[NLP]]/[[LLM]]) and sequence-based models like neural machine translation (NMT). It was introduced to address the limitations of earlier models, like [[Recurrent Neural Networks|RNN]] and [[LSTM]]..."
  },
  "auc": {
    "title": "AUC",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\AUC.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "ROC (Receiver Operating Characteristic)"
    ],
    "inlinks": [
      "precision-recall_curve"
    ],
    "summary": "AUC (Area Under the Curve) is a metric for binary classification problems, representing the area under the [[ROC (Receiver Operating Characteristic)]] Key Concepts Represents the area under the ROC curve. AUC values range from 0 to 1, where 1 indicates perfect classification and 0.5 suggests no discriminative power (equivalent to random guessing). Roc and Auc Score The roc_auc_score is a function from the sklearn.metrics module in Python that computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. It is a widely used metric for evaluating the performance of binary classification models. Key Points about roc_auc_score:..."
  },
  "automated_feature_creation": {
    "title": "Automated Feature Creation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Automated Feature Creation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Feature Engineering",
      "Automated Feature Creation"
    ],
    "inlinks": [
      "automated_feature_creation"
    ],
    "summary": "Question: Can autodetect meaningful features. LINK Rough notes [[Feature Engineering]] was an ad-hoc manual process that depended on domain knowledge, intuition, data exploration and creativity. However, this process is dataset-dependent, time-consuming, tedious, subjective, and it is not a scalable solution. [[Automated Feature Creation]] automatically generates features using a framework; these features can be filtered using Feature Selection to avoid feature explosion. Below you can find some popular open source libraries for automated feature engineering: Pycaret \u2013 PyCaret Featuretools for advanced usage Home What is Featuretools? \u2014 Featuretools 1.1.0 documentation Optuna \u2013A hyperparameter optimization framework Feature-engine A Python library for Feature..."
  },
  "aws_lambda": {
    "title": "AWS Lambda",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\AWS Lambda.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Event Driven Events",
      "S3 bucket"
    ],
    "inlinks": [],
    "summary": "AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that allows you to run code without provisioning or managing servers. AWS Lambda is a powerful tool for building scalable, event-driven applications without the overhead of managing server infrastructure. With AWS Lambda, you can execute your code in response to various events, such as HTTP requests via Amazon API Gateway, changes to data in an Amazon S3 bucket, updates to a DynamoDB table, or messages arriving in an Amazon SQS queue. Key features of AWS Lambda include: [[Event Driven Events]]: AWS Lambda functions are triggered by events,..."
  },
  "azure": {
    "title": "Azure",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Azure.md",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_engineering_tools"
    ],
    "summary": "Public cloud computing platform from Microsoft offering various services like infrastructure, data storage, and machine learning."
  },
  "b-tree": {
    "title": "B-tree",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\B-tree.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_index"
    ],
    "summary": ""
  },
  "backpropagation": {
    "title": "Backpropagation in Neural Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Backpropagation.md",
    "tags": [
      "deep_learning",
      "ml_optimisation",
      "statistics"
    ],
    "aliases": [
      "Backprop",
      "BP",
      "backward propagation"
    ],
    "outlinks": [
      "Gradient Descent",
      "Supervised Learning",
      "Regularisation",
      "Adam Optimizer",
      "vanishing and exploding gradients problem",
      "Sympy"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "fitting_weights_and_biases_of_a_neural_network",
      "deep_learning",
      "activation_function",
      "forward_propagation",
      "named_entity_recognition"
    ],
    "summary": "[!Summary] Backpropagation is an essential algorithm in the training of neural networks and iteratively correcting its mistakes. It involves a process of calculating the gradient of the loss function $L(\\theta)$ concerning each weight in the network, allowing the system to update its weights via [[Gradient Descent]]. This process helps minimize the difference between predicted outputs and actual target values. Mathematically, the chain rule of calculus is employed to propagate errors backward through the network. Each layer in the network computes a partial derivative that is used to adjust the weights. This iterative approach continues until a convergence criterion is met,..."
  },
  "bag_of_words": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bag of words.md",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "ML_Tools",
      "Bag_of_Words.py",
      "TF-IDF"
    ],
    "inlinks": [
      "nlp",
      "naive_bayes",
      "word2vec",
      "tf-idf"
    ],
    "summary": "In [[ML_Tools]] see: [[Bag_of_Words.py]] In the context of natural language processing (NLP), the Bag of Words (BoW) model is a simple and commonly used ==method for text representation==. It converts text data into numerical form by treating each ==document as a collection of individual words, disregarding grammar and word order==. Here's how it works: Vocabulary Creation: A vocabulary is created from the entire corpus, which is a list of all unique words appearing in the documents. Vector Representation: Each document is represented as a vector, where each element corresponds to a word in the vocabulary. The value of each element..."
  },
  "bagging": {
    "title": "Bagging",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bagging.md",
    "tags": [
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "Model Ensemble",
      "Overfitting",
      "Random Forests"
    ],
    "inlinks": [
      "random_forests",
      "imbalanced_datasets",
      "bias_and_variance",
      "model_ensemble"
    ],
    "summary": "Overview: Bagging, short for Bootstrap Aggregating, is an [[Model Ensemble]] technique designed to improve the stability and accuracy of machine learning algorithms. It works by ==training multiple instances of the same learning algorithm on different subsets of the training data== and then ==combining their predictions.== How Bagging Works: Bootstrap Sampling: Bagging involves creating multiple subsets of the training data by sampling with replacement. This means that each subset, or \"bootstrap sample,\" is drawn randomly from the original dataset, and some data points may appear multiple times in a subset while others may not appear at all. Parallel Training: Each bootstrap..."
  },
  "bag_of_words.py": {
    "title": "Bag_of_Words.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bag_of_Words.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bag_of_words"
    ],
    "summary": "Summary of What the Script Does: It takes a dataset of text (movie reviews in this case) and processes it to remove HTML tags, non-alphabetic characters, and stopwords. It transforms the cleaned text into numerical features using the Bag of Words model, where each word in the reviews is counted and represented as a feature. It prints a sample of the top features (words) that were extracted from the reviews. This is a typical text preprocessing pipeline used to prepare textual data for machine learning models."
  },
  "bandit_example_output": {
    "title": "Bandit example output",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bandit example output.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Bandit_Example_Nonfixed.py",
      "Bandit_Example_Fixed.py"
    ],
    "inlinks": [
      "tool.bandit"
    ],
    "summary": "Complicated example output of bandit Running bandit on [[ML_Tools]] file [[Bandit_Example_Nonfixed.py]] gives. Fixing this gives [[Bandit_Example_Fixed.py]] ```bandit [main] INFO profile include tests: None [main] INFO profile exclude tests: None [main] INFO cli include tests: None [main] INFO cli exclude tests: None [main] INFO running on Python 3.10.8 Run started:2025-01-11 17:19:41.806346 Test results: Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module. Severity: Low Confidence: High CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html) More Info: https://bandit.readthedocs.io/en/1.8.0/blacklists/blacklist_imports.html#b404-import-subprocess Location: .\\Bandit_Example.py:1:0 1 import subprocess 2 import os 3 import pickle Issue: [B403:blacklist] Consider possible security implications associated with pickle module. Severity: Low Confidence: High CWE: CWE-502..."
  },
  "bandit_example_fixed.py": {
    "title": "Bandit_Example_Fixed.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bandit_Example_Fixed.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bandit_example_output"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Fixed.py"
  },
  "bandit_example_nonfixed.py": {
    "title": "Bandit_Example_Nonfixed.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bandit_Example_Nonfixed.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bandit_example_output",
      "tool.bandit"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Nonfixed.py"
  },
  "bash": {
    "title": "Bash",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bash.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Bash_folder"
    ],
    "inlinks": [
      "command_line",
      "command_prompt",
      "powershell_vs_bash"
    ],
    "summary": "Automation Scripts In [[ML_Tools]], see: [[Bash_folder]] Basic Commands Show Current Directory: bash pwd Display Contents of a Text File: bash cat filename.txt Search for a Word in a File: bash grep \"word\" filename.txt Replace Text in a File (Output Only): bash sed 's/old/new/g' filename.txt Writing and Running a Bash Script Create a Script: bash nano hello.sh Add: bash #!/bin/bash echo \"Hello, $(whoami)! Welcome to Bash scripting!\" Save and exit: Ctrl + O, Enter, Ctrl + X. Make the Script Executable: bash chmod +x hello.sh Run the Script: bash ./hello.sh Useful Bash Automation Tips Clear Screen: bash clear Keyboard Shortcut: Ctrl..."
  },
  "batch_normalisation": {
    "title": "Batch Normalisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Batch Normalisation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "vanishing and exploding gradients problem",
      "Overfitting",
      "Neural network",
      "Normalisation vs Standardisation",
      "Pasted image 20241219071904.png"
    ],
    "inlinks": [
      "normalisation"
    ],
    "summary": "Links: - Batch normalization | What it is and how to implement it Can be used to handle [[vanishing and exploding gradients problem]] and [[Overfitting]] problems within [[Neural network]]. First note: [[Normalisation vs Standardisation]] How does Batch normalisation work? Batch normalisation works by first standardising the inputs, then scales linearly - coefficients determined through training. This occurs between each layer. Outcomes of this process: - epochs take longer, but less epochs are required. Benefits: - Batch normalisation occurs at each layer, so do not need separate normalisation step for input data. - What about bias? We do not need bias..."
  },
  "batch_processing": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Batch Processing.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "Apache Spark",
      "Distributed Computing",
      "Databricks",
      "Batch Processing"
    ],
    "inlinks": [
      "lambda_architecture",
      "alternatives_to_batch_processing",
      "data_streaming",
      "batch_processing",
      "publish_and_subscribe",
      "ds_&_ml_portal",
      "map_reduce",
      "hadoop"
    ],
    "summary": "Batch Processing is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them together in a single batch. [[Apache Spark]] is the leading technology for batch processing, offering scalable and distributed data processing. It can handle unmanageable data sizes by using parallelism and [[Distributed Computing]] A key concept in batch processing is MapReduce: - Map: Splits the data into smaller, manageable pieces for parallel processing. - Reduce: Aggregates the processed data results from the individual tasks. - Order: The order of Map and Reduce steps is flexible; the..."
  },
  "bellman_equations": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bellman Equations.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "What are the Bellman equations that are used in RL?"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "[[What are the Bellman equations that are used in RL?]] Equations here may not be accurate. In reinforcement learning, Bellman's equations are fundamental to understanding how agents make decisions to maximize rewards over time. They are used to describe the relationship between the value of a state and the values of its successor states. There are two main types of Bellman's equations: Bellman Equation for State Value Function (V): This equation expresses the value of a state as the expected return starting from that state and following a particular policy. It is defined as: $$ V(s) = \\sum_{a} \\pi(a|s) \\sum_{s'}..."
  },
  "benefits_of_data_transformation": {
    "title": "Benefits of Data Transformation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Benefits of Data Transformation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Transformation",
      "Interoperability",
      "Data Quality"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Benefits of [[Data Transformation]] Efficiency: Faster query performance. [[Interoperability]]: Converting data into the required format for target systems. Enrichment: Adding contextual data for better insights. [[Data Quality]]: Validating, cleansing, and deduplicating data."
  },
  "bernoulli": {
    "title": "Bernoulli",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bernoulli.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parametric_vs_non-parametric_models",
      "distributions"
    ],
    "summary": ""
  },
  "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\BERT Pretraining of Deep Bidirectional Transformers for Language Understanding.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": ""
  },
  "bert": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\BERT.md",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": null,
    "outlinks": [
      "Transformer",
      "NLP",
      "Google",
      "BERT Pretraining of Deep Bidirectional Transformers for Language Understanding",
      "Vector Embedding|embedding",
      "Positional Encoding",
      "Transfer Learning",
      "Transformer",
      "Transfer Learning",
      "Named Entity Recognition|NER",
      "Summarisation",
      "Sentence Similarity"
    ],
    "inlinks": [
      "transformers_vs_rnns",
      "word2vec.py",
      "vector_embedding",
      "attention_mechanism",
      "rag",
      "small_language_models",
      "lstm",
      "ds_&_ml_portal",
      "transformer"
    ],
    "summary": "BERT (==Bidirectional Encoder Representations from [[Transformer]]==) is used in [[NLP]]processing, developed by [[Google]]. Introduced in the paper \"[[BERT Pretraining of Deep Bidirectional Transformers for Language Understanding]]\" in 2018. It is forward & backward looking in the context. BERT is a stack of encoders -learning context. Input [[Vector Embedding|embedding]]: - [[Positional Encoding]]: passes location info to encoder - Sentence embeddings: differences between sentences - Token embeddings Training of BERT: - Masked Language modelling (hiding words) - Next Sentence Prediction Fine tuning ([[Transfer Learning]]) BERT model: - New output layer dependent Resources: - What is BERT and how does it work? |..."
  },
  "bertscore": {
    "title": "BERTScore",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\BERTScore.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "accessing_gen_ai_generated_content"
    ],
    "summary": ""
  },
  "bias_and_variance": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bias and variance.md",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": null,
    "outlinks": [
      "Overfitting",
      "Regularisation",
      "Boosting",
      "Bagging"
    ],
    "inlinks": [
      "machine_learning_algorithms",
      "cross_validation",
      "overfitting"
    ],
    "summary": "Related to [[Overfitting]] Ways to Reduce Bias and Variance: - [[Regularisation]] - [[Boosting]] - [[Bagging]] What is Bias in Machine Learning? Bias occurs when a model produces ==consistently unfair or inaccurate results==, usually caused during training due to design choices. What Does High Bias Mean for a Machine Learning Model? High bias refers to a situation where a model has a strong and often ==simplistic assumption== about the underlying data, leading to underfitting. It is biased to the data. What is the Variance of a Machine Learning Model? Variance measures how much a ==model's predictions change when trained on different..."
  },
  "big_data": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Big Data.md",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "Apache Spark",
      "Hadoop",
      "Scala",
      "Databricks",
      "Data Storage",
      "Apache Spark|Spark",
      "Big Data"
    ],
    "inlinks": [
      "big_data",
      "parquet",
      "databricks",
      "scala",
      "hadoop"
    ],
    "summary": "The concept of Big Data revolves around datasets that are too large or complex to be managed using traditional data processing techniques. It\u2019s characterized by four main attributes, commonly referred to as the ==Four V\u2019s:== Volume: The sheer amount of data being generated, often in terabytes, petabytes, or even exabytes. Variety: The diversity in data types, including structured, semi-structured, and unstructured data (e.g., text, images, videos). Velocity: The speed at which data is generated and needs to be processed in real-time or near-real-time. Veracity: The uncertainty or quality of the data, addressing issues like noise, biases, or incomplete data. Big..."
  },
  "big_o_notation": {
    "title": "What is the Big-O Notation?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Big O Notation.md",
    "tags": [
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "mathematics"
    ],
    "summary": "Big-O Notation is an analysis of the algorithm using Big \u2013 O asymptotic notation. Mostly related to computing rather than storage Doing things not exponentially, such as copying the same data many times, will save lots of performance and money. We can express algorithmic complexity using the big-O notation. For a problem of size N: - A constant-time function/method is \u201corder 1\u201d : O(1) - A linear-time function/method is \u201corder N\u201d : O(N) - A quadratic-time function/method is \u201corder N squared\u201d : O(N^2)"
  },
  "bigquery": {
    "title": "BigQuery",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\BigQuery.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Warehouse",
      "Google"
    ],
    "inlinks": [
      "google_cloud_platform",
      "elt"
    ],
    "summary": "cloud-based [[Data Warehouse]] BigQuery is a fully managed, serverless data warehouse offered by [[Google]] Cloud Platform (GCP). It is designed to handle large-scale data analytics and allows users to run fast SQL queries on massive datasets. Serverless Architecture: BigQuery is serverless, meaning users do not need to manage any infrastructure. Google handles the provisioning of resources, scaling, and maintenance, allowing users to focus on analyzing data. Scalability: BigQuery can scale to handle petabytes of data, making it suitable for large datasets and complex queries. SQL Support: BigQuery supports standard SQL, making it accessible to users familiar with SQL syntax. It..."
  },
  "binary_classification": {
    "title": "Binary Classification",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Binary Classification.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Classification"
    ],
    "inlinks": [
      "determining_threshold_values",
      "fitting_weights_and_biases_of_a_neural_network",
      "use_cases_for_a_simple_neural_network_like",
      "cosine_similarity",
      "typical_output_formats_in_neural_networks",
      "logistic_regression",
      "precision-recall_curve",
      "activation_function",
      "ds_&_ml_portal"
    ],
    "summary": "Binary classification is a type of [[Classification]] task that involves predicting one of two possible classes or outcomes. It is used in scenarios where the goal is to categorize data into two distinct groups, such as spam vs. not spam in email filtering or disease vs. no disease in medical diagnosis."
  },
  "binder": {
    "title": "Binder",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Binder.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://mybinder.org/"
  },
  "boosting": {
    "title": "Boosting",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Boosting.md",
    "tags": [
      "model_architecture",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "Model Ensemble",
      "Weak Learners",
      "Weak Learners",
      "Ada boosting",
      "Gradient Boosting",
      "XGBoost",
      "Interpretability"
    ],
    "inlinks": [
      "bias_and_variance",
      "gradient_boosting",
      "model_ensemble",
      "gradient_boosting_regressor",
      "ada_boosting"
    ],
    "summary": "Boosting is a type of [[Model Ensemble]] in machine learning that focuses on improving the accuracy of predictions by building a ==sequence of models==. Each subsequent model focuses on correcting the errors made by the previous ones. It combines [[Weak Learners]] (models that are slightly better than random guessing) to create a strong learner. Key Concepts of Boosting: Sequential Learning: Boosting involves training models sequentially. Each new model is trained to correct the errors made by the previous models. This means that the models are not independent of each other; instead, ==each model is built on the mistakes of the..."
  },
  "bootstrap": {
    "title": "Bootstrap",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Bootstrap.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "sampling with replacement from an original dataset."
  },
  "boxplot": {
    "title": "Boxplot",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Boxplot.md",
    "tags": [
      "#statistics",
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "standardised/Outliers",
      "Distributions",
      "Data Cleansing"
    ],
    "inlinks": [
      "variance",
      "violin_plot",
      "distributions",
      "anomaly_detection"
    ],
    "summary": "A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the dataset. Key Components Uses: - Identifying [[standardised/Outliers]]. - Understanding the spread and skewness of the data [[Distributions]]. - Comparing distributions across different categories. - Need to remove then in order to do [[Data Cleansing]]. Components: - Minimum: The smallest data point excluding outliers. - First Quartile (Q1): The median of the lower half of the dataset. - Median (Q2): The..."
  },
  "business_intelligence": {
    "title": "What is Business Intelligence",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\business intelligence.md",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "rollup",
      "Single Source of Truth"
    ],
    "inlinks": [
      "data_scientist",
      "granularity",
      "data_ingestion"
    ],
    "summary": "Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization\u2019s business decisions. The new term is Data Engineer. The language of a BI engineer is SQL. Goals of BI BI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail: [[rollup]] capability - (data) Visualization over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance. Drill-down possibilities - from the above high-level overview drill down the very details to figure out..."
  },
  "business_observability": {
    "title": "Business observability",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Business observability.md",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "Model Observability|observability",
      "Data Collection",
      "Data Visualisation"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "Business [[Model Observability|observability]] refers to the ability to gain insights into the internal state and performance of a business through the continuous monitoring and analysis of data. It involves collecting, analyzing, and visualizing data from various sources to understand how different parts of the business are functioning and to identify areas for improvement. Business observability aims to provide a comprehensive view of operations, customer interactions, and other critical aspects to enable data-driven decision-making. It helps businesses to detect issues early, optimize operations, enhance customer experiences, and drive growth and innovation. Key components of business observability include: [[Data Collection]]: Gathering data..."
  },
  "career_interest": {
    "title": "Career Interest",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Career Interest.md",
    "tags": [],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "This is a portal to notes that I find relevant to my career:"
  },
  "casual_inference": {
    "title": "Casual Inference",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Casual Inference.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "missing data problem"
  },
  "catboost": {
    "title": "CatBoost",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\CatBoost.md",
    "tags": null,
    "aliases": [
      "CAT"
    ],
    "outlinks": [
      "Gradient Boosting",
      "categorical",
      "Hyperparameter|Hyperparameter tuning"
    ],
    "inlinks": [
      "gradient_boosting",
      "optuna",
      "lightgbm_vs_xgboost_vs_catboost"
    ],
    "summary": "CatBoost is a [[Gradient Boosting]] library developed by Yandex, designed to handle [[categorical]] features efficiently and provide robust performance with minimal [[Hyperparameter|Hyperparameter tuning]] It is particularly useful in scenarios where datasets contain a significant number of categorical variables. Key Advantages Handling Categorical Features: CatBoost natively processes categorical features without the need for extensive preprocessing like one-hot encoding, which simplifies the workflow and reduces the risk of introducing errors during data preparation. Robustness to Overfitting: It employs techniques such as ordered boosting and per-feature scaling to reduce overfitting, making it a reliable choice for complex datasets. Performance: CatBoost offers competitive performance..."
  },
  "central_limit_theorem": {
    "title": "Central Limit Theorem",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Central Limit Theorem.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Why is the Central Limit Theorem important when working with small sample sizes",
      "Central Limit Theorem"
    ],
    "inlinks": [
      "z-test",
      "statistics",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "central_limit_theorem"
    ],
    "summary": "The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the original distribution. [[Why is the Central Limit Theorem important when working with small sample sizes]] Key Points Mean of Sampling Distribution: The mean of the sampling distribution is equal to the mean of the original population. Variance of Sampling Distribution: The variance of the sampling distribution is the population variance divided by the sample size ((n)), making it (n) times smaller. Applicability: The CLT applies when calculating the sum or..."
  },
  "chain_of_thought": {
    "title": "Chain of thought",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Chain of thought.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling",
      "deepseek",
      "llm"
    ],
    "summary": "Chain of Thought (CoT) reasoning in AI systems is a cognitive-inspired framework that improves the performance of large language models (LLMs) by explicitly guiding the AI through intermediate reasoning steps. Advantages of Chain of Thought: Improved Interpretability: Since the model outputs intermediate steps, it's easier for humans to understand how the final answer was reached. Better Performance on Complex Tasks: CoT allows the model to handle multi-step reasoning more effectively. Easier Debugging: If there's an error in reasoning, it can be spotted at a specific step in the chain, which aids in model fine-tuning and debugging."
  },
  "change_management": {
    "title": "Change Management",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Change Management.md",
    "tags": [
      "business"
    ],
    "aliases": [
      "Change Program"
    ],
    "outlinks": [],
    "inlinks": [
      "prevention_is_better_than_the_cure",
      "data_quality"
    ],
    "summary": "Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a desired future state. It involves - preparing, - supporting, - and helping people to adopt change in order to drive organizational success and outcomes. Effective change management helps - minimize resistance, - improves engagement, - and increases the likelihood of successful outcomes. The process typically includes: Planning: Identifying the need for change, defining the change, and developing a strategy to implement it. Communication: Clearly explaining the reasons for the change, the benefits, and the impact on the organization and its people. Training..."
  },
  "checksum": {
    "title": "Checksum",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Checksum.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data storage",
      "algorithms",
      "Security"
    ],
    "inlinks": [
      "data_integrity"
    ],
    "summary": "A checksum is a value calculated from a data set that is used to verify the integrity of that data. It acts as a fingerprint for the data, allowing systems to detect errors or alterations that may occur during storage, processing, or transmission. When data is sent or stored, a checksum is generated based on the contents of the data. This checksum is then sent or stored alongside the data. Upon retrieval or receipt, the checksum is recalculated from the data and compared to the original checksum. If the two checksums match, it indicates that the data has remained unchanged..."
  },
  "chi-squared_test": {
    "title": "Chi-Squared Test",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Chi-Squared Test.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Chi-Squared Test The Chi-squared test is used to determine if there is a significant association between categorical variables. It assesses whether the observed frequencies in a contingency table differ from the expected frequencies, assuming the data is independent."
  },
  "choosing_a_threshold": {
    "title": "Choosing a Threshold",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Choosing a Threshold.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ROC (Receiver Operating Characteristic)",
      "Precision-Recall Curve",
      "Cost-Sensitive Analysis"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal threshold depends on the specific problem and the desired trade-off between different types of errors: Manual Selection: Based on domain expertise or prior knowledge, choose a threshold that seems reasonable. Receiver Operating Characteristic ([[ROC (Receiver Operating Characteristic)]]) Curve Analysis: Plot the true positive rate (TPR) against the false positive rate (FPR) for different threshold values. The optimal threshold often lies near the \"elbow\" of the ROC curve, where a small increase in FPR results in a significant increase in TPR. [[Precision-Recall Curve]] Analysis: Plot the precision against the recall for different threshold values. The optimal threshold often lies..."
  },
  "choosing_the_number_of_clusters": {
    "title": "Choosing the Number of Clusters",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Choosing the Number of Clusters.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "granularity",
      "WCSS and elbow method",
      "Silhouette Analysis"
    ],
    "inlinks": [
      "neural_network_classification"
    ],
    "summary": "The optimal number of clusters ([[clustering]]) depends on the data and the desired level of [[granularity]]. Here are some common approaches: Elbow Method: [[WCSS and elbow method]]: Plot the within-cluster sum of squares (WCSS) as a function of the number of clusters. The optimal number of clusters is often the point where the WCSS starts to decrease slowly. [[Silhouette Analysis]]: Calculate the silhouette coefficient for each data point, which measures how similar a data point is to its own cluster compared to other clusters. The optimal number of clusters 1 is often the one that maximizes the average silhouette coefficient.T"
  },
  "ci-cd": {
    "title": "CI-CD",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\CI-CD.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Continuous Integration",
      "Continuous Delivery/Deployment",
      "Software Development Life Cycle",
      "Gitlab",
      "Docker"
    ],
    "inlinks": [
      "devops",
      "gitlab"
    ],
    "summary": "CI/CD stands for [[Continuous Integration]] and [[Continuous Delivery/Deployment]]. It is a set of practices aimed at streamlining and accelerating the [[Software Development Life Cycle]]. The main goals of CI/CD are to improve software quality, reduce integration issues, and deliver updates to users more frequently and reliably. Tools and Technologies - [[Gitlab]] - [[Docker]]"
  },
  "class_separability": {
    "title": "Class Separability",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Class Separability.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Imbalanced Datasets",
      "classification",
      "accuracy",
      "feature engineering"
    ],
    "inlinks": [],
    "summary": "If you have a perfectly balanced dataset (unlike [[Imbalanced Datasets]]) but still experience poor [[classification]] [[accuracy]], class separability might be an issue due to the following reasons: Overlapping Classes: The features of different classes may overlap significantly, making it difficult for the model to distinguish between them. If the decision boundaries are not well-defined, the model may struggle to classify instances correctly. Complex Decision Boundaries: The underlying relationship between the features and the classes may be complex, requiring a more sophisticated model to capture the nuances. If the model is too simple, it may not be able to learn the..."
  },
  "classification_report": {
    "title": "Classification Report",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Classification Report.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Precision",
      "Recall",
      "F1 Score",
      "ML_Tools",
      "Evaluation_Metrics.py",
      "Pasted image 20240404163858.png|500"
    ],
    "inlinks": [
      "precision"
    ],
    "summary": "The classification_report function in sklearn.metrics is used to evaluate the performance of a classification model. It provides a summary of key metrics for each class, including precision, recall, F1-score, and support. Function Signature python sklearn.metrics.classification_report( y_true, y_pred, , labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn' ) Parameters: - y_true: Array of true labels. - y_pred: Array of predicted labels. - labels: (Optional) List of label indices to include in the report. - target_names: (Optional) List of string names for the labels. - sample_weight: (Optional) Array of weights for each sample. - digits: Number of decimal places for formatting output. - output_dict:..."
  },
  "classification": {
    "title": "Classification",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Classification.md",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "Supervised Learning",
      "Regression",
      "Naive Bayes",
      "Decision Tree",
      "Support Vector Machines",
      "K-nearest neighbours",
      "Neural network",
      "Model Ensemble",
      "Interpretability"
    ],
    "inlinks": [
      "precision_or_recall",
      "precision",
      "k-nearest_neighbours",
      "accuracy",
      "cross_entropy",
      "ds_&_ml_portal",
      "class_separability",
      "supervised_learning",
      "recall",
      "confusion_matrix",
      "typical_output_formats_in_neural_networks",
      "neural_network_classification",
      "machine_learning_algorithms",
      "learning_styles",
      "imbalanced_datasets",
      "binary_classification",
      "decision_tree",
      "loss_function",
      "gini_impurity",
      "support_vector_machines"
    ],
    "summary": "Classification is a type of [[Supervised Learning]] in machine learning, where the algorithm learns from labeled data to predict which category or class a new, unlabeled data point belongs to. The goal is to assign the correct label to input data based on patterns learned from the training set. Examples of Classifiers Classifier: A model used for classification tasks, predicting discrete labels or categories. For example, determining whether an email is spam or not, or identifying the species of a flower based on its features. This contrasts with a Regressor ([[Regression]]), which predicts continuous values. [[Naive Bayes]] [[Decision Tree]] [[Support..."
  },
  "claude": {
    "title": "Claude",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Claude.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Claude is better for code and uses Artifact for tracking code changes. Claude is crazy see: https://youtu.be/RudrWy9uPZE?t=473"
  },
  "cleaning_terminal_path": {
    "title": "cleaning terminal path",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\cleaning terminal path.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.youtube.com/watch?v=18hUejOK0qk cmd prompt $g powershell ```powershell $profile microsfot_Powershell_profile have function prompt{ $p = -path \"$p> \" } ``` getting the script working https://stackoverflow.com/questions/41117421/ps1-cannot-be-loaded-because-running-scripts-is-disabled-on-this-system"
  },
  "click_implementation.py": {
    "title": "Click_Implementation.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Click_Implementation.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "python_click"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Click_Implementation.py This script implements a command-line interface (CLI) tool using Python's click library. The CLI allows users to interact with a JSON file, enabling them to view keys, retrieve values, and update key-value pairs. Functionality Overview CLI Initialization (cli function) Serves as the main command group. Accepts a JSON file (document) as an argument. Reads the JSON file and stores its content in ctx.obj, making it accessible to all subcommands. Displaying Keys (show_keys command) Lists all top-level keys in the JSON document. Retrieving a Value (get_value command) Accepts a key as an argument. Prints the corresponding value if the key..."
  },
  "cloud_providers": {
    "title": "What are the top Cloud Providers?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cloud Providers.md",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "Databricks",
      "Data Warehouse",
      "Data Lakehouse|Lakehouse",
      "Scaling Server",
      "Load Balancing",
      "Memory Caching"
    ],
    "inlinks": [
      "data_storage",
      "parquet",
      "storage_layer_object_store"
    ],
    "summary": "Among the biggest cloud providers are AWS, Microsoft Azure, Google Cloud. Whereas [[Databricks]] ( Databrick) and Snowflake provide dedicated [[Data Warehouse]]and [[Data Lakehouse|Lakehouse]] solutions Features [[Scaling Server]] [[Load Balancing]] [[Memory Caching]]"
  },
  "clustering": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Clustering.md",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "Unsupervised Learning",
      "standardised/Outliers|anomalies",
      "Anomaly Detection",
      "K-means",
      "DBScan",
      "Hierarchical Clustering",
      "Gaussian Mixture Models",
      "Interpretability",
      "Feature Scaling",
      "Correlation",
      "Dendrograms"
    ],
    "inlinks": [
      "unsupervised_learning",
      "anomaly_detection_with_clustering",
      "ds_&_ml_portal",
      "silhouette_analysis",
      "model_parameters",
      "multicollinearity",
      "data_mining_-_crisp",
      "neural_network_classification",
      "machine_learning_algorithms",
      "tf-idf",
      "feature_selection",
      "wcss_and_elbow_method",
      "learning_styles",
      "choosing_the_number_of_clusters",
      "gaussian_mixture_models",
      "correlation",
      "problem_definition",
      "covariance_structures",
      "dbscan"
    ],
    "summary": "Clustering involves grouping a set of data points into subsets or clusters based on inherent patterns or similarities. It is an [[Unsupervised Learning]]technique used for tasks like customer segmentation and [[standardised/Outliers|anomalies]] detection. The primary goal of clustering is to organize data by grouping similar items. Applications Customer Segmentation: Group customers with similar purchasing behavior or demographics for targeted marketing. Image Segmentation: Group pixels in an image based on color or texture to identify objects or regions. [[Anomaly Detection]]: Identify clusters of normal behavior to detect anomalies that deviate significantly from these clusters. Methods [[K-means]] [[DBScan]] [[Hierarchical Clustering]] [[Gaussian Mixture Models]]..."
  },
  "clustering_dashboard.py": {
    "title": "Clustering_Dashboard.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Clustering_Dashboard.py.md",
    "tags": [
      "code_snippet"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "dash"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Clustering_Dashboard.py"
  },
  "clustermap": {
    "title": "Clustermap",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Clustermap.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Preprocessing|Preprocess",
      "Dendrograms"
    ],
    "inlinks": [],
    "summary": "Clustermap Related to: [[Preprocessing|Preprocess]] Purpose: Identify which features are most similar using [[Dendrograms]]. Visualization: Regions of color show clustering, similar to a heatmap. Functionality: Performs clustering on both rows and columns. Requirements: Input should be numerical; data needs to be scaled. python import seaborn as sns sns.clustermap(x_scaled, cmap='mako', standard_scale=0) # 0 for rows, 1 for columns Resources Video Explanation Seaborn Clustermap Documentation"
  },
  "code_diagrams": {
    "title": "Code Diagrams",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Code Diagrams.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Documentation & Meetings",
      "Classes",
      "Mermaid",
      "Architecture Diagram",
      "Sequence diagram"
    ],
    "inlinks": [],
    "summary": "[[Documentation & Meetings]] There are class diagrams showing the hierarchy of classes [[Classes]] (Object orientated). Done in [[Mermaid]]. Overall [[Architecture Diagram]]: showing how software components interact. [[Sequence diagram]] of componets interact. Sequence diagraph: how the componets interact Architecture diagram : main componts fitting together"
  },
  "columnar_storage": {
    "title": "Columnar Storage",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Columnar Storage.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Querying"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "duckdb",
      "database_storage",
      "types_of_database_schema",
      "row-based_storage",
      "vectorized_engine"
    ],
    "summary": "A database storage technique that stores ==data by columns== rather than rows, Useful for read-heavy operations and ==large-scale data analytics==, as it enables the retrieval of specific columns without the need to access the entire row. Columnar Storage Example (Analytical Workloads)**: | order_id | customer_id | order_date | order_amount | |-------------|---------------|--------------|----------------| | 1 | 101 | 2024-10-01 | $100 | | 2 | 102 | 2024-10-02 | $150 | | 3 | 103 | 2024-10-03 | $200 | In columnar storage, the data would be stored by columns, like: - customer_id: [101, 102, 103] If you're querying for the total..."
  },
  "command_line": {
    "title": "Command Line",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Command Line.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "PowerShell",
      "Powershell vs Bash",
      "Bash",
      "Command Prompt"
    ],
    "inlinks": [],
    "summary": "The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts, and perform various tasks. [[PowerShell]] [[Powershell vs Bash]] [[Bash]] [[Command Prompt]]"
  },
  "command_prompt": {
    "title": "Command Prompt",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Command Prompt.md",
    "tags": [
      "software"
    ],
    "aliases": [
      "cmd"
    ],
    "outlinks": [
      "Bash",
      "PowerShell"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_line"
    ],
    "summary": "Command Prompt (cmd) is a command-line interpreter on Windows systems that allows users to execute commands to perform various basic tasks. Below are some common tasks that can be performed in cmd, along with examples: Related to: - [[Bash]] 1. Navigating the File System Changing Directories: cmd cd C:\\path\\to\\directory Changes the current directory to C:\\path\\to\\directory. Listing Files and Directories: cmd dir Lists the files and directories in the current directory. 2. Managing Files and Directories Creating a Directory: cmd mkdir newfolder Creates a new directory named newfolder. Deleting a Directory: cmd rmdir /s /q newfolder Deletes the directory newfolder and..."
  },
  "common_security_vulnerabilities_in_software_development": {
    "title": "Common Security Vulnerabilities in Software Development",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Common Security Vulnerabilities in Software Development.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Security",
      "Software Development Portal",
      "tool.bandit",
      "Input is Not Properly Sanitized",
      "Why JSON is Better than Pickle for Untrusted Data",
      "SQL Injection"
    ],
    "inlinks": [
      "testing",
      "security",
      "tool.bandit"
    ],
    "summary": "[[Security]] vulnerabilities can be encountered and mitigated in [[Software Development Portal]]. In this not describe potential security risks in their applications. Useful Tools - [[tool.bandit]] Examples Command Injection General Description: Command injection is a security vulnerability that occurs when an attacker is able to execute arbitrary commands on the host operating system via a vulnerable application. This typically happens when user input is improperly handled and passed to a system shell. Example: The dangerous_subprocess function uses subprocess.call with shell=True, which can lead to command injection if user input is not properly sanitized. python import subprocess def dangerous_subprocess(user_input): subprocess.call(user_input, shell=True) Mitigation:..."
  },
  "common_table_expression": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Common Table Expression.md",
    "tags": [
      "database",
      "querying"
    ],
    "aliases": [
      "CTE"
    ],
    "outlinks": [
      "Views",
      "Querying|Queries",
      "DE_Tools",
      "Views",
      "Recursive Algorithm"
    ],
    "inlinks": [
      "views"
    ],
    "summary": "A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a [[Views]]. Serve as temporary views for a single [[Querying|Queries]]. sql WITH cte_query AS (SELECT \u2026 subquery ...) SELECT main query ... FROM/JOIN with cte_query ... In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/ExplorationsSQLite/Utilities/Common_Table_Expression.ipynb Non-Recursive CTE The non-recursive are simple where CTE is used to ==avoid SQL duplication== by referencing a name instead of the actual SQL statement. See [[Views]] simplification usage. sql WITH avg_per_store AS (SELECT store, AVG(amount) AS average_order FROM orders..."
  },
  "communication_principles": {
    "title": "Communication principles",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Communication principles.md",
    "tags": [
      "communication"
    ],
    "aliases": [],
    "outlinks": [
      "Pasted image 20240916075433.png",
      "Pasted image 20240916075439.png"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20240916075433.png]] ![[Pasted image 20240916075439.png]]"
  },
  "communication_techniques": {
    "title": "Communication Techniques",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Communication Techniques.md",
    "tags": [
      "communication"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Overview Using these structured communication bridges can enhance clarity and engagement, especially in spontaneous or high-stakes discussions. Tips for Using Communication Bridges 1. Start Small: Begin by integrating 2-3 bridges that feel natural to you. 2. Observe Reactions: Notice how listeners respond when you clarify changes, summarize key points, or highlight actions. 3. Practice Consistency: Make these bridges a regular part of your speaking style. Speak More Clearly: 8 Precise Steps to Improve Communication 1. Context Bridge Purpose: Aligns everyone by setting the context before diving into details. How to Use: Start with phrases like: \"At a high level...\" \"This..."
  },
  "comparing_llm": {
    "title": "Comparing LLM",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Comparing LLM.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "LLM"
    ],
    "inlinks": [],
    "summary": "Use lmarena.ai as a bench marking tool. [[LLM]] web dev arena text to image leader board"
  },
  "comparing_ensembles.py": {
    "title": "Comparing_Ensembles.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Comparing_Ensembles.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Ensemble"
    ],
    "inlinks": [
      "model_ensemble"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Comparing_Ensembles.py This script explores various [[Model Ensemble]] techniques in machine learning, demonstrating their effectiveness in improving prediction accuracy through programmatic examples [!Important Notes] - {{Note 1: Consider adding more ensemble techniques like Stacking or Isolated Forests for a comprehensive comparison.}} - {{Note 2: Observe how different base estimators impact the performance of the ensemble methods.}} [!Follow-Up Questions] - {{How can ensemble methods be optimized for larger datasets?}} - {{What are the trade-offs between different ensemble techniques in terms of complexity and performance?}}"
  },
  "components_of_the_database": {
    "title": "Components of the database",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Components of the database.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Fact Table",
      "Dimension Table",
      "Obsidian_CSP0FnAVD1.png"
    ],
    "inlinks": [
      "database",
      "data_engineering_portal"
    ],
    "summary": "[[Fact Table]] in main table that [[Dimension Table]] connect to them. ![[Obsidian_CSP0FnAVD1.png]]"
  },
  "computer_science": {
    "title": "Computer Science",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Computer Science.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Algorithms"
    ],
    "inlinks": [],
    "summary": "[[Algorithms]]"
  },
  "concatenate": {
    "title": "Concatenate",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Concatenate.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": ""
  },
  "conceptual_data_model": {
    "title": "conceptual data model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\conceptual data model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_schema"
    ],
    "summary": ""
  },
  "conceptual_model": {
    "title": "Conceptual Model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Conceptual Model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ER Diagrams"
    ],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Conceptual Model - Entities: Customer, Order, Book - Relationships: Customers place Orders, Orders include Books Conceptual Model - Focuses on high-level business requirements. - Defines important data entities and their relationships. - Tools: [[ER Diagrams]], ER Studio, DbSchema."
  },
  "concurrency": {
    "title": "Concurrency",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Concurrency.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DE_Tools"
    ],
    "inlinks": [
      "database_techniques",
      "transaction",
      "sqlite"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb"
  },
  "confidence_interval": {
    "title": "Confidence Interval",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Confidence Interval.md",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "statistics",
      "model_interpretability",
      "data_analysis"
    ],
    "summary": "A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with a confidence level, such as 95%, indicating the probability that the interval captures the true parameter. Key Points - Confidence Level: The likelihood that the interval includes the true parameter (e.g., 95%). - Purpose: Quantifies the uncertainty of an estimate, providing a range rather than a single value. Example A 95% confidence interval for a mean of (50, 60) suggests that, in repeated sampling, 95% of such intervals would contain the true mean."
  },
  "confusion_matrix": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Confusion Matrix.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "Classification",
      "Pasted image 20240120215414.png",
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "Specificity",
      "Recall",
      "Pasted image 20240116205937.png|500",
      "Pasted image 20240116210541.png|500"
    ],
    "inlinks": [
      "accuracy",
      "evaluation_metrics",
      "logistic_regression"
    ],
    "summary": "Description A Confusion Matrix is a table used to evaluate the performance of a [[Classification]] model. It provides a detailed breakdown of the model's predictions across different classes, showing the number of true positives, true negatives, false positives, and false negatives. Purpose The confusion matrix helps identify where the classifier is making errors, indicating where it is \"confused\" in its predictions. Structure ![[Pasted image 20240120215414.png]] Structure True Positives (TP): Correctly predicted positive instances. False Positives (FP): Incorrectly predicted positive instances (Type 1 error). True Negatives (TN): Correctly predicted negative instances. False Negatives (FN): Incorrectly predicted negative instances (Type 2 error)...."
  },
  "continuous_delivery_-_deployment": {
    "title": "Continuous Delivery - Deployment",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Continuous Delivery - Deployment.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Deployment"
    ],
    "inlinks": [],
    "summary": "Continuous Delivery - Ensures that code changes are automatically prepared for a release to production. - Builds, tests, and releases are automated, but the deployment is manual. Continuous Deployment: - Extends continuous delivery by automating the deployment process. - Every change that passes the automated tests is deployed to production automatically. [[Model Deployment]] A continuous integration and continuous deployment (CI/CD) pipeline is a series of steps that must be performed in order to deliver a new version of software"
  },
  "continuous_integration": {
    "title": "Continuous Integration",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Continuous Integration.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "data_engineer",
      "ci-cd"
    ],
    "summary": "Developers frequently integrate code into a shared repository. Automated builds and tests are run to detect issues early. Encourages smaller, more manageable code changes."
  },
  "converting_categorical_variables_to_a_dummy_indicators": {
    "title": "Converting categorical variables to a dummy indicators",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Converting categorical variables to a dummy indicators.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "convolutional_neural_networks": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Convolutional Neural Networks.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Deep Learning",
      "Feature Extraction",
      "Pasted image 20241006124829.png|500",
      "Pasted image 20241006124735.png|500"
    ],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Convolutional networks, or CNNs, are specialized [[Deep Learning]] architectures designed for processing data with grid-like structures, such as images. They use convolutional layers with learnable filters to extract spatial features from the input data. The convolutional operation involves sliding these filters across the input, performing element-wise multiplications and summations to create feature maps. CNNs are particularly effective for image classification, object detection, and image segmentation tasks. Primarily used in image recognition and processing tasks. CNNs use convolutional layers to automatically detect spatial patterns in images, like edges and textures. Pooling: The idea of pooling in convolutional neural networks is to..."
  },
  "correlation_vs_causation": {
    "title": "Correlation vs Causation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Correlation vs Causation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Correlation"
    ],
    "inlinks": [
      "statistics",
      "correlation"
    ],
    "summary": "What is the meaning of [[Correlation]] does not imply causation? Correlation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. Correlation: Indicates an association between variables but does not imply that changes in one variable cause changes in the other. Causation: Suggests a direct cause-and-effect relationship between variables, requiring experimentation to establish."
  },
  "correlation": {
    "title": "Correlation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Correlation.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Correlation vs Causation",
      "standardised/Outliers",
      "Covariance",
      "Feature Selection",
      "multicollinearity",
      "Heatmap",
      "Clustering",
      "'var1', 'target'"
    ],
    "inlinks": [
      "pca_principal_components",
      "eda",
      "filter_methods",
      "multicollinearity",
      "data_selection_in_ml",
      "heatmap",
      "correlation_vs_causation",
      "covariance",
      "ds_&_ml_portal",
      "clustering",
      "feature_selection"
    ],
    "summary": "Use in understanding relationships between variables in data analysis. While it helps identify associations, it's important to remember that ==correlation does not imply causation.== Visualization tools like heatmaps and clustering can aid in identifying and interpreting these relationships effectively. What is Correlation?: A measure of the strength and direction of the relationship between two variables. Description Correlation measures the relationship between two variables, indicating how they change together. It ranges from -1 to 1: -1: Perfect negative correlation 0: No correlation 1: Perfect positive correlation Key Points [[Correlation vs Causation]]: Correlation does not imply causation. While correlation highlights associations, causation..."
  },
  "cosine_similarity": {
    "title": "Cosine Similarity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cosine Similarity.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Metric",
      "Binary Classification",
      "TF-IDF"
    ],
    "inlinks": [
      "word2vec.py",
      "vector_database"
    ],
    "summary": "Cosine similarity is a [[Metric]] used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from -1 to 1, where 1 indicates identical orientation, 0 indicates orthogonality, and -1 indicates opposite orientation. Cosine similarity is commonly used in text analysis, information retrieval, and recommendation systems to compare document similarity, user preferences, or item features. In [[Binary Classification]], cosine similarity can be used as a feature to help distinguish between two classes. For instance, in text classification tasks, you might represent documents as vectors using techniques like [[TF-IDF]]. By calculating the cosine..."
  },
  "cost_function": {
    "title": "Cost Function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cost Function.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Optimisation",
      "Loss Function",
      "Loss versus Cost function",
      "Model Parameters",
      "Gradient Descent",
      "Pasted image 20241216202825.png|500",
      "Pasted image 20241216202917.png|500",
      "Reward Function",
      "Reinforcement learning"
    ],
    "inlinks": [
      "lbfgs",
      "model_parameters_tuning",
      "loss_versus_cost_function",
      "gradient_descent",
      "momentum",
      "loss_function",
      "optimisation_techniques",
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model"
    ],
    "summary": "The concept of a Cost Function is central to [[Model Optimisation]], particularly in training models. A cost function, also known as a loss function or error function, is a mathematical function used in optimization and machine learning to measure the difference between predicted values and actual values. It quantifies the error or \"cost\" of a model's predictions. The goal of many machine learning algorithms is to minimize this cost function, thereby improving the accuracy of the model. Common examples of cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks. Relation to [[Loss Function]]:..."
  },
  "cost-sensitive_analysis": {
    "title": "Cost-Sensitive Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cost-Sensitive Analysis.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "Model Parameters"
    ],
    "inlinks": [
      "determining_threshold_values",
      "imbalanced_datasets",
      "choosing_a_threshold"
    ],
    "summary": "Cost-sensitive analysis in machine learning refers to the practice of incorporating the costs associated with different types of errors into the model training and evaluation process. This approach is particularly useful in scenarios where the consequences of false positives and false negatives are not equal, and it is important to ==minimize the overall cost== rather than just the error rate. In cost-sensitive analysis, the objective is to find the best [[Model Parameters]] that minimize the overall cost, rather than just focusing on minimizing the error rate. This involves considering the costs associated with different types of classification errors, as defined..."
  },
  "covariance_structures": {
    "title": "Covariance Structures",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Covariance Structures.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "covariance",
      "variance",
      "Distributions|Distribution",
      "Statistics",
      "Principal Component Analysis|PCA",
      "Clustering",
      "Gaussian Mixture Models",
      "Data Analysis"
    ],
    "inlinks": [
      "kmeans_vs_gmm",
      "gaussian_mixture_models"
    ],
    "summary": "A covariance structure in general refers to the way variability and relationships between variables (or dimensions) are modeled and described in a dataset. It specifies how the data points are distributed in space, particularly focusing on the relationships between variables and their individual variances. Key Components of Covariance Structure Covariance Matrix: - A mathematical representation of the [[covariance]] structure for multiple variables. It shows the [[variance]] of each variable along the diagonal and the covariances between variables off the diagonal. - For a dataset with $p$ variables: $$ \\Sigma = \\begin{bmatrix} \\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\dots & \\text{Cov}(X_1, X_p)..."
  },
  "covariance_vs_correlation": {
    "title": "Covariance vs Correlation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Covariance vs Correlation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Covariance",
      "interpretability|interpretable"
    ],
    "inlinks": [],
    "summary": "[[Covariance]] provides a basic measure of how two variables move together, correlation offers a more [[interpretability|interpretable]] and standardized way to understand their relationship. Definition: - Covariance measures the degree to which two variables change together. It can take any value, which makes it difficult to interpret the strength of the relationship. - Correlation, on the other hand, is a standardized measure of the relationship between two variables, ranging from -1 to 1. This standardization allows for easier interpretation of the strength and direction of the relationship. Formula: - Correlation is derived from covariance. The formula for the correlation coefficient $r$between..."
  },
  "covariance": {
    "title": "Covariance",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Covariance.md",
    "tags": [
      "statistics",
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "correlation",
      "Gaussian Mixture Models"
    ],
    "inlinks": [
      "gaussian_mixture_models",
      "covariance_vs_correlation",
      "correlation",
      "covariance_structures"
    ],
    "summary": "In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between the variables. Specifically, covariance can be defined as follows: Positive Covariance: If the covariance is positive, it means that as one variable increases, the other variable tends to also increase. Conversely, if one variable decreases, the other variable tends to decrease as well. Negative Covariance: If the covariance is negative, it indicates that as one variable increases, the other variable tends to decrease, and vice versa. Zero Covariance: A covariance close to zero suggests that..."
  },
  "covering_index": {
    "title": "Covering Index",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Covering Index.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Database Index|Index"
    ],
    "inlinks": [
      "database_index"
    ],
    "summary": "Like an [[Database Index|Index]] but for partial indexes?"
  },
  "cron_jobs": {
    "title": "Cron jobs",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cron jobs.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "cross_entropy": {
    "title": "Cross Entropy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cross Entropy.md",
    "tags": [
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "Loss function",
      "Classification",
      "categorical data",
      "ML_Tools",
      "Cross_Entropy_Single.py",
      "Cross_Entropy.py",
      "Cross_Entropy_Net.py"
    ],
    "inlinks": [
      "neural_scaling_laws",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "decision_tree",
      "cross_entropy.py",
      "language_model_output_optimisation",
      "loss_function",
      "gini_impurity_vs_cross_entropy",
      "ds_&_ml_portal"
    ],
    "summary": "Cross entropy is a [[Loss function]] used in [[Classification]] tasks, particularly for [[categorical data]]. The cross entropy loss function is particularly effective for multi-class classification problems, where the goal is to assign an input to one of several categories. ==Cross entropy measures confidence.== Cross entropy works by measuring the (difference/loss) ==dissimilarity between two probability distributions==: the true distribution (actual class labels) and the predicted distribution (model's output probabilities). Fit of Predictions: - A low cross entropy loss means the predicted probabilities are close to the true labels (e.g., assigning high probability to the correct class). - A high loss indicates..."
  },
  "cross_validation": {
    "title": "Cross Validation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cross Validation.md",
    "tags": [
      "#evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "overfitting",
      "Model Optimisation",
      "Model Evaluation",
      "Hyperparameter",
      "Model Validation",
      "Bias and variance",
      "ML_Tools",
      "KFold_Cross_Validation.py",
      "Time Series",
      "Data Leakage"
    ],
    "inlinks": [
      "hyperparameter_tuning",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "decision_tree",
      "model_optimisation",
      "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression",
      "model_evaluation",
      "kaggle_abalone_regression_example",
      "train-dev-test_sets",
      "model_selection",
      "overfitting"
    ],
    "summary": "Cross-validation is a statistical technique used in machine learning to ==assess how well a model will generalize== to an independent dataset. It is a crucial step in the model-building process because it helps ensure that the model is not [[overfitting]] or underfitting the training data. Cross-validation is a technique used in machine learning and statistics to evaluate the performance ([[Model Optimisation]]) of a predictive model. It provides a robust evaluation by splitting the training data into smaller chunks and training the model multiple times. K-Fold Cross-Validation: Involves dividing the dataset into ( k ) equal-sized subsets (called \"folds\") and using..."
  },
  "crosstab": {
    "title": "Crosstab",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Crosstab.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DE_Tools"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "groupby_vs_crosstab"
    ],
    "summary": "Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables. Here's an example: ```python Sample DataFrame df = pd.DataFrame({ 'Category': ['A', 'B', 'A', 'B', 'A'], 'Subcategory': ['X', 'X', 'Y', 'Y', 'X'] }) Cross-tabulation of 'Category' and 'Subcategory' crosstab = pd.crosstab(df['Category'], df['Subcategory']) print(crosstab) ``` Input Category Subcategory 0 A X 1 B X 2 A Y 3 B Y 4 A X Output: Subcategory X Y Category A 2 1 B 1 1 In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb"
  },
  "cross_entropy.py": {
    "title": "Cross_Entropy.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cross_Entropy.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Mean Squared Error",
      "Cross Entropy"
    ],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py Generalized Script Description: Dataset: Uses the Iris dataset from sklearn to classify flower species. Preprocessing: One-hot encodes the target labels and splits the data into training and testing sets. Model: Trains a multinomial logistic regression model to predict probabilities for each class. Cross Entropy Calculation: Computes cross entropy loss for all predictions in the test set. Visualization: Plots a histogram to show the distribution of loss values across the test samples. Summary Statistics: Outputs mean, median, maximum, and minimum loss values for analysis. This approach provides insight into the model's performance by analyzing the spread and typical values of..."
  },
  "cross_entropy_single.py": {
    "title": "Cross_Entropy_Single.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cross_Entropy_Single.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cross_entropy"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py Example Let's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class label being A. The true label in one-hot encoded form would be [1, 0, 0]. Assume the model predicts the following probabilities for this data point: Probability of class A: 0.7 Probability of class B: 0.2 Probability of class C: 0.1 The predicted probability vector is [0.7, 0.2, 0.1]. To calculate the cross entropy loss for this example, we use the formula: $L = -\\sum_{i=1}^{N} y_i \\log(p_i)$ Substituting the values: For class A: $y_1 =..."
  },
  "crud": {
    "title": "CRUD",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\CRUD.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database",
      "database_management_system_(dbms)",
      "row-based_storage",
      "rest_api"
    ],
    "summary": "Create,Read,Update,Delete."
  },
  "cryptography": {
    "title": "Cryptography",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Cryptography.md",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "Security",
      "Node.JS",
      "JavaScript",
      "Hash"
    ],
    "inlinks": [],
    "summary": "Cryptography is the foundation of digital [[Security]], enabling privacy and secure communication over the internet. Examples are implemented in [[Node.JS]] (using crypto module) and are written in [[JavaScript]]. Resources: - 7 Cryptography Concepts EVERY Developer Should Know - https://fireship.io/lessons/node-crypto-examples/ [[Hash]] (Chop and mix) A hashing function takes an input of any length and outputs a fixed-length value, ensuring: The same input always produces the same output. It is computationally expensive to reverse the hash. It has a low probability of collisions. Create a Hash in Node.js ```javascript const { createHash } = require('crypto'); function hash(str) { return createHash('sha256').update(str).digest('hex'); } let..."
  },
  "current_challenges_within_the_energy_sector": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Current challenges within the energy sector.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "Current challenges within the energy sector"
    ],
    "inlinks": [
      "current_challenges_within_the_energy_sector"
    ],
    "summary": "[[Current challenges within the energy sector]] related to reinforcement learning and that can be progressed with recent technological advances"
  },
  "dagster": {
    "title": "What is Dagster?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\dagster.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Data Lifecycle Management",
      "declarative"
    ],
    "inlinks": [
      "etl",
      "data_management",
      "directed_acyclic_graph_(dag)"
    ],
    "summary": "Dagster is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [[Data Lifecycle Management]] lifecycle, with integrated lineage and observability, a [[declarative]] programming model, and best-in-class testability. Key features are: - Manage your data assets with code - A single pane of glass for your data platform"
  },
  "dash": {
    "title": "Dash",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dash.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "ML_Tools",
      "Clustering_Dashboard.py",
      "Plotly"
    ],
    "inlinks": [
      "dashboarding"
    ],
    "summary": "Dash is an open-source framework for building interactive web applications using Python. It is particularly well-suited for data visualization and dashboard creation. Dash integrates with popular libraries such as Plotly, Pandas, and NumPy, making it ideal for creating dynamic and interactive visualizations. In [[ML_Tools]] see [[Clustering_Dashboard.py]] Key Components of Dash 1. Dash App: The main application instance, created using dash.Dash(__name__). 2. Dash HTML Components (dash_html_components): Provides wrappers for standard HTML elements (e.g., html.Div, html.H1). 3. Dash Core Components (dash_core_components): Includes interactive UI components like graphs, dropdowns, sliders, and more (e.g., dcc.Graph, dcc.Dropdown). 4. Callback Functions: Used to make components interactive..."
  },
  "dashboarding": {
    "title": "Dashboarding",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dashboarding.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Dash",
      "Streamlit.io"
    ],
    "inlinks": [
      "react"
    ],
    "summary": "[[Dash]] [[Streamlit.io]]"
  },
  "data_ai_education_at_work": {
    "title": "Data AI Education at Work",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data AI Education at Work.md",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Introduction Organizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are equipped with the necessary skills to leverage these technologies effectively. Integrating data and AI education into organizational strategies is essential for maintaining competitiveness and fostering a culture of continuous learning. By addressing these areas, organizations can better prepare their workforce for the evolving technological landscape. Practical Steps for Integration Access to Training: Provide clear guidance on how to access training courses. Offer details on accessing training funds and budgets. Partner with training providers to offer..."
  },
  "data_analysis_portal": {
    "title": "Data Analysis Portal",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Analysis Portal.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "data_analysis": {
    "title": "Data Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Analysis.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Analyst",
      "EDA",
      "Statistics",
      "Data Visualisation",
      "Hypothesis testing",
      "Confidence Interval"
    ],
    "inlinks": [
      "eda",
      "data_lifecycle_management",
      "duckdb",
      "data_roles",
      "data_analyst",
      "alternatives_to_batch_processing",
      "fact_table",
      "statistical_assumptions",
      "ds_&_ml_portal",
      "covariance_structures"
    ],
    "summary": "What is it? Usually done with a [[Data Analyst]].After processing, data is analyzed to extract meaningful insights and derive value from the data. Types of analysis: Exploration and understanding: - [[EDA]]: Involves exploring data sets to find patterns, anomalies, or relationships without having a specific hypothesis in mind. It is often used in the initial stages of data analysis to generate insights. - Descriptive: ==Focuses on summarizing historical data to understand what has happened== in the past. It often involves the use of [[Statistics]] measures and [[Data Visualisation]] tools to present data trends and patterns. - Diagnostic: ==Seeks to understand..."
  },
  "data_analyst": {
    "title": "Data Analyst",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Analyst.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Analysis",
      "Data Collection",
      "Data Cleansing",
      "EDA",
      "Distributions|distribution",
      "Statistics",
      "hypothesis testing",
      "Data Visualisation",
      "Documentation & Meetings"
    ],
    "inlinks": [
      "data_roles",
      "data_analysis"
    ],
    "summary": "Summary: - Gathers and processes data to generate reports. - Communicates insights and findings to management - Conducts [[Data Analysis]]. Key responsibilities of a data analyst: Define Objectives: Clearly outline the goals of the analysis to guide the process. [[Data Collection]]: Gather relevant data from various sources, ensuring accuracy, completeness, and timeliness. [[Data Cleansing]]: Clean the data to remove errors, duplicates, and inconsistencies for reliable findings. Data Exploration: Perform exploratory data analysis ([[EDA]]) to understand data structure, [[Distributions|distribution]], and relationships. Choose the Right Tools: Utilize appropriate tools and software for analysis, such as Excel, R, Python, SQL, or specialized platforms...."
  },
  "data_architect": {
    "title": "Data Architect",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Architect.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Architect - Designs and manages the data infrastructure. - Ensures data is stored, organized, and accessible for analysis."
  },
  "data_archive_graph_analysis": {
    "title": "Data Archive Graph Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Archive Graph Analysis.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Dataview",
      "Graph View",
      "Graph Analysis Plugin"
    ],
    "inlinks": [],
    "summary": "Use the following to [[Dataview]] [[Graph View]] Check out [[Graph Analysis Plugin]] Convert Dataview to CSV"
  },
  "data_asset": {
    "title": "data asset",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\data asset.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_product"
    ],
    "summary": ""
  },
  "data_cleansing": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Cleansing.md",
    "tags": [
      "data_transformation",
      "data_cleaning",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "Data Quality",
      "standardised/Outliers|Handling Outliers",
      "Handling Missing Data",
      "Handling Different Distributions",
      "DE_Tools",
      "Data Selection",
      "Deleting rows or filling them with the mean is not always best "
    ],
    "inlinks": [
      "pandas_stack",
      "preprocessing",
      "data_transformation",
      "data_analyst",
      "why_use_er_diagrams",
      "anomaly_detection",
      "boxplot",
      "fuzzywuzzy"
    ],
    "summary": "Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [[Data Quality]] for analysis. Involves: [[standardised/Outliers|Handling Outliers]] [[Handling Missing Data]] [[Handling Different Distributions]] In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning/Dataframe_Cleaing.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning Related terms: - [[Data Selection]] Follow-up questions: - [[Deleting rows or filling them with the mean is not always best ]]"
  },
  "data_collection": {
    "title": "Data Collection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Collection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Quality",
      "Imbalanced Datasets"
    ],
    "inlinks": [
      "preprocessing",
      "business_observability",
      "data_analyst"
    ],
    "summary": "Determine the [[Data Quality]] and quantity of data required and get it. [[Imbalanced Datasets]]"
  },
  "data_contract": {
    "title": "Data Contract",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Contract.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Contract",
      "dbt",
      "API",
      "Data Contract",
      "data contract",
      "Data Quality",
      "Pasted image 20250312163351.png"
    ],
    "inlinks": [
      "data_contract",
      "data_quality"
    ],
    "summary": "[[Data Contract]] pattern to handle schema changes Pattern to apply to organisation using tools they have. Tooling: - [[dbt]] Data contracts help prevent preventable data issues while increasing collaboration and reducing costs. A data contract is an agreed interface between the generators of data and its consumers. It sets the expectations around that data, defines how it should be governed, and facilitates the explicit generation of quality data that meets the business requirements. Interfaces: - [[API]] for data. A document to codify what has been agreed. Q: How does the Data Contract allow for contextual rules? Example the same schema..."
  },
  "data_distribution": {
    "title": "Data Distribution",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Distribution.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_management",
      "data_lifecycle_management"
    ],
    "summary": "Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. This can involve supplying data to - business applications, - reporting systems, - or other data-driven processes, - ensuring that stakeholders have access to the information they need for decision-making and operations."
  },
  "data_drift": {
    "title": "Data Drift",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Drift.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Performance Drift"
    ],
    "inlinks": [
      "performance_drift",
      "model_observability"
    ],
    "summary": "Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead to decreased model performance, as the model may struggle to make accurate predictions on data that differ from its training set. Regular monitoring and prompt response to data drift are essential to maintain the effectiveness of ML models in dynamic production environments. Concepts: Data drift involves changes in input data distributions Concept drift pertains to alterations in the relationship between inputs and outputs. [[Performance Drift]] drift relates to changes in model outputs. Training-Serving Skew: This..."
  },
  "data_engineer": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Engineer.md",
    "tags": [
      "career",
      "field"
    ],
    "aliases": [
      "Data Engineering"
    ],
    "outlinks": [
      "Data Pipeline",
      "Data Management",
      "Data Engineering Portal",
      "Data Roles",
      "Data Engineering Tools",
      "continuous integration",
      "Documentation & Meetings"
    ],
    "inlinks": [
      "data_engineering",
      "apache_spark",
      "databricks",
      "data_roles"
    ],
    "summary": "The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on - automating the data collection, - processing, - and analysis workflows, - solving how systems manage and handle the flow of data. ==Develops data pipelines and ensures data flow between systems.== Resources: - Link Key Responsibilities: Infrastructure Design and Maintenance: Data engineers design, build, and maintain the necessary infrastructure to collect, process, and store large amounts of data. This infrastructure is crucial for ensuring data is accessible and usable for analysis and reporting. [[Data Pipeline]]: Support Role:..."
  },
  "data_engineering_portal": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Engineering Portal.md",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "MySql",
      "PostgreSQL",
      "Structured Data",
      "Spreadsheets vs Databases",
      "Database Management System (DBMS)",
      "Components of the database",
      "Relating Tables Together",
      "Turning a flat file into a database",
      "Database Techniques"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] They facilitate efficient CRUD.md operations and transactional processing (OLTP.md), structured by a Database Schema.md that organizes data into tables and relationships. Key Features [[Structured Data]]: Organized for efficient CRUD operations, allowing reliable access. Relational Databases: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy. Structure - Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval. Flexibility - Databases have a flexible schema that adapts to evolving requirements, unlike..."
  },
  "data_engineering_tools": {
    "title": "Data Engineering Tools",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Engineering Tools.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Cloud",
      "SQL",
      "Azure",
      "Amazon S3|S3",
      "Data Ingestion",
      "Apache Kafka",
      "Data Storage",
      "dbt"
    ],
    "inlinks": [
      "data_storage",
      "data_engineer",
      "data_ingestion"
    ],
    "summary": "Snowflake: [[Cloud]]-based data warehousing for scalable storage and processing. Microsoft SQL Server: [[SQL]]-based relational database management. [[Azure]] SQL Database: Managed relational database service on Azure. Azure Data Lake Storage: Scalable storage for big data analytics. SQL and T-SQL: Query languages for managing and querying relational databases. AWS [[Amazon S3|S3]]: Storage for data lakes. [[Data Ingestion]] Tools and Technologies: - [[Apache Kafka]] - AWS Kinesis: A cloud service for real-time data processing, enabling the collection and analysis of streaming data. - Google Pub/Sub: A messaging service that allows for asynchronous communication between applications, supporting real-time data ingestion. [[Data Storage]] Tools: Amazon..."
  },
  "data_engineering": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Engineering.md",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "Data Management",
      "Data Engineer",
      "Apache Airflow",
      "Prefect",
      "Data Pipeline"
    ],
    "inlinks": [
      "data_storage",
      "normalisation",
      "data_transformation_in_data_engineering"
    ],
    "summary": "The definition from the Fundamentals of Data Engineering, as it\u2019s one of the most recent and complete: Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, [[Data Management]], DataOps, data architecture, orchestration, and software engineering. A [[Data Engineer]] today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the Data Engineering Lifecycle and..."
  },
  "data_governance": {
    "title": "What is Data Governance?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Governance.md",
    "tags": [
      "business",
      "#data_governance"
    ],
    "aliases": [],
    "outlinks": [
      "Data Observability"
    ],
    "inlinks": [
      "master_data_management",
      "data_steward",
      "data_roles",
      "ai_governance",
      "data_storage",
      "data_principles"
    ],
    "summary": "Data governance is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals. It establishes the processes and responsibilities that ensure the Data Quality and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods. Data Governance: Focuses on ensuring that data is managed consistently and adheres to policies, often working in tandem with [[Data Observability]] to enforce quality standards."
  },
  "data_hierarchy_of_needs": {
    "title": "The Data Hierarchy of Needs",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Hierarchy of Needs.md",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "Pasted image 20241005170237.png|500"
    ],
    "inlinks": [],
    "summary": "![[Pasted image 20241005170237.png|500]] The Data Hierarchy of Needs is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow\u2019s hierarchy, progressing from basic data needs to advanced capabilities: Data Collection: (bottom) Start by collecting raw data from various sources, ensuring it's stored securely and reliably. Data Storage and Access: Organize and store data so it's easily accessible for those who need it, using databases or data warehouses. Data Cleaning and Preparation: Clean, preprocess, and transform data to ensure it\u2019s accurate, consistent, and ready for analysis. Data Analytics: Analyze the prepared data to generate insights,..."
  },
  "data_ingestion": {
    "title": "Data Ingestion",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Ingestion.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Database",
      "API",
      "Data Streaming",
      "Data Pipeline",
      "Data Quality",
      "Scalability",
      "Latency",
      "business intelligence",
      "Machine Learning",
      "Data Engineering Tools",
      "Data Ingestion"
    ],
    "inlinks": [
      "data_warehouse",
      "data_lifecycle_management",
      "data_pipeline",
      "data_engineering_tools",
      "data_ingestion",
      "data_storage"
    ],
    "summary": "Data ingestion is the process of collecting and importing raw data from various sources ([[Database]], [[API]], [[Data Streaming]] services) into a system for processing and analysis, and can be performed in batch and realtime ingestion. The goal is to gather raw data that can be processed and analyzed. Used for building [[Data Pipeline]] Challenges - [[Data Quality]]: Ensuring that the ingested data is accurate, complete, and consistent. - [[Scalability]]: Handling large volumes of data efficiently as the data sources grow. - [[Latency]]: Minimizing the delay between data generation and processing, especially in real-time scenarios. Use Cases: - Data ingestion is..."
  },
  "data_integration": {
    "title": "What is Data Integration?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Integration.md",
    "tags": [
      "data_storage",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Single Source of Truth",
      "Data Virtualization"
    ],
    "inlinks": [
      "data_virtualization",
      "apache_kafka"
    ],
    "summary": "Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [[Single Source of Truth]]. Manual Integration Manual integration involves analysts manually logging into source systems, analyzing and/or exporting data, and creating reports. Disadvantages of Manual Integration: Time-consuming: The process requires significant time investment. Security Risks: Analysts need access to multiple operational systems. Performance Issues: Running analytics on non-optimized systems can interfere with their functioning. Outdated Reports: Data changes frequently, leading to quickly outdated reports. [[Data Virtualization]] Data virtualization is a method that allows access to data without needing to..."
  },
  "data_integrity": {
    "title": "Data Integrity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Integrity.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Database",
      "Checksum",
      "Hash",
      "Data Integrity"
    ],
    "inlinks": [
      "data_lifecycle_management",
      "performance_dimensions",
      "hash",
      "data_pipeline",
      "relating_tables_together",
      "apache_kafka",
      "transaction",
      "soft_deletion",
      "data_integrity",
      "anomaly_detection",
      "data_principles"
    ],
    "summary": "Data integrity refers to the - accuracy, - consistency, and - reliability of data throughout its lifecycle. It ensures that data remains ==unaltered== and ==trustworthy==, whether it is being - stored, - processed, - or transmitted. Maintaining data integrity involves implementing measures to prevent unauthorized access, corruption, or loss of data. In the context of [[Database]] and information systems, data integrity can be enforced through: Validation Rules: Ensuring that data entered into a system meets certain criteria. Access Controls: Limiting who can view or modify data. Backups: Regularly saving copies of data to prevent loss. Error Checking: Using [[Checksum]] or..."
  },
  "data_lake": {
    "title": "What is a Data Lake?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Lake.md",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "unstructured data",
      "structured data",
      "unstructured data"
    ],
    "inlinks": [
      "event_driven_events",
      "fabric",
      "databricks",
      "data_storage",
      "hadoop"
    ],
    "summary": "A Data Lake is a storage system with vast amounts of [[unstructured data]] and [[structured data]], stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.). Definition: A repository that ==stores diverse data types==, including structured, semi-structured, and unstructured data. If cant fit into a database. Features: - Versatility: Can accommodate various data formats, including videos, images, documents, and more. - Raw Data Storage: Preserves data in its raw form, suitable for..."
  },
  "data_lakehouse": {
    "title": "What is a Data Lakehouse?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Lakehouse.md",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Lakehouse"
    ],
    "outlinks": [
      "Data Management",
      "Data Management",
      "Data Warehouse|Warehouse",
      "ACID Transaction",
      "ACID Transaction",
      "Database Schema|schema"
    ],
    "inlinks": [
      "single_source_of_truth"
    ],
    "summary": "A Data Lakehouse open [[Data Management]] architecture that combines the flexibility, cost-efficiency, and scale of Data Lake with the data management and ACID transactions of Data Warehouse with Data Lake Table Formats (Delta Lake, Apache Iceberg & Apache Hudi) that enable Business Intelligence (BI) and Machine Learning (ML) on all data. A data lakehouse is an emerging architectural approach that combines the best features of data lakes and data warehouses to provide a unified platform for storing, processing, and analyzing large volumes of structured and unstructured data. Here\u2019s a breakdown of its key characteristics and benefits: The data lakehouse architecture..."
  },
  "data_leakage": {
    "title": "Data Leakage",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Leakage.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_selection_in_ml",
      "cross_validation",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data"
    ],
    "summary": "Data Leakage refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly optimistic model performance. It occurs when the model has access to data it shouldn't during training, such as future information or test data, which can result in misleading evaluation metrics and poor generalization to new data."
  },
  "data_lifecycle_management": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Lifecycle Management.md",
    "tags": [
      "data_management",
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "data integrity",
      "Software Development Life Cycle",
      "Data Ingestion",
      "Data Storage",
      "Preprocessing",
      "Data Analysis",
      "Data Visualisation",
      "Data Distribution",
      "Performance Dimensions"
    ],
    "inlinks": [
      "data_principles",
      "dagster",
      "data_lineage"
    ],
    "summary": "This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. Used for maintaining [[data integrity]], optimizing performance, and ensuring that data-driven decisions are based on accurate and timely information. Not the same as the [[Software Development Life Cycle]] Key Stages of Full Lifecycle Management [[Data Ingestion]] [[Data Storage]] [[Preprocessing]] [[Data Analysis]] [[Data Visualisation]] [[Data Distribution]] Data engineers must evaluate and select tools and technologies based on several [[Performance Dimensions]]"
  },
  "data_lineage": {
    "title": "What is Data Lineage?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\data lineage.md",
    "tags": [
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "Data Lifecycle Management"
    ],
    "inlinks": [
      "declarative",
      "dbt",
      "model_observability"
    ],
    "summary": "Data lineage uncovers the [[Data Lifecycle Management]] life cycle of data. It aims to show the complete data flow from start to finish. Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. This includes all Data Transformation (what changed and why)."
  },
  "data_literacy": {
    "title": "What is Data Literacy?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\data literacy.md",
    "tags": [
      "#business"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Data literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This skill set is crucial for employees across various levels of an organization, especially as data-driven decision-making becomes increasingly important. Organizations should invest in data literacy training programs to empower their employees with the necessary skills to effectively engage with data. A data-literate employee can read charts, draw correct conclusions, recognize when data is being used inappropriately or misleadingly, and gain a deeper understanding of the business domain. This enables them to communicate more effectively using..."
  },
  "data_management": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Management.md",
    "tags": [
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "Data Pipeline",
      "data quality",
      "Apache Airflow",
      "Dagster",
      "Database Management System (DBMS)",
      "Master Data Management",
      "Data Distribution",
      "Data Management"
    ],
    "inlinks": [
      "master_data_management",
      "data_engineering",
      "data_lakehouse",
      "data_engineer",
      "database_schema",
      "digital_twin",
      "spreadsheets_vs_databases",
      "data_pipeline",
      "data_steward",
      "data_roles",
      "data_management",
      "difference_between_snowflake_to_hadoop",
      "data_storage",
      "data_principles"
    ],
    "summary": "Data management involves overseeing processes to maintain data integrity and quality. It includes: Responsibility: Identifying accountable individuals or teams. Issue Resolution: Mechanisms for detecting and addressing data-related problems. Data management ensures that a [[Data Pipeline]] operates efficiently, focusing on monitoring errors, performance issues, and [[data quality]]. Tools: - [[Apache Airflow]] - Prefect - [[Dagster]] Related Concepts: - [[Database Management System (DBMS)]] - [[Master Data Management]] - [[Data Distribution]] [[Data Management]] Tags: #data_management, #data_quality"
  },
  "data_mining_-_crisp": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Mining - CRISP.md",
    "tags": [
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "Clustering"
    ],
    "inlinks": [],
    "summary": "CRISP-DM stands for Cross-Industry Standard Process for Data Mining, a widely adopted framework that provides a structured approach to planning, organizing, and conducting data mining projects. It was developed in 1996 by Daimler-Benz, SPSS, and NCR. CRISP-DM provides a flexible, iterative process that allows for refinement at any stage, making it adaptable to various industries and types of data mining projects. mermaid graph TD A[Business Understanding] --> B[Data Understanding] B --> C[Data Preparation] C --> D[Modeling] D --> E[Evaluation] E --> F[Deployment] F --> A Business Understanding: Focuses on understanding the project\u2019s objectives from a business perspective. The goal is..."
  },
  "data_modelling": {
    "title": "Data Modelling",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Modelling.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "Conceptual Model",
      "Logical Model",
      "Physical Model",
      "ER Diagrams",
      "Data Modelling"
    ],
    "inlinks": [
      "data_modelling",
      "database_schema"
    ],
    "summary": "Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. This helps in organizing and structuring the data so it can be efficiently managed and utilized. Data modelling ensures that data is logically structured and organized, making it easier to store, retrieve, and manipulate in a database. Workflow of Data Modeling: 1) [[Conceptual Model]] 2) [[Logical Model]] 3) [[Physical Model]] Types of Modeling: - Relational: Organizes data into tables. - Object-Oriented: Focuses on objects and their state changes, e.g., robots in a car factory. - Entity: Uses [[ER Diagrams]]..."
  },
  "data_observability": {
    "title": "What is Data Observability?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Observability.md",
    "tags": [
      "#data_orchestration",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "Data Quality",
      "standardised/Outliers|anomalies",
      "dbt",
      "dbt",
      "Data Observability"
    ],
    "inlinks": [
      "declarative",
      "data_quality",
      "model_observability",
      "prevention_is_better_than_the_cure",
      "data_observability",
      "data_governance"
    ],
    "summary": "Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [[Data Quality]], reliability, and availability. It covers various aspects, such as data quality, pipeline health, metadata management, and infrastructure performance. By tracking key metrics and [[standardised/Outliers|anomalies]], it helps detect issues like data freshness problems, schema changes, or pipeline failures before they impact downstream processes or users. Categories of Observability Auto-profiling Data: Automatically tracks data attributes, such as row count, column types, data distributions, and schema changes. - Bigeye: Provides ML-driven threshold tests and automatic alerts when data drifts beyond expected ranges. - Datafold:..."
  },
  "data_orchestration": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Orchestration.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [
      "#data_orchestration"
    ],
    "outlinks": [
      "Data Pipeline to Data Products"
    ],
    "inlinks": [],
    "summary": "Data orchestration refers to the process of managing and coordinating the flow of data across various systems and environments, particularly in complex and heterogeneous cloud settings. A Data Orchestrator is responsible for modeling dependencies between different tasks, ensuring that data is processed and moved efficiently from one stage to another. It integrates with both legacy systems and modern cloud-based tools, as well as data lakes and data warehouses. The orchestration process involves invoking computations, such as executing business logic in languages like SQL and Python, and applying machine learning models at the appropriate times. These actions can be triggered based..."
  },
  "data_pipeline_to_data_products": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Pipeline to Data Products.md",
    "tags": [
      "#question",
      "data_orchestration",
      "anomaly_detection",
      "data_pipeline",
      "data_products"
    ],
    "aliases": [],
    "outlinks": [
      "Data Pipeline",
      "Data Product"
    ],
    "inlinks": [
      "data_pipeline",
      "data_orchestration"
    ],
    "summary": "The journey from [[Data Pipeline]] to [[Data Product]] involves transforming raw data into valuable insights or applications that can be used to drive business decisions. This process typically includes several stages, each with its own set of tasks and objectives. Read more on Data Orchestration Trends: The Shift From Data Pipelines to Data Products. Workflow Define Objectives: Understand the business goals and what insights or products are needed. Design the Pipeline: Plan the architecture and select appropriate tools for each stage of the pipeline. Implement and Test: Build the pipeline, ensuring data flows smoothly from ingestion to product delivery. Test..."
  },
  "data_pipeline": {
    "title": "Data Pipeline",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Pipeline.md",
    "tags": [
      "data_pipeline"
    ],
    "aliases": [
      "ETL Pipeline"
    ],
    "outlinks": [
      "Data Integrity",
      "Data Ingestion",
      "Data Transformation",
      "Data Storage",
      "Preprocessing|Data Preprocessing",
      "Data Management",
      "Data Pipeline to Data Products",
      "Data Pipeline"
    ],
    "inlinks": [
      "data_engineering",
      "data_engineer",
      "data_pipeline",
      "data_ingestion",
      "data_management",
      "data_pipeline_to_data_products"
    ],
    "summary": "A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can be stored, analyzed, and used to generate insights. It ensures that data flows smoothly and efficiently through different stages, maintaining data quality and [[Data Integrity]]. By implementing a data pipeline, organizations can automate data workflows, reduce manual effort, and ensure timely and accurate data delivery for decision-making. Workflow [[Data Ingestion]] [[Data Transformation]] [[Data Storage]] [[Preprocessing|Data Preprocessing]] [[Data Management]] Other steps: Design: - Define the objectives and requirements of the data pipeline. - Choose appropriate tools..."
  },
  "data_principles": {
    "title": "Data Principles",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Principles.md",
    "tags": [
      "data_quality",
      "data_governance"
    ],
    "aliases": null,
    "outlinks": [
      "Data Quality",
      "Data Governance",
      "data integrity",
      "security",
      "data management",
      "Data Lifecycle Management",
      "Documentation & Meetings",
      "Performance Dimensions"
    ],
    "inlinks": [],
    "summary": "Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically. [[Data Quality]] Ensure data is accurate, complete, reliable, and up-to-date. High-quality data is crucial for making informed decisions. [[Data Governance]]: Establish clear policies and procedures for data management, including roles and responsibilities, to ensure [[data integrity]] and compliance with regulations. Data Privacy: Protect personal and sensitive information by adhering to privacy laws and regulations, such as GDPR or CCPA, and implementing appropriate security measures. Data Security: Safeguard data against unauthorized access, breaches, and other security threats through encryption, access controls, and regular [[security]] audits...."
  },
  "data_product": {
    "title": "What is a Data Product?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Product.md",
    "tags": [
      "data_management",
      "business",
      "business_intelligence"
    ],
    "aliases": [],
    "outlinks": [
      "data asset"
    ],
    "inlinks": [
      "data_pipeline_to_data_products"
    ],
    "summary": "A data product is \"a product that facilitates an end goal through data\". Delivering the final output, which could be dashboards, reports, or machine learning models. For example Recommendation systems or predictive analytics dashboards. It applies more product thinking, whereas the \"Data Product\" essentially is a dashboard, report, and table in a Data Warehouse or a Machine Learning model. Sometimes Data Products are also called [[data asset]]."
  },
  "data_quality": {
    "title": "What is Data Quality?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Quality.md",
    "tags": [
      "#data_quality"
    ],
    "aliases": [],
    "outlinks": [
      "Data Observability",
      "Change Management",
      "Prevention Is Better Than The Cure",
      "Data Observability",
      "Data Contract",
      "Change Management"
    ],
    "inlinks": [
      "eda",
      "benefits_of_data_transformation",
      "data_collection",
      "prevention_is_better_than_the_cure",
      "data_observability",
      "ds_&_ml_portal",
      "data_cleansing",
      "determining_threshold_values",
      "neural_network_classification",
      "machine_learning_algorithms",
      "data_management",
      "data_principles",
      "master_data_management",
      "declarative",
      "performance_dimensions",
      "data_roles",
      "why_use_er_diagrams",
      "data_validation",
      "data_contract",
      "data_selection_in_ml",
      "data_ingestion"
    ],
    "summary": "Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis. Definition: Data quality refers to the ==accuracy, consistency, and reliability of data.== It is essential for maintaining trust in data-driven processes and outcomes. Importance: The principle of \"garbage in, garbage out\" highlights that poor-quality data leads to poor model performance. Related terms: - [[Data Observability]] - [[Change Management]] - [[Prevention Is Better Than The Cure]] Related terms: - [[Data Observability]] - [[Data Contract]] - [[Change Management]]"
  },
  "data_reduction": {
    "title": "Data Reduction",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Reduction.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Dimensionality Reduction",
      "Sampling",
      "variance"
    ],
    "inlinks": [
      "preprocessing"
    ],
    "summary": "Reducing the volume of data through techniques: [[Dimensionality Reduction]] [[Sampling]]: Use subsets of data for training to speed up the process and address issues like imbalanced data representation. Remove features with zero or low [[variance]] and redundant features to improve model performance."
  },
  "data_roles": {
    "title": "Data Roles",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Roles.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Steward",
      "Data quality",
      "data quality",
      "Data Governance",
      "data management",
      "Data Engineer",
      "Data Scientist",
      "Data analysis",
      "ML Engineer",
      "Data Architect",
      "Data Analyst"
    ],
    "inlinks": [
      "data_engineer"
    ],
    "summary": "A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. The team collaborates across various functions to ensure data integrity, accessibility, and usability. Key Roles and Responsibilities | Role | Focus Area | Key Responsibilities | | ---------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------- | | [[Data Steward]] | [[Data quality]] & governance | Enforces data policies, resolves [[data quality]] issues, manages metadata. | | [[Data Governance]] Team | Policy & compliance | Defines [[data management]] rules, ensures regulatory adherence. | | [[Data Engineer]] | Data infrastructure | Builds..."
  },
  "data_science": {
    "title": "Data Science",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Science.md",
    "tags": [
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "Scientific Method",
      "unstructured data",
      "statistics"
    ],
    "inlinks": [],
    "summary": "A field that uses the [[Scientific Method]], algorithms, and systems to ==extract knowledge== and insights from structured and [[unstructured data]]. It combines techniques from [[statistics]], computer science, and domain expertise to analyze and interpret complex data sets, enabling informed decision-making and predictive modeling. Resources: - https://scikit-learn.org/stable/auto_examples/index.html"
  },
  "data_scientist": {
    "title": "Data Scientist",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Scientist.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Business Intelligence"
    ],
    "inlinks": [
      "data_roles"
    ],
    "summary": "Data Scientist - Utilizes [[Business Intelligence]] (BI) tools to analyze data. - Works with data lakes to extract insights. - Develops and deploys production Machine Learning (ML) models for predictions."
  },
  "data_selection_in_ml": {
    "title": "Data Selection in ML",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Selection in ML.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Optimisation",
      "Data Quality",
      "Imbalanced Datasets",
      "Imbalanced Datasets|class imbalance",
      "Distributions",
      "distributions",
      "Data Transformation",
      "Correlation",
      "multicollinearity",
      "Dimensionality Reduction",
      "feature selection",
      "Data Leakage"
    ],
    "inlinks": [
      "data_selection"
    ],
    "summary": "When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[[Model Optimisation]] and the insights you can derive from it. Here are key factors to consider: Relevance: Ensure that the features (input variables) you select are relevant to the problem you are trying to solve. Irrelevant features can introduce noise and reduce model accuracy. Quality: [[Data Quality]] Assess the quality of the data, including checking for missing values, outliers, and errors. Poor quality data can lead to inaccurate models. Quantity: Consider the size of your dataset. More data can lead to better models, but it..."
  },
  "data_selection": {
    "title": "Data Selection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Selection.md",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "DE_Tools",
      "Data Selection in ML"
    ],
    "inlinks": [
      "data_cleansing",
      "how_do_you_do_the_data_selection",
      "pandas",
      "data_transformation"
    ],
    "summary": "Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame. In [[DE_Tools]] we explore how to do Data Selection with Pandas - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/selection.ipynb Related: - [[Data Selection in ML]] Examples Selecting Columns You can select a single column from a DataFrame using either bracket notation or dot notation: python df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}) column_a = df['A'] # or df.A Selecting Rows by Index To select rows by their index position, you can use slicing: python rows_0_to_2 = df[0:3] # Selects the first three..."
  },
  "data_steward": {
    "title": "Data Steward",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Steward.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Steward",
      "data governance",
      "data management"
    ],
    "inlinks": [
      "data_steward",
      "data_roles"
    ],
    "summary": "A Data Steward is responsible for ensuring the quality, integrity, and governance of an organization's data assets. They act as a bridge between business users, IT teams, and data governance policies, ensuring that data is well-defined, accurate, and used appropriately. Key Responsibilities of a Data Steward Data Quality Management \u2013 Ensuring data accuracy, completeness, consistency, and reliability across systems. Metadata Management \u2013 Documenting data definitions, relationships, and lineage. Data Governance Compliance \u2013 Implementing policies, standards, and best practices for data handling. Master Data Management (MDM) \u2013 Managing critical business data entities like customers, products, and suppliers. Collaboration with Stakeholders \u2013..."
  },
  "data_storage": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Storage.md",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "data_management"
    ],
    "outlinks": [
      "Data Engineering",
      "Data Ingestion",
      "Data Transformation",
      "Querying",
      "data management",
      "Data Transformation",
      "Data Warehouse",
      "storage layer object store\\|Object Store",
      "Database",
      "SQLite",
      "NoSQL",
      "Data Warehouse",
      "Data Lake",
      "NoSQL",
      "Cloud Providers",
      "Amazon S3",
      "Data Governance",
      "Data Engineering Tools"
    ],
    "inlinks": [
      "data_warehouse",
      "data_lifecycle_management",
      "big_data",
      "data_pipeline",
      "data_engineering_tools",
      "apache_kafka",
      "parquet",
      "checksum"
    ],
    "summary": "Data storage is a fundamental aspect of [[Data Engineering]], influencing processes such as - (occurring after [[Data Ingestion]]) - [[Data Transformation]] - [[Querying]] - [[data management]]. Storing the [[Data Transformation]] data in a database or [[Data Warehouse]] for easy access and analysis. Types of Storage Data storage encompasses various methods and technologies for storing, retrieving, and managing data. The choice of storage method significantly impacts ==data retrieval efficiency== and consistency | Storage Type | Description | | -------------------------------------------- | ----------------------------------------------------------------------------------------------------- | | [[storage layer object store\\|Object Store]] | The gold standard for data lakes, ideal for unstructured data such as..."
  },
  "data_streaming": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Streaming.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": null,
    "outlinks": [
      "batch processing",
      "Publish and Subscribe",
      "Apache Kafka",
      "Alternatives to Batch Processing",
      "Data Streaming"
    ],
    "inlinks": [
      "distributed_computing",
      "lambda_architecture",
      "data_ingestion",
      "data_streaming",
      "publish_and_subscribe",
      "alternatives_to_batch_processing"
    ],
    "summary": "Data Streaming is used for real-time data processing, allowing continuous flow and processing of data as it arrives. This is different from [[batch processing]], which handles data in chunks. The key to data streaming is the [[Publish and Subscribe]] [[Apache Kafka]] Example: - Companies like Netflix use Kafka to handle billions of messages daily, powering real-time recommendations, analytics, and user activity tracking. [[Alternatives to Batch Processing]] [[Data Streaming]] Tags: #data_workflow"
  },
  "data_terms": {
    "title": "Data Terms",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Terms.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "data_transformation_in_data_engineering": {
    "title": "Data transformation in Data Engineering",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data transformation in Data Engineering.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Engineering",
      "ETL",
      "ELT",
      "ETL vs ELT"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Data transformation in [[Data Engineering]] is a key step in data pipelines, often part of: ETL (Extract, Transform, Load) [[ETL]]: Data is transformed before loading into the target system. ELT (Extract, Load, Transform) [[ELT]]: Data is loaded first, then transformed for analysis. EtLT (Extract, \u201ctweak\u201d, Load, Transform: A hybrid approach combining elements of ETL and ELT. Related: - [[ETL vs ELT]]for a comparison."
  },
  "data_transformation_in_machine_learning": {
    "title": "Data transformation in Machine Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data transformation in Machine Learning.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Supervised Learning",
      "Encoding Categorical Variables"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Transforming raw data into a meaningful format is necessary for building effective models. [[Supervised Learning]]: Annotating datasets with correct labels (e.g., labeling images of apples vs. other fruits). Manual & Automated Labeling: Using human annotators or leveraging existing labeled datasets (e.g., Google reCAPTCHA). Feature Scaling & Encoding: Applying normalization and encoding to categorical variables. [[Encoding Categorical Variables]]: Converting categorical data into numerical format for machine learning models."
  },
  "data_transformation_with_pandas": {
    "title": "Data Transformation with Pandas",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Transformation with Pandas.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "pandas",
      "Merge",
      "Concatenate",
      "Joining Datasets",
      "Pandas join vs merge",
      "Multi-level index",
      "Aggregation",
      "Pandas Stack",
      "Crosstab",
      "DE_Tools",
      "Pasted image 20250323081817.png"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Using [[pandas]] we can do the following: [[Merge]] [[Concatenate]] [[Joining Datasets]] [[Pandas join vs merge]] [[Multi-level index]] [[Aggregation]] [[Pandas Stack]] [[Crosstab]] A summary of transformations steps can be helpful: |Step|Operation|Result| |---|---|---| |1|set_index|Rows get hierarchical keys| |2|stack|Wide \u2192 long with 3-level row index| |3|reset + extract|Parse variable names into fields| |4|pivot|Tidy format with metric columns| |5|unstack|Wide format with MultiIndex columns| In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation Related terms: Split-Apply-Combine ![[Pasted image 20250323081817.png]]"
  },
  "data_transformation": {
    "title": "What is data transformation?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Transformation.md",
    "tags": [
      "data_cleaning",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "Data Cleansing",
      "Structuring and organizing data",
      "Aggregation",
      "Data Selection",
      "Joining Datasets",
      "Normalisation of data",
      "Normalised Schema",
      "Data Transformation with Pandas",
      "Data transformation in Data Engineering",
      "Data transformation in Machine Learning",
      "Benefits of Data Transformation"
    ],
    "inlinks": [
      "pandas_stack",
      "eda",
      "preprocessing",
      "handling_missing_data",
      "duckdb",
      "data_pipeline",
      "dbt",
      "data_selection_in_ml",
      "benefits_of_data_transformation",
      "pandas",
      "standardisation",
      "melt",
      "activation_function",
      "data_storage",
      "etl"
    ],
    "summary": "Data transformation is the process of converting data from one format to another. Data transformation may involve: - [[Data Cleansing]] - [[Structuring and organizing data]] - [[Aggregation]] - [[Data Selection]] - [[Joining Datasets]] - [[Normalisation of data]] - [[Normalised Schema]] Others: - Sorting: Arranging data in a logical order. - Validating: Ensuring data integrity and accuracy. - Data Type Conversion: Changing data types (e.g., converting strings to integers). - Schema Normalization: Ensuring a consistent data structure for efficiency. Related: - [[Data Transformation with Pandas]] - [[Data transformation in Data Engineering]] - [[Data transformation in Machine Learning]] - [[Benefits of Data..."
  },
  "data_validation": {
    "title": "Data Validation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Validation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Quality",
      "type checking",
      "TypeScript",
      "Pydantic"
    ],
    "inlinks": [
      "pydantic",
      "excel_&_sheets",
      "pyright_vs_pydantic"
    ],
    "summary": "Data Validation: Error Prevention: It ensures data accuracy by preventing incorrect or inappropriate data entries. Consistent Data Entry: Helps maintain consistency across large datasets by controlling what users can input. Efficiency: By providing drop-down lists or constraints, it reduces the chances of manual errors. **Better [[Data Quality]]: Validating input ensures that your data is clean and ready for analysis or reporting without requiring additional checks. [[type checking]] [[TypeScript]] [[Pydantic]]"
  },
  "data_virtualization": {
    "title": "Data Virtualization",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Virtualization.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data integration"
    ],
    "inlinks": [
      "semantic_layer",
      "data_integration"
    ],
    "summary": "Organizations may also consider adopting a data virtualization solution to integrate their data. In this type of [[data integration]], data from multiple sources is left in place and is ==accessed== via a virtualization layer so that it ==appears== as a single data store. This virtualization layer makes use of adapters that translate queries executed on the virtualization layer into a format that each connected source system can execute. The virtualization layer then combines the responses from these source systems into a single result. This data integration strategy is sometimes used when a BI tool like Tableau needs to access data..."
  },
  "data_visualisation": {
    "title": "Data Visualisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Visualisation.md",
    "tags": [
      "data_analysis"
    ],
    "aliases": null,
    "outlinks": [
      "Tableau",
      "PowerBI",
      "Looker Studio"
    ],
    "inlinks": [
      "eda",
      "data_lifecycle_management",
      "business_observability",
      "data_analyst",
      "melt",
      "dimensionality_reduction",
      "grouped_plots",
      "tableau",
      "data_analysis"
    ],
    "summary": "Data visualization involves presenting data in a visual format, enabling stakeholders to quickly grasp insights and make informed decisions. Effective visualization tools include dashboards and reports. Can generate reports using: - [[Tableau]] - [[PowerBI]] - [[Looker Studio]]"
  },
  "data_warehouse": {
    "title": "What is a Data Warehouse?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Data Warehouse.md",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": [
      "Warehouse",
      "DWH"
    ],
    "outlinks": [
      "Querying",
      "Data Ingestion",
      "ETL",
      "Data Storage",
      "Documentation & Meetings"
    ],
    "inlinks": [
      "bigquery",
      "databricks_vs_snowflake",
      "cloud_providers",
      "snowflake",
      "fabric",
      "fact_table",
      "data_storage",
      "dimensional_modelling",
      "single_source_of_truth"
    ],
    "summary": "A Data Warehouse (DWH) is a centralized repository designed for [[Querying]] and analysis, storing large volumes of structured data from various sources within an organization. It supports reporting and decision-making by providing a consolidated view of data. Key Features [[Data Ingestion]] Integration: Combines data from diverse sources (e.g., transactional databases, CRM systems) into a single repository, ensuring consistency. Subject-Oriented: Organizes data around key business areas (e.g., sales, finance) rather than operational processes. Non-Volatile: Data remains unchanged once entered, preserving historical data for long-term analysis. Time-Variant: Stores data with a time dimension, enabling historical analysis and trend identification. Components Data Sources:..."
  },
  "database_index": {
    "title": "Database Index",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database Index.md",
    "tags": [
      "database_optimisation"
    ],
    "aliases": [
      "Indexing",
      "Index"
    ],
    "outlinks": [
      "DE_Tools",
      "Covering Index",
      "database",
      "Querying|queries",
      "B-tree"
    ],
    "inlinks": [],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Indexing/Indexing.ipynb Related terms: - [[Covering Index]] - Partial Index (Index with where clause) Indexing is a technique used to ==speed up data retrieval== in [[database]]. It achieves this by creating a separate structure, known as an index, that organizes specific columns of data for faster access. Better than scanning. Commonly created on ==primary keys== (unique for item) and foreign keys. Indexes can also be created across multiple tables to enhance the performance of complex queries, especially those that involve joins. A special type of index, called a ==covering index==, includes all the necessary data within the..."
  },
  "database_management_system_(dbms)": {
    "title": "Database Management System (DBMS)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database Management System (DBMS).md",
    "tags": [
      "database",
      "data_management"
    ],
    "aliases": [
      "DBMS"
    ],
    "outlinks": [
      "SQLite",
      "PostgreSQL",
      "MySql",
      "MongoDB",
      "Oracle",
      "CRUD"
    ],
    "inlinks": [
      "database",
      "data_management",
      "data_engineering_portal"
    ],
    "summary": "A Database Management System (DBMS) is software that allows you to interact with and manage databases. Easiest to use: - [[SQLite]] - [[PostgreSQL]] Others: - [[MySql]] - [[MongoDB]] - [[Oracle]] These systems enable users to perform [[CRUD]] operations while maintaining data integrity and providing tools for backup, security, and optimization. Can be proprietary (paid, with support) or Open source (free, self-supported)."
  },
  "database_schema": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database Schema.md",
    "tags": [
      "data_modeling",
      "database_structure"
    ],
    "aliases": [
      "schema",
      "Schema"
    ],
    "outlinks": [
      "Database Schema|schema",
      "Database",
      "Data Management",
      "Data Modelling",
      "conceptual data model",
      "Types of Database Schema",
      "Implementing Database Schema"
    ],
    "inlinks": [
      "structured_data"
    ],
    "summary": "A [[Database Schema|schema]] is the structure that defines how data is organized in a [[Database]], used in [[Data Management]]. It specifies the tables, columns, relationships, and constraints within the database. The schema is used for ensuring data is stored consistently and can be queried efficiently. Definition and Components: A database schema represents the ==structure== around the data, including tables, views, fields, relationships, and various other elements like indexes and triggers. It provides a framework for organizing and understanding data. Importance of ==Structure==: Without a schema, data can be chaotic and difficult to ==interpret==. A well-defined schema organizes data, making it..."
  },
  "database_storage": {
    "title": "What are Data Processing Techniques (row-based, columnar, vectorized)?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database Storage.md",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "database",
      "Columnar Storage",
      "Row-based Storage",
      "Vectorized Engine"
    ],
    "inlinks": [],
    "summary": "Methods and optimizations for storing, retrieving, and processing data in [[database]] systems. [[Columnar Storage]] [[Row-based Storage]] [[Vectorized Engine]]"
  },
  "database_techniques": {
    "title": "Database Techniques",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database Techniques.md",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "Soft Deletion",
      "Concurrency",
      "Race Conditions",
      "Querying",
      "SQL Joins",
      "Stored Procedures",
      "Database Index|Indexing",
      "Query Plan",
      "Vacuum"
    ],
    "inlinks": [
      "database",
      "sql",
      "data_engineering_portal",
      "melt"
    ],
    "summary": "Techniques: - [[Soft Deletion]] - [[Concurrency]] - [[Race Conditions]] - [[Querying]] - [[SQL Joins]] - [[Stored Procedures]] - Cleaning: Use Levenshtein Distance (if SQLite extension is available) to group similar entries. - [[Database Index|Indexing]] - [[Query Plan]] - [[Vacuum]]"
  },
  "database": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Database.md",
    "tags": [
      "database",
      "data_storage",
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "MySql",
      "PostgreSQL",
      "MongoDB",
      "CRUD",
      "OLTP",
      "Database Schema|schema",
      "Structured Data",
      "Spreadsheets vs Databases",
      "Database Management System (DBMS)",
      "Components of the database",
      "Relating Tables Together",
      "Turning a flat file into a database",
      "Database Techniques"
    ],
    "inlinks": [
      "database_schema",
      "database_storage",
      "slowly_changing_dimension",
      "microsoft_access",
      "data_integrity",
      "data_ingestion",
      "rollup",
      "database_index",
      "software_design_patterns",
      "sql",
      "data_storage",
      "structured_data",
      "single_source_of_truth"
    ],
    "summary": "Databases manage large data volumes with scalability, speed, and flexibility. Key systems include: [[MySql]] [[PostgreSQL]] [[MongoDB]] They facilitate efficient [[CRUD]] operations and transactional processing ([[OLTP]]) structured by [[Database Schema|schema]] that organizes data into tables and relationships. Key Features - [[Structured Data]]: Organized for efficient CRUD operations, allowing reliable access. - Relational Databases: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy. Structure - Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval. Flexibility - Databases have a flexible schema that adapts to evolving..."
  },
  "databricks_vs_snowflake": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Databricks vs Snowflake.md",
    "tags": [
      "software",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "Databricks",
      "Snowflake",
      "Data Warehouse"
    ],
    "inlinks": [
      "databricks"
    ],
    "summary": "Comparison between [[Databricks]] and [[Snowflake]]: Databricks is a versatile platform that emphasizes collaborative data science and engineering through interactive notebooks, making it suitable for advanced analytics and machine learning applications. Snowflake, on the other hand, focuses on [[Data Warehouse]] and offers a robust SQL interface for analytics, making it a preferred choice for organizations prioritizing data storage and reporting capabilities. | Feature | Databricks | Snowflake | |------------------------------|-------------------------------------------------------|---------------------------------------------------| | Primary Functionality | Unified analytics platform for big data processing and machine learning. | Cloud-based data warehousing and analytics platform. | | Data Processing | Built on Apache Spark, optimized for..."
  },
  "databricks": {
    "title": "Databricks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Databricks.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "big data",
      "Apache Spark",
      "data engineer",
      "Data Lake",
      "Data Warehouse|warehouse",
      "Hadoop",
      "Databricks vs Snowflake"
    ],
    "inlinks": [
      "big_data",
      "batch_processing",
      "cloud_providers",
      "databricks_vs_snowflake"
    ],
    "summary": "Databricks Overview [!Summary] Databricks is a cloud-based platform for [[big data]] processing built on [[Apache Spark]]. It provides an integrated workspace for collaboration among [[data engineer]]s, data scientists, and analysts. Databricks on Azure simplifies Spark deployment by offering auto-scaling clusters, real-time analytics, and integration with various Azure services, such as Azure [[Data Lake]] for large-scale data storage. Cloud Platform Compatibility: - Supports the big three cloud providers (AWS, Azure, GCP). - Integration with Other Technologies: - Combines capabilities of: - Apache Spark - Delta Lake - MLflow - Data Lakehouse Architecture: - Represents a combination of a data [[Data Warehouse|warehouse]]..."
  },
  "datasets": {
    "title": "Datasets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Datasets.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Time Series"
    ],
    "inlinks": [
      "forecasting_exponential_smoothing.py",
      "handling_different_distributions",
      "train-dev-test_sets",
      "mnist",
      "forecasting_autoarima.py"
    ],
    "summary": "This note collects notes on datasets that are good examples for exploring various concepts. Heart Failure Prediction Dataset Link: Heart Failure Prediction Dataset Useful for: Exploring predictive modeling in healthcare. Time Series Exploration Description: There is a dataset with seasonality, bikes, which can be used to explore [[Time Series]] concepts. Numenta Anomaly Benchmark (NAB) Link: Numenta Anomaly Benchmark (NAB) Columns: timestamp, value Description: NAB is used to evaluate and compare the performance of different anomaly detection algorithms on a diverse set of time series data. It includes real-world and artificial time series data covering domains such as finance, transportation, and..."
  },
  "dbscan": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DBScan.md",
    "tags": [
      "clustering"
    ],
    "aliases": null,
    "outlinks": [
      "Clustering",
      "K-means",
      "standardised/Outliers"
    ],
    "inlinks": [
      "unsupervised_learning",
      "anomaly_detection_with_clustering",
      "clustering",
      "isolated_forest"
    ],
    "summary": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a [[Clustering]] algorithm that groups together data points ==based on density==. It is particularly useful when K-means doesn't work well, such as in datasets with complex shapes or when there are outliers. Used when [[K-means]] doesn't work: DBSCAN handles datasets with ==irregular cluster shapes== and is not sensitive to outliers like K-means. When you have nesting of clusters: It can identify clusters of varying shapes and sizes without needing to predefine the number of clusters, unlike K-means. Groups core points to make clusters: DBSCAN identifies core points, which have many nearby..."
  },
  "dbt": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\dbt.md",
    "tags": [
      "software"
    ],
    "aliases": [
      "data build tool"
    ],
    "outlinks": [
      "Data Transformation",
      "SQL",
      "Documentation & Meetings",
      "Data Lineage",
      "ELT",
      "dbt"
    ],
    "inlinks": [
      "dbt",
      "data_engineering_tools",
      "jinja_template",
      "elt",
      "data_contract",
      "data_observability"
    ],
    "summary": "Data build tool is an open-source framework designed for [[Data Transformation]] within a modern data stack. It enables analysts and engineers to transform, model, and manage data using [[SQL]] while ==adhering to software engineering best practices== like version control, testing, and [[Documentation & Meetings]]. Key Concepts of dbt: SQL-based Transformation: dbt allows users to write SQL queries to define transformations and models, making it accessible for analysts who are already familiar with SQL. It doesn't handle extraction or loading of data, but focuses purely on transforming data that is already in a data warehouse. Modular and Reusable Models: dbt encourages..."
  },
  "debugging_ipynb": {
    "title": "Debugging ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Debugging ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "debugging jupyter cells https://www.youtube.com/watch?v=CY6uZIoF_kQ Sometimes dissapears: https://stackoverflow.com/questions/72671709/vs-code-debug-cell-disappears-arbitrarily-in-jupyter-notebook-view"
  },
  "debugging": {
    "title": "Debugging",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Debugging.md",
    "tags": [
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "Testing",
      "Software Development Life Cycle",
      "ML_Tools",
      "Debugging.py",
      "Testing_unittest.py",
      "Testing_Pytest.py",
      "Types of Computational Bugs",
      "StackBiz",
      "TypeScript",
      "Git"
    ],
    "inlinks": [
      "pydantic",
      "types_of_computational_bugs",
      "pyright"
    ],
    "summary": "Debugging is the process of identifying, analyzing, and resolving bugs or defects in software while [[Testing]]. It is a critical part of the [[Software Development Life Cycle]], ensuring that applications function correctly and efficiently. Debugging involves several techniques and tools to pinpoint the source of errors and fix them. In [[ML_Tools]] see: - [[Debugging.py]] - [[Testing_unittest.py]] - [[Testing_Pytest.py]] Key Concepts in Debugging [[Types of Computational Bugs]]: Understanding the types of bugs, such as cumulative rounding errors, integer overflow, and race conditions, is essential for effective debugging. How to Manage/View Bugs: Console Log/Dir: Use console logging to output variable values and..."
  },
  "debugging.py": {
    "title": "Debugging.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Debugging.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Debugging.py This script includes examples of logging, using breakpoints, and reproducing a simple bug for practice Key Concepts Demonstrated in the Script Logging in Python: The script uses Python's logging module to record debug, info, error, and warning messages. This helps track the flow of execution and diagnose issues. Reproduce the Bug: The script intentionally includes a division by zero bug to demonstrate how to identify and fix it. Breakpoints: You can set a breakpoint in your IDE at the line where result = divide_numbers(num1, num2) to inspect the values of num1 and num2. Automated Testing: The script includes a..."
  },
  "decision_tree": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Decision Tree.md",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "Supervised Learning",
      "Gini impurity",
      "Pasted image 20240404154526.png|500",
      "Gini Impurity",
      "Cross Entropy",
      "Gini Impurity vs Cross Entropy",
      "Overfitting",
      "pruning",
      "Cross Validation",
      "classification",
      "Hyperparameter",
      "GridSeachCv",
      "interpretability|interpretable",
      "Decision Tree"
    ],
    "inlinks": [
      "supervised_learning",
      "model_parameters",
      "xgboost",
      "weak_learners",
      "decision_tree",
      "gradient_boosting",
      "embedded_methods",
      "classification",
      "machine_learning_algorithms",
      "regularisation_of_tree_based_models",
      "gradient_boosting_regressor",
      "gini_impurity",
      "ada_boosting",
      "ds_&_ml_portal",
      "random_forests",
      "why_and_when_is_feature_scaling_necessary",
      "feature_scaling"
    ],
    "summary": "A Decision Tree is a type of [[Supervised Learning]] algorithm used to predict a target variable based on input features. It involves splitting data into subsets to create a tree-like model. Decision Tree Structure are a flowchart-like model where each internal node represents a decision based on a feature, branches represent outcomes, and leaf nodes represent final predictions. Splits data recursively based on feature importance, forming a tree-like structure. The decision tree algorithm calculates the [[Gini impurity]] for each possible split and selects the one with the lowest impurity. Use to make predictions on new data, the algorithm traverses the..."
  },
  "declarative": {
    "title": "What is declarative?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\declarative.md",
    "tags": [
      "data_orchestration",
      "field"
    ],
    "aliases": [],
    "outlinks": [
      "SQL",
      "data lineage",
      "Data Observability",
      "Data Quality"
    ],
    "inlinks": [
      "pydantic",
      "dagster"
    ],
    "summary": "In a declarative data pipeline, the focus is on what needs to be achieved, not how it should be executed. You define the desired outcome or the data products, and the system takes care of the underlying execution details, such as the order in which tasks are performed. This is in contrast to an imperative pipeline, where the developer explicitly specifies the steps and the order in which they should be executed. Here's a breakdown of the key aspects: Declarative Programming: Focuses on ==what== needs to be done. Describes the desired state or result without dictating the control flow or..."
  },
  "deep_learning_frameworks": {
    "title": "Deep Learning Frameworks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Deep Learning Frameworks.md",
    "tags": "deep_learning drafting",
    "aliases": [],
    "outlinks": [
      "Sci-kit Learn"
    ],
    "inlinks": [],
    "summary": "Watch Overview Video TensorFlow Focus: TensorFlow is a comprehensive open-source platform for machine learning. It provides a flexible and comprehensive ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML, and developers easily build and deploy ML-powered applications. Integration: TensorFlow can implement a wide range of machine learning algorithms, including those available in [[Sci-kit Learn]], making it versatile for various applications. Modularity: Its modular architecture allows users to deploy computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Parallelization: TensorFlow is optimized for..."
  },
  "deep_learning": {
    "title": "Deep Learning Overview",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Deep Learning.md",
    "tags": [
      "deep_learning"
    ],
    "aliases": [
      "DL"
    ],
    "outlinks": [
      "Backpropagation",
      "Gradient Descent",
      "PyTorch",
      "TensorFlow",
      "What is the role of gradient-based optimization in training deep learning models. ",
      "Explain different gradient descent algorithms, their advantages, and limitations.",
      "LLM",
      "Neural network|Neural Network",
      "Stochastic Gradient Descent|SGD",
      "Optimisation techniques",
      "Transfer Learning"
    ],
    "inlinks": [
      "pytorch",
      "optimising_neural_networks",
      "convolutional_neural_networks",
      "adam_optimizer",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "neural_network",
      "ds_&_ml_portal"
    ],
    "summary": "[!Summary] Deep learning is a subset of machine learning that uses neural networks to process large-scale data for tasks like image and speech recognition, natural language processing, and recommendation systems. A neural network consists of layers of nodes where each node performs weighted sums of its inputs, applies activation functions like ReLU or sigmoid, and produces an output. [[Backpropagation]] is the primary algorithm for training neural networks by minimizing error through [[Gradient Descent]]. Regularization techniques, such as dropout, prevent overfitting. Popular frameworks like [[PyTorch]] and [[TensorFlow]] facilitate deep learning model development. Questions: - [[What is the role of gradient-based optimization..."
  },
  "deep_q-learning": {
    "title": "Deep Q-Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Deep Q-Learning.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Q-Learning",
      "reinforcement learning",
      "Neural network",
      "Pasted image 20250220133838.png"
    ],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": "Deep [[Q-Learning]] is a type of [[reinforcement learning]] algorithm that combines Q-Learning with [[Neural network]]. Necessary when Q-Table grows too large. Updates the weights in the model. ![[Pasted image 20250220133838.png]] Key Concepts Target Network Purpose: The target network is used to stabilize the training process in Deep Q-Learning. When is it needed?: It is needed when updating the Q-values to prevent oscillations and divergence during training. How it works: The target network is a copy of the main Q-network and is used to generate target Q-values. It is updated less frequently than the main network, often using a technique called..."
  },
  "deepseek": {
    "title": "DeepSeek",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DeepSeek.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "LLM",
      "Chain of thought",
      "Distillation",
      "security",
      "jevon paradox",
      "Edge Machine Learning Models"
    ],
    "inlinks": [],
    "summary": "[[LLM]] example open source optimising for preformance vs efficency. Deepseek leading in efficency o3 mini [[Chain of thought]] can see it - ui choice [[Distillation]] - use gpt output and trains on it. [[security]] drafting [[jevon paradox]] - as cost decreases usage increases edge inference [[Edge Machine Learning Models]] The Genius of DeepSeek\u2019s 57X Efficiency Boost key value caching impacts attention block compute scaling: linear increased memory usage Solution: multi-query attention vs mutli - head attention Grouped-query attention Multi-head latent attention - deepseek uses uses compresses latent space linear algebra absorbed weights at training Access to Advanced AI Features Without..."
  },
  "deleting_rows_or_filling_them_with_the_mean_is_not_always_best": {
    "title": "Deleting rows or filling them with the mean is not always best",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Deleting rows or filling them with the mean is not always best.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "demand_forecasting": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Demand forecasting.md",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "Reinforcement learning|RL"
    ],
    "inlinks": [
      "use_of_rnns_in_energy_sector",
      "energy"
    ],
    "summary": "Overview: Demand response programs encourage consumers to adjust their energy usage during peak periods in response to time-based rates or other incentives. RL can optimize how these programs are implemented. Applications: Incentive Management: [[Reinforcement learning|RL]] models can dynamically adjust incentives for consumers to reduce usage during peak times based on real-time grid conditions and consumer behavior. Behavioral Adaptation: By learning from historical consumer response data, RL systems can predict how different consumers will react to incentives, allowing for more tailored and effective demand response strategies. How can we model the effects of energy consumption patterns on demand forecasting Dynamic Programming:..."
  },
  "dendrograms": {
    "title": "Dendrograms",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dendrograms.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Dendrograms",
      "Pasted image 20240405173403.png"
    ],
    "inlinks": [
      "clustermap",
      "heatmaps_dendrograms.py",
      "dendrograms",
      "clustering"
    ],
    "summary": "Dendrograms show close vectors is the data where taken as a vector. Can tell which ==features are the most similar== with [[Dendrograms]] ![[Pasted image 20240405173403.png]]"
  },
  "dependency_manager": {
    "title": "dependency manager",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\dependency manager.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Virtual environments",
      "requirements.txt",
      "TOML",
      "Poetry"
    ],
    "inlinks": [
      "poetry"
    ],
    "summary": "[[Virtual environments]] [[requirements.txt]] [[TOML]] [[Poetry]]"
  },
  "determining_threshold_values": {
    "title": "Determining Threshold Values",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Determining Threshold Values.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Binary Classification",
      "Evaluation Metrics",
      "Imbalanced Datasets|Class Imbalance",
      "Data Quality",
      "Evaluation Metrics",
      "ROC (Receiver Operating Characteristic)",
      "Precision-Recall Curve",
      "Cost-Sensitive Analysis"
    ],
    "inlinks": [],
    "summary": "In [[Binary Classification]] problems, a threshold value is used to convert predicted probabilities into discrete class labels. The choice of threshold significantly impacts the model's performance, affecting [[Evaluation Metrics]]. Important Considerations: * [[Imbalanced Datasets|Class Imbalance]]: If the classes are imbalanced, the choice of threshold can be significantly affected. Techniques like oversampling, undersampling, or using weighted loss functions can help mitigate the impact of class imbalance. * [[Data Quality]]: The quality of the training data can also influence the choice of threshold. If the data is noisy or contains outliers, the chosen values may not be optimal. * Choose [[Evaluation Metrics]]..."
  },
  "devops": {
    "title": "What is DevOps?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DevOps.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Software Development Portal",
      "CI-CD",
      "DataOps"
    ],
    "inlinks": [
      "software_development_portal",
      "software_development_life_cycle",
      "machine_learning_operations"
    ],
    "summary": "DevOps refers to practices for collaboration and automation between [[Software Development Portal]] (Dev) and IT operations (Ops) teams, aiming for faster, more reliable software delivery. Integration: It integrates the work of software development and operations teams by fostering a culture of collaboration and shared responsibility. Principles: - Emerges from Agile principles. - Emphasizes collaboration between development and operations teams. Approach: - Utilizes continuous integration and continuous delivery ([[CI-CD]]) to ensure frequent code changes and quick feedback loops. - Enables rapid and reliable updates with high levels of automation and efficiency. Goals: - Ensures existing processes are optimized and streamlined. Related..."
  },
  "difference_between_databricks_vs._snowflake": {
    "title": "Difference between Databricks vs. Snowflake",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Difference between Databricks vs. Snowflake.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "difference_between_snowflake_to_hadoop": {
    "title": "Difference between snowflake to hadoop",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Difference between snowflake to hadoop.md",
    "tags": [
      "software_architecture",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "Data Management",
      "Snowflake",
      "Hadoop"
    ],
    "inlinks": [],
    "summary": "Snowflake and Hadoop are both [[Data Management]] systems, but they serve different purposes and have distinct architectures and functionalities. In summary, Snowflake and Hadoop are both powerful tools for managing and analyzing data, but they are optimized for different types of workloads and use cases. Snowflake excels in ==cloud-based data warehousing== and real-time analytics, while Hadoop is suited for ==large-scale data processing== and storage in a distributed environment. [[Snowflake]] [[Hadoop]] Key Differences Deployment: Snowflake: Cloud-based, requires no hardware or infrastructure management by users. Hadoop: Can be deployed on-premises or in the cloud, but typically requires more hands-on management. Ease of..."
  },
  "differentation": {
    "title": "Differentation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Differentation.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Forward Mode Automatic Differentiation uses dual numbers implemented in tensor flow see also Reverse Mode Automatic Differentiation Fast,Flexible,Exact"
  },
  "digital_transformation": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Digital Transformation.md",
    "tags": [
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "Data Audit",
      "Digital Transformation",
      "Digital Transformation",
      "Change Management|Change Program"
    ],
    "inlinks": [
      "digital_transformation"
    ],
    "summary": "==\"Digital transformation starts with data centralisation\"== To digitally transform your department, you'll need to approach the process in a structured and strategic way that addresses both technological and organizational changes. [[Data Audit]] Understand Department Processes: Identify current workflows, key processes, and technologies being used. Identify Pain Points: Collect feedback from employees on inefficiencies, bottlenecks, and areas for improvement. ==Data Collection and Analysis==: Review existing data and determine how it's being used (or underutilized) in decision-making processes. 2. Define Clear Objectives Set Transformation Goals: Align transformation efforts with the broader goals of the organization. For example, improving efficiency, enhancing customer experience,..."
  },
  "digital_twin": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Digital twin.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "Energy",
      "Energy",
      "Data Management"
    ],
    "inlinks": [],
    "summary": "[!Summary] A digital twin is a virtual representation of a physical object, system, or process that mirrors its real-world counterpart in real-time. This digital model is used to simulate, monitor, analyze, and optimize the physical entity by continuously updating based on data collected from sensors, devices, or other inputs. The concept is widely applied in industries such as manufacturing, healthcare, [[Energy]], smart cities, and more to improve decision-making, predictive maintenance, and efficiency. A digital twin is a powerful tool for enhancing real-time decision-making, optimizing processes, and predicting future performance by bridging the physical and digital worlds. Its applications continue to..."
  },
  "dimension_table": {
    "title": "Dimension Table",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dimension Table.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "star schema",
      "Facts",
      "Fact Table",
      "Dimension Table"
    ],
    "inlinks": [
      "slowly_changing_dimension",
      "dimension_table",
      "fact_table",
      "dimensional_modelling",
      "components_of_the_database"
    ],
    "summary": "A dimension table is a key component of a [[star schema]] or snowflake schema in a data warehouse. It provides descriptive attributes (or dimensions) related to the [[Facts]] stored in a fact table. They provide the context and descriptive information necessary for analyzing the quantitative data stored in fact tables (e.g., product names, customer demographics, time periods). Descriptive Attributes: Dimension tables contain qualitative data that describe the entities involved in the business process. For example, a product dimension table might include attributes such as product name, category, brand, and manufacturer. Primary Key: Each dimension table has a primary key that..."
  },
  "dimensional_modelling": {
    "title": "Dimensional Modelling",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dimensional Modelling.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": null,
    "outlinks": [
      "Data Warehouse",
      "queries",
      "Star Schema",
      "Fact Table",
      "Facts",
      "Dimension Table",
      "Grain",
      "Performance Dimensions",
      "Dimensional Modelling"
    ],
    "inlinks": [
      "dimensional_modelling",
      "granularity"
    ],
    "summary": "Dimensional modeling is a design technique used in [[Data Warehouse]]used to structure data for efficient ==retrieval== and analysis. It is particularly well-suited for organizing data in a way that supports complex [[queries]] and reporting, making it easier for business users to understand and interact with the data. Dimensional modeling is a foundational technique in building data warehouses and is often associated with methodologies like the ==Kimball== approach, which emphasizes the use of [[Star Schema]] and the importance of understanding business processes and user requirements. Key Concepts in Dimensional Modeling [[Fact Table]] & [[Facts]] [[Dimension Table]] [[Grain]] Benefits of Dimensional Modeling:..."
  },
  "dimensionality_reduction": {
    "title": "Dimensionality Reduction",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dimensionality Reduction.md",
    "tags": [
      "ml_process",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "Preprocessing",
      "Data Visualisation",
      "Principal Component Analysis",
      "t-SNE",
      "Linear Discriminant Analysis",
      "Explain the curse of dimensionality"
    ],
    "inlinks": [
      "data_reduction",
      "preprocessing",
      "addressing_multicollinearity",
      "learning_styles",
      "unsupervised_learning",
      "feature_selection",
      "data_selection_in_ml",
      "factor_analysis",
      "manifold_learning",
      "vector_embedding",
      "explain_the_curse_of_dimensionality",
      "machine_learning_algorithms",
      "t-sne",
      "principal_component_analysis",
      "interview_notepad",
      "feature_engineering",
      "ds_&_ml_portal",
      "feature_extraction"
    ],
    "summary": "Dimensionality reduction is a step in the [[Preprocessing]] phase of machine learning that helps simplify models, enhance interpretability, and improve computational efficiency. Its a technique used to reduce the number of input variables (features) in a dataset while retaining as much information as possible. This process is essential for several reasons: Improves Model Performance: Reducing the number of features can help improve the performance of machine learning models by minimizing overfitting and reducing noise. Enhances Visualization: It allows for easier [[Data Visualisation]] of high-dimensional data by projecting it into lower dimensions (e.g., 2D or 3D). Reduces Computational Cost: Fewer features..."
  },
  "dimensions": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dimensions.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "OLAP (online analytical processing)|OLAP",
      "Facts"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "Dimensions are the categorical buckets that can be used to segment, filter, or group\u2014such as sales amount region, city, product, color, and distribution channel. Traditionally known from [[OLAP (online analytical processing)|OLAP]]cubes with Bus Matrixes, and Dimensional Modeling. They provide context to the [[Facts]]."
  },
  "directed_acyclic_graph_(dag)": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Directed Acyclic Graph (DAG).md",
    "tags": [
      "math",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Apache Airflow",
      "dagster",
      "Prefect"
    ],
    "inlinks": [
      "pyspark",
      "mathematics",
      "apache_airflow"
    ],
    "summary": "DAG stands for Directed Acyclic Graph. A DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point. It's a popular way of building data pipelines in tools like [[Apache Airflow]], [[dagster]], [[Prefect]]. It clearly defines the data lineage. As well, it's made for a functional approach where you have the idempotency to restart pipelines without side-effects."
  },
  "directory_structure": {
    "title": "Directory Structure",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Directory Structure.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "To make a file tree \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short - delimited description, e.g. \u2502 1.0-jqp-initial-data-exploration. \u2502 \u251c\u2500\u2500 reports <-..."
  },
  "distillation": {
    "title": "Distillation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Distillation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Small Language Models",
      "Pasted image 20250130074219.png"
    ],
    "inlinks": [
      "small_language_models",
      "deepseek",
      "llm"
    ],
    "summary": "training smaller models with larger. [[Small Language Models]] ![[Pasted image 20250130074219.png]]"
  },
  "distributed_computing": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Distributed Computing.md",
    "tags": [
      "data_management",
      "data_processing",
      "cloud_computing"
    ],
    "aliases": null,
    "outlinks": [
      "Hadoop",
      "Apache Spark",
      "edge computing",
      "scalability",
      "Amazon S3",
      "Data Streaming",
      "kubernetes",
      "latency"
    ],
    "inlinks": [
      "batch_processing",
      "publish_and_subscribe",
      "map_reduce"
    ],
    "summary": "Distributed Computing is essential for managing massive data volumes by distributing tasks across multiple servers or machines. This enables scalability and efficient data processing. [[Hadoop]] is a framework that handles both data storage and processing across clusters of servers: - It ensures scalability: can easily grow as more data is added. - Provides redundancy: data is replicated across servers to prevent loss in case of failures. Tools like [[Apache Spark]] are built to process data in these distributed environments, allowing for fast, parallel processing across the cluster. Distributed computing is central to modern data handling, driven by frameworks like Spark..."
  },
  "distributions": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Distributions.md",
    "tags": [
      "statistics",
      "drafting"
    ],
    "aliases": [
      "Distribution"
    ],
    "outlinks": [
      "ML_Tools",
      "Distribution_Analysis.py",
      "Uniform",
      "Bernoulli",
      "Binomial",
      "Poisson",
      "Gaussian Distribution",
      "hypothesis testing",
      "Pasted image 20250308191945.png",
      "ML_Tools",
      "Feature_Distribution.py",
      "Violin plot",
      "Boxplot"
    ],
    "inlinks": [
      "eda",
      "handling_different_distributions",
      "fitting_weights_and_biases_of_a_neural_network",
      "data_selection_in_ml",
      "gaussian_distribution",
      "statistics",
      "train-dev-test_sets",
      "gaussian_mixture_models",
      "precision-recall_curve",
      "gini_impurity_vs_cross_entropy",
      "boxplot",
      "ds_&_ml_portal",
      "variance",
      "feature_selection"
    ],
    "summary": "In [[ML_Tools]] see: - [[Distribution_Analysis.py]] Discrete Distributions These distributions have probabilities concentrated on specific values. [[Uniform]] Distribution: All outcomes are equally likely. Example: Drawing a card from a shuffled deck. A boxplot can be meaningful if there\u2019s variation in the distribution. Since the values are discrete, the boxplot will show the range and quartiles effectively. [[Bernoulli]] Distribution: Represents two possible outcomes. Example: Coin flip (heads or tails), true/false scenarios. A bar chart or frequency plot would be better for visualizing the proportions. or rolling a dice. [[Binomial]] Distribution: Represents the number of successes in a sequence of Bernoulli trials. Example:..."
  },
  "distribution_analysis.py": {
    "title": "Distribution_Analysis.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Distribution_Analysis.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Distribution_Analysis.py The goodness-of-fit results represent the p-values from the Kolmogorov-Smirnov (KS) test, which assesses how well the data fits each distribution. Here's how to interpret these values: Higher p-value \u2192 The distribution is a better fit. Lower p-value \u2192 The distribution is a poor fit (likely not the correct model for the data). Threshold: A common significance level is 0.05. If p > 0.05, we do not reject the hypothesis that the data follows this distribution. If p < 0.05, we reject the hypothesis, meaning the data likely does not follow that distribution. Example using penguins.csv column \"bill_depth_mm\" Goodness-of-fit results..."
  },
  "docker_image": {
    "title": "Docker Image",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Docker Image.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "docker"
    ],
    "summary": "A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. Docker images are used to create Docker containers, which are instances of these images running in an isolated environment. Docker images are used for: Consistency: They ensure that software runs the same way regardless of where it is deployed, whether on a developer's laptop, a test server, or in production. Portability: Docker images can be easily shared and moved across different environments, making it easier to deploy applications. Version Control:..."
  },
  "docker": {
    "title": "Docker",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Docker.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Docker Image"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "Utilizes Docker images [[Docker Image]] to set up containers for consistent development and testing environments. Containers can include necessary dependencies like Python and pip. Tutorial: - The Only Docker Tutorial You Need To Get Started Docker Volumes - storing data Docker Compose Multi use containers docker init command - generated Dockerfile - image - compose.yaml - .dockerignore"
  },
  "documentation_&_meetings": {
    "title": "Documentation & Meetings",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Documentation & Meetings.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pdoc",
      "Mermaid",
      "Technical Design Doc Template",
      "Pull Request Template",
      "Experiment Plan Template",
      "Retrospective Template",
      "Data Request Template",
      "One Pager Template",
      "Meeting Notes Template",
      "One Pager Template",
      "Meeting Notes Template",
      "Data Request Template",
      "Postmortem Template",
      "Retrospective Template",
      "One Pager Template",
      "1-on-1 Template",
      "Feedback Template"
    ],
    "inlinks": [
      "data_warehouse",
      "code_diagrams",
      "data_engineer",
      "fishbone_diagram",
      "dbt",
      "software_development_portal",
      "pyright",
      "data_analyst",
      "data_principles",
      "ipynb"
    ],
    "summary": "Tools [[pdoc]] \u2013 Auto-generate Python API documentation [[Mermaid]] \u2013 Create diagrams and flowcharts from text in a Markdown-like syntax Templates Project & Technical Meetings [[Technical Design Doc Template]] \u2013 Document system architecture, components, data flow [[Pull Request Template]] \u2013 Checklist and summary for PRs (code, tests, reviewers, notes) [[Experiment Plan Template]] \u2013 Hypothesis, variables, metrics, and analysis plan [[Retrospective Template]] \u2013 Guide for reviewing past sprint/project cycles Data & Analytics Meetings [[Data Request Template]] \u2013 Intake form for new analysis or data extract needs [[One Pager Template]] \u2013 Summarize a project, idea, or proposal on a single page [[Meeting Notes..."
  },
  "dropout": {
    "title": "Dropout",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dropout.md",
    "tags": [
      "deep_learning",
      "ml_optimisation"
    ],
    "aliases": null,
    "outlinks": [
      "Regularisation",
      "Neural network",
      "overfitting"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network"
    ],
    "summary": "Dropout is a [[Regularisation]] technique used in [[Neural network]] training to prevent [[overfitting]]. It works by randomly dropping units (neurons) during training, which helps the network to not rely too heavily on any single neuron. Purpose - The main goal of dropout is to improve the generalization of the model by reducing over-reliance on specific neurons. This encourages the network to learn more robust features that are useful in different contexts. How It Works - During each training iteration, a subset of neurons is randomly selected and ignored (dropped out). This means their contribution to the activation of downstream neurons..."
  },
  "ds_&_ml_portal": {
    "title": "DS & ML Portal",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DS & ML Portal.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Supervised Learning",
      "Unsupervised Learning",
      "Reinforcement learning",
      "Deep Learning",
      "Learning rate",
      "Overfitting",
      "Regularisation",
      "Hyperparameter",
      "Hyperparameter Tuning",
      "Model Optimisation",
      "Model Selection",
      "Vanishing and exploding gradients problem",
      "Feature Selection",
      "Feature Engineering",
      "Imbalanced Datasets",
      "Outliers",
      "Anomaly Detection",
      "Multicollinearity",
      "Dimensionality Reduction",
      "Clustering",
      "Classification",
      "Binary Classification",
      "Support Vector Machines",
      "Decision Tree",
      "Random Forests",
      "K-nearest neighbours",
      "Logistic Regression",
      "Regression",
      "Linear Regression",
      "Gradient Descent",
      "Gradient Boosting",
      "XGBoost",
      "BERT",
      "LSTM",
      "Recurrent Neural Networks",
      "Transformer",
      "Attention mechanism",
      "Neural network",
      "Cost Function",
      "Loss function",
      "Cross Entropy",
      "Evaluation Metrics",
      "Model Evaluation",
      "Accuracy",
      "Precision",
      "Recall",
      "Machine Learning Algorithms",
      "Optimisation techniques",
      "Optimisation function",
      "Model Ensemble",
      "Batch Processing",
      "Apache Spark",
      "Sklearn",
      "Distributions",
      "Statistics",
      "Correlation",
      "Data Analysis",
      "Data Quality",
      "Principal Component Analysis",
      "Interpretability",
      "RAG"
    ],
    "inlinks": [
      "machine_learning_operations",
      "orthogonalization"
    ],
    "summary": "Machine Learning Fundamentals [[ML_Tools]] [[Supervised Learning]] [[Unsupervised Learning]] [[Reinforcement learning]] [[Deep Learning]] Model Training and Optimisation [[Learning rate]] [[Overfitting]] [[Regularisation]] [[Hyperparameter]] [[Hyperparameter Tuning]] [[Model Optimisation]] [[Model Selection]] [[Vanishing and exploding gradients problem]] Feature Engineering and Data Handling [[Feature Selection]] [[Feature Engineering]] [[Imbalanced Datasets]] [[Outliers]] [[Anomaly Detection]] [[Multicollinearity]] [[Dimensionality Reduction]] [[Clustering]] Machine Learning Models Classification Models [[Classification]] [[Binary Classification]] [[Support Vector Machines]] [[Decision Tree]] [[Random Forests]] [[K-nearest neighbours]] [[Logistic Regression]] Regression Models [[Regression]] [[Linear Regression]] Boosting and Optimisation [[Gradient Descent]] [[Gradient Boosting]] [[XGBoost]] Deep Learning and Neural Networks [[BERT]] [[LSTM]] [[Recurrent Neural Networks]] [[Transformer]] [[Attention mechanism]] [[Neural network]] Model Evaluation and..."
  },
  "duckdb_in_python": {
    "title": "DuckDB in python",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DuckDB in python.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "duckdb"
    ],
    "summary": "To use DuckDB in Python, you can follow these steps to install the DuckDB library and perform basic operations such as creating a database, running queries, and manipulating data. Here's a simple guide: Step 1: Install DuckDB You can install DuckDB using pip. Open your terminal or command prompt and run: bash pip install duckdb Example: Full Code Here\u2019s a complete example that incorporates all the steps: ```python import duckdb Step 1: Connect to an in-memory database conn = duckdb.connect(database=':memory:') Step 2: Create a table conn.execute(\"\"\" CREATE TABLE users ( id INTEGER, name VARCHAR, age INTEGER ) \"\"\") Step 3:..."
  },
  "duckdb_vs_sqlite": {
    "title": "DuckDB vs SQLite",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DuckDB vs SQLite.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DuckDB",
      "SQLite",
      "Python",
      "columnar storage"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "Choosing between [[DuckDB]] and [[SQLite]] for data processing in [[Python]] depends on your specific use case and requirements. While SQLite is an excellent choice for lightweight applications, local data storage, and simple transactional workloads. DuckDB shines in scenarios involving complex analytical queries, large datasets, and data science workflows. If your primary focus is on data analysis and you need high performance for analytical tasks, DuckDB may be the better option. However, if you need a simple, lightweight database for small-scale applications, SQLite could be more appropriate. 1. Performance for Analytical Queries DuckDB is optimized for analytical workloads and can handle..."
  },
  "duckdb": {
    "title": "DuckDB",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\DuckDB.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DuckDB in python",
      "DuckDB vs SQLite",
      "Columnar Storage",
      "Parquet",
      "Data Analysis",
      "Data Transformation",
      "Querying"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "vectorized_engine"
    ],
    "summary": "DuckDB is an open-source analytical database management system designed for efficient data processing and analysis. It is optimized for running complex queries on large datasets and is particularly well-suited for data science and analytics tasks. Here are some key features and characteristics of DuckDB: Resources: - https://duckdb.org/ - [[DuckDB in python]] - [[DuckDB vs SQLite]] Key Features In-Memory Processing: DuckDB operates primarily in-memory, which allows for fast query execution and data manipulation. [[Columnar Storage]] It uses a columnar storage format, which is efficient for analytical queries that often involve aggregations and scans over large datasets. SQL Support: DuckDB supports SQL..."
  },
  "dummy_variable_trap": {
    "title": "Dummy variable trap",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Dummy variable trap.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "multicollinearity",
      "one-hot encoding"
    ],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Key Takeaways: The dummy variable trap occurs due to [[multicollinearity]], where ==one dummy variable can be perfectly predicted from others.== Dropping one dummy variable avoids this issue and ensures that the model has a reference category against which the other categories are compared. This approach leads to a well-conditioned model and allows for more interpretable regression coefficients. Dummy Variable Trap The dummy variable trap refers to a scenario in which there is multicollinearity in your dataset when you create dummy variables for categorical features. Specifically, it occurs when one of the dummy variables in a set of dummy variables can..."
  },
  "eda": {
    "title": "EDA",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\EDA.md",
    "tags": [
      "data_exploration",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "Data Analysis",
      "distributions",
      "correlation",
      "Data Quality",
      "standardised/Outliers",
      "standard deviation",
      "Data Visualisation",
      "correlation",
      "Data Transformation",
      "ML_Tools",
      "EDA_Pandas.py"
    ],
    "inlinks": [
      "scientific_method",
      "preprocessing",
      "factor_analysis",
      "data_analyst",
      "data_analysis"
    ],
    "summary": "Exploratory [[Data Analysis]] (EDA) is an approach to analyzing datasets to summarize their main characteristics, often utilizing visual methods. EDA helps users to: Understand the Data's Structure: Gain insights into the organization and format of the data. Detect Patterns: Identify trends and patterns within the data. Decide on Statistical Techniques: Choose appropriate statistical methods by examining [[distributions]] and [[correlation]]. Select Variables: Determine which variables to include in further analysis. Handle [[Data Quality]]: Address issues related to data quality and integrity. Spot Anomalies and [[standardised/Outliers]]: Identify unusual data points that may affect analysis. Generate and Test Hypotheses: Formulate hypotheses and validate..."
  },
  "eda_pandas.py": {
    "title": "EDA_Pandas.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\EDA_Pandas.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "eda"
    ],
    "summary": ""
  },
  "edge_machine_learning_models": {
    "title": "Edge Machine Learning Models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Edge Machine Learning Models.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "PyTorch",
      "ONNX"
    ],
    "inlinks": [
      "small_language_models",
      "deepseek"
    ],
    "summary": "Edge ML refers to deploying machine learning models directly on edge devices, such as IoT sensors, smartphones, or embedded systems, instead of relying on cloud-based processing. This is crucial in scenarios requiring low-latency, real-time decision-making, or environments with limited connectivity. Key Characteristics of Edge ML Models: Low Latency Models running at the edge can make real-time decisions with minimal delay. This is critical in applications like autonomous vehicles, industrial automation, or real-time health monitoring, where delays can have serious consequences. Reduced Bandwidth Usage By processing data locally, edge ML models reduce the need to send large amounts of data to..."
  },
  "education_and_training": {
    "title": "Education and Training",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Education and Training.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Adaptive Learning Systems Overview: Adaptive learning systems use technology to tailor educational experiences to individual student needs. RL is instrumental in personalizing these systems. Applications: Personalized Learning Paths: RL algorithms can create customized learning paths for students based on their performance, preferences, and engagement levels, adapting content delivery in real-time. Feedback and Assessment: Adaptive systems can provide immediate feedback based on student responses, reinforcing concepts through targeted exercises and adjusting difficulty levels as needed. Engagement Strategies: By analyzing student interactions, RL can suggest motivational strategies, such as gamification elements or timely reminders, to keep students engaged and motivated."
  },
  "elastic_net": {
    "title": "Elastic Net",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Elastic Net.md",
    "tags": [
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "Lasso",
      "Ridge"
    ],
    "inlinks": [
      "embedded_methods"
    ],
    "summary": "This method combines both L1 ([[Lasso]]) and L2 ([[Ridge]]) regularization by adding both absolute and squared penalties to the loss function. It strikes a balance between Ridge and Lasso. It is particularly useful when you have high-dimensional datasets with highly correlated features. The Elastic Net loss function is: $$\\text{Loss} = \\text{MSE} + \\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2$$ where $\\lambda_1$ controls the L1 regularization and $\\lambda_2$ controls the L2 regularization. Code ```python from sklearn.linear_model import ElasticNet Initialize an Elastic Net model model = ElasticNet(alpha=0.1, l1_ratio=0.5) # l1_ratio controls the L1/L2 mix model.fit(X_train, y_train) ```"
  },
  "elt": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ELT.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "ETL vs ELT",
      "BigQuery",
      "dbt",
      "ELT"
    ],
    "inlinks": [
      "elt",
      "data_transformation_in_data_engineering",
      "etl",
      "dbt"
    ],
    "summary": "ELT (Extract, Load, Transform) is a data integration approach that involves three main steps: Extract (E): Data is extracted from a source system. Load (L): The raw data is loaded into a destination system, such as a data warehouse. Transform (T): Transformation of the data occurs within the destination system after the data has been loaded. This approach contrasts with the traditional ETL (Extract, Transform, Load) method, where data is transformed before reaching the destination. For a detailed comparison, see [[ETL vs ELT]] Advantages of ELT The shift from ETL to ELT has been facilitated by several factors: Cost Efficiency:..."
  },
  "embedded_methods": {
    "title": "Embedded Methods",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Embedded Methods.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Feature Selection",
      "Filter method",
      "Wrapper Methods",
      "Regularisation",
      "Loss function",
      "Lasso",
      "Elastic Net",
      "Lasso",
      "Ridge",
      "Decision Tree",
      "Random Forests",
      "Gradient Boosting",
      "interpretability"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Embedded methods for [[Feature Selection]] ==integrate feature selection directly into the model training process.== Embedded methods provide a convenient and efficient approach to feature selection by seamlessly integrating it into the model training process, ultimately leading to models that are more parsimonious and potentially more interpretable. Incorporated into Model Training: Unlike [[Filter method]] and [[Wrapper Methods]], which involve feature selection as a separate step from model training, embedded methods perform feature selection simultaneously with model training. This means that feature importance or relevance is determined within the context of the model itself. Regularization Techniques: Embedded methods commonly use [[Regularisation]] techniques..."
  },
  "emergent_behavior": {
    "title": "emergent behavior",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\emergent behavior.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "agent-based_modelling"
    ],
    "summary": ""
  },
  "encoding_categorical_variables": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Encoding Categorical Variables.md",
    "tags": [
      "code_snippet",
      "regressor",
      "data_cleaning"
    ],
    "aliases": null,
    "outlinks": [
      "Regression",
      "Regression",
      "Feature Engineering"
    ],
    "inlinks": [
      "naive_bayes",
      "data_transformation_in_machine_learning"
    ],
    "summary": "Overview Categorical variables need to be converted into numerical representations to be used in models, particularly in [[Regression]] analysis. This process is essential for transforming categorical results into a format that algorithms can interpret. Label Encoding This method assigns a unique integer to each category in the variable. ```python from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() var1_cat = df['var1'] # Replace df with your DataFrame var1_encoded = label_encoder.fit_transform(var1_cat) `` For example, ifdf[col]contains the categories['apple', 'banana', 'orange'], theLabelEncoderwould transform them into[0, 1, 2]`. However, keep in mind that this encoding can imply an order or hierarchy in the data, which might..."
  },
  "energy_abm": {
    "title": "Energy ABM",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Energy ABM.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Agent-Based Modelling|ABM"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy ABM Complex Systems Understanding: Energy systems involve numerous stakeholders (producers, consumers, regulators) with diverse interests and behaviors. [[Agent-Based Modelling|ABM]] helps capture this complexity, providing a clearer picture of system dynamics. Adaptive Behavior: Agents in ABM can adapt their behavior based on interactions, mirroring how consumers and producers might respond to incentives or changes in the market. Scenario Analysis: ABM allows for \"what-if\" analyses, enabling stakeholders to explore different scenarios, such as the impact of implementing new technologies or policies on energy systems. Data-Driven Insights: With the rise of smart meters and IoT devices, ABM can leverage real-time data to..."
  },
  "energy_storage": {
    "title": "Energy Storage",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Energy Storage.md",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Energy Storage Battery farms exist. Stored energy can be traded. Stored energy can be stored using distributed system such as EV cars."
  },
  "energy": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Energy.md",
    "tags": [
      "energy"
    ],
    "aliases": [],
    "outlinks": [
      "Smart Grids",
      "Energy Storage",
      "Demand forecasting",
      "Network Design",
      "Energy ABM",
      "How to model to improve demand forecasting",
      "Differential Equations",
      "Stochastic Modeling",
      "Agent-Based Modelling",
      "Regression",
      "Neural network|Neural Network"
    ],
    "inlinks": [
      "interview_notepad",
      "industries_of_interest",
      "digital_twin"
    ],
    "summary": "Areas of interest: - [[Smart Grids]] - [[Energy Storage]] - [[Demand forecasting]] - [[Network Design]] - [[Energy ABM]] Questions: - [[How to model to improve demand forecasting]] - What patterns can be identified in consumer behavior data to inform energy pricing strategies? - How can predictive maintenance be implemented using data from smart sensors in energy infrastructure? Techniques: - [[Differential Equations]]: Used to model dynamic systems in energy generation and consumption. For example, they can describe the behavior of power systems over time or the thermal dynamics of energy storage systems. - [[Stochastic Modeling]]: Involves random variables to model uncertainties..."
  },
  "environment_variables": {
    "title": "Environment Variables",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Environment Variables.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Solution 1: Set Environment Variables Permanently (Recommended) This ensures that environment variables persist across sessions. On Windows (Permanent) Open Control Panel \u2192 System \u2192 Advanced system settings \u2192 Environment Variables. Under System Variables, click New. Variable Name: PG_USER Variable Value: postgres Click New again. Variable Name: PG_PASSWORD Variable Value: your_password Click OK and restart your computer. Once restarted, Jupyter Notebook should be able to access the variables."
  },
  "epoch": {
    "title": "Epoch",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Epoch.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An epoch in machine learning is a single pass through the entire training dataset. The number of epochs, denoted as $N$, determines how many times the data is applied to the model. Why Use Multiple Epochs? - Repetition for Learning: The data is applied to the model $N$ times to improve learning and accuracy. For example, if $N = 10$, the model will see the entire dataset 10 times. Example Epoch 1/10 6250/6250 [==============================] - 6s 910us/step - loss: 0.1782 Epoch 1/10: Indicates the model is currently on the first epoch out of a total of 10. Batches: For efficiency,..."
  },
  "er_diagrams": {
    "title": "ER Diagrams",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ER Diagrams.md",
    "tags": [
      "database",
      "database_design",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [
      "Why use ER diagrams",
      "Mermaid"
    ],
    "inlinks": [
      "relating_tables_together",
      "types_of_database_schema",
      "conceptual_model",
      "implementing_database_schema",
      "why_use_er_diagrams",
      "data_modelling"
    ],
    "summary": "ER Diagrams are a visual representation of the database structure. Related: - [[Why use ER diagrams]] - [[Mermaid]] Example Entities are tables in the database. Here, we have: Employee table Department table Attributes (Columns) - Each entity has attributes (columns): Relationship - ==Each Employee belongs to one Department== - Each Department has many Employees. - This is a One-to-Many (1:M) relationship. Mermaid ER Diagram mermaid erDiagram Employee { INTEGER id PK STRING name INTEGER dept_id FK } Department { INTEGER dept_id PK STRING name } Employee }o--|| Department : \"works_in\" Explanation of the Arrows }o--|| \u2192 One-to-Many (1:M) One Department..."
  },
  "estimator": {
    "title": "Estimator",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Estimator.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests",
      "statistics",
      "maximum_likelihood_estimation"
    ],
    "summary": "Given a sample an estimator is a formula that approximates a population parameter i.e feature"
  },
  "etl_pipeline_example": {
    "title": "ETL Pipeline Example",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ETL Pipeline Example.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "\"domains\",\"country\",\"web_pages\",\"name\""
    ],
    "inlinks": [
      "etl"
    ],
    "summary": "Link [Github](https://github.com/syalanuj/youtube/blob/main/de_fundamentals_python/etl.py 1. Extract using a API Get data via api or download. 2. Transform Put into a pandas df. 3. Load Save df as a database. Save SQLite database. Save as table. Run functions ```python \"\"\" Python Extract Transform Load Example \"\"\" %% import requests import pandas as pd from sqlalchemy import create_engine def extract()-> dict: \"\"\" This API extracts data from http://universities.hipolabs.com \"\"\" API_URL = \"http://universities.hipolabs.com/search?country=United+States\" data = requests.get(API_URL).json() return data def transform(data:dict) -> pd.DataFrame: \"\"\" Transforms the dataset into desired structure and filters\"\"\" df = pd.DataFrame(data) print(f\"Total Number of universities from API {len(data)}\") df = df[df[\"name\"].str.contains(\"California\")] print(f\"Number..."
  },
  "etl_vs_elt": {
    "title": "ETL vs. ELT",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ETL vs ELT.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "ETL"
    ],
    "inlinks": [
      "elt",
      "data_transformation_in_data_engineering",
      "etl"
    ],
    "summary": "ETL (Extract, Transform, and Load) and ELT (Extract, Load, and Transform) are two paradigms for moving data from one system to another. ==ELT is most friendly for analysts== The main difference between them is that when an ETL approach is used, data is transformed before it is loaded into a destination system. On the other hand, in the case of ELT, any required transformations are done after the data has been written to the destination and are then done inside the destination -- often by executing SQL commands. The difference between these approaches is easier to understand by a visual..."
  },
  "etl": {
    "title": "What is ETL?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ETL.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "Data Transformation",
      "ELT",
      "ETL vs ELT",
      "ELT",
      "Apache Airflow",
      "dagster",
      "ETL Pipeline Example",
      "ETL vs ELT",
      "ETL"
    ],
    "inlinks": [
      "data_warehouse",
      "data_transformation_in_data_engineering",
      "slowly_changing_dimension",
      "fabric",
      "etl_vs_elt",
      "etl",
      "reverse_etl"
    ],
    "summary": "ETL (Extract, Transform, Load) is a data integration process that involves moving data from one system to another. It consists of three main stages: Extract: Collecting data from various sources, such as databases, APIs, or flat files. This may involve setting up API connections to pull data from multiple sources. Transform: Cleaning and converting the data into a usable format. This includes filtering, aggregating, and joining data to create a unified dataset. See [[Data Transformation]]. Load: Inserting the transformed data into a destination system, such as a data warehouse (organized), database, or data lake (unorganized). Historical Context The ETL paradigm..."
  },
  "etlt": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\EtLT.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "[!important] EtLT refers to Extract, \u201ctweak\u201d, Load, Transform, and can be thought of an extension to the ELT approach to data integration. When compared to ELT, the EtLT approach incorporates an additional light \u201ctweak\u201d (small \u201ct\u201d) transformation, which is done on the data after it is extracted from the source and before it is loaded into the destination."
  },
  "evaluating_language_models": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Evaluating Language Models.md",
    "tags": [
      "evaluation",
      "language_models"
    ],
    "aliases": [],
    "outlinks": [
      "LMSYS",
      "LLM",
      "Elo Rating System",
      "Bradley-Terry Model",
      "Prompting"
    ],
    "inlinks": [],
    "summary": "The [[LMSYS]] Chatbot Arena is a platform where various large language models ([[LLM]]s), including versions of GPT and other prominent models like LLaMA or Claude, are compared through side-by-side interactions. The performance of these models is evaluated using human feedback based on the quality of their generated responses. The arena employs several techniques to rank and compare models: [[Elo Rating System]]: Adapted from chess, this system rates models based on their relative performance in head-to-head competitions. When one model's response is preferred over another's, the winning model gains points while the losing model loses points. The rating difference helps determine..."
  },
  "evaluation_metrics": {
    "title": "Evaluation Metrics",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Evaluation Metrics.md",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": [],
    "outlinks": [
      "Confusion Matrix",
      "Accuracy",
      "Precision",
      "Recall",
      "Precision or Recall",
      "F1 Score",
      "Recall",
      "Specificity",
      "Pasted image 20241222091831.png",
      "ML_Tools",
      "Evaluation_Metrics.py",
      "Why Type 1 and Type 2 matter",
      "F1 Score",
      "Pasted image 20241217073706.png"
    ],
    "inlinks": [
      "determining_threshold_values",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "recall",
      "test_loss_when_evaluating_models",
      "model_evaluation",
      "train-dev-test_sets",
      "neural_network_classification",
      "imbalanced_datasets",
      "forecasting_autoarima.py",
      "model_selection",
      "metric",
      "ds_&_ml_portal"
    ],
    "summary": "Description [[Confusion Matrix]] [[Accuracy]] [[Precision]] [[Recall]] [[Precision or Recall]] [[F1 Score]] [[Recall]] [[Specificity]] ![[Pasted image 20241222091831.png]] Resources: Link to good website describing these In [[ML_Tools]] see: [[Evaluation_Metrics.py]] Types of predictions Types of predictions in evaluating models. Also see [[Why Type 1 and Type 2 matter]] True Positive (TP): - This occurs when the model correctly predicts the positive class. For example, if the model predicts that an email is spam and it actually is spam, that's a true positive. False Positive (FP): - Also known as a ==\"Type I error,\"== this occurs when the model incorrectly predicts the positive class...."
  },
  "event_driven_events": {
    "title": "Event Driven Events",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Event Driven Events.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Lake",
      "Event Driven Microservices",
      "Business observability",
      "Monolith Architecture",
      "Event Driven Microservices",
      "API Driven Microservices"
    ],
    "inlinks": [
      "event_driven_microservices",
      "aws_lambda",
      "event_driven"
    ],
    "summary": "Events can be stored in a [[Data Lake]] and analysed to find patterns/predictions. [[Event Driven Microservices]] allow for [[Business observability]] [[Monolith Architecture]] [[Event Driven Microservices]] [[API Driven Microservices]]"
  },
  "event_driven_microservices": {
    "title": "Event Driven Microservices",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Event Driven Microservices.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "software architecture",
      "microservices",
      "Event Driven Events"
    ],
    "inlinks": [
      "event_driven_events",
      "event_driven"
    ],
    "summary": "Event-driven microservices refer to a [[software architecture]] pattern where [[microservices]] communicate and coordinate their actions through the production, detection, consumption, and reaction to [[Event Driven Events]]. This approach is designed to create a more decoupled and scalable system, where services can operate independently and react to changes in real-time. Event-driven microservices are particularly useful for applications that require high scalability, real-time processing, and flexibility. They are commonly used in domains like e-commerce, IoT, financial services, and any system where real-time data processing and responsiveness are critical. However, they also introduce challenges in terms of event schema management, eventual consistency, and..."
  },
  "event_driven": {
    "title": "Event Driven",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Event Driven.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Scalability",
      "Event Driven Events",
      "Event-Driven Architecture",
      "Event Driven Microservices"
    ],
    "inlinks": [],
    "summary": "Event-driven refers to a ==programming paradigm== or architectural style where the flow of the program is determined by events\u2014changes in state or conditions that trigger specific actions or responses. In this model, components of a system communicate through events, which can be generated by user interactions, system changes, or external sources. Key Concepts of Event-Driven Architecture: Events: An event is a significant change in state or an occurrence that can trigger a response. For example, a user clicking a button, a file being uploaded, or a sensor detecting a change in temperature. Event Producers: These are components or services that..."
  },
  "event-driven_architecture": {
    "title": "Event-Driven Architecture",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Event-Driven Architecture.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "publish_and_subscribe",
      "alternatives_to_batch_processing",
      "event_driven"
    ],
    "summary": ""
  },
  "everything": {
    "title": "Everything",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Everything.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Can we search with descriptions ? Tips use \\ to match in paths i.e \\playground can copy file new window crl+ n Use | to get or search search syntax"
  },
  "excel_&_sheets": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Excel & Sheets.md",
    "tags": [
      "software",
      "business"
    ],
    "aliases": null,
    "outlinks": [
      "standardised/GSheets|GSheets",
      "Data Validation"
    ],
    "inlinks": [],
    "summary": "Links Google sheets example folder see [[standardised/GSheets|GSheets]] Excel Example folder: Desktop/Example_Examples Tools common to Excel and Sheets Vlookup Table | Product ID | Product Name | Price | | -------------- | ---------------- | --------- | | 1001 | Apple | $1.00 | | 1002 | Banana | $0.50 | | 1003 | Orange | $0.80 | $0.50=VLOOKUP(1002, Table, 3, FALSE) Pivot table Excel specific index-match index-match-match Excel: Evaluate Formula Evaluate Formula is a feature in Excel that allows you to step through complex formulas to see how Excel calculates the result. This tool is helpful for debugging or understanding how..."
  },
  "explain_different_gradient_descent_algorithms,_their_advantages,_and_limitations.": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Explain different gradient descent algorithms, their advantages, and limitations..md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "deep_learning"
    ],
    "summary": ""
  },
  "explain_the_curse_of_dimensionality": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Explain the curse of dimensionality.md",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "Ngrams",
      "Language Models",
      "NLP",
      "Dimensionality Reduction",
      "Multidimensional Scaling",
      "Feature Selection",
      "Regularisation",
      "Manifold Learning"
    ],
    "inlinks": [
      "dimensionality_reduction"
    ],
    "summary": "The curse of dimensionality refers to the various phenomena that arise when working with data in high-dimensional spaces. Increased Data ==Sparsity==: As the number of dimensions grows, the available data becomes increasingly sparse, making it difficult for algorithms to find ==meaningful patterns==. This sparsity can lead to poor generalization performance, as the algorithm might not have enough data points in each region of the input space to learn a robust model. ==Distance Metric Issues:== In high-dimensional spaces, traditional distance metrics like Euclidean distance become less effective, as the relative difference between the nearest and farthest points diminishes. This can make..."
  },
  "exploration_vs._exploitation": {
    "title": "Exploration vs. Exploitation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Exploration vs. Exploitation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Reinforcement learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "One of the major challenges in [[Reinforcement learning]] is balancing exploration (trying new actions) and exploitation (choosing the best-known actions). The ==epsilon-greedy strategy== is commonly used, where a small probability (epsilon) allows for exploration while primarily exploiting the best-known actions."
  },
  "exploration": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Exploration.md",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "policy"
    ],
    "summary": ""
  },
  "f1_score": {
    "title": "F1 Score",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\F1 Score.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Precision",
      "Recall",
      "Harmonic Mean"
    ],
    "inlinks": [
      "confusion_matrix",
      "evaluation_metrics",
      "classification_report",
      "model_observability"
    ],
    "summary": "Definition The F1 Score is the harmonic mean of precision and recall. It provides a balanced view of both metrics and is particularly useful when: There is an uneven class distribution. Both precision and recall are important. The formula for the F1 Score is: $$F1 = 2 \\times \\frac{{\\text{Precision} \\times \\text{Recall}}}{{\\text{Precision} + \\text{Recall}}} $$ Where: - [[Precision]] is calculated as $\\frac{{\\text{True Positives}}}{{\\text{True Positives} + \\text{False Positives}}}$. - [[Recall]] is calculated as $\\frac{{\\text{True Positives}}}{{\\text{True Positives} + \\text{False Negatives}}}$. The F1 Score is the harmonic mean of precision and recall, which ensures that the score is high only when both precision and..."
  },
  "fabric": {
    "title": "Fabric",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Fabric.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Microsoft",
      "Data Factory",
      "Synapse",
      "PowerBI",
      "Relational Database",
      "Data Warehouse",
      "T-SQL",
      "Data Lake",
      "Scala",
      "PySpark",
      "Data Lake",
      "ETL"
    ],
    "inlinks": [],
    "summary": "Fabric is a unified analytics platform that operates in the cloud, eliminating the need for local installations. It provides an integrated environment for data analysis, similar to how [[Microsoft]] Office serves as a suite for productivity tasks. Key Features Unified Platform: Combines various data tools and services into a single platform, streamlining data analysis and management. Cloud-Based: Operates entirely in the cloud, ensuring accessibility and scalability without the need for local installations. Integrated Environment: Offers a cohesive environment for data analysis, integrating tools like [[Data Factory]], [[Synapse]], and [[PowerBI]]. Components Data Factory: Facilitates data integration and transformation. Synapse: Acts as..."
  },
  "fact_table": {
    "title": "Fact Table",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Fact Table.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Database Schema|schema",
      "data warehouse",
      "Facts",
      "data analysis",
      "Dimension Table",
      "granularity",
      "Fact Table"
    ],
    "inlinks": [
      "facts",
      "dimension_table",
      "fact_table",
      "granularity",
      "dimensional_modelling",
      "components_of_the_database"
    ],
    "summary": "A fact table is a central component of a star [[Database Schema|schema]] or snowflake schema in a [[data warehouse]], it stores [[Facts]]. Essential for [[data analysis]] in a data warehouse, providing the numerical data that can be analyzed in conjunction with the descriptive data stored in dimension tables. Measures: Fact tables contain measurable, quantitative data known as \"facts\" or \"measures.\" Examples include sales revenue, quantity sold, profit, or any other numeric data that can be aggregated. Foreign Keys: Fact tables include foreign keys that reference [[Dimension Table]]s. These keys link the fact table to related dimensions, allowing for contextual analysis...."
  },
  "factor_analysis": {
    "title": "Factor Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Factor Analysis.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dimensionality reduction",
      "EDA",
      "ML_Tools",
      "Factor_Analysis.py"
    ],
    "inlinks": [],
    "summary": "Factor Analysis (FA) is a statistical method used for: - [[dimensionality reduction]], - [[EDA]] - or latent variable detection It identifies underlying relationships between observed variables by modeling them as linear combinations of a smaller number of ==unobserved latent factors==. In simpler terms, it helps reduce a large number of variables into fewer factors while retaining the core information and structure of the data. It assumes that observed variables are influenced by some common latent factors and unique errors. Key Features of Factor Analysis: Latent Factors: These are unobserved variables that capture the shared variance among observed variables. Variance Decomposition:..."
  },
  "factor_analysis.py": {
    "title": "Factor_Analysis.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Factor_Analysis.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "factor_analysis"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Factor_Analysis.py 1. Factor Loadings Table This table shows how strongly each feature (e.g., sepal length (cm)) is correlated with the two extracted factors (Factor 1 and Factor 2). Rows: Represent the extracted factors (Factor 1 and Factor 2). Columns: Represent the original features of the Iris dataset. Values: Represent the \"loading\" or contribution of each feature to the factor. Higher absolute values indicate a stronger relationship between a feature and a factor. Interpretation: Factor 1 (Row 0): Strongly influenced by petal length (cm) (loading = 1.757902) and petal width (cm) (loading = 0.731005). Moderately influenced by sepal length (cm) (loading..."
  },
  "facts": {
    "title": "Facts",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Facts.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Fact Table"
    ],
    "inlinks": [
      "dimension_table",
      "fact_table",
      "dimensions",
      "granularity",
      "dimensional_modelling"
    ],
    "summary": "Facts are quantitative data points that are typically stored in the [[Fact Table]]. They represent measurable events or metrics, such as sales revenue or quantities sold."
  },
  "fastapi": {
    "title": "FastAPI",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\FastAPI.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pydantic",
      "ML_Tools",
      "FastAPI_Example.py"
    ],
    "inlinks": [
      "pydantic",
      "api",
      "model_deployment",
      "pyright_vs_pydantic"
    ],
    "summary": "FastAPI is a modern web framework for building APIs with Python. It is designed to be fast and easy to use, leveraging Python's type hints to provide features like: Automatic generation of OpenAPI documentation. Input data validation based on Python's type annotations. Asynchronous request handling with native support for asyncio. High performance, as it is built on Starlette and [[Pydantic]]. Key Features Automatic validation: Based on type hints and Pydantic models. Interactive API docs: Automatically generated Swagger UI and ReDoc. Asynchronous support: Full support for async functions. Dependency injection: Built-in support for dependencies. How to Run ==In [[ML_Tools]] see: [[FastAPI_Example.py]]==..."
  },
  "fastapi_example.py": {
    "title": "FastAPI_Example.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\FastAPI_Example.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fastapi"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/FastAPI_Example.py Explanation of New Features Path and Query Parameter Metadata: Added descriptions and constraints for better validation and autogenerated documentation. Nested Models: Demonstrated hierarchical data validation with the User model that includes a list of Item instances. Partial Updates: Introduced a PATCH endpoint to allow partial updates of fields using the Body method. Static Data: Provided a status endpoint that returns static information about the API. Returning Key-Value Data: Added a summary endpoint to showcase mock data. Main What the Script Does Starts the FastAPI Application When you run the script, it starts a web server using Uvicorn. This makes..."
  },
  "feature_engineering": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Engineering.md",
    "tags": [
      "#ml_process",
      "#ml_optimisation",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "Dimensionality Reduction",
      "C1_W2_Lab07_FeatureEngLecture.png"
    ],
    "inlinks": [
      "feature_selection_and_creation",
      "model_optimisation",
      "preprocessing",
      "regression",
      "automated_feature_creation",
      "ds_&_ml_portal",
      "class_separability",
      "encoding_categorical_variables"
    ],
    "summary": "Its the term given to the iterative process of building good features for a better model. Its the process that makes relevant features (using formulas and relations between others). We use it when we have a refined and optimised model. What does it involve - Create new features from existing ones (e.g., ratios, interactions). - Transform features to better capture non-linear relationships. - [[Dimensionality Reduction]] if necessary. The main techniques of feature engineering: - are selection (picking subset), - learning (picking the best), - extraction and combination(combining). Example: Predicting house prices. Raw features might be square footage, number of bedrooms,..."
  },
  "feature_evaluation": {
    "title": "Feature Evaluation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Evaluation.md",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Note Garbage in garbae out. It is the features that What is involved: Want to assess the usefulness of chosen features Measuring feature importance: Quantifying the contribution of each feature to the model's predictions. This can be done through various methods like statistical tests, model-specific importance scores, or permutation importance. Analysing feature relationships: Investigating correlations and redundancy, Assessing feature impact on model performance: Example: When are we done: **Reaching a stable and satisfactory model performance **No further room for improvement **Understanding the model's decision-making process"
  },
  "feature_extraction": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Extraction.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "Dimensionality Reduction",
      "interpretability",
      "Attention mechanism",
      "LLM",
      "Activation atlases",
      "Attention mechanism|Attention mechanism",
      "Dimensionality Reduction",
      "Dimensionality Reduction",
      "Activation atlases"
    ],
    "inlinks": [
      "convolutional_neural_networks"
    ],
    "summary": "Summary: In machine learning, Feature extraction is the process of transforming raw data into a set of useful features that can be effectively used by algorithms. It involves identifying and selecting key attributes or characteristics of the data that are most relevant to the problem at hand. Feature extraction helps improve both the performance and efficiency of machine learning models. Feature extraction simplifies complex data by transforming it into a smaller set of ==informative features.== Techniques like [[Dimensionality Reduction]] help retain the most significant information, improving model performance and [[interpretability]]. It is conceptually similar to the way the [[Attention mechanism]]..."
  },
  "feature_importance": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Importance.md",
    "tags": [
      "ml_process",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "Model Building",
      "interpretability",
      "Feature Selection",
      "Gini Impurity",
      "SHapley Additive exPlanations",
      "Local Interpretable Model-agnostic Explanations",
      "SHapley Additive exPlanations",
      "Local Interpretable Model-agnostic Explanations",
      "Random Forests",
      "XGBoost"
    ],
    "inlinks": [
      "model_evaluation",
      "feature_selection_vs_feature_importance",
      "shapley_additive_explanations"
    ],
    "summary": "Feature importance refers to ==techniques that assign scores to input features== (predictors) in a machine learning model to ==indicate their relative impact on the model's predictions.== Feature importance is typically assessed ==after== [[Model Building]]. It involves analyzing the trained model to determine the impact of each feature on the predictions. Feature importance helps in: improving model [[interpretability]], identifying key predictors, and possibly performing [[Feature Selection]] to reduce dimensionality, and refining performance The ==outcome== is a ranking or scoring of features based on their importance. By understanding which features contribute the most to the predictions, you can focus on the most..."
  },
  "feature_scaling": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Scaling.md",
    "tags": [
      "data_cleaning",
      "data_processing"
    ],
    "aliases": [],
    "outlinks": [
      "preprocessing",
      "Principal Component Analysis",
      "Gradient Descent",
      "Naive Bayes",
      "Decision Tree",
      "Linear Discriminant Analysis",
      "Normalisation",
      "Standardisation",
      "Pasted image 20241224083928.png"
    ],
    "inlinks": [
      "preprocessing",
      "clustering",
      "why_and_when_is_feature_scaling_necessary"
    ],
    "summary": "Used in preparing data for machine learning models. Feature Scaling is a [[preprocessing]] step in machine learning that involves adjusting the range and distribution of feature values. This ensures that all features contribute equally to the model's performance, especially when they are measured on different scales, which is particularly important for distance-based algorithms, [[Principal Component Analysis]], and optimization techniques like [[Gradient Descent]]. By using methods like normalization and standardization, you can enhance the performance and accuracy of your models. See sklearn.preprocessing Examples of algorithms not affected by feature scaling are [[Naive Bayes]], [[Decision Tree]], and [[Linear Discriminant Analysis]]. Why Use..."
  },
  "feature_selection_and_creation": {
    "title": "Feature selection and creation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature selection and creation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Feature Selection",
      "Feature Engineering"
    ],
    "inlinks": [],
    "summary": "[[Feature Selection]] [[Feature Engineering]] After the data is ready. Which features have the best value, which play the biggest role. Combining features to simplify the How to select features. - Correlation between each two (poor) - Stepwise regression - Lasso and ridge regression When selecting features we ask: - Can we control it/select it? - Can we control it easily what do we gain from it - is it a sensible variable?"
  },
  "feature_selection_vs_feature_importance": {
    "title": "Feature Selection vs Feature Importance",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Selection vs Feature Importance.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Feature Selection",
      "Feature Importance",
      "interpretability"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "Summary [[Feature Selection]] is about choosing which features to include in the model ==before training==, aiming to improve model performance and efficiency. [[Feature Importance]] is about understanding the role and impact of each feature ==after the model has been trained,== providing insights into the model's decision-making process. Use for [[interpretability]] of the model, but they are applied at different stages and serve different purposes."
  },
  "feature_selection": {
    "title": "Feature Selection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature Selection.md",
    "tags": [
      "ml_process",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "Feature Selection vs Feature Importance",
      "Filter Methods",
      "Wrapper Methods",
      "Embedded Methods",
      "Model Evaluation",
      "Correlation",
      "Heatmap",
      "Clustering",
      "Dimensionality Reduction",
      "Principal Component Analysis",
      "SVD",
      "Heatmap",
      "Correlation",
      "Variance",
      "Distributions",
      "ANOVA"
    ],
    "inlinks": [
      "pca_principal_components",
      "feature_selection_and_creation",
      "feature_selection_vs_feature_importance",
      "filter_methods",
      "preprocessing",
      "ridge",
      "data_selection_in_ml",
      "regularisation",
      "embedded_methods",
      "explain_the_curse_of_dimensionality",
      "p_values",
      "wrapper_methods",
      "logistic_regression_statsmodel_summary_table",
      "correlation",
      "linear_regression",
      "feature_importance",
      "ds_&_ml_portal",
      "lasso"
    ],
    "summary": "Purpose: The primary goal of feature selection is to identify and retain the most relevant features for model training while ==removing irrelevant or redundant ones==. This helps in simplifying the model, reducing overfitting, and improving computational efficiency. Process: Feature selection is typically performed before model training. It involves evaluating features based on certain criteria or algorithms to decide which features to keep or discard. Through an iterative process, feature selection experiments with different methods, adjusts parameters, and evaluates model performance until an optimal set of features is found. See [[Feature Selection vs Feature Importance]] Methods The choice of feature selection..."
  },
  "feature_distribution.py": {
    "title": "Feature_Distribution.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feature_Distribution.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributions"
    ],
    "summary": ""
  },
  "feed_forward_neural_network": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feed Forward Neural Network.md",
    "tags": [
      "deep_learning",
      "classifier"
    ],
    "aliases": [
      "FFNN"
    ],
    "outlinks": [
      "Neural network",
      "supervised learning",
      "forward propagation",
      "recurrent neural networks",
      "Loss function",
      "Overfitting",
      "Ridge|L2",
      "Recurrent Neural Networks"
    ],
    "inlinks": [
      "transformer",
      "types_of_neural_networks"
    ],
    "summary": "A Feedforward Neural Network (FFNN) is the simplest type of [[Neural network]]. In this model, connections between neurons do not form a cycle, allowing data to flow in one direction\u2014from the input layer, through the hidden layers, to the output layer\u2014without any loops or backward connections. This straightforward design is primarily used for [[supervised learning]] tasks. Structure Information flows in one direction: input \u2192 hidden layers \u2192 output. During [[forward propagation]], input data is passed through the network, processed by each layer, and an output is produced. Unlike [[recurrent neural networks]] (RNNs), FFNNs do not share information or weights between..."
  },
  "feedback_template": {
    "title": "Feedback Template",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Feedback Template.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Praise: I really appreciate your work on this add here FYI: It's really not a big deal, but I'm letting you know just in case. add here Suggestion: I\u2019m fairly confident this would help, but I can live without it add here Recommendation: This could be holding you back add here Plea: It\u2019s almost at the breaking point if it\u2019s not already there. add here"
  },
  "filter_method": {
    "title": "Filter method",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Filter method.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "wrapper_methods",
      "embedded_methods"
    ],
    "summary": ""
  },
  "filter_methods": {
    "title": "filter methods",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\filter methods.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Feature Selection",
      "Correlation",
      "ANOVA",
      "categorical",
      "Feature Selection"
    ],
    "inlinks": [
      "feature_selection"
    ],
    "summary": "For [[Feature Selection]] Pearson [[Correlation]] Coefficient: Measures the linear correlation between two continuous variables. Features with low correlation with the target variable are considered less relevant. Features with high correlation among themselves might be redundant. Mutual Information: Measures the amount of information obtained about one variable through another variable. High mutual information indicates strong dependency between features and the target variable. Can handle both continuous and categorical variables. used to rank or score features based on their relevance to the target variable. joint probability distribution information theory, [[ANOVA]] (Analysis of Variance): Assesses the differences in means among groups of a..."
  },
  "firebase": {
    "title": "Firebase",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Firebase.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "AWS"
    ],
    "inlinks": [],
    "summary": "Googles version of [[AWS]] Setup basics Project idea: Set up a basic emailer app."
  },
  "fishbone_diagram": {
    "title": "Fishbone diagram",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Fishbone diagram.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Documentation & Meetings",
      "Documentation & Meetings",
      "Pasted image 20250312162034.png"
    ],
    "inlinks": [],
    "summary": "Fishbone diagram [[Documentation & Meetings]] Root cause analysis: [[Documentation & Meetings]] - 5 Y's - Fishbone diagram: start at issue at head - ![[Pasted image 20250312162034.png]] - People and ownership: Who is entering the data: the source data"
  },
  "fitting_weights_and_biases_of_a_neural_network": {
    "title": "Fitting weights and biases of a neural network",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Fitting weights and biases of a neural network.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Parameters",
      "loss function",
      "Dropout",
      "ML_Tools",
      "distributions",
      "Forward Propagation",
      "Binary Classification",
      "Backpropagation",
      "Gradient Descent",
      "Stochastic Gradient Descent|SGD",
      "Regularisation",
      "Ridge",
      "Dropout",
      "hyperparameter",
      "Learning Rate",
      "Optimisation techniques"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "For a neural network model, fitting weights and biases involves optimizing these [[Model Parameters]] so the model learns to map input features ($X$) to target outputs ($y$) effectively. This is achieved through the training process, which minimizes the error between predictions and actual values. Best Practices - Use appropriate weight initializations like He or Xavier. - Choose a suitable [[loss function]] for the task. - Optimize using advanced optimizers like Adam or RMSprop. - Experiment with batch sizes, epochs, and learning rates. - Apply regularization (L2, [[Dropout]]) to prevent overfitting. - Monitor validation metrics and use early stopping. In [[ML_Tools]]..."
  },
  "flask": {
    "title": "Flask",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Flask.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pasted image 20240922202938.png",
      "Flask"
    ],
    "inlinks": [
      "jinja_template",
      "model_deployment",
      "flask"
    ],
    "summary": "software web app framework for writing web pages uses decorators ![[Pasted image 20240922202938.png]] [[Flask]] Flask app example https://www.youtube.com/watch?v=wBCEDCiQh3Q&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl You can run a flask app in google colab and then share it publicly with ngrok. flask app saved on github."
  },
  "folder_tree_diagram": {
    "title": "Folder Tree Diagram",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Folder Tree Diagram.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Links Simple method https://www.digitalcitizen.life/how-export-directory-tree-folder-windows/ More general https://superuser.com/questions/272699/how-do-i-draw-a-tree-file-structure Treeviz Graphviz tree /a /f >output.doc"
  },
  "forecasting_autoarima.py": {
    "title": "Forecasting_AutoArima.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Forecasting_AutoArima.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Time Series Forecasting",
      "Datasets",
      "model parameters",
      "ML_Tools",
      "Evaluation Metrics"
    ],
    "inlinks": [
      "forecasting_exponential_smoothing.py",
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_AutoArima.py ARIMA (AutoRegressive Integrated Moving Average) is a popular [[Time Series Forecasting]]method that models the autocorrelations within the data. It is particularly useful for datasets with trends and patterns that are not seasonal. Not perfect, think of stock trading data and the inability to predict the future. Key Concepts ARIMA Components: - AR (AutoRegressive): Uses the dependency between an observation and a number of lagged observations. - I (Integrated): Involves differencing the data to make it stationary. - MA (Moving Average): Models the dependency between an observation and a residual error from a moving average model applied to lagged observations...."
  },
  "forecasting_baseline.py": {
    "title": "Forecasting_Baseline.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Forecasting_Baseline.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Baseline.py Baseline methods are essential for establishing a performance benchmark. They provide insights into the data's underlying patterns and help in assessing the effectiveness of more sophisticated forecasting models. By comparing advanced models against these baselines, you can determine if the added complexity is justified by improved accuracy. Methods Implemented: - Mean Forecasting: Uses the average of all past values as the forecast for future periods. - Naive Forecasting: The last observed value is used as the forecast for all future periods. - Seasonal Naive Forecasting: Uses the value from the previous seasonal period to forecast the future. - Drift..."
  },
  "forecasting_exponential_smoothing.py": {
    "title": "Forecasting_Exponential_Smoothing.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Forecasting_Exponential_Smoothing.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Time Series Forecasting",
      "Datasets",
      "Forecasting_AutoArima.py"
    ],
    "inlinks": [
      "time_series_forecasting"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Exponential_Smoothing.py Exponential smoothing models are a set of [[Time Series Forecasting]] techniques that apply weighted averages of past observations, with the weights decaying exponentially over time. These methods are useful for capturing different components of time series data, such as level, trend, and seasonality. However, their effectiveness depends on the nature of the data. For [[Datasets]] with simple patterns, these models can be quite effective, but for more complex series, alternative methods may be necessary. Methods Implemented Simple Exponential Smoothing (SES): - Description: Suitable for forecasting data without trends or seasonality. It applies a constant smoothing factor to past observations...."
  },
  "foreign_key": {
    "title": "Foreign Key",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Foreign Key.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together",
      "turning_a_flat_file_into_a_database"
    ],
    "summary": "A foreign key is a field in one table that uniquely identifies a row in another table, linking to the primary key of that table. For example, DepartmentID in the ==Employees== table links to DepartmentID in the Departments table. Foreign keys establish relationships between tables and maintain referential integrity by ensuring valid connections between records. Departments Table | DepartmentID | DepartmentName | |--------------|----------------------| | 1 | Human Resources | | 2 | IT | | 3 | Marketing | Employees Table | EmployeeID | EmployeeName | DepartmentID | |------------|--------------|---------------| | 101 | Alice | 1 | | 102 | Bob..."
  },
  "forward_propagation": {
    "title": "Forward Propagation in Neural Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Forward Propagation.md",
    "tags": [
      "deep_learning",
      "statistics"
    ],
    "aliases": [
      "feedforward pass",
      "forward pass"
    ],
    "outlinks": [
      "activation function",
      "vanishing and exploding gradients problem",
      "Backpropagation"
    ],
    "inlinks": [
      "fitting_weights_and_biases_of_a_neural_network",
      "feed_forward_neural_network"
    ],
    "summary": "[!Summary] Forward propagation is the process by which input data moves through a neural network, layer by layer, to produce an output. During this process, each layer\u2019s weights and biases are applied to the input data, and an activation function is used to transform the data at each layer. Mathematically, for each layer, the input $x$ is transformed into an output $y$ through the equation $y = f(Wx + b)$, where $W$ represents the weights, $b$ is the bias, and $f$ is the activation function (e.g., ReLU, sigmoid). The output from one layer becomes the input to the next, and..."
  },
  "functional_programming": {
    "title": "What is Functional Programming?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\functional programming.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pyright"
    ],
    "summary": "Functional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and declarative as opposed to imperative. It's getting more popular with the rise of Functional Data Engineering. See also Programming Languages."
  },
  "fuzzywuzzy": {
    "title": "Fuzzywuzzy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Fuzzywuzzy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Cleansing"
    ],
    "inlinks": [],
    "summary": "Tool used for correcting spelling with pandas. [[Data Cleansing]]"
  },
  "gaussian_distribution": {
    "title": "Gaussian Distribution",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gaussian Distribution.md",
    "tags": null,
    "aliases": [
      "normally distributed data"
    ],
    "outlinks": [
      "Distributions"
    ],
    "inlinks": [
      "standardisation",
      "distributions"
    ],
    "summary": "Common assumption for a [[Distributions]]."
  },
  "gaussian_mixture_models": {
    "title": "Gaussian Mixture Models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gaussian Mixture Models.md",
    "tags": [
      "clustering"
    ],
    "aliases": [
      "GMM"
    ],
    "outlinks": [
      "distributions",
      "K-means",
      "Clustering",
      "ML_Tools",
      "Gaussian_Mixture_Model_Implementation.py",
      "Kmeans vs GMM",
      "Covariance Structures",
      "Covariance",
      "K-means",
      "Anomaly Detection",
      "Pasted image 20250126135722.png|500"
    ],
    "inlinks": [
      "covariance",
      "machine_learning_algorithms",
      "covariance_structures",
      "clustering"
    ],
    "summary": "Gaussian Mixture Models (GMMs) represent data as a mixture of multiple Gaussian [[distributions]], with each cluster corresponding to a different Gaussian component. GMMs are more effective than [[K-means]] because they consider the distributions of the data rather than relying solely on distance metrics. Soft [[Clustering]] technique. In [[ML_Tools]] see: [[Gaussian_Mixture_Model_Implementation.py]] [[Kmeans vs GMM]] GMMs can have difference [[Covariance Structures]] Key Concepts Gaussian Components: Each Gaussian distribution is characterized by its mean and [[Covariance]]. Likelihood: The likelihood of a data point belonging to a cluster is given by the formula: $$ P(X | C_k) = \\pi_k \\cdot \\mathcal{N}(X | \\mu_k, \\Sigma_k)..."
  },
  "gaussian_model": {
    "title": "Gaussian Model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gaussian Model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pasted image 20241230202826.png|500"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "Gaussian Model (Univariate) Formula: $p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)$ Steps: Estimate $\\mu$ and $\\sigma^2$ from the data. Compute the probability density for each data point. Points with low probabilities (below a threshold $\\epsilon$) are considered anomalies. ![[Pasted image 20241230202826.png|500]] 5. Multivariate Gaussian Distribution Steps: Extend the Gaussian model to include covariance across features. Fit the multivariate Gaussian model: $p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)$ $\\mu$: Mean vector $\\Sigma$: Covariance matrix Threshold low-probability examples to identify anomalies."
  },
  "gaussian_mixture_model_implementation.py": {
    "title": "Gaussian_Mixture_Model_Implementation.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gaussian_Mixture_Model_Implementation.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/Gaussian_Mixture_Model_Implementation.py Follow-Up Questions How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency? What are the implications of choosing different covariance types in GMMs?"
  },
  "general_linear_regression": {
    "title": "General Linear Regression",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\General Linear Regression.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Linear Regression",
      "t-test",
      "ANOVA"
    ],
    "inlinks": [],
    "summary": "[[Linear Regression]] [[t-test]] - to compare means between two populations. [[ANOVA]] - tests"
  },
  "generative_adversarial_networks": {
    "title": "Generative Adversarial Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Generative Adversarial Networks.md",
    "tags": null,
    "aliases": [
      "GAN"
    ],
    "outlinks": [],
    "inlinks": [
      "types_of_neural_networks"
    ],
    "summary": "Composed of two neural networks, a generator, and a discriminator, that compete against each other. GANs are used for tasks like generating realistic images or videos."
  },
  "generative_ai_from_theory_to_practice": {
    "title": "Generative AI From Theory to Practice",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Generative AI From Theory to Practice.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "LLM",
      "Markov chain",
      "Tokenisation",
      "NLP",
      "LLM",
      "Pasted image 20240524130607.png",
      "Software Development Life Cycle",
      "call summarisation",
      "Pasted image 20240524131311.png|500",
      "Ngrams",
      "RAG",
      "Pasted image 20240524131603.png",
      "GAN",
      "LLM"
    ],
    "inlinks": [],
    "summary": "Objective: How do LLMs work and operate. Enabling [[LLM]]'s at scale: Explore recent AI and Generative AI language models Steps Math on words: Turn words into coordinates. Statistics on words: Given context what is the probability of whats next. Vectors on words. Cosine similarity How train: Use [[Markov chain]] for prediction of the next [[Tokenisation]] Tokeniser: map from token to number Pre-training: tokenise input using [[NLP]] techqinues [[LLM]] looks at context: nearby tokens, in order to predict different implmentationg for differnet languages. Differnet tokenisers or translating after. Journey to scale: Demos, POC (plan to scale): understand limitations Beyond experiments and..."
  },
  "generative_ai": {
    "title": "Generative AI",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Generative AI.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "how_to_reduce_the_need_for_gen_ai_responses",
      "typical_output_formats_in_neural_networks",
      "guardrails",
      "inference_versus_prediction",
      "accessing_gen_ai_generated_content"
    ],
    "summary": ""
  },
  "get_data": {
    "title": "Get data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Get data.md",
    "tags": [
      "data_collection"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "What is involved: df = pd.read_csv('Categorical.csv') Gather relevant data from appropriate sources, addressing any quality or privacy concerns. ```python Get textbook data using for example: import re def read_file(filename): with open(filename, \"r\", encoding='UTF-8') as file: contents = file.read().replace('\\n\\n',' ').replace('[edit]', '').replace('\\ufeff', '').replace('\\n', ' ').replace('\\u3000', ' ') return contents text = read_file('Data various/Monte_Cristo.txt') text_start = [m.start() for m in re.finditer('VOLUME ONE', text)] text_end = [m.start() for m in re.finditer('End of Project Gutenberg', text)] text = text[text_start[1]:text_end[0]] ``` How would you approach a colleague who is hesitant to share their data? ? - explain the purpose and benefits - ensure confidentiality (GDPR) with..."
  },
  "gini_impurity_vs_cross_entropy": {
    "title": "Gini Impurity vs Cross Entropy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gini Impurity vs Cross Entropy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Gini Impurity",
      "Cross Entropy",
      "Distributions"
    ],
    "inlinks": [
      "decision_tree"
    ],
    "summary": "When working with decision trees, both [[Gini Impurity]] and [[Cross Entropy]] are metrics used to evaluate the quality of a split. They help determine how well a feature separates the classes in a dataset. Gini Impurity Definition: Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node. Computation: Generally faster to compute than cross-entropy because it does not involve logarithms. Use Case: Often used in the CART (Classification and Regression Trees) algorithm. It is a good default choice for classification tasks due to its..."
  },
  "gini_impurity": {
    "title": "Gini Impurity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gini Impurity.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Decision Tree",
      "Classification",
      "Regression Metrics"
    ],
    "inlinks": [
      "gini_impurity_vs_cross_entropy",
      "decision_tree",
      "feature_importance"
    ],
    "summary": "Gini impurity is a metric used in decision trees to measure the degree or probability of misclassification in a dataset. It is associated with the leaves of a [[Decision Tree]] and helps determine the best split at each node. Calculation Mathematical Formula: Gini impurity is calculated as the probability of incorrectly classifying a randomly chosen element if it were randomly labelled according to the distribution of labels in the subset. Formula: $$ \\text{Gini Impurity} = 1 - \\sum_{i=1}^{n} p_i^2 $$ where $p_i$ is the probability of an element being classified into a particular class. Usage Decision Trees: Gini impurity is..."
  },
  "gis": {
    "title": "GIS",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\GIS.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Web Map Tile Service (WMTS)",
      "Web Feature Server (WFS)",
      "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)",
      "shapefile"
    ],
    "inlinks": [
      "shapefile",
      "web_feature_server_(wfs)",
      "web_map_tile_service_(wmts)"
    ],
    "summary": "Geographic information system. File formats: The Web Map Tile Service (WMTS) and Web Feature Server (WFS) are both specifications used in the field of Geographic Information Systems (GIS) to serve different types of geographic data over the web. The primary differences between them lie in the type of data they serve and how they serve it. [[Web Map Tile Service (WMTS)]] [[Web Feature Server (WFS)]] [[Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)]] [[shapefile]] There are free GIS softwares"
  },
  "git": {
    "title": "Git",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Git.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "How to do git commit messages properly"
    ],
    "inlinks": [
      "how_to_do_git_commit_messages_properly",
      "powershell_vs_bash",
      "debugging"
    ],
    "summary": "tags: - software Do git bash here. git status git add . (adds all) git status git commit -m \"\" git push Notes https://www.youtube.com/watch?v=xnR0dlOqNVE Git Fork vs. Git Clone [[How to do git commit messages properly]] Examples Git: Common Issues and Fixes Git can be frustrating, especially when things go wrong. This guide provides practical solutions to common Git mistakes, explained in simple terms. https://ohshitgit.com/ \ud83d\udd04 Undoing Mistakes I messed up badly! Can I go back in time? Yes! Use Git\u2019s reflog to find a previous state: ```bash git reflog Find the index of the state before things broke git..."
  },
  "gitlab-ci.yml": {
    "title": "gitlab-ci.yml",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\gitlab-ci.yml.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The purpose of a gitlab-ci.yml file is to define and configure the GitLab CI/CD pipeline for automating tasks such as building, testing, and deploying your code. It is the core configuration file that GitLab uses to orchestrate and execute CI/CD workflows in a repository. Key Purposes: Automation of Workflows: Automates repetitive tasks like running tests, building applications, linting code, and deploying updates. Pipeline Definition: Specifies the stages (e.g., build, test, deploy) and their sequence. Defines the jobs within each stage and their respective commands. Consistency and Reliability: Ensures consistent execution of tasks across environments, reducing errors caused by manual intervention...."
  },
  "gitlab": {
    "title": "Gitlab",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gitlab.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "CI-CD"
    ],
    "inlinks": [
      "ci-cd"
    ],
    "summary": "GitLab CI CD Tutorial for Beginners Crash Course Provides managed runners to execute [[CI-CD]] pipelines. Integrates with version control systems to automate the CI/CD process."
  },
  "google_cloud_platform": {
    "title": "Google Cloud Platform",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Google Cloud Platform.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Kubernetes",
      "BigQuery",
      "NoSQL",
      "BigQuery",
      "SQL",
      "MySQL",
      "standardised/Firebase"
    ],
    "inlinks": [],
    "summary": "Google Cloud Platform is a suite of cloud computing services offered by Google. It provides a range of services including computing, storage, and application development that run on Google hardware. Resources: Introduction to Google Cloud Compute Engine Description: GCP's Infrastructure as a Service (IaaS) offering, allowing users to run virtual machines on Google's infrastructure. Features: Custom Machine Types: Create VMs with custom configurations. Preemptible VMs: Costeffective, shortlived instances for batch jobs and faulttolerant workloads. Sustained Use Discounts: Automatic discounts for prolonged usage. Persecond Billing: Charges calculated per second for cost savings. Use Cases: Suitable for web hosting, data processing, and..."
  },
  "google_my_maps_data_extraction": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Google My Maps Data Extraction.md",
    "tags": [],
    "aliases": null,
    "outlinks": [
      "**Google Apps Script**"
    ],
    "inlinks": [],
    "summary": "Summary: This guide covers the key workflows and tools for managing and processing location data in Google Sheets and Google My Maps. Suppose we have marks on Google My Maps. In order to extract the location of markers to a google sheet. Export Marker Data from Google My Maps as KML/CSV. Convert KML to CSV Use Apps Script in Google Sheets to extract data like addresses or postal codes from coordinates. Extract Data from Google My Maps Export Custom Markers: Open Google My Maps. Use the menu (three dots) to select Export to KML. The exported file will contain marker..."
  },
  "gradient_boosting_regressor": {
    "title": "Gradient Boosting Regressor",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gradient Boosting Regressor.md",
    "tags": [
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "Boosting",
      "Model Ensemble",
      "Decision Tree"
    ],
    "inlinks": [],
    "summary": "https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor [[Boosting]] The GradientBoostingRegressor from the sklearn.ensemble module is a model used for regression tasks. It builds an [[Model Ensemble]] of [[Decision Tree]] in a sequential manner, where each tree tries to correct the errors made ==by the previous ones==. Here\u2019s a breakdown of the key parameters: loss: Specifies the loss function to optimize. Default is 'squared_error', which is the least-squares loss function. Other options like 'absolute_error' can be used for robustness against outliers. learning_rate: Controls the contribution of each tree to the final prediction. A smaller value (e.g., 0.01) makes the model learn more slowly, but it can lead..."
  },
  "gradient_boosting": {
    "title": "Gradient Boosting",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gradient Boosting.md",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "Model Building",
      "Boosting",
      "Gradient Descent",
      "Weak Learners",
      "Decision Tree",
      "Weak Learners",
      "Decision Tree",
      "Model Ensemble",
      "Weak Learners",
      "Loss Function",
      "learning rate",
      "LightGBM",
      "XGBoost",
      "CatBoost",
      "Machine Learning Algorithms",
      "heterogeneous features",
      "Overfitting"
    ],
    "inlinks": [
      "xgboost",
      "embedded_methods",
      "lightgbm_vs_xgboost_vs_catboost",
      "boosting",
      "catboost",
      "use_of_rnns_in_energy_sector",
      "ds_&_ml_portal"
    ],
    "summary": "Gradient Boosting is a technique used for building predictive models [[Model Building]], particularly in tasks like regression and classification. It combines the concepts of [[Boosting]] and [[Gradient Descent]] to create strong models by sequentially combining multiple [[Weak Learners]] ([[Decision Tree]]. Key Idea: Instead of fitting a single strong model, Gradient Boosting builds multiple weak learners sequentially. Each new model focuses on ==correcting the mistakes made by the previous ones== by fitting to the residuals (differences between observed and predicted values). Gradient Boosting builds an ensemble of [[Weak Learners]] (usually [[Decision Tree]]) sequentially. Each new model focuses on the errors of..."
  },
  "gradient_descent": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gradient Descent.md",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "GD"
    ],
    "outlinks": [
      "Optimisation function",
      "Loss function",
      "Cost Function",
      "Obsidian_EPIqLAto5w.png|500",
      "Obsidian_FEGflF5RKQ.png|500",
      "learning rate",
      "Obsidian_M4mzGSAx7d.png|500",
      "Stochastic Gradient Descent",
      "Stochastic Gradient Descent",
      "Stochastic Gradient Descent",
      "Batch gradient descent",
      "Batch gradient descent",
      "Mini-batch gradient descent",
      "Mini-batch gradient descent",
      "Batch gradient descent",
      "Stochastic Gradient Descent",
      "Gradient Descent",
      "Optimisation techniques",
      "Model Parameters",
      "loss function",
      "cost function",
      "Gradient Descent",
      "Cost Function",
      "Gradient Descent",
      "Gradient Descent#",
      "Pasted image 20241224082847.png"
    ],
    "inlinks": [
      "xgboost",
      "pytorch",
      "standardisation",
      "momentum",
      "ds_&_ml_portal",
      "cost_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "backpropagation",
      "adam_optimizer",
      "lightgbm",
      "learning_rate",
      "z-normalisation",
      "gradient_boosting",
      "optimisation_function",
      "linear_regression",
      "model_parameters_vs_hyperparameters",
      "deep_learning",
      "logistic_regression_in_sklearn_&_gradient_descent",
      "gradient_descent",
      "stochastic_gradient_descent",
      "optimisation_techniques",
      "optimising_a_logistic_regression_model",
      "feature_scaling"
    ],
    "summary": "Gradient descent is an [[Optimisation function]] used to minimize errors in a model by adjusting its parameters iteratively. It works by moving in the direction of the steepest decrease of the [[Loss function]]. Uses the difference quotient. The step size is important between derivatives (small then slow) (if large then might miss minimum). With Stochastic method we can don't need to the entire data set again, we can just add the new information to get improvement. Gradient descent uses the entire data set. Used to find the min/max of [[Cost Function]]. Given any point on the cost function surfaces. Then..."
  },
  "gradio": {
    "title": "Gradio",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Gradio.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Overview"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "Gradio is an open-source platform that simplifies the process of ==creating user interfaces== for machine learning models. It allows users to quickly build interactive demos and applications for their models without extensive front-end development knowledge. Main uses: Interactive Interfaces: Gradio provides a simple way to create web-based interfaces where users can interact with machine learning models by uploading files, entering text, or adjusting sliders. Rapid Prototyping: It enables quick prototyping and sharing of machine learning models, making it easier to demonstrate model capabilities to stakeholders or gather user feedback. Ease of Integration: Gradio can be easily integrated with popular machine..."
  },
  "grain": {
    "title": "Grain",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Grain.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "granularity"
    ],
    "inlinks": [
      "dimensional_modelling"
    ],
    "summary": "Grain - Definition: The level of detail or [[granularity]] of the data stored in the fact table. - Importance: Defining the grain is crucial as it determines what each record in the fact table represents (e.g., individual transactions, daily summaries)."
  },
  "grammar_method": {
    "title": "Grammar method",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Grammar method.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "can understand the Grammar as a method for acceptable sentences."
  },
  "granularity": {
    "title": "What is Granularity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\granularity.md",
    "tags": [
      "database",
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [
      "Dimensional Modelling",
      "Fact Table",
      "Dimensions",
      "Facts",
      "business intelligence",
      "semantic layer",
      "OLAP"
    ],
    "inlinks": [
      "grain",
      "transaction",
      "choosing_the_number_of_clusters",
      "rollup",
      "fact_table"
    ],
    "summary": "Definition of Grain in [[Dimensional Modelling]] - The grain of a [[Fact Table]] defines what a single row in the table represents. It is the level of detail captured by the fact table. - Declaring the grain is essential because it sets the foundation for the entire dimensional model. It determines how detailed the data will be. Importance of Grain Declaration: - The grain must be established before selecting [[Dimensions]] and [[Facts]] because all dimensions and facts must align with the grain. - This alignment ensures consistency across the data model, which is critical for the performance and usability of..."
  },
  "graph_analysis_plugin": {
    "title": "Graph Analysis Plugin",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Graph Analysis Plugin.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_archive_graph_analysis"
    ],
    "summary": ""
  },
  "graph_neural_network": {
    "title": "Graph Neural Network",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Graph Neural Network.md",
    "tags": [],
    "aliases": [
      "GNN"
    ],
    "outlinks": [
      "Recommender systems"
    ],
    "inlinks": [
      "graphrag"
    ],
    "summary": "Resources: - How Graph Neural Networks Are Transforming Industries Use cases: - [[Recommender systems]] i.e. Uber, Pinterest (PinSage) - Traffic Prediction - Deepmind in google maps - Weather forecasting - GraphCast - Deepmind - Data Mining - Relational Deep Learning - Material Science - Deepmind - GNome - Density function theory. - Drug Discovery - MIT - antibiotic activities"
  },
  "graphrag": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\GraphRAG.md",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "graph database"
    ],
    "outlinks": [
      "GraphRAG",
      "RAG",
      "Knowledge Graph",
      "Named Entity Recognition",
      "Neo4j",
      "Graph Neural Network",
      "How to search within a graph",
      "Text2Cypher",
      "interpretability",
      "ML_Tools",
      "Wikipedia_API.py"
    ],
    "inlinks": [
      "relationships_in_memory",
      "how_to_search_within_a_graph",
      "agentic_solutions",
      "graphrag"
    ],
    "summary": "[[GraphRAG]] is a [[RAG]] framework that utilizes [[Knowledge Graph]]s to enhance information retrieval and processing. A significant aspect of this framework is the use of large language models (LLMs) for [[Named Entity Recognition]] (NER) within [[Neo4j]]. [[Graph Neural Network]] Related Terms [[How to search within a graph]] [[Text2Cypher]]: This feature allows users to interact with the graph in a user-friendly manner, converting natural language queries into Cypher queries. How to move datasets into a graph database. Graphrag patterns. The role of [[interpretability]] in understanding graph-based retrieval. Implementation I discovered an insightful LinkedIn post discussing the potential of knowledge graphs: This..."
  },
  "grep": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Grep.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "grep.png"
    ],
    "inlinks": [],
    "summary": "![[grep.png]]"
  },
  "gridseachcv": {
    "title": "GridSeachCv",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\GridSeachCv.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "GridSeachCv",
      "Hyperparameter",
      "Hyperparameter",
      "Pasted image 20240128194244.png|500"
    ],
    "inlinks": [
      "gridseachcv",
      "hyperparameter_tuning",
      "model_selection",
      "decision_tree"
    ],
    "summary": "Used [[GridSeachCv]] to search through the [[Hyperparameter]] space ```python rf_regressor = RandomForestRegressor(random_state=42) grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error') grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ Model Training with best hyperparameters rf_regressor = RandomForestRegressor(**best_params, random_state=42) rf_regressor.fit(X_train, y_train) ``` Given a parameter grid of [[Hyperparameter]], a model, then you model it on the hypers, then gives you the best hypers, that gives the highest cross validation performance. ![[Pasted image 20240128194244.png|500]]"
  },
  "groupby_vs_crosstab": {
    "title": "Groupby vs Crosstab",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Groupby vs Crosstab.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Groupby",
      "Crosstab",
      "DE_Tools"
    ],
    "inlinks": [
      "groupby"
    ],
    "summary": "In pandas, [[Groupby]] and [[Crosstab]] serve related but distinct purposes for data ==aggregation== and summarization. groupby is more flexible for aggregation and transformations, whereas crosstab is specifically designed for creating frequency tables and exploring the relationship between categorical variables. In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Key Differences Purpose: groupby: Used for performing aggregate functions (sum, mean, count, etc.) on grouped data. crosstab: Used for generating frequency tables or contingency tables. Output: groupby: Returns a DataFrame with aggregated values. crosstab: Returns a DataFrame with counts or specified aggregation functions applied across two or more columns. Usage: groupby: Can be used with multiple..."
  },
  "groupby": {
    "title": "Groupby",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Groupby.md",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "Groupby vs Crosstab",
      "DE_Tools",
      "Pasted image 20250323081619.png"
    ],
    "inlinks": [
      "pandas_stack",
      "aggregation",
      "pd.grouper",
      "handling_missing_data",
      "multi-level_index",
      "melt",
      "groupby_vs_crosstab"
    ],
    "summary": "Groupby is a versatile method in pandas used to group data based on one or more columns, and then perform aggregate functions on the grouped data. Related: - [[Groupby vs Crosstab]] In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation/group_by.ipynb Implementation ```python Sample DataFrame df = pd.DataFrame({'Category': ['A', 'B', 'A', 'B', 'A'],'Values': [10, 20, 30, 40, 50]}) Group by 'Category' and calculate the sum of 'Values' grouped = df.groupby('Category').sum() print(grouped) Output: Values Category A 90 B 60 ``` ![[Pasted image 20250323081619.png]]"
  },
  "grouped_plots": {
    "title": "Grouped plots",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Grouped plots.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Visualisation",
      "Pasted image 20250402212849.png"
    ],
    "inlinks": [
      "melt"
    ],
    "summary": "Related: - [[Data Visualisation]] - pairplots ```python import seaborn as sns import matplotlib.pyplot as plt Load example dataset tips = sns.load_dataset(\"tips\") Facet Grid Example g = sns.FacetGrid(tips, col=\"sex\", row=\"time\") g.map_dataframe(sns.histplot, x=\"total_bill\", bins=20) plt.show() ``` ![[Pasted image 20250402212849.png]]"
  },
  "gru": {
    "title": "GRU",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\GRU.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "recurrent_neural_networks",
      "lstm"
    ],
    "summary": ""
  },
  "gsheets": {
    "title": "GSheets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\GSheets.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "QUERY GSheets"
    ],
    "inlinks": [],
    "summary": "Useful functions: - [[QUERY GSheets]] - ARRAYFORMULA - Indirect Accessing google sheets from a script: https://www.youtube.com/watch?v=zCEJurLGFRk"
  },
  "guardrails": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Guardrails.md",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "Generative AI",
      "Guardrails",
      "Prompting",
      "Data Observability|monitoring"
    ],
    "inlinks": [
      "guardrails"
    ],
    "summary": "Controlling a [[Generative AI]] in business through the use of [[Guardrails]] ensures that the AI remains aligned with specific business goals and avoids unintended or harmful outputs. Guardrails are essential for maintaining security, compliance, and reliability in AI systems. Here's an outline based on your notes: 1. Input Guardrails Prompt Injection Control: [[Prompting]] To prevent users from prompting the AI in ways that could result in harmful or inappropriate responses, filtering or validating inputs can be essential. This reduces the risk of the model being \"jailbroken\" (i.e., forced to generate outputs outside its intended use case). Topic Restriction: Limit the..."
  },
  "hadoop": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hadoop.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Batch Processing",
      "Apache Spark|Spark",
      "Batch Processing",
      "Batch Processing",
      "Big Data",
      "Data Lake"
    ],
    "inlinks": [
      "distributed_computing",
      "big_data",
      "parquet",
      "databricks",
      "difference_between_snowflake_to_hadoop",
      "map_reduce"
    ],
    "summary": "Hadoop provides the backbone for distributed storage and computation. It uses HDFS (Hadoop Distributed File System) to split large datasets across clusters of servers, while MapReduce enables parallel processing. It\u2019s well-suited for [[Batch Processing]]asks, though newer tools like [[Apache Spark|Spark]] often outperform Hadoop in terms of speed and ease of use. Architecture: Open-Source Framework: Hadoop is an open-source framework for distributed storage and processing of large datasets using clusters of commodity hardware. Distributed File System: The Hadoop Distributed File System (HDFS) stores data across multiple machines, providing high throughput access to data. MapReduce: Originally designed for [[Batch Processing]] using the..."
  },
  "handling_different_distributions": {
    "title": "Handling Different Distributions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Handling Different Distributions.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "distributions",
      "Model Robustness",
      "Accuracy",
      "Datasets",
      "Preprocessing",
      "model selection",
      "ML_Tools",
      "Preprocessing"
    ],
    "inlinks": [
      "train-dev-test_sets",
      "data_cleansing"
    ],
    "summary": "Handling different [[distributions]] is needed for developing robust, fair, and accurate machine learning models that can adapt to a wide range of data environments. Importance of Handling Different Distributions [[Model Robustness]]: Ensures models generalize well to new, unseen data. Bias Mitigation: Prevents bias in predictions by accommodating diverse data types. Improved [[Accuracy]]: Fine-tunes models for better accuracy across varied [[Datasets]]. Maintains model effectiveness across different data sources. Decision Making: Informs [[Preprocessing]], [[model selection]], and evaluation strategies. Resources Video: Training and Testing on Different Distributions Example Scenario High-resolution photos (many) vs. amateur photos (small number) exhibit different distributions. Strategy for Handling..."
  },
  "handling_missing_data": {
    "title": "Handling Missing Data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Handling Missing Data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Transformation",
      "DE_Tools",
      "Handling_Missing_Data_Basic.ipynb",
      "Handling_Missing_Data.ipynb",
      "Groupby",
      "'var1', 'var2'"
    ],
    "inlinks": [
      "xgboost",
      "data_cleansing",
      "pandas",
      "outliers"
    ],
    "summary": "Missing data can provide insights into the data collection process. It's important to determine whether the missing data is randomly distributed or specific to certain features. Filling in data is a type of [[Data Transformation]]. In [[DE_Tools]]see: - [[Handling_Missing_Data_Basic.ipynb]] - [[Handling_Missing_Data.ipynb]] Resources: - https://scikit-learn.org/stable/modules/impute.html Identifying Missing Data How do you find which features have the most missing data? To find where missing values (NA) are located in your dataset, use the following commands: python df.isnull().sum() df.isna().sum() df[df.columns[df.isnull().sum() > 0].tolist()].info() Treating Missing Values There are two main strategies for handling missing values: removing them or replacing them. Remove Missing Values: -..."
  },
  "handling_missing_data.ipynb": {
    "title": "Handling_Missing_Data.ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Handling_Missing_Data.ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data.ipynb"
  },
  "handling_missing_data_basic.ipynb": {
    "title": "Handling_Missing_Data_Basic.ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Handling_Missing_Data_Basic.ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "handling_missing_data"
    ],
    "summary": "https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data_Basic.ipynb"
  },
  "handwritten_digit_classification": {
    "title": "Handwritten Digit Classification",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Handwritten Digit Classification.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pasted image 20241006124356.png|800"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "![[Pasted image 20241006124356.png|800]]"
  },
  "hash": {
    "title": "Hash",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hash.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data integrity"
    ],
    "inlinks": [
      "cryptography",
      "data_integrity"
    ],
    "summary": "A hash is a fixed-size string of characters that is generated from input data of any size using a hash function. Hashes are used to ensure [[data integrity]] by providing a unique representation of the data, making it easy to detect any changes or alterations. Key Characteristics of Hashes: Deterministic: The same input will always produce the same hash output. Fixed Size: Regardless of the size of the input data, the output hash will always be of a fixed length (e.g., SHA-256 produces a 256-bit hash). Fast Computation: Hash functions are designed to compute the hash value quickly. Pre-image Resistance:..."
  },
  "heatmap": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Heatmap.md",
    "tags": [
      "code_snippet",
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "Correlation",
      "Multicollinearity",
      "ML_Tools",
      "Heatmaps_Dendrograms.py"
    ],
    "inlinks": [
      "pca_principal_components",
      "multicollinearity",
      "heatmaps_dendrograms.py",
      "correlation",
      "feature_selection"
    ],
    "summary": "Description A heatmap is a two-dimensional graphical representation of data where individual values are represented by colors. It is particularly useful for visualizing numerical data organized in a table-like format. A heatmap is a graphical representation of data where individual values are represented by colors. It is useful for visualizing numerical data and analyzing the correlation between features. A heatmap is a visualization tool for analyzing the [[Correlation]] between features in a dataset. In the context of correlation analysis, a heatmap can display the correlation coefficients between different features in a dataset. By using a heatmap, you can easily identify..."
  },
  "heatmaps_dendrograms.py": {
    "title": "Heatmaps_Dendrograms.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Heatmaps_Dendrograms.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Heatmap",
      "Dendrograms"
    ],
    "inlinks": [
      "heatmap"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations\\Preprocess\\Correlation\\Heatmaps_Dendrograms.py See: - [[Heatmap]] - [[Dendrograms]]"
  },
  "heterogeneous_features": {
    "title": "heterogeneous features",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\heterogeneous features.md",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "Preprocessing"
    ],
    "inlinks": [
      "gradient_boosting"
    ],
    "summary": "Description In machine learning, heterogeneous features refer to a situation where the input data contains a variety of different types of features. Let's break it down: 1. Features: Features are the individual measurable properties or characteristics of the data used for making predictions in a machine learning model. For example, in a dataset about houses, features could include the number of bedrooms, square footage, location, and whether it has a garden. 2. Homogeneous vs. Heterogeneous: Homogeneous Features: In some datasets, all features are of the same type, such as numerical or categorical. For instance, a dataset containing only numerical features..."
  },
  "hierarchical_clustering": {
    "title": "Hierarchical Clustering",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hierarchical Clustering.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "clustering"
    ],
    "summary": "Hierarchical clustering builds a treelike structure of clusters, with similar clusters merged together at higher levels. Hierarchical clustering builds a tree-like structure of clusters, with similar clusters merged together at higher levels."
  },
  "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data": {
    "title": "High cross validation accuracy is not directly proportional to performance on unseen test data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\High cross validation accuracy is not directly proportional to performance on unseen test data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Cross Validation",
      "Data Leakage",
      "preprocessing",
      "hyperparameter tuning",
      "imbalanced datasets",
      "Evaluation Metrics",
      "Distributions|distribution"
    ],
    "inlinks": [],
    "summary": "Reasons a Model with High [[Cross Validation]] Accuracy May Perform Poorly on Unseen Test Data [[Data Leakage]]: - Information from test folds leaks into training, inflating CV accuracy. - Solution: Apply [[preprocessing]] independently within each CV fold. Overfitting: - Model captures noise in training data, leading to high CV but low test accuracy. - Solution: Use simpler models, regularization, and evaluate test performance during [[hyperparameter tuning]]. Insufficient Cross-Validation Folds: - Too few folds lead to high variance in performance estimates. - Solution: Use more folds (e.g., 5- or 10-fold CV) for reliable estimates. Over-Optimized Hyperparameters: - Excessive tuning results in..."
  },
  "how_businesses_use_gen_ai": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How businesses use Gen AI.md",
    "tags": [
      "business",
      "GenAI",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "transactional journeys",
      "model performance"
    ],
    "inlinks": [],
    "summary": "Businesses leverage generative AI to transform various operations, using models like OpenAI, Gemini (Google Cloud), Anthropic, and Meta models. These models provide services through cloud providers, making them accessible via APIs. Key use cases include: Content Creation: Generative AI can produce text, images, code, and even videos, enhancing marketing, design, and communication efforts. Customer Support: AI chatbots and assistants automate customer interactions, reducing response times and improving service quality. Data Analysis & Insights: Models help businesses analyze large datasets, enabling predictive analytics and trend forecasting. Customization: Personalization of products and services, such as tailored recommendations or [[transactional journeys]]/customer experiences, is..."
  },
  "how_do_we_evaluate_of_llm_outputs": {
    "title": "How do we evaluate of LLM Outputs",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How do we evaluate of LLM Outputs.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "LLM",
      "LLM Evaluation Metrics",
      "What are the best practices for evaluating the effectiveness of different prompts",
      "Prompt engineering"
    ],
    "inlinks": [
      "llm"
    ],
    "summary": "Methods for assessing the quality and relevance of LLM-generated outputs, critical for improving model performance. The evaluation of [[LLM]] outputs involves various methodologies to assess their quality and relevance. Important Evaluating LLM outputs requires both quantitative metrics ([[LLM Evaluation Metrics]]) and qualitative assessments (human judgment). The iterative feedback loop from evaluations informs model improvements and prompt engineering strategies. Follow up questions How does the inclusion of diverse datasets impact the robustness of LLM evaluations [[What are the best practices for evaluating the effectiveness of different prompts]] Related Topics [[Prompt engineering]] in natural language processing"
  },
  "how_do_you_do_the_data_selection": {
    "title": "how do you do the data selection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\how do you do the data selection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "how do you do the data selection",
      "Data Selection"
    ],
    "inlinks": [
      "how_do_you_do_the_data_selection"
    ],
    "summary": "When you sample a dataset, [[how do you do the data selection]]? [[Data Selection]] A: By randomly sampling, by time period (use a feature).."
  },
  "how_is_reinforcement_learning_being_combined_with_deep_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How is reinforcement learning being combined with deep learning.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "Reinforcement learning",
      "Deep Learning"
    ],
    "inlinks": [],
    "summary": "The sources touch upon reinforcement learning as an area beyond the scope of their discussion. However, the combination of [[Reinforcement learning]] with [[Deep Learning]] has shown remarkable results in recent years, particularly in areas like game playing and robotics. Exploring the potential of this combination in other domains and developing new algorithms that effectively integrate deep learning representations with reinforcement learning principles could lead to significant advancements in artificial intelligence."
  },
  "how_is_schema_evolution_done_in_practice_with_sql": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How is schema evolution done in practice with SQL.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "how_llms_store_facts": {
    "title": "How LLMs store facts",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How LLMs store facts.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Multilayer Perceptrons",
      "Vector Embedding",
      "Johnson\u2013Lindenstrauss lemma",
      "interpretability",
      "LLM",
      "Anthropic"
    ],
    "inlinks": [],
    "summary": "How might LLMs store facts Not solved How do [[Multilayer Perceptrons]] store facts? Different directions encode information in [[Vector Embedding]] space. MLP's are blocks of vectors, these are acted on my the context matrix [[Johnson\u2013Lindenstrauss lemma]] Sparse Autoencoder - used in [[interpretability]] of [[LLM]] responses See [[Anthropic]] posts - https://transformer-circuits.pub/2022/toy_model/index.html#adversarial - https://transformer-circuits.pub/2023/monosemantic-features"
  },
  "how_to_do_git_commit_messages_properly": {
    "title": "How to do git commit messages properly",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to do git commit messages properly.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Git"
    ],
    "inlinks": [
      "git"
    ],
    "summary": "Structure of a goof [[Git]] Commit Message Subject Line Keep it short (50 characters or less). Use the imperative mood (e.g., \"Fix bug\" instead of \"Fixed bug\"). Capitalize the first letter. Do not end with a period. Body (Optional) Separate from the subject line with a blank line. Explain the \"what\" and \"why\" of the changes, not the \"how\". Wrap text at 72 characters. Footer (Optional) Include references to issues or pull requests (e.g., \"Closes #123\"). Add any additional notes or metadata. Good Examples: Fix a Bug Fix incorrect login redirection The login redirection was leading to an unauthorized page..."
  },
  "how_to_model_to_improve_demand_forecasting": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to model to improve demand forecasting.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": ""
  },
  "how_to_normalise_a_merged_table": {
    "title": "How to normalise a merged table",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to normalise a merged table.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Normalised Schema"
    ],
    "inlinks": [
      "normalisation",
      "powerquery",
      "normalised_schema"
    ],
    "summary": "See: [[Normalised Schema]] Splitting out tables. Resource: Database Normalization for Beginners | How to Normalize Data w/ Power Query (full tutorial!)"
  },
  "how_to_reduce_the_need_for_gen_ai_responses": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to reduce the need for Gen AI responses.md",
    "tags": [
      "GenAI",
      "business"
    ],
    "aliases": [],
    "outlinks": [
      "Generative AI",
      "caching",
      "transactional journeys"
    ],
    "inlinks": [],
    "summary": "Reducing the need for frequent [[Generative AI]] (Gen AI) responses can be done by leveraging techniques such as [[caching]] and setting up predefined [[transactional journeys]]. Here's a breakdown: Caching AI Responses: Caching allows storing frequently requested AI responses and reusing them. This reduces the number of queries to the AI model, thus lowering both response time and cost. For example, common queries like \"How do I reset my password?\" can be cached for quick reuse without engaging the AI model each time (1). Predefined Transactional Journeys: For repetitive tasks (e.g., \"I want to close my account\"), predefined ==workflows== or \"journeys\"..."
  },
  "how_to_search_within_a_graph": {
    "title": "How to search within a graph",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to search within a graph.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "How to search within a graph",
      "standardised/Vector Embedding",
      "Text2Cypher",
      "How to search within a graph",
      "GraphRAG",
      "Knowledge Graph",
      "Pasted image 20241004074458.png"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "vector_embedding",
      "graphrag"
    ],
    "summary": "[[How to search within a graph]] Vector Search with Graph Context [[standardised/Vector Embedding]] plays a crucial role in enhancing search capabilities: Comparison of Vector-Only vs. Graph-RAG: - Vector-only searches may lack context, while Graph-RAG utilizes graph traversal to provide richer, multi-step context. - This leads to more complex and informative responses. Contextual Prompts: - Context is used to answer prompts (in JSON format). With graph traversal, this context involves more steps, allowing for more elaborate retrieval queries. [[Text2Cypher]] [[How to search within a graph]] Node Embedding Useful in [[GraphRAG]] is understanding the relationships of nodes in a [[Knowledge Graph]] using..."
  },
  "how_to_use_sklearn_pipeline": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\How to use Sklearn Pipeline.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "hugging_face": {
    "title": "Hugging Face",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hugging Face.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Transformer",
      "Transformer"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "Hugging Face is open-source platform known for its contributions to natural language processing (NLP) and machine learning. It provides a comprehensive library called [[Transformer]], which includes pre-trained models for tasks such as text classification, translation, summarization, and question answering. Hugging Face is widely used for: Access to Pre-trained Models: Offers a vast collection of state-of-the-art models that can be easily fine-tuned for specific NLP tasks. Ease of Use: Simplifies the implementation of complex NLP models with user-friendly APIs. Community and Collaboration: Hosts a vibrant community where researchers and developers share models and datasets, fostering collaboration and innovation in AI. [[Transformer]]..."
  },
  "hyperparameter_tuning": {
    "title": "Hyperparameter Tuning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hyperparameter Tuning.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Cross Validation",
      "GridSeachCv",
      "standardised/Optuna",
      "Regularisation",
      "Cross Validation",
      "Interpretable Decision Trees",
      "Random Forests",
      "ML_Tools",
      "Hyperparameter_tuning_GridSearchCV.py"
    ],
    "inlinks": [
      "xgboost",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "hyperparameter",
      "test_loss_when_evaluating_models",
      "pycaret",
      "ds_&_ml_portal",
      "optuna"
    ],
    "summary": "Objective: - Tune the model\u2019s hyperparameters to improve performance. For example, in regularized linear regression, the main hyperparameter to tune is the regularization strength (e.g., alpha in Ridge or Lasso). - Use [[Cross Validation]] to evaluate the model\u2019s performance with different hyperparameters. Optimization Techniques: - [[GridSeachCv]]: Exhaustively searches through a specified subset of hyperparameters. - Random Search: Randomly samples from the hyperparameter space, often more efficient than grid search. - [[standardised/Optuna]] - [[Regularisation]]: Often part of the hyperparameter tuning process, especially in models prone to overfitting. Key Considerations - Balance Between Exploration and Exploitation: Ensure a good balance between exploring..."
  },
  "hyperparameter": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hyperparameter.md",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "Neural network",
      "K-nearest neighbours|KNN",
      "Hyperparameter Tuning",
      "Model Parameters",
      "Model parameters vs hyperparameters"
    ],
    "inlinks": [
      "weak_learners",
      "decision_tree",
      "fitting_weights_and_biases_of_a_neural_network",
      "gridseachcv",
      "cross_validation",
      "adam_optimizer",
      "kaggle_abalone_regression_example",
      "isolated_forest",
      "learning_rate",
      "random_forests",
      "momentum",
      "neural_network",
      "pycaret",
      "k-means",
      "ds_&_ml_portal",
      "optuna",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Hyperparameters are parameters set before training that control the learning process, such as: - the number of nodes in a [[Neural network]] - or k in [[K-nearest neighbours|KNN]]. The best ones are found with [[Hyperparameter Tuning]]. Also see: - [[Model Parameters]] - [[Model parameters vs hyperparameters]]"
  },
  "hypothesis_testing": {
    "title": "Hypothesis testing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Hypothesis testing.md",
    "tags": [
      "statistics"
    ],
    "aliases": null,
    "outlinks": [
      "p values",
      "inference",
      "Statistics",
      "Testing"
    ],
    "inlinks": [
      "statistical_tests",
      "statistics",
      "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes",
      "data_analyst",
      "testing",
      "statistical_assumptions",
      "t-test",
      "distributions",
      "data_analysis"
    ],
    "summary": "Used to draw inferences about population parameters based on sample data. The process involves the formulation of two competing hypotheses: the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$). Key Concepts Null Hypothesis ($H_0$): The hypothesis that there is no effect or no difference, which we seek to test. Alternative Hypothesis ($H_1$): The hypothesis that indicates the presence of an effect or a difference. P-value: A measure that helps determine the strength of the evidence against $H_0$. A small p-value (typically $< 0.05$) suggests that we reject $H_0$, indicating that the observed effect is statistically significant. Decision Making Accepting $H_0$:..."
  },
  "imbalanced_datasets": {
    "title": "Imbalanced Datasets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Imbalanced Datasets.md",
    "tags": [
      "data_quality",
      "data_cleaning",
      "data_exploration"
    ],
    "aliases": [
      "Class Imbalance"
    ],
    "outlinks": [
      "Classification",
      "Regression",
      "ML_Tools",
      "Imbalanced_Datasets_SMOTE.py",
      "SMOTE (Synthetic Minority Over-sampling Technique)",
      "Cost-Sensitive Analysis",
      "Bagging",
      "Random Forests",
      "Model Evaluation",
      "Evaluation Metrics",
      "imbalanced datasets",
      "Anomaly Detection",
      "anomaly detection",
      "Transfer Learning"
    ],
    "inlinks": [
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "data_selection_in_ml",
      "data_collection",
      "neural_network_classification",
      "imbalanced_datasets",
      "accuracy",
      "precision-recall_curve",
      "ds_&_ml_portal",
      "class_separability"
    ],
    "summary": "Handling imbalanced datasets to ensure robustness of models is a common challenge in machine learning, particularly in classification tasks where one class significantly outnumbers the other(s). In [[Classification]] tasks, an imbalanced dataset can lead to a model that ==performs well on the majority class but poorly on the minority class==. This is because the model may learn to predict the majority class more often due to its prevalence. For [[Regression]] tasks, handling outliers or data skewness might be necessary. In [[ML_Tools]] see: - [[Imbalanced_Datasets_SMOTE.py]] Examples Consider a scenario where you have an imbalanced dataset of resumes, with a majority of..."
  },
  "imbalanced_datasets_smote.py": {
    "title": "Imbalanced_Datasets_SMOTE.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Imbalanced_Datasets_SMOTE.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "SMOTE (Synthetic Minority Over-sampling Technique)|SMOTE",
      "Imbalanced Datasets|class imbalance",
      "Random Forests",
      "Logistic Regression",
      "Support Vector Machines|SVM",
      "recall",
      "accuracy"
    ],
    "inlinks": [
      "imbalanced_datasets"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Imbalanced_Datasets_SMOTE.py Demonstrating the Value of Resampling in Imbalanced Classification This example highlights the effectiveness of resampling techniques, such as [[SMOTE (Synthetic Minority Over-sampling Technique)|SMOTE]], in addressing [[Imbalanced Datasets|class imbalance]] issues in classification tasks. By implementing the following strategies, the setup ensures a measurable improvement in model performance: Severe Imbalance and Dataset Size: Utilizing a larger dataset with a severe imbalance ratio (e.g., 99:1) makes the impact of resampling more apparent. This imbalance necessitates resampling for the model to predict the minority class accurately. Choice of Classifier: Switching from robust classifiers like [[Random Forests]] to more sensitive ones like [[Logistic Regression]]..."
  },
  "immutable_vs_mutable": {
    "title": "Immutable vs mutable",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Immutable vs mutable.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Python"
    ],
    "inlinks": [
      "python"
    ],
    "summary": "[[Python]] list being mutable Side effect ```python def get_largest_numbers(numbers, n): numbers. sort() return numbers[-n:] nums [2, 3, 4, 1,34, 123, 321, 1] print(nums) largest = get_largest_numbers(nums, 2) print(nums) ```"
  },
  "impact_of_multicollinearity_on_model_parameters": {
    "title": "Impact of multicollinearity on model parameters",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Impact of multicollinearity on model parameters.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multicollinearity"
    ],
    "summary": "See https://youtu.be/StSAJIZuqws?t=655 ```R ) Monte Carlo Simulation: Multicollinearity & Harm results = expand_grid( rho = seq(0, 0.95, 0.05), rep = 1:1000 ) %>% mutate( sim = map(rho, function(p) { set.seed(runif(1, 1, 10000) %>% ceiling) R = matrix(c(1, p, p, 1), nrow = 2, ncol = 2, byrow = TRUE) Sigma = cor2cov(R, c(1, 1)) data = MASS :: mvrnorm(n= 30, mu = c(0, 0), Sigma = Sigma) %>% as_tibble %>% mutate( Y=1+0.5V1+0.5V2+ rnorm(30) ) model = 1m(Y ~ V1 + V2, data = data) summary(model) $coefficients %>% as_tibble }) ```"
  },
  "imperative": {
    "title": "What is imperative?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\imperative.md",
    "tags": [
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "An imperative pipeline tells ==how to proceed== at each step in a procedural manner. In contrast, a declarative data pipeline does not tell the order it needs to be executed but instead ==allows each step/task to find the best time and way to run.== The how should be taken care of by the tool, framework, or platform running on. For example, update an asset when upstream data has changed. Both approaches result in the same output. However, the declarative approach benefits from leveraging compile-time query planners and considering runtime statistics to choose the best way to compute and find patterns..."
  },
  "implementing_database_schema": {
    "title": "Implementing Database Schema",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Implementing Database Schema.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Many-to-Many Relationships",
      "ER Diagrams"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "To manage and create a database schema in SQLite, you can use the following commands: To view all commands used to create a database, execute: .schema To view the schema for a specific table, use: .schema table To run a schema from a file, use: .read schema.sql Creating a Database Schema When creating a database schema, follow these steps: Identify the Tables: Determine which tables are necessary for your data. Define Columns: Specify the columns for each table. Choose Data Types: Select appropriate data types for each column. Establish Keys: Define primary and foreign keys to maintain data integrity. Set..."
  },
  "in_ner_how_would_you_handle_ambiguous_entities": {
    "title": "In NER how would you handle ambiguous entities",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\In NER how would you handle ambiguous entities.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Handling ambiguous entities in Named Entity Recognition (NER) can be quite challenging. Here are some strategies that can be employed: Contextual Analysis: Utilize the surrounding ==context== of the ambiguous entity to determine its correct classification. For example, the word \"Apple\" could refer to the fruit or the company, but the context in which it appears can help disambiguate its meaning. Disambiguation Models: Implement additional models specifically designed for entity disambiguation. These models can leverage knowledge bases or ontologies to determine the most likely entity based on context. Multi-label Classification: Instead of forcing a single label, allow for multiple possible labels..."
  },
  "in-memory_format": {
    "title": "What is an In-Memory Format?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\in-memory format.md",
    "tags": [
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "The term \"in-memory format\" refers to the way data is stored and managed directly in a ==computer's RAM== (Random Access Memory) rather than on disk storage like a hard drive or SSD. This approach is used to optimize performance, as accessing data in RAM is significantly faster than accessing data on disk. In-memory formats are often used in applications that require high-speed data processing, such as real-time analytics, caching systems, and certain types of databases (e.g., in-memory databases like Redis or SAP HANA). By keeping data in memory, these systems can reduce latency and improve throughput, enabling faster data retrieval..."
  },
  "incremental_synchronization": {
    "title": "incremental synchronization",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\incremental synchronization.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "soft_deletion"
    ],
    "summary": ""
  },
  "industries_of_interest": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Industries of interest.md",
    "tags": [
      "career"
    ],
    "aliases": [],
    "outlinks": [
      "Energy",
      "Telecommunications",
      "Education and Training",
      "What algorithms or models are used within the energy sector",
      "What algorithms or models are used within the telecommunication sector",
      "Reinforcement learning",
      "Markov Decision Processes"
    ],
    "inlinks": [],
    "summary": "Industries to investigate related to my background & interests: - [[Energy]] - [[Telecommunications]] - [[Education and Training]] Both Reinforcement Learning and Explainable AI offer exciting opportunities for mathematicians to contribute significantly. Your deep mathematical understanding allows you to tackle complex problems, develop new methodologies, and provide theoretical foundations for emerging techniques. Exploratory Questions - [[What algorithms or models are used within the energy sector]] - [[What algorithms or models are used within the telecommunication sector]] [[Reinforcement learning]] Stochastic Processes: Your background will allow you to delve into the mathematical properties of [[Markov Decision Processes]] MDPs, optimizing transition dynamics, and improving..."
  },
  "inference_versus_prediction": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\inference versus prediction.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "inference",
      "Generative AI",
      "prediction",
      "LLM"
    ],
    "inlinks": [
      "inference"
    ],
    "summary": "[[inference]] is similar to prediction, ==but in the context of [[Generative AI]],== it is more specific to the application of a pre-trained model to ==produce an output from new input data==. While [[prediction]] often refers to tasks like classification or regression, inferencing in Gen AI refers to generating novel outputs, such as text, images, or audio, based on learned patterns. The key distinction is that in generative models, inferencing not only predicts but ==creates new data== (like text or images) rather than assigning categories or predicting numerical values, as in traditional machine learning models. For example: - In a language..."
  },
  "inference": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\inference.md",
    "tags": null,
    "aliases": [
      "inferencing"
    ],
    "outlinks": [
      "inference versus prediction"
    ],
    "inlinks": [
      "small_language_models",
      "hypothesis_testing",
      "inference_versus_prediction"
    ],
    "summary": "Inferencing involves prediction, but the output is more generative and creative in nature. [[inference versus prediction]]"
  },
  "information_theory": {
    "title": "information theory",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\information theory.md",
    "tags": [
      "math"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "language_model_output_optimisation",
      "mathematics"
    ],
    "summary": "Information theory is a mathematical framework for quantifying the transmission, processing, and storage of information. Information theory has profound implications and applications across various domains, providing the theoretical foundation for understanding and optimizing how information is communicated and processed. Entropy: Often referred to as Shannon entropy, it measures the average amount of uncertainty or surprise associated with random variables. In essence, it quantifies the amount of information contained in a message or dataset. Information: In information theory, ==information is defined as the reduction in uncertainty==. When you receive a message, the amount of information it provides is related to how..."
  },
  "input_is_not_properly_sanitized": {
    "title": "Input is Not Properly Sanitized",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Input is Not Properly Sanitized.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Input is Not Properly Sanitized When we say that ==\"input is not properly sanitized,\"== it means that the input data from users or external sources is not being adequately checked or cleaned before being processed by the application. Proper sanitization involves validating and filtering input to ensure it is safe and expected, preventing malicious data from causing harm. Without proper sanitization, applications can be vulnerable to various attacks, such as: Command Injection: Malicious commands can be executed on the server. SQL Injection: Malicious SQL queries can be executed against a database. Cross-Site Scripting (XSS): Malicious scripts can be injected into..."
  },
  "interoperable": {
    "title": "interoperable",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\interoperable.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "multi-level_index"
    ],
    "summary": ""
  },
  "interpretability": {
    "title": "Interpretability",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\interpretability.md",
    "tags": [
      "drafting",
      "model_explainability"
    ],
    "aliases": [
      "explainability",
      "interpretable"
    ],
    "outlinks": [],
    "inlinks": [
      "xgboost",
      "addressing_multicollinearity",
      "embedded_methods",
      "boosting",
      "feature_importance",
      "statistical_assumptions",
      "ds_&_ml_portal",
      "model_interpretability",
      "model_ensemble",
      "classification",
      "machine_learning_algorithms",
      "regression_metrics",
      "accessing_gen_ai_generated_content",
      "clustering",
      "regularisation",
      "principal_component_analysis",
      "pca_explained_variance_ratio",
      "feature_selection_vs_feature_importance",
      "model_observability",
      "how_llms_store_facts",
      "small_language_models",
      "graphrag",
      "feature_extraction"
    ],
    "summary": "Links Interpretability Importance https://christophm.github.io/interpretable-ml-book/index.html Interpretability Interpretability in machine learning (ML) is about understanding the reasoning behind a model's predictions. It involves making the model's decision-making process comprehensible to humans, which is crucial for trust, debugging, and ensuring fairness and reliability. Importance of Interpretability - Trust: Stakeholders are more likely to trust models they understand. - Debugging: Easier to identify and fix issues in interpretable models. - Bias Detection: Helps identify biases in data and model predictions. - Social Acceptance: Models that can explain their decisions are more socially acceptable. - Fairness and Reliability: Ensures models are fair and reliable, especially..."
  },
  "interpreting_logistic_regression_model_parameters": {
    "title": "Interpreting logistic regression model parameters",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Interpreting logistic regression model parameters.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Logistic Regression"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "How do this in terms of odds, probabilities ,odds ratio. [[Logistic Regression]]"
  },
  "interquartile_range_(iqr)_detection": {
    "title": "Interquartile Range (IQR) Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Interquartile Range (IQR) Detection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "univariate data"
    ],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_IQR.py Context: The IQR method is a robust and widely used statistical technique for identifying outliers, especially in [[univariate data]]. It is based on the distribution of data and is less sensitive to extreme values compared to methods reliant on mean and standard deviation. Steps: - Compute the IQR: - The IQR is the range within which the central 50% of the data lies. - Formula: $\\text{IQR} = Q3 - Q1$ where: - $Q1$: The first quartile (25th percentile) - $Q3$: The third quartile (75th percentile). Determine the bounds: Define lower and upper bounds to detect potential outliers: $\\text{Lower Bound}..."
  },
  "interview_notepad": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\interview notepad.md",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "Energy",
      "Normalised Schema",
      "Preprocessing",
      "Dimensionality Reduction",
      "Dimensionality Reduction",
      "Model Selection",
      "Linear Regression",
      "Linear Regression"
    ],
    "inlinks": [],
    "summary": "Tell about a recent project of yours;; Collaborating in the image matching kaggle competition. Obviously S2DS project too. What are some areas in this business you are interested in?;; Technical consulting, energy projects. Any area with the room to problem solve, to apply the scientific method to business problems. Areas where data can be turned into decisions. Building technical systems too for operations (feasibility projects). How do you approach prioritizing tasks in a data science project?;; Current project objectives, complexity, dependencies, and client impact. How do you handle conflicts or disagreements within a data science team?;; I promote open communication,..."
  },
  "ipynb": {
    "title": "ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ipynb.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Documentation & Meetings",
      "nbconvert"
    ],
    "inlinks": [],
    "summary": "Printing without code : [[Documentation & Meetings]]/ [[nbconvert]] https://stackoverflow.com/questions/49907455/hide-code-when-exporting-jupyter-notebook-to-html jupyter nbconvert stock_analysis.ipynb --no-input --to pdf jupyter nbconvert --to html --no-input --no-prompt phi_analysis.ipynb --clear -output jupyter nbconvert --to html --TemplateExporter.exclude_input=True Querying.ipynb"
  },
  "isolated_forest": {
    "title": "Isolation Forest and Its Use in Anomaly Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Isolated Forest.md",
    "tags": [
      "anomaly_detection",
      "data_quality"
    ],
    "aliases": [
      "iForest",
      "anomaly isolation"
    ],
    "outlinks": [
      "Model Ensemble",
      "standardised/Outliers|anomalies",
      "Hyperparameter",
      "DBSCAN",
      "Anomaly Detection with Clustering",
      "Random Forests",
      "Support Vector Machines|SVM"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "unsupervised_learning",
      "model_observability",
      "model_ensemble",
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "Isolation Forest (iForest) is an [[Model Ensemble]]-based method used for anomaly detection. It operates by isolating data points using a series of random binary splits. The key idea is that [[standardised/Outliers|anomalies]], being rare and different, are easier to isolate and thus require fewer splits. Mathematically, the isolation of a point is captured by the path length in a decision tree, where shorter paths indicate anomalies. The algorithm constructs multiple isolation trees, and the ==anomaly score of a point== is determined by the average path length across all trees. Isolation Forest is highly efficient for large datasets and is particularly useful..."
  },
  "java_vs_javascript": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Java vs JavaScript.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Java",
      "JavaScript",
      "Java",
      "JavaScript"
    ],
    "inlinks": [],
    "summary": "Difference Between [[Java]] and [[JavaScript]] Although their names are similar, Java and JavaScript are fundamentally different languages designed for different purposes. Below is a comparison between the two: | Aspect | Java | JavaScript | |----------------------|------------------------------------------------|--------------------------------------------------| | Type | Object-Oriented Programming Language | Scripting Language | | Use | General-purpose, used for desktop, mobile, and enterprise applications | Primarily used for web development (front-end and back-end) | | Execution | Runs on the Java Virtual Machine (JVM) | Runs in the browser or on server-side (Node.js) | | Compiled or Interpreted | Compiled to bytecode, then executed by the JVM..."
  },
  "javascript": {
    "title": "JavaScript",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\JavaScript.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "java_vs_javascript",
      "quartz",
      "react",
      "cryptography",
      "strongly_vs_weakly_typed_language"
    ],
    "summary": ""
  },
  "jinja_template": {
    "title": "What is a Jinja Template?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\jinja template.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Pasted image 20240922201606.png",
      "Pasted image 20240922202345.png",
      "Flask",
      "Quartz",
      "dbt",
      "Flask"
    ],
    "inlinks": [
      "quartz"
    ],
    "summary": "Resources LINK Practical jinja2 works with python 3. ![[Pasted image 20240922201606.png]] Renders templates with variable substitutions You can use tags too. ![[Pasted image 20240922202345.png]] Get gpt to generate example if necessary. can get a csv to export the data. context dictionaries are used can do html and flask. jinja used to manage web pages [[Flask]] makes me think about how [[Quartz]] is constructed. About Jinja is a fast, expressive, extensible templating engine. Special placeholders in the template allow writing code similar to Python syntax. Then the template is passed data to render the final document. Most popularized by [[dbt]]. Read..."
  },
  "johnson\u2013lindenstrauss_lemma": {
    "title": "Johnson\u2013Lindenstrauss lemma",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Johnson\u2013Lindenstrauss lemma.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "LLM"
    ],
    "inlinks": [
      "how_llms_store_facts",
      "mathematics"
    ],
    "summary": "Johnson\u2013Lindenstrauss lemma math https://youtu.be/9-Jl0dxWQs8?list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&t=1125 THe number of vectors that can be fit into a spaces grows exponentially. Useful for [[LLM]] in storing ideas. Plotting M>N almost orthogonal vectors in N-dim space Optimisation process that nudges then towards being perpendicular between 89-91 degrees ```python import torch import matplotlib.pyplot as plt from tqdm import tqdm List of vectors in some dimension, with many more vectors than there are dimensions num_vectors = 10000 vector_len = 100 big_matrix = torch.randn(num_vectors, vector_len) big_matrix /= big_matrix.norm(p=2, dim=1, keepdim=True) big_matrix.requires_grad_(True) Set up an optimization loop to create nearly-perpendicular vectors optimizer = torch.optim.Adam([big_matrix], lr=0.01) num_steps = 250 losses..."
  },
  "joining_datasets": {
    "title": "Joining Datasets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Joining Datasets.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DE_Tools",
      "SQL Joins"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "data_transformation"
    ],
    "summary": "Joining Datasets In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ```python Merge df1 = pd.DataFrame({'key': ['A', 'B'], 'value': [1, 2]}) df2 = pd.DataFrame({'key': ['A', 'B'], 'value': [3, 4]}) merged_df = pd.merge(df1, df2, on='key') Concat concat_df = pd.concat([df1, df2]) Join df1.set_index('key', inplace=True) df2.set_index('key', inplace=True) joined_df = df1.join(df2, lsuffix='_left', rsuffix='_right') ``` Merging datasets for completeness (also see [[SQL Joins]])."
  },
  "json_to_yaml": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Json to Yaml.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Json",
      "yaml"
    ],
    "inlinks": [],
    "summary": "[[Json]] [[yaml]] JSON { \"json\": [ \"rigid\", \"better for data interchange\" ], \"yaml\": [ \"slim and flexible\", \"better for configuration\" ], \"object\": { \"key\": \"value\", \"array\": [ { \"null_value\": null }, { \"boolean\": true }, { \"integer\": 1 }, { \"alias\": \"aliases are like variables\" }, { \"alias\": \"aliases are like variables\" } ] }, \"paragraph\": \"Blank lines denote\\nparagraph breaks\\n\", \"content\": \"Or we\\ncan auto\\nconvert line breaks\\nto save space\", \"alias\": { \"bar\": \"baz\" }, \"alias_reuse\": { \"bar\": \"baz\" } } ```YAML <- yaml supports comments, json does not did you know you can embed json in yaml? try uncommenting the next..."
  },
  "json": {
    "title": "Json",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Json.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "normalisation_of_data",
      "json_to_yaml",
      "semi-structured_data",
      "rest_api",
      "multi-level_index",
      "software_development_portal",
      "yaml",
      "pydantic",
      "structured_data"
    ],
    "summary": "Stands for javascript object notation records separated by commas keys & strings wrapped by double quotes good choice for data transport JSON data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet. The Raw JSON stored by Airbyte during ELT is an example of semi-structured data. This looks as follows: | | _airbyte_data| |---------| -----------| |Record 1| \\\"{'id': 1, 'name': 'Mary X'}\\\" | |Record 2| \\\"{'id': 2, 'name': 'John..."
  },
  "junction_tables": {
    "title": "Junction Tables",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Junction Tables.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": ""
  },
  "justfile": {
    "title": "Justfile",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Justfile.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "Justfile is a command runner designed to streamline workflows by allowing users to define simple, reusable commands for common tasks. This approach minimizes the cognitive load associated with memorizing long command sequences, enhancing productivity. The underlying concept is to create a more efficient command execution environment. By leveraging Justfile, users can automate repetitive tasks, thereby reducing the likelihood of errors and increasing consistency in execution. The implications for industry practices include improved efficiency in software development and operations, as well as the potential for better collaboration among team members through shared command definitions. Important Justfile enables the definition of reusable..."
  },
  "k-means": {
    "title": "K-means",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\K-means.md",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "Unsupervised Learning",
      "ML_Tools",
      "K_Means.py",
      "Hyperparameter",
      "WCSS and elbow method",
      "Pasted image 20241230200255.png"
    ],
    "inlinks": [
      "kmeans_vs_gmm",
      "model_parameters",
      "unsupervised_learning",
      "algorithms",
      "gaussian_mixture_models",
      "machine_learning_algorithms",
      "dbscan",
      "clustering",
      "why_and_when_is_feature_scaling_necessary"
    ],
    "summary": "K-means clustering is an [[Unsupervised Learning]] algorithm that partitions data into (k) clusters. Each data point is assigned to the cluster with the nearest centroid. The algorithm partitions a dataset into k clusters by assigning data points to the closest cluster mean. The means are updated iteratively until convergence is achieved. In [[ML_Tools]] see: [[K_Means.py]] Key Features Unsupervised Learning: K-means organizes unlabeled data into meaningful groups without prior knowledge of the categories. [[Hyperparameter]] k: The number of clusters must be specified beforehand. The optimal number of clusters can be determined using [[WCSS and elbow method]]. Algorithm Process: 1. Randomly choose..."
  },
  "k-nearest_neighbours": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\K-nearest neighbours.md",
    "tags": [
      "classifier"
    ],
    "aliases": [
      "KNN"
    ],
    "outlinks": [
      "classification",
      "regression",
      "Recommender systems",
      "parametric vs non-parametric models",
      "K-nearest neighbours"
    ],
    "inlinks": [
      "supervised_learning",
      "unsupervised_learning",
      "k-nearest_neighbours",
      "classification",
      "ds_&_ml_portal"
    ],
    "summary": "K-nearest Neighbors is a non-parametric method used for both [[classification]] and [[regression]] tasks. It classifies a sample by a majority vote of its neighbors, assigning the sample to the class most common among its (k) nearest neighbors, where (k) is a small positive integer. How It Works Classification: When a new data point needs classification, KNN identifies its (k) nearest neighbors in the training data based on feature similarity. The class label most common among these neighbors is assigned to the new data point. Regression: For regression tasks, KNN predicts the average value of the (k) nearest neighbors. Applications [[Recommender..."
  },
  "kaggle_abalone_regression_example": {
    "title": "Kaggle Abalone regression example",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Kaggle Abalone regression example.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Hyperparameter",
      "Hyperparameter",
      "Hyperparameter",
      "Cross Validation"
    ],
    "inlinks": [],
    "summary": "Task: For each model as we tune the hyperparameters what happens to the (RMSLE) metric (scatter metric against hyperparameter). Using Root Mean Squared Logarithmic Error RMSLE to evaluate. Practice with model and feature engineering ideas, create visualizations [ ] create eda for blog post [ ] create model training optuna for blog post Questions [[Hyperparameter]] [[Hyperparameter]] tuning can be done with [[Hyperparameter]] [[Cross Validation]] Both (sklearn)StratifiedKFold and RepeatedStratifiedKFold can be very effective when used on classification problems with a severe class imbalance. They both stratify the sampling by the class label; that is, they split the dataset in such a..."
  },
  "kernelling": {
    "title": "Kernelling",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Kernelling.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Kernelling",
      "Support Vector Machines|SVM"
    ],
    "inlinks": [
      "kernelling",
      "support_vector_machines"
    ],
    "summary": "[[Kernelling]] is a technique where the [[Support Vector Machines|SVM]] uses a kernel function to map the dataset into a higher-dimensional space, making it easier to identify separable clusters that may not be apparent in the original low-dimensional space. Kernel Trick: - When the data cannot be separated by a straight line or plane in its original (low-dimensional) space, SVM uses a technique called kernelling to project the data into a higher dimension where it becomes easier to separate. - The Kernel Trick allows the transformation of data into a higher dimension without explicitly computing the transformation. There are different types..."
  },
  "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)": {
    "title": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS).md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Web Map Tile Service (WMTS)",
      "Web Feature Server (WFS)"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS) Data Type: [[Web Map Tile Service (WMTS)]]: Serves image tiles (raster data). [[Web Feature Server (WFS)]]: Serves geographic features (vector data). Use Case: WMTS: Ideal for applications needing fast rendering of static maps, such as online map viewers. WFS: Suitable for applications requiring access to and manipulation of raw geographic data, such as spatial analysis and GIS applications. Performance: WMTS: High performance due to pre-rendered and cached tiles, optimized for rapid delivery. WFS: Performance depends on the complexity of the data and queries, typically slower than WMTS due..."
  },
  "kmeans_vs_gmm": {
    "title": "Kmeans vs GMM",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Kmeans vs GMM.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "k-Means",
      "Covariance Structures",
      "Distributions|distribution"
    ],
    "inlinks": [
      "gaussian_mixture_models"
    ],
    "summary": "Key Differences Between [[k-Means]] and GMM Cluster Shape k-Means: Assumes clusters are spherical and equidistant from their centroids. GMM: Models clusters as Gaussian distributions, allowing for different shapes (e.g., ellipses) by incorporating mean and covariance matrices. GMM can model clusters of varying shapes and sizes by adjusting the [[Covariance Structures]] (e.g., full, diagonal, spherical). Probability-Based Assignments k-Means: Assigns each point deterministically to the nearest cluster centroid. GMM: Provides a probability [[Distributions|distribution]] for cluster membership, making it a soft clustering method. GMM handles overlapping clusters effectively by assigning probabilities to data points for each cluster, instead of enforcing hard boundaries like..."
  },
  "knowledge_graph_vs_rag_setup": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Knowledge graph vs RAG setup.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Knowledge Graph",
      "RAG",
      "semantic search",
      "Generative"
    ],
    "inlinks": [],
    "summary": "Comparison: Knowledge Graph vs. RAG Setup ==Knowledge Graphs are structured representations of entities and their relationships, designed primarily for querying, reasoning, and storing factual information.== ==RAG setups enhance generative models by retrieving external knowledge (from unstructured or semi-structured data) and integrating it into the generation process.== While not the same, these two concepts can be used together to build systems that combine structured knowledge retrieval with the natural language generation capabilities of RAG models. While knowledge graphs and RAG are distinct, they can be integrated to improve certain systems: - ==A RAG model could use a knowledge graph as the..."
  },
  "knowledge_graph": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Knowledge Graph.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "explainability",
      "Pasted image 20240921154214.png|600"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "knowledge_graphs_with_obsidian",
      "graphrag",
      "knowledge_graph_vs_rag_setup",
      "accessing_gen_ai_generated_content"
    ],
    "summary": "[!Summary] Knowledge graphs (KGs) enable large language models (LLMs) to generate more accurate, trustworthy AI outputs. Neo4j is leader in this space and make use of KG through such as generative AI techniques like GraphRAG. - Knowledge graphs are critical for managing complex data relationships and making strategic AI-driven decisions. - The combination of KGs and LLMs improves AI accuracy, diversity of viewpoints, and [[explainability]]. A knowledge graph is a structured representation of knowledge that captures entities (e.g., people, places, concepts) and the relationships between them. Knowledge graphs are often used to represent and store factual information in a way..."
  },
  "knowledge_graphs_with_obsidian": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Knowledge Graphs with Obsidian.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "RAG",
      "LLM",
      "LLM",
      "LLM",
      "Knowledge Graph"
    ],
    "inlinks": [],
    "summary": "[!Summary] Llama Index is a python package that can be used to create Knowledge graphs (KGs). There exists a method to integrate with Obsidian. This is a tutorial on how to set this up a [[RAG]] system with obsidian. Visualisation is done using the pyvis library. Requires API to [[LLM]]. [!important] - RAG improves [[LLM]] performance by utilizing external databases. - Llama Index facilitates the transformation of Obsidian notes into a structured Knowledge Graph. - The tutorial includes steps for setup, dependencies, and visualization. [!attention] - [[LLM]]s can still produce errors known as \"hallucinations.\" - Requires familiarity with Python and..."
  },
  "knowledge_work": {
    "title": "Knowledge Work",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Knowledge Work.md",
    "tags": [
      "career"
    ],
    "aliases": null,
    "outlinks": [
      "Scientific Method"
    ],
    "inlinks": [
      "thinking_systems"
    ],
    "summary": "Knowledge work refers to tasks that primarily involve handling or using information and require cognitive skills rather than manual labor. It is characterized by problem-solving, critical thinking, and the application of specialized knowledge. Key Characteristics Problem Solving: Knowledge work often involves identifying, analyzing, and solving complex problems. This requires creativity, analytical skills, and the ability to synthesize information from various sources. Use of the [[Scientific Method]]: Many knowledge work tasks, especially in research and development, rely on the scientific method. This involves forming hypotheses, conducting experiments, analyzing data, and drawing conclusions. Information Management: Knowledge workers must efficiently gather, process, and..."
  },
  "kubernetes": {
    "title": "What is Kubernetes?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\kubernetes.md",
    "tags": [
      "data_orchestration",
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "google_cloud_platform",
      "distributed_computing"
    ],
    "summary": "It\u2019s a platform that allows you to run and orchestrate container workloads. Kubernetes has become the de-facto standard for your cloud-native apps to (auto-) scale-out and deploy your open-source zoo fast, cloud-provider-independent. No lock-in here. Kubernetes is the move from infrastructure as code towards infrastructure as data, specifically as YAML. With Kubernetes, developers can quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down."
  },
  "k_means.py": {
    "title": "K_Means.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\K_Means.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "k-means"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/KMeans/K_Means.py Key Concepts Used in the Script Data Loading: The script reads data from a CSV file (penguins.csv) and uses a sample dataset with random features for demonstration purposes. Data Preprocessing: Standardization: Features are standardized using sklearn.preprocessing.scale and StandardScaler to ensure that all features contribute equally to the clustering process. Feature Selection: Specific features, such as bill_length_mm and bill_depth_mm, are selected for clustering. K-Means Clustering: The core clustering algorithm is applied with n_clusters=3. Outputs include cluster centroids and labels for each data point. Visualization: Scatter plots are used to display the clustering results, highlighting the cluster centroids. Evaluation of Optimal..."
  },
  "label_encoding": {
    "title": "Label encoding",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Label encoding.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": ""
  },
  "labelling_data": {
    "title": "Labelling data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Labelling data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Possible missing labelling or bias in the data, or under-represented data. Construction of the data set comes from the group collecting it. Examples: - ImageNet"
  },
  "lambda_architecture": {
    "title": "What is a Lambda Architecture?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\lambda architecture.md",
    "tags": [
      "data_modeling",
      "data_orchestration"
    ],
    "aliases": [],
    "outlinks": [
      "Data Streaming",
      "design pattern",
      "Batch Processing"
    ],
    "inlinks": [],
    "summary": "Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. [[Data Streaming]] This approach to architecture attempts to balance ==latency, throughput, and fault tolerance== using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before the presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of MapReduce. Lambda architecture is a [[design pattern]]..."
  },
  "langchain": {
    "title": "Langchain",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Langchain.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Python",
      "LLM",
      "Pandas Dataframe Agent",
      "Agentic Solutions"
    ],
    "inlinks": [
      "vector_database"
    ],
    "summary": "[[Python]] framework For building apps with [[LLM]] and interaction with them and combining models. Its end to end, through composability Example: [[Pandas Dataframe Agent]] Modules models interface prompts chains sequences of calls too a LLM Memory Indexes Agents and tools set up [[Agentic Solutions]]"
  },
  "language_model_output_optimisation": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Language Model Output Optimisation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "information theory",
      "Language Models",
      "Cross Entropy",
      "Attention Mechanism"
    ],
    "inlinks": [],
    "summary": "What techniques from [[information theory]] can be used to measure and optimize the amount of information conveyed by an language model? In information theory, several techniques can be applied to measure and optimize the amount of information conveyed by an [[Language Models]]. Entropy: Entropy measures the uncertainty or unpredictability of a random variable. In the context of language models, it can be used to quantify the uncertainty in predicting the next word in a sequence. Lower entropy indicates more predictable and informative outputs. [[Cross Entropy]]: This measures the difference between two probability distributions. For language models, cross-entropy can be used..."
  },
  "language_models_large_(llms)_vs_small_(slms)": {
    "title": "Language Models Large (LLMs) vs Small (SLMs)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Language Models Large (LLMs) vs Small (SLMs).md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "LLM",
      "SLM"
    ],
    "inlinks": [],
    "summary": "Overview Language models can be categorized into large language models ([[LLM]]) and small language models ([[SLM]]). While LLMs boast extensive general-purpose knowledge and capabilities, SLMs offer distinct advantages in certain scenarios, particularly when it comes to efficiency, resource constraints, and task-specific environments. Key Differences | Aspect | LLMs | SLMs | |--------------------|---------------------------------------------------|------------------------------------------------------| | Accuracy | Higher accuracy across broad tasks due to large datasets and extensive training. | Comparable performance in domain-specific tasks after fine-tuning. | | Efficiency | Computationally expensive; requires significant resources for training and inference. | More resource-efficient; suited for edge devices and real-time applications. | |..."
  },
  "language_models": {
    "title": "Language Models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Language Models.md",
    "tags": [
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "LLM",
      "Small Language Models|SLM"
    ],
    "inlinks": [
      "memory",
      "vector_embedding",
      "llm",
      "explain_the_curse_of_dimensionality",
      "small_language_models",
      "language_model_output_optimisation"
    ],
    "summary": "A language model is a machine learning model that is designed to understand, generate, and predict human language. It does this by analyzing large amounts of text data to learn the patterns, structures, and relationships between words and phrases. They work by assigning probabilities to sequences of words, allowing them to predict the next word in a sentence or generate coherent text based on a given prompt. Related to: [[LLM]] [[Small Language Models|SLM]]"
  },
  "lasso": {
    "title": "Lasso",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Lasso.md",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "Feature Selection"
    ],
    "inlinks": [
      "regularisation",
      "regression",
      "embedded_methods",
      "elastic_net"
    ],
    "summary": "L1 Regularization (Lasso Regression): In L1 regularization, a penalty proportional to the absolute value of the coefficients is added to the loss function. The L1 penalty tends to shrink some coefficients to exactly zero, effectively performing feature selection by eliminating irrelevant features. Lasso regression (Least Absolute Shrinkage and Selection Operator) is an example of a model that uses L1 regularization. The L1-regularized loss function is: $\\text{Loss} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} |w_i|$ where $\\lambda$ is the regularization parameter, $w_i$ are the model weights, and MSE is the mean squared error. For Lasso Regression (L1): ```python from sklearn.linear_model import Lasso Initialize..."
  },
  "latency": {
    "title": "Latency",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Latency.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "performance_dimensions",
      "distributed_computing",
      "data_ingestion"
    ],
    "summary": ""
  },
  "lbfgs": {
    "title": "LBFGS",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LBFGS.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Optimisation function",
      "logistic regression",
      "model parameters",
      "cost function",
      "Sklearn"
    ],
    "inlinks": [],
    "summary": "LBFGS stands for Limited-memory Broyden-Fletcher-Goldfarb-Shanno, which is an [[Optimisation function]]optimization algorithm used to find the minimum of a function. In the context of [[logistic regression]], LBFGS is a method for optimizing the cost function to find the optimal [[model parameters]] (such as the intercept and coefficients). Here's a breakdown of the key features of LBFGS: Quasi-Newton Method: LBFGS is a type of Quasi-Newton method, which approximates the inverse of the Hessian matrix (second-order derivatives of the cost function). Instead of computing the full Hessian matrix, it uses an approximation, which makes it more efficient for large datasets. Limited Memory: The..."
  },
  "learning_rate": {
    "title": "learning rate",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\learning rate.md",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "Hyperparameter",
      "Gradient Descent",
      "Loss function",
      "standardised/Optuna",
      "Adam Optimizer",
      "Gradient Descent",
      "Pasted image 20241216204925.png"
    ],
    "inlinks": [
      "xgboost",
      "weak_learners",
      "adaptive_learning_rates",
      "fitting_weights_and_biases_of_a_neural_network",
      "z-normalisation",
      "gradient_boosting",
      "adam_optimizer",
      "gradient_descent",
      "momentum",
      "linear_regression",
      "optimisation_techniques",
      "ds_&_ml_portal",
      "model_parameters_vs_hyperparameters"
    ],
    "summary": "Description The learning rate is a [[Hyperparameter]] in machine learning that ==determines the step size at which a model's parameters are updated during training==. It plays a significant role in the optimization process, particularly in algorithms like [[Gradient Descent]] which are used to minimize the [[Loss function]]. Key Points about Learning Rate: Parameter Updates: During training, the model's parameters (such as weights and biases in neural networks) are adjusted iteratively to minimize the loss function. The learning rate controls how much the parameters are changed in response to the estimated error each time the model weights are updated. Impact on..."
  },
  "learning_styles": {
    "title": "Learning Styles",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Learning Styles.md",
    "tags": [
      "model_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "continuous",
      "categorical",
      " Pasted image 20240112101344.png|500",
      "Unsupervised Learning",
      "Regression",
      "Classification",
      "Unsupervised Learning",
      "Dimensionality Reduction",
      "Clustering"
    ],
    "inlinks": [],
    "summary": "What does the data look like [[continuous]] or [[categorical]]? ![[ Pasted image 20240112101344.png|500]] [[Unsupervised Learning]] [[Regression]] [[Classification]] [[Unsupervised Learning]] [[Dimensionality Reduction]] [[Clustering]]"
  },
  "lemmatization": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\lemmatization.md",
    "tags": [
      "NLP"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "nlp",
      "normalisation_of_text"
    ],
    "summary": "Lemmatization is the process of ==reducing a word to its base or root== form, known as the \"lemma.\" Unlike stemming, which simply cuts off word endings, lemmatization considers the context and morphological analysis of the words. It ensures that the root word is a valid word in the language. ==For example, the words \"running,\" \"ran,\" and \"runs\" would all be lemmatized to \"run.\"== This process helps in normalizing text data for natural language processing tasks by grouping together different forms of a word."
  },
  "lightgbm_vs_xgboost_vs_catboost": {
    "title": "LightGBM vs XGBoost vs CatBoost",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LightGBM vs XGBoost vs CatBoost.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Gradient Boosting",
      "LightGBM",
      "XGBoost",
      "CatBoost",
      "Regularisation"
    ],
    "inlinks": [],
    "summary": "This table summarizes the key differences and strengths of each [[Gradient Boosting]] framework. | Feature/Aspect | [[LightGBM]] (LGBM) | [[XGBoost]] | [[CatBoost]] | | --------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | | Tree Growth Strategy | Leaf-wise growth, leading to deeper trees and potentially better accuracy. | Level-wise growth, resulting in more balanced trees. | Ordered boosting, reducing overfitting and improving generalization. | | Speed and Memory | High speed and low memory usage, especially with large datasets. | Balanced speed and accuracy with robust regularization options. | Competitive performance with minimal hyperparameter tuning. | | Handling Categorical Features..."
  },
  "lightgbm": {
    "title": "LightGBM",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LightGBM.md",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "LGBM"
    ],
    "outlinks": [
      "Gradient Descent"
    ],
    "inlinks": [
      "time_series_forecasting",
      "gradient_boosting",
      "optuna",
      "lightgbm_vs_xgboost_vs_catboost"
    ],
    "summary": "LightGBM is a gradient boosting framework that is designed for speed and efficiency. It is particularly well-suited for handling large datasets and high-dimensional data. Tree Growth: Splits the tree leaf-wise, which can lead to faster convergence compared to level-wise growth. Learning Rate: Similar to [[Gradient Descent]], LightGBM uses a learning rate to control the contribution of each tree. DART: A variant of LightGBM known for its performance. Parameter Definition: Requires parameters to be defined in a dictionary for model configuration. Watch Video Explanation Key Parameters Learning Rate: Controls the step size at each iteration while moving toward a minimum of..."
  },
  "linear_discriminant_analysis": {
    "title": "Linear Discriminant Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Linear Discriminant Analysis.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dimensionality_reduction",
      "feature_scaling"
    ],
    "summary": ""
  },
  "linear_regression": {
    "title": "Linear Regression",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Linear Regression.md",
    "tags": [
      "regressor"
    ],
    "aliases": [],
    "outlinks": [
      "linearity",
      "Loss function",
      "R squared",
      "Loss function",
      "Ordinary Least Squares",
      "Gradient Descent",
      "learning rate",
      "Model Evaluation",
      "R squared",
      "Adjusted R squared",
      "F-statistic",
      "Feature Selection",
      "p-values in linear regression in sklearn",
      "Pasted image 20240117145455 1.png|500",
      "Pasted image 20240124135607.png|500"
    ],
    "inlinks": [
      "supervised_learning",
      "model_parameters",
      "pytorch",
      "regression",
      "general_linear_regression",
      "outliers",
      "ridge",
      "maximum_likelihood_estimation",
      "p-values_in_linear_regression_in_sklearn",
      "logistic_regression",
      "machine_learning_algorithms",
      "interview_notepad",
      "ds_&_ml_portal"
    ],
    "summary": "Description Linear regression assumes [[linearity]] between the input features and the target variable. Assumes that the relationship between the independent variable(s) and the dependent variable is linear. During the training phase, the algorithm adjusts the slope (m) and the intercept (b) of the line to minimize the [[Loss function]]. The linear regression model is represented as: $y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n$ $y$ is the dependent variable (the variable we want to predict). $x_1, x_2, \\ldots, x_n$ are the independent variables (features or predictors). $b_0, b_1, b_2, \\ldots, b_n$ are the coefficients (weights) associated with..."
  },
  "linked_list": {
    "title": "Linked List",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Linked List.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A linked list is a linear data structure in which elements (called nodes) are linked together using pointers. Unlike arrays, linked lists do not store elements in contiguous memory locations; instead, each node contains: Data \u2013 The actual value stored in the node. Pointer (or Reference) \u2013 A reference to the next node in the sequence. Types of Linked Lists: Singly Linked List \u2013 Each node has a pointer to the next node only. Doubly Linked List \u2013 Each node has pointers to both the previous and next nodes. Circular Linked List \u2013 The last node points back to the..."
  },
  "llm_evaluation_metrics": {
    "title": "LLM Evaluation Metrics",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LLM Evaluation Metrics.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "LLM Evaluation Metrics",
      "LLM"
    ],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "llm_evaluation_metrics"
    ],
    "summary": "[[LLM Evaluation Metrics]] - BLEU, - ROUGE, - perplexity which quantify the similarity between generated text and reference outputs. [[LLM]]"
  },
  "llm": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LLM.md",
    "tags": [
      "language_models"
    ],
    "aliases": [
      "LLMs",
      "Large Language Models"
    ],
    "outlinks": [
      "How do we evaluate of LLM Outputs",
      "Memory|What is LLM memory",
      "Relationships in memory|Managing LLM memory",
      "Mixture of Experts",
      "Distillation",
      "Attention mechanism",
      "Reinforcement learning",
      "Chain of thought",
      "Language Models",
      "standardised/Vector Embedding|word embedding",
      "Transformer",
      "Transfer Learning"
    ],
    "inlinks": [
      "langchain",
      "attention_mechanism",
      "comparing_llm",
      "language_models",
      "how_do_we_evaluate_of_llm_outputs",
      "language_models_large_(llms)_vs_small_(slms)",
      "deepseek",
      "vector_database",
      "generative_ai_from_theory_to_practice",
      "knowledge_graphs_with_obsidian",
      "evaluating_language_models",
      "deep_learning",
      "scaling_agentic_systems",
      "johnson\u2013lindenstrauss_lemma",
      "llm_evaluation_metrics",
      "how_llms_store_facts",
      "rag",
      "inference_versus_prediction",
      "feature_extraction"
    ],
    "summary": "A Large Language Model (LLM) is a type of language model designed for language understanding and generation. They can perform a variety of tasks, including: Text generation Machine translation Summary writing Image generation from text Machine coding Chatbots or Conversational AI Questions [[How do we evaluate of LLM Outputs]] [[Memory|What is LLM memory]] [[Relationships in memory|Managing LLM memory]] [[Mixture of Experts]]: having multiple experts instead of one big model. [[Distillation]] Mathematics on the parameter usage [[Attention mechanism]] Use of [[Reinforcement learning]] in training [[Chain of thought]] methods in LLM's (deepseek) How do Large Language Models (LLMs) Work? Large [[Language Models]]..."
  },
  "load_balancing": {
    "title": "Load Balancing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Load Balancing.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Load balancing is a technique used to distribute incoming network traffic across multiple servers. This helps ensure both reliability and performance by preventing any single server from becoming overwhelmed with too much traffic."
  },
  "local_interpretable_model-agnostic_explanations": {
    "title": "Local Interpretable Model-agnostic Explanations",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Local Interpretable Model-agnostic Explanations.md",
    "tags": [],
    "aliases": [
      "LIME"
    ],
    "outlinks": [],
    "inlinks": [
      "model_interpretability",
      "feature_importance"
    ],
    "summary": "LIME explains individual predictions ==by approximating the model locally== with an interpretable model and calculating the feature importance based on the surrogate model. Key Points Purpose: LIME focuses on explaining individual predictions by approximating the model locally using a simpler, interpretable model (like linear regression). How it Works: For a given prediction, LIME generates perturbed samples (e.g., by modifying input features). It observes how the predictions change, thus inferring feature importance for that specific instance. Use Cases: Useful for understanding why a specific decision was made in complex black-box models. Advantage: LIME can work with any model type. It is..."
  },
  "logical_model": {
    "title": "Logical Model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Logical Model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Logical Model - Customer: CustomerID, Name, Email - Order: OrderID, OrderDate, CustomerID - Book: BookID, Title, Author - Order-Book Relationship: OrderID, BookID Logical Model - Details the attributes of each data entity. - Specifies relationships without depending on a specific database management system."
  },
  "logistic_regression_does_not_predict_probabilities": {
    "title": "Logistic Regression does not predict probabilities",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Logistic Regression does not predict probabilities.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "What is the difference between odds and probability"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "In logistic regression, the model predicts the ==odds of an event happening rather than directly predicting probabilities.== The odds are defined as: $$ \\text{Odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{p}{1-p} $$ where $p$ is the probability of success. The log-odds (or logit function) is the natural logarithm of the odds: $$ \\text{Log-Odds} = \\ln\\left(\\frac{p}{1-p}\\right) = b_0 + b_1 x $$ This ==transformation makes the relationship between the independent variables and the dependent variable linear,== allowing logistic regression to estimate the parameters $b_0$ and $b_1$. Resources: - Explanation of log odds - Explanation of log odd function [[What is the difference between odds..."
  },
  "logistic_regression_in_sklearn_&_gradient_descent": {
    "title": "Logistic regression in sklearn & Gradient Descent",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Logistic regression in sklearn & Gradient Descent.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Gradient Descent",
      "Optimisation function"
    ],
    "inlinks": [],
    "summary": "Logistic regression in sklearn & Gradient Descent sklearn's Logistic Regression implementation does not use [[Gradient Descent]] by default. Instead, it uses more sophisticated optimization techniques depending on the solver specified. These solvers are more efficient and robust for finding the optimal parameters for logistic regression. Here's a summary: [[Optimisation function]]: Solvers in sklearn's Logistic Regression** lbfgs (default in many cases): Stands for Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno. It's a quasi-Newton method, which approximates the second derivatives (Hessian matrix) to find the minimum of the cost function efficiently. liblinear: Uses the coordinate descent method for optimization. Ideal for small datasets or when penalty='l1' is..."
  },
  "logistic_regression_statsmodel_summary_table": {
    "title": "Logistic Regression Statsmodel Summary table",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Logistic Regression Statsmodel Summary table.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Sklearn",
      "MLE",
      "Feature Selection",
      "Pasted image 20240124095916.png"
    ],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "Statsmodel has this summary table unlike [[Sklearn]] Explanation of summary The dependent variable is 'duration'. The model used is a Logit regression (logistic in common lingo), while the method - Maximum Likelihood Estimation ([[MLE]]). It has clearly converged after classifying 518 observations. - The Pseudo R-squared is 0.21 which is within the 'acceptable region'. - The duration variable is significant and its coefficient is 0.0051. - The constant is also significant and equals: -1.70 (p value close to 0) - High p value, suggests to remove from model, drop one by one, ie [[Feature Selection]]. Specifically a graph such as,..."
  },
  "logistic_regression": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Logistic Regression.md",
    "tags": [
      "classifier",
      "regressor"
    ],
    "aliases": null,
    "outlinks": [
      "Regression",
      "Binary Classification",
      "Logistic Regression Statsmodel Summary table",
      "Logistic Regression does not predict probabilities",
      "Interpreting logistic regression model parameters",
      "Model Evaluation",
      "ML_Tools",
      "Regression_Logistic_Metrics.ipynb",
      "Support Vector Machines",
      "Binary Classification",
      "Linear Regression",
      "Model Evaluation",
      "Confusion Matrix",
      "Model Parameters"
    ],
    "inlinks": [
      "lbfgs",
      "model_parameters",
      "regression",
      "ridge",
      "statistics",
      "machine_learning_algorithms",
      "interpreting_logistic_regression_model_parameters",
      "roc_(receiver_operating_characteristic)",
      "statistical_assumptions",
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "imbalanced_datasets_smote.py"
    ],
    "summary": "==Logistic regression models the log-odds of the probability as a linear function of the input features.== It models the probability of an input belonging to a particular class using a logistic (sigmoid) function. The model establishes a decision boundary (threshold) in the feature space and leverages the concept of odds and log-odds to make predictions. Logistic regression is best suited for cases where the decision boundary is approximately linear in the feature space. Logistic [[Regression]] can be used for [[Binary Classification]]tasks, where the goal is to predict one of two possible outcomes (e.g., \"Yes\" or \"No\"). Related Notes: [[Logistic Regression..."
  },
  "looker_studio": {
    "title": "Looker Studio",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Looker Studio.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Google",
      "PowerBI",
      "standardised/GSheets|GSheets",
      "PostgreSQL"
    ],
    "inlinks": [
      "data_visualisation"
    ],
    "summary": "Looker studio is [[Google]] version of [[PowerBI]], but its free. Connectors to data Can connect to data sources i.e: - [[standardised/GSheets|GSheets]] - [[PostgreSQL]] Data Modelling Data models are called Blends Dashboard with Relational Database in Looker Studio Data Blending and Modeling"
  },
  "loss_function": {
    "title": "Loss function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Loss function.md",
    "tags": [
      "deep_learning",
      "model_architecture",
      "ml_optimisation"
    ],
    "aliases": [],
    "outlinks": [
      "cost function",
      "model evaluation",
      "model parameters",
      "Mean Squared Error",
      "regression",
      "Cross Entropy",
      "classification",
      "Loss versus Cost function"
    ],
    "inlinks": [
      "xgboost",
      "pytorch",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy",
      "test_loss_when_evaluating_models",
      "embedded_methods",
      "cross_entropy",
      "ds_&_ml_portal",
      "cost_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "loss_versus_cost_function",
      "typical_output_formats_in_neural_networks",
      "learning_rate",
      "neural_scaling_laws",
      "ridge",
      "gradient_boosting",
      "regularisation",
      "optimisation_function",
      "linear_regression",
      "model_parameters_vs_hyperparameters",
      "gradient_descent",
      "optimising_a_logistic_regression_model",
      "feed_forward_neural_network"
    ],
    "summary": "Loss functions are used in training machine learning models. Also known as a [[cost function]], error function, or objective function. Serves as a metric for [[model evaluation]]. Purpose: ==Measure predictive accuracy==: Measures the difference between predicted and actual values. That is they measure how well a model's predictions match the actual target values by quantifying the error between the predicted output and the true output. Goal: ==To be minimized==: The primary goal during model training is to minimize this loss, improving accuracy of predictions on unseen data. Used during training to adjust [[model parameters]] and during evaluation to assess model..."
  },
  "loss_versus_cost_function": {
    "title": "Loss versus Cost function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Loss versus Cost function.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Loss function",
      "Cost Function",
      "Model Optimisation|Optimisation",
      "Model parameters vs hyperparameters"
    ],
    "inlinks": [
      "cost_function",
      "loss_function"
    ],
    "summary": "In machine learning, the terms \"loss function\" and \"cost function\" are often used interchangeably, but they can have slightly different meanings depending on the context: [[Loss function]]: This typically refers to the function that measures the error for a single training example. It quantifies how well or poorly the model is performing on that specific example - data point. Common examples include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks. [[Cost Function]]: This is generally used to refer to the average of the loss function over the entire training dataset. It provides a measure of..."
  },
  "lstm": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\LSTM.md",
    "tags": [
      "deep_learning",
      "time_series",
      "code_snippet",
      "drafting"
    ],
    "aliases": [
      "LSTM vs. Transformer",
      "RNN vs. Transformer"
    ],
    "outlinks": [
      "vanishing and exploding gradients problem",
      "Recurrent Neural Networks",
      "Pasted image 20241015211424.png|500",
      "RNN",
      "Transformer",
      "NLP",
      "Attention mechanism",
      "BERT",
      "GRU",
      "Keras",
      "LSTM",
      "LSTM",
      "PyTorch"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "recurrent_neural_networks",
      "transformers_vs_rnns",
      "ai_engineer",
      "attention_mechanism",
      "lstm",
      "ds_&_ml_portal",
      "named_entity_recognition"
    ],
    "summary": "What is LSTM LSTM (Long Short-Term Memory) networks are a specialized type of Recurrent Neural Network (RNN) designed to overcome the [[vanishing and exploding gradients problem]] that affects traditional [[Recurrent Neural Networks]]. LSTMs address this challenge through their unique architecture. Used for tasks that require the retention of information over time, and problems involving ==sequential data.== The key strength of LSTMs is their ability to manage ==long-term dependencies== using their ==gating mechanisms==. Key Components of LSTM Networks: Resources: Understanding LSTM Networks $x_t$ input, $h_t$ output, cell state $C_t$, conveyer belt ![[Pasted image 20241015211424.png|500]] Memory Cell: - The core of an..."
  },
  "machine_learning_algorithms": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Machine Learning Algorithms.md",
    "tags": [
      "ml_process",
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "Algorithms",
      "Supervised Learning",
      "Classification",
      "Logistic Regression",
      "Support Vector Machines",
      "Naive Bayes",
      "Decision Tree",
      "Random Forests",
      "Regression",
      "Linear Regression",
      "Support Vector Regression",
      "Random Forest Regression",
      "Unsupervised Learning",
      "Clustering",
      "K-means",
      "Gaussian Mixture Models",
      "Clustering",
      "Dimensionality Reduction",
      "Dimensionality Reduction",
      "Principal Component Analysis",
      "Manifold Learning",
      "Scalability",
      "Data Quality",
      "Overfitting",
      "Bias and variance",
      "Interpretability"
    ],
    "inlinks": [
      "pandas_stack",
      "supervised_learning",
      "z-normalisation",
      "gradient_boosting",
      "model_building",
      "machine_learning",
      "use_of_rnns_in_energy_sector",
      "ds_&_ml_portal"
    ],
    "summary": "Machine learning [[Algorithms]] are used to automate tasks, extract insights, and make more informed decisions. Choosing the right algorithm for a specific problem involves understanding the task, the characteristics of the data, and the strengths and limitations of different algorithms. [[Supervised Learning]] Common [[Classification]] algorithms include: [[Logistic Regression]] [[Support Vector Machines]] [[Naive Bayes]] [[Decision Tree]] [[Random Forests]] Common [[Regression]] algorithms include: - [[Linear Regression]] - [[Support Vector Regression]] - [[Random Forest Regression]] [[Unsupervised Learning]] Common [[Clustering]] algorithms include: [[K-means]] [[Gaussian Mixture Models]] [[Clustering]] [[Dimensionality Reduction]] Common [[Dimensionality Reduction]] algorithms include: [[Principal Component Analysis]] [[Manifold Learning]] Strengths and Limitations of Machine..."
  },
  "machine_learning_operations": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Machine Learning Operations.md",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "MLOPs"
    ],
    "outlinks": [
      "DevOps",
      "DS & ML Portal",
      "Model Observability"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Machine Learning Operations (MLOps) is a set of practices and tools designed to streamline the entire lifecycle of machine learning models, from development to deployment and maintenance. It aims to integrate machine learning with [[DevOps]] principles to ensure that models are reliable, scalable, and efficient in production environments. Development: MLOps focuses on creating a seamless workflow for developing machine learning models. This includes data preprocessing, feature engineering, model building, and training. The goal is to ensure that models can be developed quickly and efficiently. See [[DS & ML Portal]]. Deployment: Once a model is developed and evaluated, MLOps facilitates its..."
  },
  "machine_learning": {
    "title": "What is Machine Learning?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Machine Learning.md",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "Machine Learning Algorithms"
    ],
    "inlinks": [
      "tensorflow",
      "data_ingestion"
    ],
    "summary": "Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning [[Machine Learning Algorithms]] use historical data as input to predict new output values."
  },
  "maintainable_code": {
    "title": "Maintainable Code",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Maintainable Code.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pydantic",
      "Pyright",
      "Testing"
    ],
    "inlinks": [
      "pyright"
    ],
    "summary": "[[Pydantic]] : runtine analysis [[Pyright]]: static analysis [[Testing]] Want robust and reliable Python applications."
  },
  "makefile": {
    "title": "Makefile",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Makefile.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "software_development_portal"
    ],
    "summary": "A Makefile is a special file used by the make build automation tool to manage the build process of a project. It defines a set of tasks to be executed, typically to compile and link a program. Here are some key functions of a Makefile: Compilation Instructions: It specifies how to compile and link the program. This includes defining the source files, the compiler to use, and any necessary flags or options. Dependencies: Makefiles list dependencies between files, ensuring that changes in source files trigger recompilation of only the necessary parts of the program. Automation: It automates repetitive tasks, such..."
  },
  "manifold_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Manifold Learning.md",
    "tags": [
      "deleted",
      "data_exploration"
    ],
    "aliases": null,
    "outlinks": [
      "Dimensionality Reduction",
      "Dimensionality Reduction",
      "Pasted image 20240127124620.png|500"
    ],
    "inlinks": [
      "explain_the_curse_of_dimensionality",
      "machine_learning_algorithms",
      "principal_component_analysis"
    ],
    "summary": "Manifold learning is a powerful approach for high-dimensional data exploration, focusing on uncovering the lower-dimensional manifold that the data resides on. These algorithms aim to identify and map the underlying low-dimensional structure, or manifold, that the data is assumed to lie on, within the high-dimensional space. This is particularly useful for reducing dimensionality while preserving the intrinsic properties of the data. Methods like ==Isomap== aim to preserve the geodesic distances between points, which better represent the data's true structure than straight-line distances in high-dimensional space. This enables effective [[Dimensionality Reduction]] for non-linear data while preserving important relationships between data points...."
  },
  "many-to-many_relationships": {
    "title": "Many-to-Many Relationships",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Many-to-Many Relationships.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Many-to-Many Relationships"
    ],
    "inlinks": [
      "implementing_database_schema",
      "many-to-many_relationships",
      "relating_tables_together"
    ],
    "summary": "Many-to-Many Relationships Occurs when multiple records in one table are associated with multiple records in another table. Need to use a junction table (also known as a bridge table or associative entity). This table will contain foreign keys that reference the primary keys of the two tables involved in the relationship. Steps to Implement Many-to-Many Relationships Identify the Entities: Determine the two entities that will participate in the many-to-many relationship. For example, consider students and courses. Create the Junction Table: Create a new table that will serve as the junction table. This table will hold the foreign keys from both..."
  },
  "map_reduce": {
    "title": "What is MapReduce?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\map reduce.md",
    "tags": [
      "data_cleaning"
    ],
    "aliases": [],
    "outlinks": [
      "Distributed Computing",
      "Hadoop",
      "Batch Processing",
      "real-time processing",
      "Apache Spark"
    ],
    "inlinks": [],
    "summary": "MapReduce is a programming model and processing technique used for processing and generating large data sets with a parallel, distributed algorithm on a cluster. [[Distributed Computing]] It is a core component of the Apache Hadoop [[Hadoop]] framework, which is designed to handle vast amounts of data across many servers. The MapReduce model simplifies data processing across large clusters by breaking down the task into two main functions: Map and Reduce. MapReduce is particularly effective for [[Batch Processing]] tasks where the data can be processed independently and aggregated later. However, it may not be the best choice for [[real-time processing]] or..."
  },
  "markov_chain": {
    "title": "Markov chain",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Markov chain.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "Is a stochastic model that describes a sequence of events in which the probability of each event depends only on the state attained in the previous event."
  },
  "markov_decision_processes": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Markov Decision Processes.md",
    "tags": [
      "model_algorithm"
    ],
    "aliases": [
      "MDP"
    ],
    "outlinks": [
      "Markov Decision Processes|MDP",
      "Markov Decision Processes",
      "Markov Decision Processes|MDP"
    ],
    "inlinks": [
      "industries_of_interest",
      "markov_decision_processes",
      "reinforcement_learning"
    ],
    "summary": "Markov Decision Process ([[Markov Decision Processes|MDP]]) is a formal framework for decision-making where outcomes depend solely on the current state (Markov property). \\ architecture [[Markov Decision Processes]] ([[Markov Decision Processes|MDP]]s): The mathematical framework for modelling decision-making, characterized by states, actions, transition probabilities, and rewards. Your understanding of probability theory and stochastic processes will be crucial here."
  },
  "master_data_management": {
    "title": "What is Master Data Management (MDM)?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\master data management.md",
    "tags": [
      "data_storage",
      "data_governance",
      "data_management"
    ],
    "aliases": [
      "mdm"
    ],
    "outlinks": [
      "Data Management",
      "source of truth",
      "Data Governance",
      "Data Quality"
    ],
    "inlinks": [
      "data_management",
      "single_source_of_truth"
    ],
    "summary": "Master data management is a method to ==centralize== master data. It's the bridge between the business that maintain the data and know them best and the data folks, and it's a tool of choice. It helps with uniformity, accuracy, stewardship, semantic consistency, and accountability of mostly enterprise master data assets. Master [Data Management] refers to the processes, technologies, and tools used to define, manage, and maintain an organization's critical data entities, such as customers, products, employees, suppliers, and locations, ==ensuring that this data is accurate, consistent, and up-to-date across all systems and departments.== The goal of MDM is to create..."
  },
  "mathematical_reasoning_in_transformers": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mathematical Reasoning in Transformers.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "Transformer|Transformer",
      "grok",
      "GPT-f"
    ],
    "inlinks": [
      "symbolic_computation",
      "reasoning_tokens"
    ],
    "summary": "transformer-based models that address mathematical reasoning either through pretraining, hybrid systems, or fine-tuning on specific mathematical tasks Challenges: General-purpose transformers [[Transformer|Transformer]] are trained primarily on large corpora of text, which include mathematical problems but lack systematic and rigorous math-specific training. This results in limited capabilities for handling complex calculations or abstract algebraic problems. Grokking in Mathematical Reasoning: This is an area of research where models are trained on small datasets of synthetic math problems to encourage grokking, a phenomenon where the model suddenly achieves near-perfect performance after extended training. Researchers are interested in how transformers might be able to \"[[grok]]\"..."
  },
  "mathematics": {
    "title": "Mathematics",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mathematics.md",
    "tags": [
      "portal",
      "math"
    ],
    "aliases": null,
    "outlinks": [
      "Johnson\u2013Lindenstrauss lemma",
      "Big O Notation",
      "Directed Acyclic Graph (DAG)",
      "information theory"
    ],
    "inlinks": [],
    "summary": "[[Johnson\u2013Lindenstrauss lemma]] [[Big O Notation]] [[Directed Acyclic Graph (DAG)]] [[information theory]]"
  },
  "maximum_likelihood_estimation": {
    "title": "Maximum Likelihood Estimation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Maximum Likelihood Estimation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Parameters",
      "Linear Regression",
      "Estimator",
      "Estimator",
      "parametric vs non-parametric models"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resource: - https://www.youtube.com/watch?v=YevSE6bRhTo Used to infer [[Model Parameters]] from collected data for example in [[Linear Regression]] ($\\beta_0,\\beta_1$). Definition: Likelihood Why is it a good tool for guessing parameter values? The likelihoods plot a distribution, the max gives the most likely. This is called the MLE. Properties of a MLE: - As more data comes in the [[Estimator]] should approach a true value - MLE is a ==consistent== [[Estimator]], i.e it gets closer to the true parameter value as the sample size grows. - Asymptotical Normal - Asymptotic Efficiency Assumptions for MLE: - Regularity [[parametric vs non-parametric models]] Likelihood is a..."
  },
  "mean_absolute_error": {
    "title": "mean absolute error",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\mean absolute error.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "mean_squared_error": {
    "title": "Mean Squared Error",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mean Squared Error.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "loss_function",
      "cross_entropy.py"
    ],
    "summary": "==Measures numerical proximity.=="
  },
  "melt": {
    "title": "Melt",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Melt.md",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [
      "Data Transformation",
      "Database Techniques",
      "Turning a flat file into a database",
      "DE_Tools",
      "Normalisation",
      "Data Visualisation",
      "Grouped plots",
      "Multi-level index",
      "Groupby"
    ],
    "inlinks": [],
    "summary": "In pandas, the melt function is used to ==transform ([[Data Transformation]]) a DataFrame from a wide format to a long format==. This is especially useful for data analysis and visualization tasks where long-format data is preferred or required. The wide format typically has multiple columns for different variables, whereas the long format has a single column for variable names and a single column for values. Related: - [[Database Techniques]] - [[Turning a flat file into a database]] In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Key Reasons to Use melt: [[Normalisation]] Wide to Long Transformation: melt helps in converting data with many columns..."
  },
  "memory_caching": {
    "title": "Memory Caching",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Memory Caching.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Memory Caching - Use in-memory caches to store frequently accessed data closer to the user, reducing latency."
  },
  "memory": {
    "title": "Memory",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Memory.md",
    "tags": null,
    "aliases": [
      "What is LLM memory",
      "context"
    ],
    "outlinks": [
      "language models",
      "Memory",
      "Semantic Relationships"
    ],
    "inlinks": [
      "memory"
    ],
    "summary": "Memory in large [[language models]] (LLMs) involves managing context windows to enhance reasoning capabilities without the high costs associated with traditional training methods. The goal of [[Memory]] is to address challenges like \"forgetting,\" where LLMs struggle to retain context across interactions. Key Concepts: Forgetting Context: Understanding how and why LLMs lose context, especially in multi-turn dialogues, and its impact on response accuracy. Forgetting occurs due to the limitations of fixed context windows, manifesting differently in single-turn (immediate forgetting) versus multi-turn interactions (progressive loss of context). Prioritization of Context: Techniques for determining which parts of the context are most relevant and..."
  },
  "merge": {
    "title": "Merge",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Merge.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_transformation_with_pandas",
      "pandas_join_vs_merge"
    ],
    "summary": ""
  },
  "mermaid": {
    "title": "Mermaid",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mermaid.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "code_diagrams",
      "documentation_&_meetings",
      "er_diagrams"
    ],
    "summary": "Step 1. Ask Chatgpt to make er diagram with mermaidcode. Step 2. Use https://mermaid.js.org/ Use obsidians built in feature: ```mermaid erDiagram SAMPLING_POINT { string notation PK \"Primary Key\" string label int easting int northing } SAMPLE { string id PK \"Primary Key\" string samplingPoint FK \"Foreign Key\" datetime sampleDateTime boolean isComplianceSample string purposeLabel string sampledMaterialTypeLabel } DETERMINAND { string notation PK \"Primary Key\" string label string definition string unitLabel } RESULT { int result PK \"Primary Key\" string sampleID FK \"Foreign Key\" string determinandNotation FK \"Foreign Key\" string resultQualifierNotation string codedResultInterpretation } SAMPLING_POINT ||--o{ SAMPLE : \"has\" SAMPLE ||--o{ RESULT..."
  },
  "metadata_handling": {
    "title": "Metadata Handling",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Metadata Handling.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "parquet"
    ],
    "summary": ""
  },
  "methods_for_handling_outliers": {
    "title": "Methods for Handling Outliers",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Methods for Handling Outliers.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "outliers"
    ],
    "summary": "Trimming Description: Removing data points identified as outliers based on criteria such as being beyond a certain number of standard deviations from the mean or outside a specified percentile range. Implementation Example: python lower_quantile = df[\"var1\"].quantile(0.01) upper_quantile = df[\"var1\"].quantile(0.99) df_no_outliers = df[(df[\"var1\"] >= lower_quantile) & (df[\"var1\"] <= upper_quantile)] Capping or Flooring Description: Setting a maximum or minimum threshold beyond which data points are considered outliers and replacing them with the threshold value. Winsorizing Description: Similar to capping and flooring, winsorizing replaces extreme values with less extreme values within a specified range, typically using percentiles."
  },
  "metric": {
    "title": "What is a Metric?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Metric.md",
    "tags": [
      "business"
    ],
    "aliases": [
      "Measure",
      "KPI"
    ],
    "outlinks": [
      "Evaluation Metrics",
      "Regression Metrics"
    ],
    "inlinks": [
      "semantic_layer",
      "cosine_similarity"
    ],
    "summary": "Metrics in Machine Learning [[Evaluation Metrics]] [[Regression Metrics]] Metrics in business A metric, also called KPI or (calculated) measure, are terms that serve as the building blocks for how business performance is both measured and defined, as knowledge of how to define an organization's KPIs. It is fundamental to have a common understanding of them. Metrics usually surface as business reports and dashboards with direct access to the entire organization. For example, think of ==operational metrics== that represent your company's performance and service level or financial metrics that describe its financial health. Calculated measures are part of metrics and apply..."
  },
  "microsoft_access": {
    "title": "Microsoft Access",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Microsoft Access.md",
    "tags": [
      "software",
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "Database"
    ],
    "inlinks": [],
    "summary": "Tasks [ ] How to update (or more so insert) into multiple related tables. How to insert existing data into a [[Database]]. SQL triggers? [ ] Investigate: Helper table to gather many small tables into one. [ ] What are typical types of databases. [ ] Questions: Make a form that accesses multiple tables. Resources Tutorial Best Practices LINK TIME Notes Why use access: Handles lots of data better than excel. Understand relationships between sources of data. Querying: Can do querying. Which might be hard to do in excel. Graphical way to make queries. Forms: Access can make it easier..."
  },
  "mini-batch_gradient_descent": {
    "title": "Mini-batch gradient descent",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mini-batch gradient descent.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "gradient_descent"
    ],
    "summary": ""
  },
  "mixture_of_experts": {
    "title": "Mixture of Experts",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Mixture of Experts.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "llm"
    ],
    "summary": "Different parts of the network focusing on parts of the questions Routing, distribution activating"
  },
  "ml_engineer": {
    "title": "ML Engineer",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ML Engineer.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_roles"
    ],
    "summary": "ML Engineer - Configures and optimizes production ML models. - Monitors the performance and accuracy of ML models in production environments."
  },
  "mnist": {
    "title": "MNIST",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\MNIST.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Datasets"
    ],
    "inlinks": [
      "neural_scaling_laws"
    ],
    "summary": "[[Datasets]]"
  },
  "model_building": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Building.md",
    "tags": "ml_optimisation evaluation",
    "aliases": "Build",
    "outlinks": [
      "Preprocessing",
      "Machine Learning Algorithms",
      "parametric vs nonparametric models",
      "Train-Dev-Test Sets",
      "Model Parameters",
      "Model Selection"
    ],
    "inlinks": [
      "train-dev-test_sets",
      "gradient_boosting",
      "feature_importance"
    ],
    "summary": "The Model Building phase follows the [[Preprocessing]] phase, where data is organized and prepared for analysis. This phase focuses on selecting and setting up the appropriate machine learning models to solve the problem at hand. Key Steps Types of Models: - Choose a model to apply based on the problem requirements and data characteristics. - Explore different [[Machine Learning Algorithms]] to find the best fit for your data. - Consider the tradeoffs between [[parametric vs nonparametric models]]. Setting Up a Model: - Divide the data into [[Train-Dev-Test Sets]] to ensure robust evaluation and tuning. - Optimize [[Model Parameters]] and configurations..."
  },
  "model_cascading": {
    "title": "Model Cascading",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Cascading.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "small_language_models"
    ],
    "summary": ""
  },
  "model_deployment": {
    "title": "Model Deployment",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Deployment.md",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": [
      "Deployment"
    ],
    "outlinks": [
      "API",
      "Flask",
      "FastAPI",
      "Sklearn Pipeline",
      "Preprocessing",
      "Gradio",
      "Streamlit.io",
      "Scalability",
      "Model Observability",
      "PyCaret"
    ],
    "inlinks": [
      "pycaret",
      "continuous_delivery_-_deployment"
    ],
    "summary": "Deploying a machine learning model involves moving it from a development environment to a production environment where it can make predictions on new data. Steps for Model Deployment Model Exporting - Use tools like joblib or pickle to serialize the model. python import joblib joblib.dump(model, 'linear_regression_model.pkl') Deployment Options - Application Integration: Embed the model into an application for real-time predictions. - [[API]] Deployment: Use frameworks like [[Flask]] or [[FastAPI]] to create an API endpoint for the model. - Automated Workflows: Integrate the model into automated data processing pipelines. Tools and Platforms [[Sklearn Pipeline]]: Streamline the deployment process by integrating [[Preprocessing]]..."
  },
  "model_ensemble": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Ensemble.md",
    "tags": [
      "deleted",
      "model_architecture"
    ],
    "aliases": null,
    "outlinks": [
      "Bagging",
      "Boosting",
      "Stacking",
      "Isolated Forest",
      "ML_Tools",
      "Comparing_Ensembles.py",
      "Interpretability"
    ],
    "inlinks": [
      "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy",
      "xgboost",
      "stacking",
      "weak_learners",
      "model_optimisation",
      "gradient_boosting",
      "classification",
      "comparing_ensembles.py",
      "boosting",
      "isolated_forest",
      "bagging",
      "regularisation_of_tree_based_models",
      "gradient_boosting_regressor",
      "ds_&_ml_portal",
      "random_forests"
    ],
    "summary": "Ensemble models in machine learning are techniques that ==combine the predictions of multiple individual models== to improve overall performance. Ensemble methods can achieve better accuracy and robustness than any single model alone. Key Concepts of Ensemble Models: 1. Diversity: The strength of ensemble models lies in the ==diversity== of the base models. Different models may capture different patterns or errors in the data, and combining them can lead to more accurate predictions. 2. Combination: Ensemble methods aggregate the predictions of individual models using ==techniques like averaging, voting, or weighted sums== to produce a final prediction. Main Ensemble Techniques: - [[Bagging]]..."
  },
  "model_evaluation_vs_model_optimisation": {
    "title": "Model Evaluation vs Model Optimisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Evaluation vs Model Optimisation.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Model Evaluation",
      "Model Optimisation"
    ],
    "inlinks": [],
    "summary": "[[Model Evaluation]]focuses on assessing a model's performance, while [[Model Optimisation]]aims to improve that performance through various techniques. Iterative Process: Model evaluation and optimization are often iterative. After evaluating a model, insights gained can guide further optimization. Conversely, after optimizing a model, it needs to be re-evaluated to ensure improvements. Feedback Loop: Evaluation provides feedback on the effectiveness of optimization efforts, helping refine the model further."
  },
  "model_evaluation": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Evaluation.md",
    "tags": [
      "evaluation",
      "deleted"
    ],
    "aliases": [],
    "outlinks": [
      "Overfitting",
      "Evaluation Metrics",
      "Regression Metrics",
      "Cross Validation",
      "Overfitting",
      "Feature Importance"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "model_optimisation",
      "model_evaluation_vs_model_optimisation",
      "neural_network_in_practice",
      "test_loss_when_evaluating_models",
      "cross_validation",
      "logistic_regression",
      "train-dev-test_sets",
      "wrapper_methods",
      "linear_regression",
      "loss_function",
      "model_selection",
      "imbalanced_datasets",
      "feature_selection"
    ],
    "summary": "Assess the model's performance using various metrics to ensure it meets the desired accuracy and reliability. Appropriate evaluation metrics are used based on the problem type (classification vs. regression), to assess how well the model predicts. ==The aim is to improve accuracy but also to generalize and avoid biases== and [[Overfitting]]. Performance Assessment: Models are evaluated on a testing set using metrics relevant to the problem type. Generalization and Bias: Evaluation includes assessing how well the model generalizes to new data and identifying any biases. For categorical classifiers: [[Evaluation Metrics]]: Use metrics such as accuracy, precision, recall, F1-score, and confusion..."
  },
  "model_interpretability": {
    "title": "Model Interpretability",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Interpretability.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability",
      "p values",
      "Confidence Interval",
      "SHapley Additive exPlanations",
      "Local Interpretable Model-agnostic Explanations"
    ],
    "inlinks": [
      "model_selection"
    ],
    "summary": "Model [[interpretability]] tools are crucial in ensuring that machine learning models are transparent, explainable, and understandable to stakeholders, particularly in industries where decisions need to be justifiable (e.g., finance, healthcare). These tools are becoming standard for ensuring trustworthiness and transparency in ML models, enabling organizations to defend model predictions in regulated industries and maintain user trust. [[p values]] and [[Confidence Interval]]: If statistical significance is needed, interpret these values to determine which features significantly contribute to the model. [[SHapley Additive exPlanations]] [[Local Interpretable Model-agnostic Explanations]] Counterfactual Explanations: Purpose: Counterfactual explanations aim to provide insight into ==how small changes in the..."
  },
  "model_observability": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Observability.md",
    "tags": [
      "deleted",
      "#model_explainability"
    ],
    "aliases": [
      "Observability"
    ],
    "outlinks": [
      "Model Validation",
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "ROC (Receiver Operating Characteristic)",
      "Performance Drift",
      "data drift",
      "Performance Drift",
      "Isolated Forest",
      "Interpretability",
      "data lineage",
      "validation",
      "Performance Drift",
      "Data Observability",
      "Model Validation"
    ],
    "inlinks": [
      "model_validation",
      "model_deployment",
      "machine_learning_operations"
    ],
    "summary": "Monitor the model's performance over time (in production). Similar to [[Model Validation]]. In the context of machine learning (ML), Observability refers to the ability to ==monitor, understand, and diagnose the performance and behaviour of ML models== in production. It encompasses the processes, tools, and techniques that help practitioners ensure models are functioning as expected and identify when they deviate from desired outcomes. Key aspects of observability in machine learning include: Observability is a process in ML, and is usually achieved through logging, metrics collection, real-time monitoring, and advanced diagnostic tools integrated into the ML pipeline. Monitoring Model Performance ==(monitoring metrics)==:..."
  },
  "model_optimisation": {
    "title": "Model Optimisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Optimisation.md",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "Optimisation"
    ],
    "outlinks": [
      "Model Parameters",
      "Hyperparameter|Hyperparameter tuning",
      "Feature Engineering",
      "Model Evaluation",
      "Cross Validation",
      "Model Ensemble"
    ],
    "inlinks": [
      "cost_function",
      "model_evaluation_vs_model_optimisation",
      "data_selection_in_ml",
      "cross_validation",
      "ds_&_ml_portal"
    ],
    "summary": "Model optimization is a step in the machine learning workflow aimed at enhancing a model's performance by fine-tuning its parameters and hyperparameters. The goal is to improve the model's accuracy, efficiency, and ability to generalize to new data. Purpose: Accuracy: Improve the model's predictive performance. Efficiency: Ensure the model runs efficiently in terms of computation and resource usage. Generalization: Enhance the model's ability to perform well on unseen data, avoiding overfitting. Process: [[Model Parameters]] tuning [[Hyperparameter|Hyperparameter tuning]] Adjust hyperparameters such as learning rate, number of layers in a neural network, and regularization strength to find the optimal configuration. Techniques like..."
  },
  "model_parameters_tuning": {
    "title": "Model Parameters Tuning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Parameters Tuning.md",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "Model Parameters",
      "Cost Function",
      "Optimisation function",
      "Optimisation techniques",
      "Pasted image 20241231142918.png"
    ],
    "inlinks": [
      "model_parameters",
      "regularisation"
    ],
    "summary": "To find optimal [[Model Parameters]]. Finding Optimal Model Parameters Parameter Space Exploration: It's useful to visualize slices of the parameter space by selecting two parameters at a time. This helps in understanding how different parameter values affect the model's performance. [[Cost Function]] The cost function is used to find the minimum error in predictions. It measures the difference between predicted and actual values, and the goal is to minimize this function to improve model accuracy. [[Optimisation function]] Ideal parameters are found using optimization functions, which adjust the model parameters to minimize the loss function. Common optimization algorithms include Gradient Descent,..."
  },
  "model_parameters_vs_hyperparameters": {
    "title": "Model parameters vs hyperparameters",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model parameters vs hyperparameters.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Parameters",
      "Loss function",
      "Gradient Descent",
      "Hyperparameter",
      "learning rate",
      "Neural network"
    ],
    "inlinks": [
      "loss_versus_cost_function",
      "hyperparameter"
    ],
    "summary": "Model parameters and hyperparameters serve different roles: [[Model Parameters]] - These are the internal variables of the model that are learned from the training data. They define the model's structure and are adjusted during the training process to minimize the [[Loss function]]. - Examples include: - the weights and biases in a neural network, - the coefficients in a linear regression model, - or the support vectors in a support vector machine. - Model parameters are directly influenced by the data and are optimized through algorithms like [[Gradient Descent]]. [[Hyperparameter]] - These are external configurations set before the training process..."
  },
  "model_parameters": {
    "title": "Model Parameters",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Parameters.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Parameters Tuning",
      "Optimisation techniques",
      "Linear Regression",
      "Logistic Regression",
      "linear regression",
      "Deep Learning|Neural Networks",
      "Support Vector Machines",
      "Decision Tree",
      "K-Means",
      "Clustering"
    ],
    "inlinks": [
      "model_parameters_tuning",
      "train-dev-test_sets",
      "cost_function",
      "fitting_weights_and_biases_of_a_neural_network",
      "hyperparameter",
      "maximum_likelihood_estimation",
      "logistic_regression",
      "model_building",
      "lbfgs",
      "model_validation",
      "cost-sensitive_analysis",
      "optimisation_function",
      "forecasting_autoarima.py",
      "neural_network",
      "model_parameters_vs_hyperparameters",
      "model_optimisation",
      "gradient_descent",
      "loss_function",
      "optimising_a_logistic_regression_model"
    ],
    "summary": "Model parameters are also called weights and biases. These parameters are adjusted during the training process to optimize the model's performance on the given task. See also: - [[Model Parameters Tuning]] - [[Optimisation techniques]] Examples [[Linear Regression]]: Coefficients (weights) for each feature in the input data. Intercept term (bias). [[Logistic Regression]]: Similar to [[linear regression]], it has coefficients for each feature and an intercept term, but it models the probability of a binary outcome. [[Deep Learning|Neural Networks]]: Weights: The connections between neurons in different layers. Biases: Additional parameters added to the weighted sum of inputs to a neuron. [[Support Vector..."
  },
  "model_preparation": {
    "title": "Model preparation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model preparation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "python model = model.fit(X_train, y_train) print(model) y_pred = model.predict(X_test) print(accuracy_score(y_expect, y_pred))"
  },
  "model_selection": {
    "title": "Model Selection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Selection.md",
    "tags": [
      "ml_process",
      "deleted",
      "evaluation"
    ],
    "aliases": [
      "Selection"
    ],
    "outlinks": [
      "Machine Learning Operations",
      "Model Evaluation",
      "GridSeachCv",
      "Random Search",
      "Cross Validation",
      "Evaluation Metrics",
      "Cross Validation",
      "Model Interpretability"
    ],
    "inlinks": [
      "handling_different_distributions",
      "test_loss_when_evaluating_models",
      "regularisation",
      "model_building",
      "interview_notepad",
      "pycaret",
      "parsimonious",
      "ds_&_ml_portal"
    ],
    "summary": "Model selection is an integral part of building a [[Machine Learning Operations]] to ensure that the best performing model is chosen for a given task, avoiding issues like overfitting or underfitting. This is a crucial step because the model's ability to ==generalize== to unseen data depends on selecting the right one. Model selection typically involves the following steps: Define candidate models: These can be models of different types (e.g., decision trees, support vector machines, neural networks) or the same model type but with varying hyperparameters. Train each model: Train all the candidate models on the training set using different algorithms..."
  },
  "model_validation": {
    "title": "Model Validation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Model Validation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "model parameters",
      "Model Observability",
      "Performance Drift"
    ],
    "inlinks": [
      "cross_validation",
      "model_observability"
    ],
    "summary": "Model Validation refers to the process of evaluating a machine learning model's performance on a separate dataset (often called the validation set) to ensure it generalizes well to new, unseen data. This step is crucial for tuning [[model parameters]], selecting the best model, and preventing overfitting. Validation helps in assessing how well the model will perform in real-world scenarios. [[Model Observability]], on the other hand, involves monitoring and understanding the model's performance and behavior in production over time. It includes tracking metrics, detecting [[Performance Drift]], and ensuring the model continues to function as expected in dynamic environments. While model validation..."
  },
  "momentum": {
    "title": "Momentum",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Momentum.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Optimisation|Optimisation",
      "Gradient Descent",
      "cost function",
      "ML_Tools",
      "Momentum.py",
      "Hyperparameter",
      "Learning Rate"
    ],
    "inlinks": [
      "adam_optimizer",
      "adaptive_learning_rates"
    ],
    "summary": "Momentum is an [[Model Optimisation|Optimisation]] technique used to accelerate the [[Gradient Descent]] algorithm by incorporating the concept of inertia. It helps in reducing oscillations and speeding up convergence, especially in scenarios where the [[cost function]] has a complex landscape (surface). Momentum helps in dampening oscillations and achieving faster convergence. Momentum is a technique that helps accelerate gradient descent by adding a fraction of the previous update to the current update. Formula: $$ v_{t+1} = \\beta v_t + (1 - \\beta) \\nabla_{\\theta} J(\\theta) $$ $$ \\theta_{t+1} = \\theta_t - \\alpha v_{t+1} $$ Where: - $v_t$ is the velocity (the accumulated gradient)...."
  },
  "momentum.py": {
    "title": "Momentum.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Momentum.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "momentum"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Momentum.py"
  },
  "mongodb": {
    "title": "MongoDB",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\MongoDB.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database",
      "database_management_system_(dbms)"
    ],
    "summary": ""
  },
  "monolith_architecture": {
    "title": "Monolith Architecture",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Monolith Architecture.md",
    "tags": [
      "software_architecture"
    ],
    "aliases": [],
    "outlinks": [
      "software architecture",
      "microservices"
    ],
    "inlinks": [
      "event_driven_events"
    ],
    "summary": "A monolith, in the context of [[software architecture]], refers to a ==single, unified application where all components and functionalities are interconnected and interdependent==. In a monolithic architecture, the entire application is typically built as a ==single codebase==, and all functions and modules are tightly coupled. While monolithic architectures can be simpler to develop and deploy initially, they can become cumbersome as the application grows in complexity. Many organizations eventually transition to [[microservices]] or other modular architectures to improve scalability, flexibility, and maintainability. However, monoliths can still be effective for smaller applications or teams with limited resources. When we talk about..."
  },
  "monte_carlo_simulation": {
    "title": "Monte Carlo Simulation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Monte Carlo Simulation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Resources: - https://www.youtube.com/watch?v=r7cn3WS5x9c Algorithms that use repeated random sampling. Monte Carlo: random How does the randomness in data generation impact the randomness of the paramaeter calculation. Simulation study: 1) FIGURE OUT A WAY TO APPROXIMATE A PROCESS WITH A RANDOM NUMBER GENERATOR 2) GENERATE THE DATA AND CALCULATE A VALUE OF INTEREST(I.E.MEAN, BIAS, COVERAGE) 3) REPEAT STEPS 1& 2 MANY TIMES TO LEARN ABOUT THE UNCERTAINTY IN THIS VALUE Simulation studies:"
  },
  "multi-agent_reinforcement_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Multi-Agent Reinforcement Learning.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "reinforcement_learning"
    ],
    "summary": ""
  },
  "multi-head_attention": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Multi-head attention.md",
    "tags": [
      "deleted",
      "deep_learning"
    ],
    "aliases": [],
    "outlinks": [
      "Attention mechanism"
    ],
    "inlinks": [
      "attention_mechanism",
      "transformer"
    ],
    "summary": "Summary ==Aggregates different perspectives== This approach allows the model to attend to different parts of the input sequence simultaneously, capturing various aspects of the context more effectively. Like a hydra, it focuses on different aspects of the context. Getting a finer understanding. Multi-head attention captures more context by dividing the input processing into multiple independent attention heads. Each head focuses on different parts of the input and captures diverse types of relationships, both local and global. This parallelism allows the model to learn multiple perspectives simultaneously, enriching its understanding of the input sequence and improving performance on complex tasks like..."
  },
  "multi-level_index": {
    "title": "Multi-level index",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Multi-level index.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "interoperable",
      "JSON",
      "Groupby",
      "Pandas Stack",
      "DE_Tools"
    ],
    "inlinks": [
      "pandas_stack",
      "structuring_and_organizing_data",
      "pd.grouper",
      "data_transformation_with_pandas",
      "melt"
    ],
    "summary": "Multi-level indexing in pandas\u2014also called hierarchical indexing\u2014enables you to work with higher-dimensional data in a 2D DataFrame. It's particularly useful for working with grouped or nested data structures. Why use multi-level index: - MultiIndex makes your data [[interoperable]] - Enables systematic slicing and aggregation - Logical grouping of variables Operations like .stack() and .unstack() rely on MultiIndex to move between long and wide formats. - In a flat DataFrame, reshaping often requires column renaming or pivoting. - With MultiIndex, it's structured and reversible. - Stack can be used to make a multi index from a flat dataframe. If you need..."
  },
  "multicollinearity": {
    "title": "Multicollinearity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Multicollinearity.md",
    "tags": [
      "code_snippet",
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Correlation",
      "Addressing Multicollinearity",
      "Impact of multicollinearity on model parameters",
      "Heatmap",
      "Clustering",
      "Correlation"
    ],
    "inlinks": [
      "dummy_variable_trap",
      "regression",
      "addressing_multicollinearity",
      "ridge",
      "data_selection_in_ml",
      "heatmap",
      "statistics",
      "correlation",
      "ds_&_ml_portal"
    ],
    "summary": "When two or more regressors are in [[Correlation]] Multicollinearity refers to the ==instability== of a model due to ==highly correlated independent variables.== It occurs when two or more independent variables in a regression model are highly correlated, which can make it difficult to determine the individual effect of each variable on the dependent variable. Multicollinearity affects regression models primarily because it leads to instability in the estimated coefficients of the independent variables. Also see: - [[Addressing Multicollinearity]] - [[Impact of multicollinearity on model parameters]] Related: - Multicollinearity hurts your hypothesis test - Correlation increases bias in the estimated parameters -..."
  },
  "multinomial_naive_bayes": {
    "title": "Multinomial Naive bayes",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Multinomial Naive bayes.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "naive_bayes"
    ],
    "summary": ""
  },
  "mysql": {
    "title": "MySql",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\MySql.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database",
      "database_management_system_(dbms)",
      "google_cloud_platform",
      "data_engineering_portal"
    ],
    "summary": "MySQL has more ==granularity== with types than SQLite. For example, an integer could be TINYINT, SMALLINT, MEDIUMINT, INT or BIGINT based on the size of the number we want to store. The following table shows us the size and range of numbers we can store in each of the integer types. Tags Tags: #relational_database, #data_management"
  },
  "naive_bayes": {
    "title": "Naive Bayes",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Naive Bayes.md",
    "tags": [
      "classifier"
    ],
    "aliases": [],
    "outlinks": [
      "Encoding Categorical Variables",
      "Bag of words",
      "Multinomial Naive bayes",
      "Gaussian Naive Bayes",
      "Pasted image 20240116184108.png|500",
      "Pasted image 20240118111554.png|500",
      "Naive Bayes"
    ],
    "inlinks": [
      "supervised_learning",
      "classification",
      "machine_learning_algorithms",
      "naive_bayes",
      "feature_scaling"
    ],
    "summary": "Can values for X,y be categroical ? [[Encoding Categorical Variables]] BernoulliNB() Why Naive Bayes?;;Order doesn't matter, features are independent. Treated it as a [[Bag of words]]. Which ==simplifies== the above equation. Want to use this in classifiers for ML Want to understand: [[Multinomial Naive bayes]] classifer There is also: [[Gaussian Naive Bayes]] Issues To avoid having 0 probability sometimes they add ==counts== $\\alpha$ to do this. Links: https://youtu.be/PPeaRc-r1OI?t=169 Formula $$P(A|B)=P(A) \\times \\frac{P(B|A)}{P(B)}$$ Think of the line as \"given\". Examples Example 1 ![[Pasted image 20240116184108.png|500]] Example 2 In the formula above P(A) is P(+), P(B)=P(NEW) P(B|A) = P(A=0|+)... P(C=0|+) P(A=0,B=1,C=0) is..."
  },
  "named_entity_recognition": {
    "title": "Technical Analysis of Named Entity Recognition",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Named Entity Recognition.md",
    "tags": [
      "NLP",
      "model_algorithm"
    ],
    "aliases": [
      "NER",
      "Entity Recognition"
    ],
    "outlinks": [
      "NLP|Natural Language Processing",
      "LSTM",
      "backpropagation",
      "structured data",
      "How does the choice of training data affect the performance of NER models",
      "What are the challenges of NER in multilingual contexts",
      "Why is named entity recognition (NER) a challenging task",
      "In NER how would you handle ambiguous entities",
      "NLP"
    ],
    "inlinks": [
      "graphrag"
    ],
    "summary": "Named Entity Recognition (NER) is a subtask of [[NLP|Natural Language Processing]] (NLP) that involves identifying and classifying key entities in text into predefined categories such as names, organizations, locations. The process typically employs algorithms like Conditional Random Fields (CRFs) or deep learning models such as Bi-directional [[LSTM]] (Long Short-Term Memory) networks. Mathematically, NER can be framed as a sequence labeling problem where the goal is to assign a label $y_i$ to each token $x_i$ in a sentence. The model learns from annotated datasets, optimizing parameters to maximize the likelihood $P(y|x)$ using techniques like [[backpropagation]]. NER has significant implications in information..."
  },
  "nbconvert": {
    "title": "nbconvert",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\nbconvert.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "ipynb"
    ],
    "summary": ""
  },
  "neo4j": {
    "title": "neo4j",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\neo4j.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "text2cypher",
      "graphrag"
    ],
    "summary": ""
  },
  "network_design": {
    "title": "Network Design",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Network Design.md",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "energy"
    ],
    "summary": "Mixed-Integer Programming: Handles problems where some variables must be integers, commonly used in optimizing network design and capacity planning. How to systems interact."
  },
  "neural_network_classification": {
    "title": "Neural Network Classification",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Neural Network Classification.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Neural network",
      "Classification",
      "Deep Learning|neural networks",
      "Choosing a Threshold",
      "clustering",
      "Choosing the Number of Clusters",
      "Imbalanced Datasets",
      "Data Quality",
      "Evaluation Metrics"
    ],
    "inlinks": [],
    "summary": "Choosing Thresholds/Clusters in [[Neural network]] [[Classification]] When working with [[Deep Learning|neural networks]], the output is often a probability distribution across different classes. To make a final classification decision, we need to convert these probabilities into discrete class labels. This is typically done by comparing the probabilities against a threshold or by clustering them. Threshold-Based Classification In threshold-based classification, we set a specific probability value as the threshold. If the probability of a class exceeds this threshold, the input is classified as belonging to that class. Otherwise, it's classified as belonging to another class or as \"unknown.\" [[Choosing a Threshold]] Clustering-Based..."
  },
  "neural_network_in_practice": {
    "title": "Neural network in Practice",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Neural network in Practice.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Neural network",
      "ML_Tools",
      "SparseCategorialCrossentropy or CategoricalCrossEntropy",
      "Model Evaluation"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "This guide provides practical insights into building and using [[Neural network]]. Refer to [[ML_Tools]] for more details: Neural_Net_Build.py Softmax Placement at the End Numerical stability is crucial. One way to enhance stability is by grouping the softmax function with the loss function rather than placing it at the output layer. Building the Model Final Dense Layer: - Use a 'linear' activation function, which means no activation is applied. This setup allows the model to output raw logits. Model Compilation: - When compiling the model, specify from_logits=True in the loss function to indicate that the outputs are raw logits. python loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)..."
  },
  "neural_network": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Neural network.md",
    "tags": [
      "#deep_learning",
      "drafting"
    ],
    "aliases": [
      "Neural Network"
    ],
    "outlinks": [
      "Neural network|Neural Network",
      "Deep Learning",
      "Types of Neural Networks",
      "Neural network in Practice",
      "Hyperparameter",
      "Model Parameters",
      "activation function",
      "Fitting weights and biases of a neural network",
      "Optimisation techniques",
      "Normalisation"
    ],
    "inlinks": [
      "over_parameterised_models",
      "batch_normalisation",
      "word2vec",
      "activation_function",
      "types_of_neural_networks",
      "deep_q-learning",
      "ds_&_ml_portal",
      "hyperparameter",
      "typical_output_formats_in_neural_networks",
      "classification",
      "ai_engineer",
      "neural_network_classification",
      "optimising_neural_networks",
      "ridge",
      "regularisation",
      "model_parameters_vs_hyperparameters",
      "recurrent_neural_networks",
      "dropout",
      "neural_network_in_practice",
      "feed_forward_neural_network"
    ],
    "summary": "A [[Neural network|Neural Network]] is a computational model inspired by biological neural networks in the human brain. It consists of layers of interconnected nodes (neurons) that process and transmit information. Neural networks are fundamental to [[Deep Learning]]. Resources: - Keras Guide Also see: - [[Types of Neural Networks]] - [[Neural network in Practice]] Key Components The number of starting nodes depends on the input parameter, similar for output. The width and depth of the net are called [[Hyperparameter]]. Neurons (Nodes): - The basic units of a neural network. Each neuron receives input, processes it, and passes it to the next..."
  },
  "neural_scaling_laws": {
    "title": "Neural Scaling Laws",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Neural Scaling Laws.md",
    "tags": [
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "compute efficent frontier",
      "Pasted image 20241017072233.png|500",
      "validation loss",
      "LLM|LLMs",
      "Pasted image 20241017072732.png|500",
      "Loss function",
      "Cross Entropy",
      "Entropy of natural language",
      "Cross entropy loss",
      "MNIST",
      "Pasted image 20241017073743.png|500",
      "Pasted image 20241017074030.png|500",
      "Resolution limited scaling",
      "Cross entropy loss",
      "intrinsic dimension of natural language"
    ],
    "inlinks": [],
    "summary": "Even scaled model cannot cross tthe [[compute efficent frontier]] ![[Pasted image 20241017072233.png|500]] [[validation loss]] Neural scaling laws. That is error rates scale with compute,model size and dataset size, independant of model aritechture. Can we drive to 0? Same laws apear in video and image models. LLMs are auto progressive models. Theorical results guiding experiemental - saving compute time. How [[LLM|LLMs]] work ![[Pasted image 20241017072732.png|500]] During training we know next value, hence we have a [[Loss function]] to help learning. L1 - loss functions [[Cross Entropy]]-loss function (uses negative log of probability). Why is cross enropy used over L1? unabigious next..."
  },
  "ngrams": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Ngrams.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "NLP",
      "text classification"
    ],
    "inlinks": [
      "explain_the_curse_of_dimensionality",
      "generative_ai_from_theory_to_practice",
      "nlp"
    ],
    "summary": "N-grams are used in NLP that allow for the analysis of text data by breaking it down into smaller, manageable sequences. An N-gram is a contiguous sequence of n items (or tokens) from a given sample of text or speech. In the context of natural language processing ([[NLP]]) and text analysis, these items are typically words or characters. N-grams are used to analyze and ==model the structure of language==, and they can help in various tasks such as [[text classification]]. Types of N-grams Unigram: An N-gram where n = 1. It represents individual words or tokens. For example, in the..."
  },
  "nlp": {
    "title": "Natural Language Processing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\NLP.md",
    "tags": [
      "NLP"
    ],
    "aliases": [
      "Natural Language Processing"
    ],
    "outlinks": [
      "Preprocessing",
      "Normalisation of Text",
      "lemmatization",
      "Part of speech tagging",
      "Bag of words",
      "TF-IDF",
      "standardised/Vector Embedding",
      "One Hot Encoding",
      "Ngrams",
      "Grammar method",
      "Summarisation",
      "nltk"
    ],
    "inlinks": [
      "transformer",
      "sentence_similarity",
      "generative_ai_from_theory_to_practice",
      "ngrams",
      "attention_mechanism",
      "explain_the_curse_of_dimensionality",
      "normalisation",
      "rag",
      "bert",
      "tokenisation",
      "lstm",
      "named_entity_recognition"
    ],
    "summary": "Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It encompasses various techniques and models to process and analyze large amounts of natural language data. Key Concepts [[Preprocessing]] [[Normalisation of Text]]: The process of converting text into a standard format, which may include lowercasing, removing punctuation, and stemming or [[lemmatization]]. [[Part of speech tagging]]: Assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text. Models [[Bag of words]]: Represents text data by counting the occurrence of each word in a document, ignoring grammar and word order. It takes..."
  },
  "nltk": {
    "title": "nltk",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\nltk.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": ""
  },
  "node.js": {
    "title": "Node.JS",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Node.JS.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cryptography"
    ],
    "summary": ""
  },
  "non-parametric_tests": {
    "title": "Non-parametric tests",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Non-parametric tests.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "normalisation_of_data": {
    "title": "Normalisation of data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Normalisation of data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "JSON",
      "API",
      "PostgreSQL"
    ],
    "inlinks": [
      "normalisation",
      "normalised_schema",
      "data_transformation"
    ],
    "summary": "Normalization is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from a nested, dynamically typed source like a [[JSON]] [[API]] to a relational destination like [[PostgreSQL]], normalization is the process that un-nests JSON from the source into a relational table format that uses the appropriate column types in the destination."
  },
  "normalisation_of_text": {
    "title": "Normalisation of Text",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Normalisation of Text.md",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "Preprocessing",
      "Tokenisation",
      "Stemming",
      "lemmatization"
    ],
    "inlinks": [
      "nlp",
      "normalisation"
    ],
    "summary": "[[Preprocessing]] in NLP tasks is called Normalization involves reducing words to their base or root form, converting them to lowercase, and removing stop words. Processes What are some steps involved in the pre-processing of a text?. These include making the text lower case, removal of punctuation, tokenize the text (split up the words in a sentence), remove stop words as they convey grammar rather than meaning, word stemming (reduce words to their stems). [[Tokenisation]]: Used to separate words or sentences. [[Stemming]]: returns part of a words that doesnt change ie breaks, breakthrough gives break. Use ```python from nltk.stem.porter import PorterStemmer..."
  },
  "normalisation_vs_standardisation": {
    "title": "Normalisation vs Standardisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Normalisation vs Standardisation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Normalisation",
      "standardisation",
      "Pasted image 20241219071120.png"
    ],
    "inlinks": [
      "normalisation",
      "batch_normalisation"
    ],
    "summary": "Key Differences: [[Normalisation]] changes the range of the data, while standardisation changes the data distribution. Normalisation is preferred when the data does not follow a Gaussian distribution, whereas [[standardisation]] is used when the data is normally distributed. ![[Pasted image 20241219071120.png]]"
  },
  "normalisation": {
    "title": "Normalisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Normalisation.md",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "Z-Normalisation",
      "Standardisation",
      "Normalisation vs Standardisation",
      "Batch Normalisation",
      "Data Engineering",
      "Normalisation of data",
      "Normalised Schema",
      "How to normalise a merged table",
      "NLP",
      "Normalisation of Text"
    ],
    "inlinks": [
      "normalisation_vs_standardisation",
      "melt",
      "neural_network",
      "anomaly_detection",
      "feature_scaling"
    ],
    "summary": "Standardizing data distributions for consistency. In ML: - [[Z-Normalisation]] - [[Standardisation]] - [[Normalisation vs Standardisation]] - [[Batch Normalisation]] In [[Data Engineering]]: - [[Normalisation of data]] - [[Normalised Schema]] - [[How to normalise a merged table]] In [[NLP]]: - [[Normalisation of Text]] ```python --- 15. GroupBy with Transformation (Using transform to align with original dataframe) df['Value_transformed'] = df.groupby('Category')['Value'].transform(lambda x: x - x.mean()) get the mean value for each category print(df.groupby('Category')['Value'].mean()) print(\"\\nTransformed Values with mean subtracted (transform()):\") print(df.sort_values('Category')) ```"
  },
  "normalised_schema": {
    "title": "What is Normalization?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Normalised Schema.md",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "How to normalise a merged table",
      "Normalisation of data"
    ],
    "inlinks": [
      "data_transformation",
      "snowflake_schema",
      "types_of_database_schema",
      "normalisation",
      "why_use_er_diagrams",
      "interview_notepad",
      "how_to_normalise_a_merged_table"
    ],
    "summary": "In a normalized schema, data is organized into multiple related tables to minimize redundancy and dependency, and improve data integrity. This approach is often used in transactional databases (OLTP) to ensure data integrity. However, it can lead to complex queries and slower performance for analytical queries. Normalization involves organizing the columns (attributes) and tables (relations) in a database to ensure proper enforcement of dependencies through database integrity constraints. This is achieved by applying formal rules during the creation of a new database design or ==decomposition== (improvement of an existing database design) process. First Normal Form (1NF): Eliminate duplicate data by..."
  },
  "nosql": {
    "title": "NoSQL",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\NoSQL.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_storage",
      "google_cloud_platform",
      "sql_vs_nosql"
    ],
    "summary": "(Not Only SQL):** ==Non-relational== database management systems offering flexibility and scalability for unstructured or document-based data. NoSQL Databases: Accommodate unstructured data and can be represented through graph theory or document-based structures, allowing for flexible data models."
  },
  "notebooklm": {
    "title": "NotebookLM",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\NotebookLM.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Archive",
      "NotebookLM"
    ],
    "inlinks": [
      "notebooklm"
    ],
    "summary": "https://www.youtube.com/watch?v=EOmgC3-hznM key topics chat interface takes into account resources. save to note- to dave. how to select and folders - from obsidian [[Data Archive]] for this ? A getter of some kind can convert muiltple notes into a single note. Can add website as source. project context - similar projects notes Focus knowledge retrieval - get info from sources (folders) FAQ Note: #portal can help with file extraction rem utils function (for [[NotebookLM]])"
  },
  "npy_files_a_numpy_array_storage": {
    "title": "npy Files A NumPy Array storage",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\npy Files A NumPy Array storage.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "A .npy file is a binary file format specifically designed to store a single NumPy array. NumPy, or Numerical Python, is a powerful library in Python used for numerical computing and data analysis. Why Use .npy Files? Efficiency: Storing data in binary format is generally more efficient than storing it in text-based formats like CSV or JSON. This means faster read/write operations and less disk space usage. Preserves Data Structure: .npy files maintain the exact structure and data type of the NumPy array, ensuring that the data can be loaded back into memory without any loss of information. Simple Format:..."
  },
  "olap_(online_analytical_processing)": {
    "title": "What is OLAP (Online Analytical Processing)?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\OLAP (online analytical processing).md",
    "tags": [
      "database",
      "data_cleaning"
    ],
    "aliases": [
      "OLAP"
    ],
    "outlinks": [
      "Excel pivot table"
    ],
    "inlinks": [],
    "summary": "OLAP, or Online Analytical Processing, is a category of database technology. OLAP systems allow organizations to gain insights by examining data across various dimensions, such as time, product, and region. [[Excel pivot table]] Key Features of OLAP & Operations Query Performance Aggregation and Summarization across dimensions. Slicing: Extracting a single layer of data from the cube by selecting a specific dimension (e.g., sales for Q1). Dicing: Selecting a subcube by specifying values for multiple dimensions. Drill Down: Moving from a summary level to a more detailed level (e.g., from yearly to monthly sales). Roll Up: Aggregating data to a higher..."
  },
  "oltp_(online_transactional_processing)": {
    "title": "oltp (online transactional processing)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\oltp (online transactional processing).md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "oltp": {
    "title": "OLTP",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\OLTP.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "row-based_storage",
      "database"
    ],
    "summary": "In online transaction processing (OLTP), information systems typically facilitate and manage transaction-oriented applications. It's the opposite of OLAP (Online Analytical Processing)."
  },
  "one_pager_template": {
    "title": "One Pager Template",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\One Pager Template.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Proposal: [Project name] About this doc Metadata about this document. Describe the scope and current status. This doc is a proposal for [feature or change]. Upon approval, we will look to have this prioritized as a project and do a full Technical Design Document. | | | |---|---| |Sign off deadline|Date| |Status|Draft| |Author(s)|Name 1, Name 2| Sign offs Name 1 Name 2 Add your name here to sign off Problem What is the problem being solved? What are the pain points? What is the current solution and why is not good enough? High level goal Why should we do this?..."
  },
  "one-hot_encoding": {
    "title": "One-hot encoding",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\One-hot encoding.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Dummy variable trap",
      "Label encoding",
      "Why does label encoding give different predictions from one-hot encoding",
      "ML_Tools",
      "One_hot_encoding.py"
    ],
    "inlinks": [
      "dummy_variable_trap",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ],
    "summary": "Related terms: - Why do we need to drop one of the dummy columns? [[Dummy variable trap]]: ==Dummy variables & One-hot encoding are fundamentally different from [[Label encoding]]== [[Why does label encoding give different predictions from one-hot encoding]] One-hot encoding is a technique used to convert categorical data into a numerical format that can be used by machine learning algorithms. It is particularly useful when dealing with categorical variables that have no ordinal relationship. In one-hot encoding, each category is transformed into a binary vector. If there are ( n ) unique categories, each category is represented by a vector..."
  },
  "one_hot_encoding.py": {
    "title": "One_hot_encoding.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\One_hot_encoding.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Explorations\\Preprocess\\One_hot_encoding\\One_hot_encoding.py This script demonstrates how to preprocess categorical variables and apply linear regression for house price prediction. Key steps include: Data Loading: It loads a dataset of house prices. Dummy Variables: It creates dummy variables for the 'town' column using pd.get_dummies() and merges them with the original dataframe. Dummy Variable Trap: It drops one dummy variable to avoid multicollinearity (dummy variable trap). Feature and Target Split: It separates the dataset into features (X) and the target variable (price). Model Training: A Linear Regression model is trained on the data. Predictions: It predicts house prices based on various features and evaluates..."
  },
  "optimisation_function": {
    "title": "Optimisation function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Optimisation function.md",
    "tags": [
      "ml_optimisation",
      "model_selection"
    ],
    "aliases": null,
    "outlinks": [
      "Model Parameters",
      "Loss function",
      "Optimisation function",
      "Model Parameters",
      "Gradient Descent",
      "Optimisation techniques"
    ],
    "inlinks": [
      "lbfgs",
      "model_parameters_tuning",
      "gradient_descent",
      "optimisation_function",
      "ds_&_ml_portal",
      "optimising_a_logistic_regression_model",
      "logistic_regression_in_sklearn_&_gradient_descent"
    ],
    "summary": "Optimization functions adjust the [[Model Parameters]] to minimize the [[Loss function]], which measures how well the model performs. This is a fundamental step in training machine learning models. General Optimization Process: The [[Optimisation function]] (e.g., LBFGS, Newton-CG) iteratively updates the [[Model Parameters]] by: 1. Calculating the gradient of the loss function with respect to the parameters. 2. Updating the parameters in the direction of the negative gradient (as described in [[Gradient Descent]]). This process is repeated until: - The cost function converges (i.e., the change in the loss function becomes negligible), or - The maximum number of iterations is reached...."
  },
  "optimisation_techniques": {
    "title": "Optimisation techniques",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Optimisation techniques.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Adam Optimizer",
      "Stochastic Gradient Descent",
      "standardised/Optuna",
      "Gradient Descent",
      "Cost Function",
      "Learning Rate",
      "Sklearn",
      "Adaptive Learning Rates"
    ],
    "inlinks": [
      "model_parameters",
      "model_parameters_tuning",
      "fitting_weights_and_biases_of_a_neural_network",
      "gradient_descent",
      "optimisation_function",
      "neural_network",
      "ds_&_ml_portal",
      "deep_learning"
    ],
    "summary": "Optimisation techniques - [[Adam Optimizer]] - RMSprop - [[Stochastic Gradient Descent]] - [[standardised/Optuna]] [[Gradient Descent]] - Iteratively updates parameters using the gradient of the [[Cost Function]] with respect to the parameters. - Requires careful tuning of the [[Learning Rate]] ($\\alpha$), which controls the size of each update. Optimization Solvers in [[Sklearn]] : Scikit-learn solvers improve on Gradient Descent by leveraging advanced techniques: - Use second-order information, such as approximations to the Hessian matrix. - Achieve faster and more reliable convergence compared to Gradient Descent. - Automatically adapt step sizes [[Adaptive Learning Rates]], eliminating the need for manual tuning."
  },
  "optimising_a_logistic_regression_model": {
    "title": "Optimising a Logistic Regression Model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Optimising a Logistic Regression Model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Logistic Regression",
      "loss function",
      "cost function",
      "sklearn",
      "Optimisation function",
      "Model Parameters",
      "Ridge",
      "Gradient Descent"
    ],
    "inlinks": [],
    "summary": "Optimising a [[Logistic Regression]] Model In sklearn, the logistic regression model uses an optimization algorithm to find the best parameters (intercept and coefficients) that minimize a loss function, typically the logistic loss (cross-entropy loss). Here's an overview of how it works: Optimization process: sklearn optimizes the logistic regression parameters using iterative solvers (like LBFGS, newton-cg, or liblinear) that minimize the logistic loss function.sklearn is using one of these optimization algorithms (likely lbfgs or liblinear by default) to minimize the logistic loss function. Objective Function (Loss Function) The objective of logistic regression is to minimize the logistic [[loss function]] (also called..."
  },
  "optimising_neural_networks": {
    "title": "Optimising Neural Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Optimising Neural Networks.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Deep Learning",
      "Neural network"
    ],
    "inlinks": [
      "orthogonalization"
    ],
    "summary": "[[Deep Learning]] Ways to improve in using a [[Neural network]] more data, bigger network, diverse training set, try dropout, change network architechure. ==Need strategies that will point towards whats the best methods to try.=="
  },
  "optuna": {
    "title": "Optuna",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Optuna.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "hyperparameter",
      "Hyperparameter Tuning",
      "LightGBM",
      "XGBoost",
      "CatBoost"
    ],
    "inlinks": [],
    "summary": "Optuna is a [[hyperparameter]] optimization framework used to automatically tune hyperparameters for machine learning models. Optuna automates the process of tuning hyperparameters by defining an objective function, testing different hyperparameter combinations, training the model, and evaluating its performance. The best set of hyperparameters is chosen based on the performance metric (e.g., test accuracy) returned by the objective function. [[Hyperparameter Tuning]] Benefits of Using Optuna Efficient Search: Utilizes algorithms like TPE (Tree-structured Parzen Estimator) to search the hyperparameter space more efficiently than grid search. Dynamic Search Space: Can explore continuous, categorical, and discrete spaces. Automatic Pruning: Supports pruning of unpromising trials..."
  },
  "ordinary_least_squares": {
    "title": "Ordinary Least Squares",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Ordinary Least Squares.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pytorch",
      "linear_regression"
    ],
    "summary": "Derivation of Coefficients: - OLS derives the coefficients by setting the partial derivatives of the SSE with respect to each coefficient to zero. This results in a set of normal equations that can be solved to find the optimal coefficients. - In matrix form, the solution can be expressed as: $$b=(X^{T}X)^{-1} X^{T}y$$ - Here, $X$ is the matrix of input features (including a column of ones for the intercept), is the vector of observed values, and is the vector of coefficients."
  },
  "orthogonalization": {
    "title": "Orthogonalization",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Orthogonalization.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Adam Optimizer",
      "Regularisation",
      "Optimising Neural Networks",
      "DS & ML Portal"
    ],
    "inlinks": [],
    "summary": "link When training ML model need Orthogonalization in order to Determine what to tune, and observe the effect it has. Each button does one thing. Example: Car with Accelerator and angle of steering wheel. Assumptions (controls for tuning): - model works well with cost functions - Try [[Adam Optimizer]], bigger network - work on training set - [[Regularisation]] - Try bigger training set - works on test set of data - Try bigger training set. - works well in real life. - Change training set - Change cost function. Avoid early stopping as effects network size, and training set size...."
  },
  "outliers": {
    "title": "Outliers",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Outliers.md",
    "tags": [
      "statistics",
      "anomaly_detection",
      "data_cleaning"
    ],
    "aliases": [
      "anomalies",
      "Handling Outliers"
    ],
    "outlinks": [
      "Linear Regression",
      "Handling Missing Data",
      "Methods for Handling Outliers",
      "Anomaly Detection"
    ],
    "inlinks": [
      "ds_&_ml_portal"
    ],
    "summary": "Outliers are data points that differ significantly from other observations in the dataset. They can skew and mislead the training of machine learning models, especially those sensitive to the scale of data, such as [[Linear Regression]]. They can sway the generality of the model, skewing predictions and increasing the standard deviation. Related Concepts: - Handling outliers in similar to [[Handling Missing Data]] - [[Methods for Handling Outliers]] - [[Anomaly Detection]] Why Removing Outliers May Improve Regression but Harm Classification Impact on Regression Model: Regression models, particularly linear regression, are sensitive to outliers because they attempt to minimize the sum of..."
  },
  "over_parameterised_models": {
    "title": "Over parameterised models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Over parameterised models.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Neural network"
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Neural network]] Universal approximation theory"
  },
  "overfitting": {
    "title": "Overfitting in Machine Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Overfitting.md",
    "tags": [
      "model_architecture"
    ],
    "aliases": [
      "model overfitting",
      "high variance models"
    ],
    "outlinks": [
      "Regularisation",
      "Cross Validation",
      "Cross Validation",
      "Cross Validation",
      "Bias and variance"
    ],
    "inlinks": [
      "dropout",
      "bias_and_variance",
      "decision_tree",
      "ridge",
      "gradient_boosting",
      "model_evaluation",
      "cross_validation",
      "batch_normalisation",
      "bagging",
      "machine_learning_algorithms",
      "parsimonious",
      "ds_&_ml_portal",
      "feed_forward_neural_network"
    ],
    "summary": "[!Summary] Overfitting in machine learning occurs when a model captures not only the underlying patterns in the training data ==but also the noise==, leading to poor performance on unseen data, and is unable to generalise. Mathematically, overfitting results in a model with low bias but high variance, meaning it adapts too closely to the training data and fails to generalize well. Key methods to address overfitting include [[Regularisation]] (such as $L_1$ and $L_2$ regularization), [[Cross Validation]], and simpler models. In statistical terms, it indicates a model with high complexity and too many parameters relative to the amount of training data,..."
  },
  "p_values": {
    "title": "p values",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\p values.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Feature Selection"
    ],
    "inlinks": [
      "p-values_in_linear_regression_in_sklearn",
      "model_interpretability",
      "hypothesis_testing",
      "statistics"
    ],
    "summary": "A p-value is a measure of the evidence against a null hypothesis. p-values indicate whether an effect exists Used in [[Feature Selection]]"
  },
  "p-values_in_linear_regression_in_sklearn": {
    "title": "p-values in linear regression in sklearn",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\p-values in linear regression in sklearn.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "p values",
      "Linear Regression"
    ],
    "inlinks": [
      "sklearn",
      "linear_regression"
    ],
    "summary": "Question How to include [[p values]] in sklearn for a [[Linear Regression]]? import scipy.stats as stat. You can modify the class of LinearRegression() from sklearn to include them C:\\Users\\RhysL\\Desktop\\DS_Obs\\1_Inbox\\Work\\Udemy\\Part_5_Advanced_Statistical_Methods_(Machine_Learning)\\multiple_linear_regression\\sklearn - How to properly include p-values.ipynb What is f_regression and why can it compute p values? from sklearn.feature_selection import f_regression p_values = f_regression(x,y)[1] p_values link We will look into: f_regression f_regression finds the F-statistics for the simple regressions created with each of the independent variables In our case, this would mean running a simple linear regression on GPA where SAT is the independent variable and a simple linear regression on GPA..."
  },
  "pandas_dataframe_agent": {
    "title": "Pandas Dataframe Agent",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas Dataframe Agent.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "langchain"
    ],
    "summary": "Example: https://github.com/AssemblyAI/youtube-tutorials/tree/main/pandas-dataframe-agent Follow: https://www.youtube.com/watch?v=ZIfzpmO8MdA&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=7 Can as pandas questions to a dataframe. Types of questions: - what is the max value of \"col1\""
  },
  "pandas_join_vs_merge": {
    "title": "Pandas join vs merge",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas join vs merge.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Merge"
    ],
    "inlinks": [
      "data_transformation_with_pandas"
    ],
    "summary": "In pandas, both .join() and pd.merge() are used to combine DataFrames, but they differ in syntax, defaults, and use cases. [[Merge]] is better than Join. |Feature|df.join()|pd.merge()| |---|---|---| |Default key|Uses index of caller and index/column of other|Requires explicit column(s) to merge on| |Syntax style|Method on a DataFrame|Function (pd.merge(left, right))| |Column join|Must specify on= and use one column from each|Can merge on multiple columns| |Index join|Default behavior (index-to-index)|Requires left_index=True, right_index=True| |Suffixes|lsuffix, rsuffix|suffixes=('_x', '_y')| |Complex joins|Not well-suited|Supports full SQL-style joins| |Use case|Simple joins on index or one column|Complex joins with control over join behavior| data_cleaning #data_integration"
  },
  "pandas_pivot_table": {
    "title": "Pandas Pivot Table",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas Pivot Table.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "DE_Tools"
    ],
    "inlinks": [
      "pandas_stack",
      "aggregation"
    ],
    "summary": "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html Pivot Table: Summarize Data python df = pd.DataFrame({'A': ['foo', 'foo', 'bar'], 'B': ['one', 'two', 'one'], 'C': [1, 2, 3]}) pivot_table = df.pivot_table(values='C', index='A', columns='B', aggfunc='sum') In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb"
  },
  "pandas_stack": {
    "title": "Pandas Stack",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas Stack.md",
    "tags": [
      "data_transformation",
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "Pandas Pivot Table",
      "Pandas_Common.py",
      "Pandas_Stack.py",
      "DE_Tools",
      "Data Transformation",
      "Data Cleansing",
      "Groupby",
      "Multi-level index",
      "Machine Learning Algorithms"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "multi-level_index"
    ],
    "summary": "Tool for reshaping data, particularly when you need to pivot a DataFrame ([[Pandas Pivot Table]]) from a wide format to a long format. See: - [[Pandas_Common.py]] - [[Pandas_Stack.py]] In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Why Use stack? ([[Data Transformation]]) Data Reshaping: - ==Wide to Long Format==: Convert a DataFrame from a wide format to a long format, which is often preferred for statistical models and visualizations. Handling Multi-Index DataFrames: - Simplifying Structure: Move the inner level of a column MultiIndex to the row index, simplifying the DataFrame's structure. [[Data Cleansing]]: - Aggregation and Operations: Facilitate data cleaning by allowing aggregation or..."
  },
  "pandas": {
    "title": "Pandas",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Pandas_Common.py",
      "Handling Missing Data",
      "Data Selection",
      "Data Transformation"
    ],
    "inlinks": [
      "data_transformation_with_pandas",
      "pyspark"
    ],
    "summary": "In [[ML_Tools]] see: - [[Pandas_Common.py]] Areas: - [[Handling Missing Data]] - [[Data Selection]] - [[Data Transformation]]"
  },
  "pandas_common.py": {
    "title": "Pandas_Common.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas_Common.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack",
      "pandas"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Common.py"
  },
  "pandas_stack.py": {
    "title": "Pandas_Stack.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pandas_Stack.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pandas_stack"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pandas_Stack.py"
  },
  "parametric_tests": {
    "title": "Parametric tests",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Parametric tests.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "parametric_vs_non-parametric_models": {
    "title": "parametric vs non-parametric models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\parametric vs non-parametric models.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Statistics",
      "regression",
      "Bernoulli",
      "support vector machines"
    ],
    "inlinks": [
      "k-nearest_neighbours",
      "maximum_likelihood_estimation"
    ],
    "summary": "Parametric Models In [[Statistics]] Definition: Models that summarize data with a ==set of parameters of fixed size, regardless of the number of data points.== Characteristics: - Assumes a specific form for the function mapping inputs to outputs (e.g., linear regression assumes a linear relationship). - Requires estimation of a finite number of parameters. - Generally faster to train and predict due to their simplicity. - Risk of underfitting if the model assumptions do not align well with the data. Examples: - Linear [[regression]], logistic regression, neural networks (with a fixed architecture), [[Bernoulli]] Non-parametric Models Definition: Models that do not assume..."
  },
  "parametric_vs_non-parametric_tests": {
    "title": "parametric vs non-parametric tests",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\parametric vs non-parametric tests.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Parametric tests ",
      "Statistical Assumptions",
      "Non-parametric tests "
    ],
    "inlinks": [
      "statistics"
    ],
    "summary": "[[Parametric tests ]] are statistical tests that make ==assumptions about the distribution== of the data. For example, a t-test assumes that the data is normally distributed. Non-parametric tests do not make assumptions about the distribution of the data. Parametric tests are generally more powerful than non-parametric tests, but they are only valid if the data meets the [[Statistical Assumptions]] of the test. [[Non-parametric tests ]] are less powerful than parametric tests, but they can be used on any type of data, regardless of the distribution."
  },
  "parquet": {
    "title": "Parquet",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Parquet.md",
    "tags": [
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "Big Data",
      "Data Storage",
      "Hadoop",
      "Apache Spark",
      "Schema Evolution",
      "Metadata Handling",
      "Cloud Providers"
    ],
    "inlinks": [
      "duckdb"
    ],
    "summary": "A Parquet file is a columnar storage file format specifically designed for storing large amounts of data efficiently. It is commonly used in [[Big Data]] ecosystems due to its optimised performance for both storage and querying. [[Data Storage]] Key Features of Parquet: Columnar Storage Format: Data is stored column by column instead of row by row. This design is highly efficient for analytical queries that access only specific columns, reducing the amount of data read. Optimised for Big Data: Parquet is designed for distributed systems like Apache [[Hadoop]], [[Apache Spark]], and other big data processing tools. Compression: It supports multiple..."
  },
  "parsimonious": {
    "title": "parsimonious",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\parsimonious.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Selection",
      "Overfitting"
    ],
    "inlinks": [
      "adjusted_r_squared"
    ],
    "summary": "Parsimonious refers to a principle in [[Model Selection]] and statistical modeling that emphasizes ==simplicity==. In the context of regression and other statistical models, a parsimonious model is one that explains the data with the fewest possible parameters or predictors while still providing a good fit. A parsimonious model is one that achieves a good balance between simplicity and explanatory power. Key Points about Parsimonious Models: Simplicity: A parsimonious model avoids unnecessary complexity. It uses only the essential variables that contribute meaningfully to the prediction or explanation of the outcome. Avoiding [[Overfitting]]: By keeping the model simple, a parsimonious approach helps..."
  },
  "part_of_speech_tagging": {
    "title": "Part of speech tagging",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Part of speech tagging.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "nlp"
    ],
    "summary": "Part of speech tagging : assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text Part-of-speech tagging involves assigning a specific part-of-speech category (such as noun, verb, adjective, etc.) to each word in a text python from nltk import pos_tag pos_tag(temp[:20]) will get outputs such as [('history', 'NN'), ('poland', 'NN'), ('roots', 'NNS'), ('early', 'JJ')."
  },
  "pca_explained_variance_ratio": {
    "title": "PCA Explained Variance Ratio",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PCA Explained Variance Ratio.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "PCA Explained Variance Ratio",
      "interpretability"
    ],
    "inlinks": [
      "pca_explained_variance_ratio",
      "principal_component_analysis"
    ],
    "summary": "[[PCA Explained Variance Ratio]] - The variance explained by each principal component is printed using pca.explained_variance_ratio_. - The sum of the explained variances is calculated and printed, which should ideally equal 1 (indicating that all variance in the data is accounted for by the principal components). The explained variance refers to how much of the total variance in the original dataset is captured or explained by each principal component (PC) in Principal Component Analysis (PCA). In the context of PCA: Explained Variance: After performing PCA, each principal component corresponds to a certain amount of variance in the original data. The..."
  },
  "pca_principal_components": {
    "title": "PCA Principal Components",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PCA Principal Components.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "PCA_Analysis.ipynb",
      "Pasted image 20250317093551.png",
      "Heatmap",
      "Correlation",
      "Feature Selection"
    ],
    "inlinks": [
      "principal_component_analysis"
    ],
    "summary": "The principal components (or the new axes that explain the most variance) are stored in pca.components_ and displayed as a DataFrame for easier reading Interpretating See [[PCA_Analysis.ipynb]] ![[Pasted image 20250317093551.png]] How to Interpret the PCA Heatmap This heatmap represents the principal component loadings, which show how strongly each original feature contributes to each principal component (PC). Understanding the Heatmap Content Rows = Principal Components (PCs) Each row corresponds to a principal component (e.g., PC1, PC2, PC3, etc.). PC1 captures the most variance, PC2 captures the second-most, and so on. Columns = Original Features Each column represents an original feature from..."
  },
  "pca-based_anomaly_detection": {
    "title": "PCA-Based Anomaly Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PCA-Based Anomaly Detection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "PCA_Based_Anomaly_Detection.py"
    ],
    "inlinks": [
      "anomaly_detection",
      "principal_component_analysis"
    ],
    "summary": "For implementation, see: [[ML_Tools]]: - [[PCA_Based_Anomaly_Detection.py]]"
  },
  "pca_analysis.ipynb": {
    "title": "PCA_Analysis.ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PCA_Analysis.ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Principal Component Analysis|PCA"
    ],
    "inlinks": [
      "pca_principal_components",
      "principal_component_analysis"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Analysis.ipynb This script performs Principal Component Analysis (PCA) on the Iris dataset to reduce its dimensionality while preserving key variance. See also [[Principal Component Analysis|PCA]] Summary: Load and Preprocess Data Loads the Iris dataset and extracts features and target labels. Scales the data to standardize feature ranges. Apply PCA (3 Components) Fits PCA to the scaled data and transforms it into three principal components. Stores the transformed data in a DataFrame with species labels. Analyze PCA Loadings & Variance Computes and stores PCA loadings (weights of original features in principal components). Computes explained variance and cumulative variance to assess PCA..."
  },
  "pca_based_anomaly_detection.py": {
    "title": "PCA_Based_Anomaly_Detection.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PCA_Based_Anomaly_Detection.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pca-based_anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/PCA/PCA_Based_Anomaly_Detection.py"
  },
  "pd.grouper": {
    "title": "pd.Grouper",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\pd.Grouper.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Groupby",
      "Multi-level index"
    ],
    "inlinks": [],
    "summary": "pd.Grouper is a utility in pandas used with .groupby() to flexibly group data by a specific column, often useful for time-based grouping, multi-index grouping, or applying custom frequency aggregation. See: - [[Groupby]] - [[Multi-level index]] Why Use pd.Grouper? Allows more readable and declarative code when working with time-indexed data. Supports multi-index groupings without restructuring your data. Enables resampling-like grouping without setting the index. Syntax python pd.Grouper(key=None, level=None, freq=None, axis=0, sort=False) Parameters key: The column name to group by. level: For MultiIndex, the level to group by. freq: Used to group time-series data (e.g., 'D' for daily, 'M' for monthly). axis:..."
  },
  "pdoc": {
    "title": "pdoc",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\pdoc.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "pdoc"
    ],
    "inlinks": [
      "pdoc",
      "documentation_&_meetings"
    ],
    "summary": "PDOC is a documentation generator specifically designed for Python projects. Here are some key features and details: Automatic Documentation: It scans your Python code and automatically generates documentation based on the docstrings you include in your code. This means that as long as you write clear comments and descriptions in your code, pdoc can create documentation without much extra work. Modern and Clean Design: The output documentation is visually appealing and easy to navigate. It uses a modern design that enhances readability, making it user-friendly for anyone who needs to understand your code. Customization Options: While pdoc generates documentation automatically,..."
  },
  "pdp_and_ice": {
    "title": "PDP and ICE",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PDP and ICE.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "interpretability|interpretable",
      "Pasted image 20241204203338.png",
      "Pasted image 20241204203413.png"
    ],
    "inlinks": [],
    "summary": "link: https://scikit-learn.org/1.5/modules/partial_dependence.html#h2009 [[interpretability|interpretable]] Regression example: ![[Pasted image 20241204203338.png]] Categorical Example ![[Pasted image 20241204203413.png]]"
  },
  "percentile_detection": {
    "title": "Percentile Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Percentile Detection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_with_statistical_methods"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_percentile.py"
  },
  "performance_dimensions": {
    "title": "Performance Dimensions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Performance Dimensions.md",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "Cost-efficiency",
      "Speed",
      "Performance",
      "Flexibility",
      "Scalability",
      "Reusability",
      "Latency",
      "Simplicity",
      "Usability",
      "Accessibility",
      "Data Integrity",
      "Data Quality",
      "Availability",
      "Interoperability",
      "Data Compatibility"
    ],
    "inlinks": [
      "data_principles",
      "dimensional_modelling",
      "data_lifecycle_management"
    ],
    "summary": "Efficiency & Performance - [[Cost-efficiency]]: Ensuring that the solutions used are cost-effective and provide value for money. - [[Speed]]: The ability to process and analyze data quickly to meet business needs. - [[Performance]]: Optimizing query performance by organizing data in a way that supports efficient retrieval. Flexibility & Scalability - [[Flexibility]]: The capability to adapt to changing requirements and data sources. - [[Scalability]]: Ensuring the system can handle increasing volumes of data without performance degradation. Can handle large volumes of data, making it suitable for enterprise-level data warehousing. - [[Reusability]]: Designing components that can be reused across different projects or..."
  },
  "performance_drift": {
    "title": "Performance Drift in Machine Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Performance Drift.md",
    "tags": [
      "deleted",
      "data_quality",
      "deleted",
      "model_explainability"
    ],
    "aliases": [
      "model degradation",
      "accuracy drift",
      "concept drift"
    ],
    "outlinks": [
      "Data Drift",
      "Data Observability|monitoring",
      "Pasted image 20250113072251.png"
    ],
    "inlinks": [
      "data_drift",
      "model_validation",
      "model_observability",
      "transfer_learning"
    ],
    "summary": "Not [[Data Drift]] TL;DR. Data drift is a change in the input data. Concept drift is a change in input-output relationships. Both often happen simultaneously. Performance drift refers to the ==gradual decline in a machine learning model's accuracy== or effectiveness over time as the underlying data distribution changes. This phenomenon occurs when the real-world data that the model is applied to differs from the data it was trained on. Mathematically, this is often represented by a shift in the joint distribution $P(X, Y)$ of the features $X$ and target variable $Y$. Performance drift can occur due to ==concept drift (==when..."
  },
  "physical_model": {
    "title": "Physical Model",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Physical Model.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "data_modelling"
    ],
    "summary": "Physical Model (for a SQL database): ```sql CREATE TABLE Customer ( CustomerID INT PRIMARY KEY, Name VARCHAR(100), Email VARCHAR(100) ); CREATE TABLE Order ( OrderID INT PRIMARY KEY, OrderDate DATE, CustomerID INT, FOREIGN KEY (CustomerID) REFERENCES Customer(CustomerID) ); CREATE TABLE Book ( BookID INT PRIMARY KEY, Title VARCHAR(100), Author VARCHAR(100) ); CREATE TABLE OrderBook ( OrderID INT, BookID INT, PRIMARY KEY (OrderID, BookID), FOREIGN KEY (OrderID) REFERENCES Order(OrderID), FOREIGN KEY (BookID) REFERENCES Book(BookID) ); ``` Physical Model - Implements the logical model in a specific database system. - Includes table structures, columns, data types, and constraints."
  },
  "poetry": {
    "title": "Poetry",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Poetry.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "dependency manager",
      "dependency manager"
    ],
    "inlinks": [
      "virtual_environments",
      "dependency_manager"
    ],
    "summary": "Modern version of setting up dependencies instead of requirements.txt ([[dependency manager]]) Primary Purpose: Poetry is a [[dependency manager]] and packaging tool for Python projects. Main Features: - Dependency management: Allows you to declare, install, and lock dependencies in the pyproject.toml file. - Package management: Can help package your Python project for distribution (e.g., publishing to PyPI). - Virtual environment management: Poetry automatically creates and manages a virtual environment for your project. - Version management: Ensures that all your project dependencies use compatible versions through its poetry.lock file, similar to npm or yarn in the JavaScript ecosystem. - Built-in publishing: Simplifies..."
  },
  "policy": {
    "title": "What is a policy in RL",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Policy.md",
    "tags": [
      "question",
      "#question"
    ],
    "aliases": [
      "policies"
    ],
    "outlinks": [
      "reinforcement learning",
      "SARSA",
      "Q-Learning",
      "exploration",
      "exploitation"
    ],
    "inlinks": [
      "reinforcement_learning",
      "q-learning"
    ],
    "summary": "In [[reinforcement learning]] (RL), a policy is a strategy or a rule that defines the actions an agent takes in a given state to achieve its goals. It essentially ==maps states of the environment to actions that the agent should take when in those states.== Policies are used in RL as they determine the behavior of the agent. The goal of many RL algorithms is to find an optimal policy that maximizes the cumulative reward the agent receives over time. This involves balancing exploration (trying new actions) and exploitation (using known actions that yield high rewards). Key Concepts On-Policy vs...."
  },
  "positional_encoding": {
    "title": "Positional Encoding",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Positional Encoding.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "bert"
    ],
    "summary": ""
  },
  "postgresql": {
    "title": "PostgreSQL",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PostgreSQL.md",
    "tags": [
      "relational_database",
      "data_management"
    ],
    "aliases": null,
    "outlinks": [
      "Tableau",
      "Adding a database to PostgreSQL",
      "Pasted image 20250329081752.png"
    ],
    "inlinks": [
      "normalisation_of_data",
      "looker_studio",
      "database",
      "database_management_system_(dbms)",
      "tableau",
      "data_engineering_portal"
    ],
    "summary": "Installation How to set up a Postgres database on your Windows 10 PC In [[Tableau]] can connect to a database here. There are plugins. Spatial objects? Connections to database tables: hosted Connecting [[Adding a database to PostgreSQL]] PGAdmin pgadmin tools: to check system information ![[Pasted image 20250329081752.png]]"
  },
  "powerbi": {
    "title": "PowerBI",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PowerBI.md",
    "tags": [
      "software",
      "data_visualization"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fabric",
      "semantic_layer",
      "looker_studio",
      "data_visualisation"
    ],
    "summary": "tutorial Business analytics tool for data visualization and reporting."
  },
  "powerquery": {
    "title": "Powerquery",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Powerquery.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "How to normalise a merged table"
    ],
    "inlinks": [],
    "summary": "[[How to normalise a merged table]]"
  },
  "powershell_versus_cmd": {
    "title": "Powershell versus cmd",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Powershell versus cmd.md",
    "tags": [
      "#software"
    ],
    "aliases": null,
    "outlinks": [
      "PowerShell",
      "Command Prompt"
    ],
    "inlinks": [],
    "summary": "PowerShell and Command Prompt (cmd) are both command-line interfaces available on Windows systems, but they differ significantly in their capabilities, syntax, and scripting abilities. Here are the key differences and examples that highlight their distinct features: [[PowerShell]]: Object-Oriented Shell: PowerShell is designed around the concept of objects rather than text streams like cmd. This makes it more powerful for scripting and automation tasks. Example: Getting detailed information about files: powershell Get-ChildItem | Select-Object Name, Length, LastWriteTime This command retrieves file objects and selects specific properties (Name, Length, LastWriteTime). Extensive Commandlets (Cmdlets): PowerShell includes a wide range of cmdlets for performing..."
  },
  "powershell_vs_bash": {
    "title": "Powershell vs Bash",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Powershell vs Bash.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "PowerShell",
      "Bash",
      "Windows Subsystem for Linux",
      "Git"
    ],
    "inlinks": [
      "command_line"
    ],
    "summary": "The choice between [[PowerShell]] and [[Bash]] largely depends on the user's needs and the environment in which they are working. Here are some considerations for each: PowerShell Windows Integration: PowerShell is deeply integrated with Windows and is ideal for managing Windows systems and applications. Object-Oriented: As mentioned earlier, PowerShell works with objects, making it easier to manipulate data and interact with .NET applications. Remote Management: It has strong capabilities for remote management of Windows systems. Scripting: PowerShell's scripting capabilities are robust, making it suitable for complex automation tasks. Bash Unix/Linux Environment: Bash is the default shell for many Unix and..."
  },
  "powershell": {
    "title": "PowerShell",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PowerShell.md",
    "tags": [
      "software"
    ],
    "aliases": [
      "Why is Powershell better than cmd"
    ],
    "outlinks": [
      "Command Prompt|cmd",
      ".NET",
      "Batch Files"
    ],
    "inlinks": [
      "powershell_versus_cmd",
      "command_line",
      "command_prompt",
      "powershell_vs_bash"
    ],
    "summary": "Why is Powershell better than [[Command Prompt|cmd]]? PowerShell is often considered better than Command Prompt (cmd) for several reasons: Object-Oriented: PowerShell is built on the [[.NET]] framework and works with objects rather than plain text. This allows for more complex data manipulation and easier handling of outputs. Powerful Scripting Capabilities: PowerShell supports advanced scripting features, including functions, loops, and error handling, making it more suitable for automation and complex tasks. Access to .NET Framework: PowerShell can leverage the full power of the .NET framework, allowing users to utilize a vast array of libraries and functionalities. Cmdlets: PowerShell uses cmdlets, which..."
  },
  "precision_or_recall": {
    "title": "Precision or Recall",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Precision or Recall.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "Precision",
      "Recall",
      "Classification",
      "Precision",
      "Recall",
      "Pasted image 20240116211130.png"
    ],
    "inlinks": [
      "precision-recall_curve",
      "evaluation_metrics",
      "precision"
    ],
    "summary": "[[Precision]] and [[Recall]] are two fundamental metrics used to evaluate the performance of a [[Classification]] model, particularly in binary classification tasks. They are related through a trade-off: improving one often comes at the expense of the other. Key Differences: - [[Precision]] focuses on the quality of positive predictions. - [[Recall]] focuses on the ability to identify all relevant positive instances Trade-off: - It is challenging to optimize both precision and recall simultaneously. Improving precision by reducing false positives may lead to an increase in false negatives, thereby reducing recall, and vice versa. - However, it's important to balance recall with..."
  },
  "precision-recall_curve": {
    "title": "Precision-Recall Curve",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Precision-Recall Curve.md",
    "tags": [],
    "aliases": [
      "PR Curve"
    ],
    "outlinks": [
      "precision",
      "recall",
      "Binary Classification",
      "precision",
      "recall",
      "ML_Tools",
      "ROC_PR_Example.py",
      "Distributions",
      "Precision or Recall",
      "AUC",
      "ROC (Receiver Operating Characteristic)",
      "Pasted image 20241231172749.png",
      "ROC (Receiver Operating Characteristic)",
      "imbalanced datasets"
    ],
    "inlinks": [
      "determining_threshold_values",
      "choosing_a_threshold"
    ],
    "summary": "A [[precision]]-[[recall]] curve is a graphical representation used to evaluate the performance of a [[Binary Classification]] model, particularly in scenarios where the classes are imbalanced. It plots [[precision]] (the positive predictive value) against [[recall]] (the true positive rate) for different threshold values. Overall, precision-recall curves are a valuable tool for assessing the tradeoffs between precision and recall, helping to choose the optimal threshold for classification based on the specific requirements of the task. Resources In [[ML_Tools]] see: [[ROC_PR_Example.py]] Sklearn Link Precision Recall Curve: Plot: The curve is generated by varying the threshold for classifying a positive instance and plotting the..."
  },
  "precision": {
    "title": "Precision",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Precision.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "Accuracy",
      "Classification",
      "Pasted image 20241222091831.png",
      "Classification Report",
      "Precision or Recall"
    ],
    "inlinks": [
      "evaluation_metrics",
      "precision_or_recall",
      "f1_score",
      "confusion_matrix",
      "classification_report",
      "model_observability",
      "precision-recall_curve",
      "ds_&_ml_portal"
    ],
    "summary": "Precision Score is a metric used to evaluate the [[Accuracy]] of a [[Classification]] model, specifically focusing on the positive class. ==How many retrieved items are relevant?== This metric indicates the accuracy of positive predictions. The formula for precision is: $$\\text{Precision} = \\frac{TP}{TP + FP}$$ where: - TP (True Positives): The number of correctly predicted positive instances. - FP (False Positives): The number of instances incorrectly predicted as positive. Importance: - Precision is crucial in scenarios where the cost of false positives is high, such as in spam detection or medical diagnosis. It helps in understanding how many of the predicted..."
  },
  "preprocessing": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Preprocessing.md",
    "tags": [
      "#ml_optimisation",
      "data_transformation",
      "data_cleaning",
      "data_collection",
      "portal"
    ],
    "aliases": [
      "Data Preprocessing",
      "Feature Preprocessing",
      "Preprocess"
    ],
    "outlinks": [
      "Data Cleansing",
      "Data Transformation",
      "Data Collection",
      "Data Reduction",
      "EDA",
      "Feature Scaling",
      "Feature Selection",
      "Dimensionality Reduction",
      "Feature Engineering"
    ],
    "inlinks": [
      "normalisation_of_text",
      "data_lifecycle_management",
      "handling_different_distributions",
      "high_cross_validation_accuracy_is_not_directly_proportional_to_performance_on_unseen_test_data",
      "model_deployment",
      "model_building",
      "nlp",
      "dimensionality_reduction",
      "pycaret",
      "interview_notepad",
      "heterogeneous_features",
      "feature_scaling"
    ],
    "summary": "Data Preprocessing Data Preprocessing refers to the overall process of cleaning and transforming raw data into a format that is suitable for analysis and modelling. This includes a variety of tasks, such as: Useful: - [[Data Cleansing]] - [[Data Transformation]] Others: - [[Data Collection]] - [[Data Reduction]] End goal: - [[EDA]] Feature Preprocessing Feature preprocessing refers to the process of transforming raw data into a clean data set for learning models, after Data Preprocessing. This step is crucial for improving model performance and ensuring accurate predictions [[Feature Scaling]]: Normalizing or standardizing features to ensure they are on a similar scale...."
  },
  "prevention_is_better_than_the_cure": {
    "title": "Prevention Is Better Than The Cure",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Prevention Is Better Than The Cure.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Quality",
      "Data Observability",
      "Change Management"
    ],
    "inlinks": [
      "data_quality"
    ],
    "summary": "To ensure data products are effective essential to prioritize prevention over remediation of [[Data Quality]] Prevention Preventing data quality issues is the most effective strategy. This involves identifying and addressing potential problems at the source, ensuring that data is accurately entered from the beginning. Remediation When data quality issues do arise, organizations should implement remediation strategies, including: - [[Data Observability]] Tools: Monitor data quality continuously to detect issues early. - Alerting Systems: Notify stakeholders when data quality problems are identified. - Complex ETL Processes: Manage data effectively to minimize errors. - Trust Building: Address the erosion of trust that can..."
  },
  "primary_key": {
    "title": "Primary Key",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Primary Key.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relating_tables_together"
    ],
    "summary": "A primary key (PK) is a unique identifier for each record in a database table. Uniqueness: No two records can have the same primary key value. Non-null: A primary key cannot contain null values; every record must have a valid primary key. Immutability: Ideally, the primary key should not change over time, as it serves as a stable reference for the record For example, an ISBN serves as a primary key for books, uniquely identifying each book in the database"
  },
  "principal_component_analysis": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Principal Component Analysis.md",
    "tags": [
      "data_cleaning",
      "data_visualization"
    ],
    "aliases": [
      "PCA"
    ],
    "outlinks": [
      "Dimensionality Reduction",
      "Unsupervised Learning",
      "Variance",
      "interpretability",
      "Manifold Learning",
      "t-SNE",
      "Manifold Learning",
      "ML_Tools",
      "PCA-Based Anomaly Detection",
      "PCA_Analysis.ipynb",
      "PCA Explained Variance Ratio",
      "PCA Principal Components"
    ],
    "inlinks": [
      "addressing_multicollinearity",
      "feature_scaling",
      "unsupervised_learning",
      "standardisation",
      "dimensionality_reduction",
      "machine_learning_algorithms",
      "ds_&_ml_portal",
      "feature_selection"
    ],
    "summary": "PCA is a tool for [[Dimensionality Reduction]] in [[Unsupervised Learning]] to reduce the dimensionality of data. It transforms the original data into a new coordinate system defined by the principal components, which are ==orthogonal vectors== that capture the most [[Variance]] in the data. It helps simplify models, enhances [[interpretability]], and improves computational efficiency by transforming data into a lower-dimensional space while ==retaining the most significant variance==, and reducing noise. How PCA Works ==Linear Technique==: PCA is a linear technique, meaning it assumes that the relationships between features are linear. This distinguishes it from methods like [[Manifold Learning]] which can capture..."
  },
  "probability_in_other_fields": {
    "title": "Probability in other fields",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Probability in other fields.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "problem_definition": {
    "title": "Problem Definition",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Problem Definition.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Clustering"
    ],
    "inlinks": [
      "scientific_method"
    ],
    "summary": "What is involved: Clearly articulate the problem you're trying to solve and the outcomes you expect. Follow up questions What assumption can we make based on the problem? What kind of questions are good to ask? Business Context: What are the desired outcomes and how would success be measured? What are the limitations and feasibility of using machine learning in this context? 2. Data Availability and Quality: What data is available in quantity and quality and relevant to the problem? What is the format and structure of the data? 3. Feature Engineering and Model Selection: What are the key features..."
  },
  "programming_languages": {
    "title": "programming languages",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\programming languages.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "prompt_engineering": {
    "title": "Prompt Engineering",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Prompt Engineering.md",
    "tags": [
      "language_models",
      "NLP"
    ],
    "aliases": null,
    "outlinks": [
      "prompt retrievers",
      "What are the best practices for evaluating the effectiveness of different prompts",
      "How can prompt engineering be integrated into existing NLP workflows to enhance performance"
    ],
    "inlinks": [
      "agent-based_modelling",
      "how_do_we_evaluate_of_llm_outputs"
    ],
    "summary": "Prompt engineering is a technique in the field of natural language processing (NLP), particularly when working with large language models (LLMs). It involves designing and optimizing input prompts to get the most relevant and accurate responses from these models. Techniques like [[prompt retrievers]], which include systems like UPRISE and DaSLaM, enhance the ability to retrieve and generate contextually appropriate prompts. Prompt engineering aims to guide LLMs toward producing desired outputs while minimizing ambiguity. Key Takeaways Prompt engineering optimizes input to improve LLM responses. Techniques like prompt retrievers (e.g., UPRISE, DaSLaM) enhance prompt effectiveness. Quality prompts reduce ambiguity and guide model..."
  },
  "prompt_extracting_information_from_blog_posts": {
    "title": "Prompt Extracting information from blog posts",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Prompt Extracting information from blog posts.md",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "Prompt Extracting information from blog posts You are an expert in summarizing and analyzing online content. Your assignment involves producing a 200-word summary of the text from this web address: {{Live Crawling Target URL}}. The text from URL {{Live Crawling Target URL}} is: {{Live Crawling Crawled Text}} The resulting summary must be around 200 words. Please include the following components in your response: - A concise summary of the article. - A list of key points as bullet points under the \"important\" section. - Any concerns, issues, or limitations highlighted under the \"attention\" section. - Optionally, provide an example from..."
  },
  "prompting": {
    "title": "Prompting",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Prompting.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Pasted image 20240910072458.png| 500"
    ],
    "inlinks": [
      "ai_engineer",
      "evaluating_language_models",
      "guardrails",
      "rag"
    ],
    "summary": "Pre set prompts are most useful when they are easily accessible. In obsidian copilot can select prompts with \"/\". We tag prompts with #prompt How to write prompts Link Use a formula to design a prompt. 1) Persona 2) Context 3) Task 4) Format [!Example] ![[Pasted image 20240910072458.png| 500]]"
  },
  "proportion_test": {
    "title": "Proportion Test",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Proportion Test.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "Proportion Test The proportion test is used to compare proportions between groups. It can be categorized into: - One-Sample Proportion Test: Compares the proportion of successes in a single sample to a known population proportion. - Two-Sample Proportion Test: Compares the proportions of successes between two independent samples."
  },
  "publish_and_subscribe": {
    "title": "Publish and Subscribe",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Publish and Subscribe.md",
    "tags": [
      "event_driven",
      "data_streaming"
    ],
    "aliases": null,
    "outlinks": [
      "data streaming",
      "Event-Driven Architecture",
      "Distributed Computing",
      "Scalability",
      "batch processing",
      "batch processing",
      "Apache Kafka"
    ],
    "inlinks": [
      "apache_kafka",
      "data_streaming"
    ],
    "summary": "The Publish-Subscribe (Pub-Sub) model is a messaging pattern that enables real-time data distribution by decoupling message producers from consumers. This architecture is widely used in [[data streaming]], [[Event-Driven Architecture]], and [[Distributed Computing]]. It can help in designing more efficient and [[Scalability]] and data processing architectures. Core Components This model ensures that multiple consumers can receive the same data without requiring direct connections between producers and consumers, allowing for a more scalable and flexible system. Producers: Entities or applications that generate data and publish messages to specific channels known as Topics. Each Topic represents a category or stream of data. Consumers:..."
  },
  "pull_request_template": {
    "title": "Pull Request Template",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pull Request Template.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "Tl;dr 1-liner if the context of the change is long Context A few sentences on the high level context for the change. Link to relevant design docs or discussion. This Change What this change does in the larger context. Specific details to highlight for review. Include UI change screenshots or videos if applicable: Callout 1 Callout 2 Callout 3 Test Plan Go over how you plan to test it. Your test plan should be more thorough the riskier the change is. For major changes, I like to describe how I E2E tested it and will monitor the rollout. Links link..."
  },
  "push-down": {
    "title": "What is a Push-Down?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Push-Down.md",
    "tags": [
      "database"
    ],
    "aliases": [
      "Query Pushdown"
    ],
    "outlinks": [
      "Querying|queries"
    ],
    "inlinks": [],
    "summary": "Query pushdown aims to execute as much work as possible in the source databases. Push-downs or query pushdowns push transformation logic to the source database. This reduces to store data physically and transfers them over the network. For example, a semantic layer or Data Virtualization translates the transformation logic into SQL [[Querying|queries]] and sends the SQL queries to the database. The source database runs the SQL queries to process the transformations. Pushdown optimization increases mapping performance when the source database can process transformation logic faster than the semantic layer itself."
  },
  "pycaret": {
    "title": "PyCaret",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PyCaret.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Preprocessing",
      "hyperparameter",
      "Model Deployment|Deployment",
      "ML_Tools",
      "Pycaret_Example.py",
      "hyperparameter tuning",
      "Model Selection",
      "Model Deployment"
    ],
    "inlinks": [
      "model_deployment"
    ],
    "summary": "PyCaret is an open-source, low-code Python library designed to simplify machine learning workflows. It allows users to build, evaluate, and deploy machine learning models with minimal coding and effort. PyCaret provides an end-to-end solution for automating repetitive tasks in machine learning, such as - [[Preprocessing]], - model training, - [[hyperparameter]] tuning - [[Model Deployment|Deployment]] Implementation See: https://pycaret.gitbook.io/docs/get-started/quickstart Resources: https://github.com/pycaret/pycaret/tree/master In [[ML_Tools]] see: - [[Pycaret_Example.py]] Key Features of PyCaret Ease of Use: PyCaret is designed to be beginner-friendly, enabling users to build models without deep expertise in coding. Modular Design: PyCaret supports various machine learning tasks through its modular APIs: Classification:..."
  },
  "pycaret_anomaly.ipynb": {
    "title": "Pycaret_Anomaly.ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pycaret_Anomaly.ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/Pycaret_Anomaly.ipynb"
  },
  "pycaret_example.py": {
    "title": "Pycaret_Example.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pycaret_Example.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pycaret"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Pycaret/Pycaret_Example.py"
  },
  "pydantic": {
    "title": "Pydantic",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pydantic.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "data validation",
      "ML_Tools",
      "Pydantic.py",
      "Pydantic_More.py",
      "Data Validation",
      "type checking",
      "JSON",
      "debugging",
      "FastAPI",
      "Object-Oriented Programming",
      "declarative"
    ],
    "inlinks": [
      "maintainable_code",
      "pyright_vs_pydantic",
      "data_validation",
      "fastapi"
    ],
    "summary": "Pydantic is a Python library used for [[data validation]] and settings management using Python type annotations. It provides a way to define data models with type hints, and it automatically validates and parses input data to ensure it matches the specified types. Pydantic is often used in applications where data integrity is crucial, such as in web APIs, configuration management, and data processing pipelines. It helps developers catch errors early by enforcing type constraints and providing clear error messages when data does not conform to the expected format. In [[ML_Tools]] see: - [[Pydantic.py]] - [[Pydantic_More.py]] What Pydantic Does: [[Data Validation]]..."
  },
  "pydantic.py": {
    "title": "Pydantic.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pydantic.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic.py Explanation: BaseModel: This is the base class for creating data models in Pydantic. You define your model by subclassing BaseModel and specifying fields with type annotations. Optional: Used to indicate that a field is optional. List: Used to specify a list of items, in this case, a list of strings for friends. Validator: A custom validator is used to enforce additional constraints, such as ensuring the age is positive. ValidationError: This exception is raised when the input data does not conform to the model's constraints. This script demonstrates how Pydantic can be used to validate and parse data, ensuring..."
  },
  "pydantic_more.py": {
    "title": "Pydantic_More.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pydantic_More.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "pydantic"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Pydantic_More.py Key Features Demonstrated in the Script: Nested Models: Use of the Friend model inside the User model. Custom Validators: Validating age and email fields with specific logic. Dynamic Defaults: Using datetime.now for created_at. Field Aliases: Supporting different key names during parsing and serialization. Configuration Options: Stripping whitespace and enabling strict typing. Model Inheritance: Extending the User model to create an AdminUser model. Parsing Raw Data: Demonstrating parse_raw for JSON strings."
  },
  "pyright_vs_pydantic": {
    "title": "Pyright vs Pydantic",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pyright vs Pydantic.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pyright",
      "Pydantic",
      "data validation",
      "FastAPI"
    ],
    "inlinks": [],
    "summary": "While [[Pyright]] and [[Pydantic]] serve different roles in Python development, they complement each other well. Pyright helps ensure that the code adheres to ==type constraints before execution==, while Pydantic ensures that the ==data being processed== adheres to the expected types and formats during runtime. Key Differences Purpose: Pyright is aimed at improving code quality through static analysis and type checking. Pydantic is focused on runtime [[data validation]], ensuring that the data conforms to specified types and constraints. Functionality: Pyright checks for type errors and enforces type hints during development, preventing potential issues before the code is executed. Pydantic validates and..."
  },
  "pyright": {
    "title": "Pyright",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pyright.md",
    "tags": [
      "prompt"
    ],
    "aliases": [],
    "outlinks": [
      "Documentation & Meetings",
      "functional programming",
      "Debugging",
      "Maintainable Code",
      "type checking"
    ],
    "inlinks": [
      "maintainable_code",
      "pyright_vs_pydantic"
    ],
    "summary": "Pyright is a ==static type checker== for Python that enhances code reliability by enforcing type constraints ==at compile-time.== It utilizes type hints to identify potential errors, such as type mismatches, before runtime, thereby improving code robustness. Pyright significantly reduces runtime errors by enforcing type constraints at compile-time. The use of type hints in Pyright improves code readability and maintainability, serving as [[Documentation & Meetings]] for function signatures. Related Topics Type inference in programming languages The role of type systems in [[functional programming]] [[Debugging]] [[Maintainable Code]] [[type checking]] Follow up questions How does the inclusion of Pyright impact the performance of..."
  },
  "pyspark": {
    "title": "PySpark",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PySpark.md",
    "tags": [
      "#data_orchestration",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Apache Spark",
      "Apache Spark",
      "Directed Acyclic Graph (DAG)",
      "SQL",
      "Pandas"
    ],
    "inlinks": [
      "fabric"
    ],
    "summary": "Python API for [[Apache Spark]], a ==distributed processing framework== for big data analysis and machine learning on clusters. Part of [[Apache Spark]] [[Directed Acyclic Graph (DAG)]] Interlinked with [[SQL]] queriers Tutorial Can run local. Similar to [[Pandas]]"
  },
  "pytest": {
    "title": "Pytest",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pytest.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "testing",
      "toml",
      "testing_unittest.py",
      "testing_pytest.py"
    ],
    "summary": "@pytest.fixture Explanation @pytest.fixture is a decorator in pytest used to define reusable test setup functions. It allows tests to use shared resources without redundant code. Example & Usage python Copy code import pytest @pytest.fixture def sample_data(): return {\"name\": \"John Doe\", \"age\": 30} def test_example(sample_data): assert sample_data[\"name\"] == \"John Doe\" assert sample_data[\"age\"] == 30 \ud83d\udd39 How it works: The function sample_data() is decorated with @pytest.fixture, making it a fixture. The test function test_example() receives sample_data as an argument. pytest automatically provides the fixture data when running the test. Why use fixtures? Avoids repetitive setup code. Ensures clean test environments. Can handle..."
  },
  "python_click": {
    "title": "Python Click",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Python Click.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "ML_Tools",
      "Click_Implementation.py"
    ],
    "inlinks": [],
    "summary": "Python Click, or \"Command Line Interface Creation Kit,\" is a library for building command-line interfaces (CLIs). It supports arbitrary nesting of commands, automatic help page generation, and lazy loading of subcommands. In [[ML_Tools]] see: [[Click_Implementation.py]] Installation To install Click, use pip: sh pip install click Creating a Command Group Click uses groups to organize related commands. A group serves as a container for multiple commands. ```python import click import json @click.group(\"cli\") @click.pass_context @click.argument(\"document\") def cli(ctx, document): \"\"\"An example CLI for interfacing with a document\"\"\" with open(document) as _stream: _dict = json.load(_stream) ctx.obj = _dict def main(): cli(prog_name=\"cli\") if name ==..."
  },
  "python": {
    "title": "Python",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Python.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Immutable vs mutable"
    ],
    "inlinks": [
      "duckdb_vs_sqlite",
      "langchain",
      "testing",
      "tool.ruff",
      "immutable_vs_mutable"
    ],
    "summary": "dynamic language lower learning, support object orientated [[Immutable vs mutable]]"
  },
  "pytorch_vs_tensorflow": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Pytorch vs Tensorflow.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Tensorflow",
      "pytorch",
      "keras"
    ],
    "inlinks": [
      "tensorflow"
    ],
    "summary": "[[Tensorflow]] is widely adopted but pytorch picking up Dynamic vs static graph Tensorboard is better than [[pytorch]] visualization Plain tensorflow looks pretty much like a library Abstraction is better in pytorch, even data parallelism Tf.contrib, [[keras]] to rescue"
  },
  "pytorch": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\PyTorch.md",
    "tags": [
      "software"
    ],
    "aliases": [
      "PyTorch"
    ],
    "outlinks": [
      "PyTorch",
      "Deep Learning",
      "Tensorflow",
      "Numpy",
      "Neural network|Neural Network",
      "Loss function",
      "Stochastic Gradient Descent",
      "PyTorch",
      "Stochastic Gradient Descent",
      "Neural network|Neural Network",
      "1.0, 2.0",
      "1.0, 2.0",
      "1.0, 2.0",
      "Use Cases for a Simple Neural Network Like",
      "Gradient Descent",
      "Loss function",
      "Ordinary Least Squares",
      "Stochastic Gradient Descent|SGD",
      "Linear Regression",
      "Neural network|Neural Network",
      "Parallelism",
      "Gradient Descent"
    ],
    "inlinks": [
      "pytorch",
      "recurrent_neural_networks",
      "pytorch_vs_tensorflow",
      "vector_embedding",
      "edge_machine_learning_models",
      "lstm",
      "transfer_learning",
      "deep_learning"
    ],
    "summary": "Text Generation With LSTM Recurrent Neural Networks in Python with Keras want for [[PyTorch]] Open-source [[Deep Learning]] framework with dynamic computational graphs, emphasizing flexibility and research. Similar to [[Tensorflow]]. Framework pieces: - torch: a general purpose array library similar to [[Numpy]] that can do computations on GPU when the tensor type is cast to (torch.cuda.TensorFloat) - torch.autograd: a package for building a computational graph and automatically obtaining gradients - torch.nn: a [[Neural network|Neural Network]] library with common layers and [[Loss function]] - torch.optim: an optimization package with common optimization algorithms like [[Stochastic Gradient Descent]] Basics of [[PyTorch]] Tensors arrays PyTorch..."
  },
  "q-learning": {
    "title": "Q-Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Q-Learning.md",
    "tags": [
      "regressor",
      "ml_process"
    ],
    "aliases": null,
    "outlinks": [
      "Reinforcement learning",
      "policy",
      "Bellman Equations",
      "Pasted image 20250220133556.png",
      "Exploration vs. Exploitation"
    ],
    "inlinks": [
      "reinforcement_learning",
      "policy",
      "deep_q-learning"
    ],
    "summary": "Q-learning is a value-based, model-free [[Reinforcement learning]] algorithm where the agent learns the optimal [[policy]] by updating Q-values based on the rewards received. It is particularly useful in discrete environments like grids. Uses a Q-Table which is populated by Q-values which are the maximum expected future reward for the given state and action. We improve the Q-Table in an iterative approach Resources: - Q-Learning Explained - Reinforcement Learning Tutorial Q-learning update rule: The left hand side gets updated ([[Bellman Equations]]) $$ Q_{new}(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right] $$..."
  },
  "quartz": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Quartz.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Vim",
      "jinja template",
      "JavaScript"
    ],
    "inlinks": [
      "jinja_template"
    ],
    "summary": "[[Vim]]: telescope? Search preview feature? https://www.youtube.com/watch?v=v5LGaczJaf0 How does quartz work of a software level: - Transforming text. Think [[jinja template]]. - Manipulating markdown notes - There is a diagram showing how markdown goes to html. - [[JavaScript]] for static site generators already existed."
  },
  "query_gsheets": {
    "title": "QUERY GSheets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\QUERY GSheets.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standardised/GSheets"
    ],
    "inlinks": [
      "gsheets"
    ],
    "summary": "In [[standardised/GSheets]] I want to use query, but I also want to remove certain rows based on a range of keys , can I do this ? 1. Use FILTER Inside QUERY (ArrayFormula Workaround) Since QUERY does not support dynamic NOT IN, you can first filter out the excluded keys using FILTER, then pass the result to QUERY: excel =QUERY(FILTER(A1:D, ISNA(MATCH(A1:A, X1:X10, 0))), \"SELECT Col1, Col2, Col3, Col4\", 1) FILTER(A1:D, ISNA(MATCH(A1:A, X1:X10, 0))): Removes rows where column A matches any value in X1:X10. QUERY(..., \"SELECT Col1, Col2, Col3, Col4\", 1): Runs a query on the filtered data."
  },
  "query_optimisation": {
    "title": "Query Optimisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Query Optimisation.md",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "Querying",
      "Querying",
      "Database Index|Indexing",
      "Transaction",
      "Queries",
      "Database Index|Index",
      "Transaction|Transactions",
      "Query Optimisation"
    ],
    "inlinks": [
      "query_optimisation"
    ],
    "summary": "[[Querying]] can be optimised for time, ==space efficiency==, and concurrency of queries. Optimizing SQL [[Querying]]: - Timing queries - [[Database Index|Indexing]] - Managing [[Transaction]] - and vacuuming, and handling concurrency with transactions and locks ensures efficient and reliable database performance. Timing [[Queries]]: Use .timer on to measure query execution time and identify slow queries. [[Database Index|Index]] Search Creating an index on specific columns can speed up searches: - A covering index includes all the columns required by a query, eliminating the need to access the table data. - Partial indexes cover a subset of rows, saving space while maintaining query..."
  },
  "query_plan": {
    "title": "Query Plan",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Query Plan.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Database Index|Indexing"
    ],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "What is expected to happen to the query plan if there is [[Database Index|Indexing]]?"
  },
  "querying": {
    "title": "Querying",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Querying.md",
    "tags": [
      "database",
      "data_analysis",
      "data_exploration"
    ],
    "aliases": [
      "Queries",
      "Query"
    ],
    "outlinks": [
      "DE_Tools",
      "SQL Joins",
      "SQL Injection"
    ],
    "inlinks": [
      "data_warehouse",
      "duckdb",
      "columnar_storage",
      "sqlite",
      "database_techniques",
      "query_optimisation",
      "sql",
      "data_storage"
    ],
    "summary": "Querying is the process of asking questions of data. Querying makes use of keys primary and foreign within tables. Useful Links - CS50 SQL Course In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Querying/Querying.ipynb SQL Commands and Examples - SELECT - LIMIT - ORDER - WHERE - NOT - LIKE - WITH - INSERT, UPDATE, or DELETE You can have parameterised queries so that you can pass in variables to it: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Querying/Parameterised_Queries.ipynb Related terms: - [[SQL Joins]] - [[SQL Injection]]:=Why we should not use f-strings in queries"
  },
  "quicksort": {
    "title": "QuickSort",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\QuickSort.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Recursive Algorithm"
    ],
    "inlinks": [],
    "summary": "[[Recursive Algorithm]] Quicksort Algorithm in Five Lines of Code! - Computerphile Fast algorithm (compared to say Insertion Sort) 1) Pick pivot value 2) Divide remaining numbers into two parts 3) ==sort sublists in some way== <- apply alog again 4) merge Recursion stops when nothing to pick for pivot value. ```python def quick_sort(arr, depth=0): indent = \" \" * depth # Indentation for better readability in recursion print(f\"{indent}Sorting: {arr}\") if len(arr) <= 1: print(f\"{indent}Returning sorted: {arr}\") return arr # Base case: already sorted pivot = arr[len(arr) // 2] # Choosing pivot (middle element) left = [x for x in arr..."
  },
  "r_squared": {
    "title": "R squared",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\R squared.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "regression",
      "Adjusted R squared",
      "R-squared metric not always a good indicator of model performance in regression"
    ],
    "inlinks": [
      "regression_metrics",
      "linear_regression",
      "adjusted_r_squared"
    ],
    "summary": "R\u00b2, or the coefficient of determination, ==measures the proportion of variance in the dependent variable that is explained by the independent variables== in a [[regression]] model. Interpretation: - R\u00b2 values range from 0 to 1. - A value of 1 indicates perfect predictions, meaning the model explains all the variability of the response data around its mean. - Higher R\u00b2 values signify a better fit of the model to the data. However, it can be misleading when adding more predictors, as R\u00b2 will never decrease when more variables are added to a model. See [[Adjusted R squared]]. Formula: $$R^2 =..."
  },
  "r-squared_metric_not_always_a_good_indicator_of_model_performance_in_regression": {
    "title": "R-squared metric not always a good indicator of model performance in regression",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\R-squared metric not always a good indicator of model performance in regression.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Adjusted R squared",
      "Cross Validation"
    ],
    "inlinks": [
      "r_squared"
    ],
    "summary": "R-squared (R\u00b2) is a commonly used metric for assessing the performance of regression models, but it is not always a reliable indicator of model quality. It should not be the sole criterion for evaluating model performance. It is essential to consider other metrics, such as [[Adjusted R squared]], [[Cross Validation]] results, and the overall context of the analysis. Increased Complexity: R\u00b2 will never decrease when more predictors are added to a model, even if those predictors do not have a meaningful relationship with the dependent variable. This can lead to overfitting, where the model captures noise rather than the underlying..."
  },
  "r": {
    "title": "R",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\R.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": ""
  },
  "race_conditions": {
    "title": "Race Conditions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Race Conditions.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": ""
  },
  "rag": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\RAG.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "LLM",
      "LLM",
      "NLP",
      "Generative",
      "Transformer",
      "LLM",
      "LLM",
      "Pasted image 20240928194559.png|500",
      "Prompting",
      "Generative",
      "BERT",
      "hallucinating",
      "RAG",
      "Pasted image 20241017165540.png"
    ],
    "inlinks": [
      "relationships_in_memory",
      "generative_ai_from_theory_to_practice",
      "knowledge_graphs_with_obsidian",
      "rag",
      "small_language_models",
      "graphrag",
      "knowledge_graph_vs_rag_setup",
      "ds_&_ml_portal",
      "accessing_gen_ai_generated_content"
    ],
    "summary": "Rag is a framework the help [[LLM]] be more up to date. RAG grounds the Gen AI in external data. [!Summary] Given a question sometimes the answer given is wrong, issue with [[LLM]] is no source of data and is out of date. RAG is a specific architecture used in natural language processing ([[NLP]]), where a retrieval mechanism is combined with a generative model ([[Generative]]) (often a [[Transformer]] like GPT). RAG systems are designed to ==enhance the ability of a generative model to answer questions or generate content by incorporating factual knowledge retrieved from external data sources== (such as documents,..."
  },
  "random_forest_regression": {
    "title": "Random Forest Regression",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Random Forest Regression.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Random Forest Regression : Like random forests for classification, random forest regression combines multiple regression trees to improve prediction accuracy."
  },
  "random_forests": {
    "title": "Random Forests",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Random Forests.md",
    "tags": [
      "classifier",
      "drafting"
    ],
    "aliases": [],
    "outlinks": [
      "Model Ensemble",
      "Decision Tree",
      "Random Forests",
      "Pasted image 20240128194716.png|500",
      "Pasted image 20240118145117.png|500",
      "Random Forests",
      "Decision Tree",
      "Bagging",
      "Hyperparameter"
    ],
    "inlinks": [
      "hyperparameter_tuning",
      "embedded_methods",
      "isolated_forest",
      "imbalanced_datasets",
      "bagging",
      "machine_learning_algorithms",
      "regularisation_of_tree_based_models",
      "feature_importance",
      "ada_boosting",
      "ds_&_ml_portal",
      "random_forests",
      "imbalanced_datasets_smote.py",
      "why_and_when_is_feature_scaling_necessary"
    ],
    "summary": "A random forest is an [[Model Ensemble]] of [[Decision Tree]]s. Take many decision trees decisions to get better result. What is the Random Forest method;; an ensemble learning method based on constructing multiple decision trees during training and combining their predictions through averaging. Random Forests are flexibility, robustness, and ability to handle high-dimensional data, as well as their resistance to overfitting. What is an issue with [[Random Forests]];; susceptible to overfitting, especially when dealing with noisy or high-dimensional data. Proper tuning of hyperparameters like the number of trees and maximum depth is crucial to mitigate this. Random forests combine multiple..."
  },
  "react": {
    "title": "React",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\React.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "JavaScript",
      "Dashboarding"
    ],
    "inlinks": [],
    "summary": "React is a [[JavaScript]] library developed by Meta for building user interfaces, particularly in web development. Related to: - [[Dashboarding]] Core Concepts React's component-based architecture allows for reusable UI elements, enhancing maintenance and testing. It uses a Virtual DOM for efficient updates, minimizing direct DOM manipulation. Data flows unidirectionally from parent to child components. Main Use Cases React is ideal for Single Page Applications (SPAs) that load once and update dynamically, as well as for complex, interactive user interfaces and real-time applications like dashboards. Common Tools Popular UI libraries include Tailwind CSS and shadcn/ui."
  },
  "reasoning_tokens": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Reasoning tokens.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Mathematical Reasoning in Transformers"
    ],
    "inlinks": [],
    "summary": "Transformers rely on pattern recognition and language-based reasoning. Thus, reasoning tokens serve as a mechanism for token-based logical progression, allowing models like ChatGPT to simulate math insights by leveraging pattern recognition, token relationships, and sequential reasoning, even without explicit symbolic or mathematical processing built into the model itself. [[Mathematical Reasoning in Transformers]] In the context of models like ChatGPT, reasoning tokens refer to the individual pieces of language that contribute to the step-by-step logical process used by the model to solve problems, including mathematical ones. 5. Logical Continuity and Error Correction Reasoning tokens enable the model to maintain logical continuity,..."
  },
  "recall": {
    "title": "Recall",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Recall.md",
    "tags": [
      "evaluation"
    ],
    "aliases": [
      "sensitivity"
    ],
    "outlinks": [
      "Evaluation Metrics",
      "Classification",
      "Pasted image 20241222091831.png"
    ],
    "inlinks": [
      "evaluation_metrics",
      "precision_or_recall",
      "f1_score",
      "confusion_matrix",
      "classification_report",
      "model_observability",
      "precision-recall_curve",
      "roc_(receiver_operating_characteristic)",
      "ds_&_ml_portal",
      "imbalanced_datasets_smote.py"
    ],
    "summary": "Recall Score is a [[Evaluation Metrics]] used to evaluate the performance of a [[Classification]] model, focusing on the model's ability to identify all relevant instances of the positive class. It answers the question: ==How many relevant items are retrieved?== High recall means that the model is effective at identifying most of the actual spam emails. This is useful in environments where missing a spam email could lead to security risks such as in corporate email systems. The formula for recall is: $$\\text{Recall} = \\frac{TP}{TP + FN}$$ Importance - Recall is crucial in scenarios where ==missing a positive instance is costly==,..."
  },
  "recommender_systems": {
    "title": "Recommender systems",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Recommender systems.md",
    "tags": [
      "evaluation",
      "model_algorithm"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "k-nearest_neighbours",
      "tf-idf",
      "graph_neural_network"
    ],
    "summary": "Crab on Python A recommender system, or recommendation system, is a type of information filtering system that aims to predict the preferences or interests of users by analyzing their behavior and the behavior of similar users or items. These systems are widely used in various applications, such as e-commerce, streaming services, social media, and content platforms, to provide personalized recommendations to users. Key Components of Recommender Systems: User Data: Information about users, such as their preferences, ratings, purchase history, and interactions with items. Item Data: Information about the items being recommended, which can include attributes, descriptions, and metadata. Recommendation Algorithms:..."
  },
  "recurrent_neural_networks": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Recurrent Neural Networks.md",
    "tags": [
      "deep_learning",
      "time_series"
    ],
    "aliases": [
      "RNN",
      "RNNs"
    ],
    "outlinks": [
      "neural network",
      "Time Series",
      "NLP|natural language processing",
      "vanishing and exploding gradients problem",
      "LSTM",
      "Gated Recurrent Unit",
      "Pasted image 20241219073017.png",
      "Backpropagation",
      "PyTorch",
      "LSTM",
      "GRU",
      "Pasted image 20241219073440.png",
      "Use of RNNs in energy sector",
      "Transformer|Transformers",
      "Transformer|Transformer"
    ],
    "inlinks": [
      "transformer",
      "lstm",
      "types_of_neural_networks",
      "ds_&_ml_portal",
      "transformers_vs_rnns",
      "feed_forward_neural_network"
    ],
    "summary": "Recurrent Neural Networks (RNNs) are a type of [[neural network]] designed to process sequential data by maintaining a memory of previous inputs through hidden states. This makes them suitable for tasks where the order of data is needed, such as: [[Time Series]] prediction, speech recognition, and [[NLP|natural language processing]] (NLP). RNNs have loops in their architecture, ==allowing information to persist across sequence steps.== However, they face challenges with long sequences due to [[vanishing and exploding gradients problem]]. To address these issues, variants like Long Short-Term Memory ([[LSTM]]) and [[Gated Recurrent Unit]] (GRU) have been developed. Resources: Video link https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks Key..."
  },
  "recursive_algorithm": {
    "title": "Recursive Algorithm",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Recursive Algorithm.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "common_table_expression",
      "algorithms",
      "quicksort"
    ],
    "summary": ""
  },
  "regression_metrics": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regression Metrics.md",
    "tags": [
      "code_snippet",
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "standardised/Outliers",
      "Interpretability",
      "interpretability",
      "R squared",
      "Adjusted R squared",
      "standardised/Outliers",
      "Variance"
    ],
    "inlinks": [
      "gini_impurity",
      "model_evaluation",
      "adjusted_r_squared",
      "metric"
    ],
    "summary": "These metrics provide various ways to evaluate the performance of regression models. Evaluating Regression Models These metric provide: Comprehensive Evaluation: Each metric provides a different perspective on model performance. For example, while MSE and RMSE give insights into the average error magnitude, MAE provides a straightforward average error measure, and R-squared indicates how well the model explains the variance in the data. Sensitivity to [[standardised/Outliers]]: Metrics like MSE and RMSE are sensitive to outliers due to the squaring of errors, which can be useful if you want to emphasize larger errors. In contrast, MAE and Median Absolute Error are more..."
  },
  "regression": {
    "title": "Regression Analysis and its Applications",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regression.md",
    "tags": [
      "statistics",
      "regressor"
    ],
    "aliases": [
      "regressive models",
      "predictive regression",
      "linear models"
    ],
    "outlinks": [
      "Regression",
      "Linear Regression",
      "Logistic Regression",
      "Polynomial Regression",
      "Linear Regression",
      "Lasso",
      "Ridge",
      "Polynomial Regression",
      "Supervised Learning",
      "Multicollinearity",
      "linearity",
      "Polynomial Regression",
      "Linear Regression",
      "Feature Engineering"
    ],
    "inlinks": [
      "ds_&_ml_portal",
      "supervised_learning",
      "regression",
      "energy",
      "learning_styles",
      "use_cases_for_a_simple_neural_network_like",
      "k-nearest_neighbours",
      "r_squared",
      "typical_output_formats_in_neural_networks",
      "logistic_regression",
      "classification",
      "machine_learning_algorithms",
      "loss_function",
      "parametric_vs_non-parametric_models",
      "imbalanced_datasets",
      "encoding_categorical_variables"
    ],
    "summary": "[!Summary] [[Regression]] analysis is a statistical method used to ==predict== a continuous variable based on one or more predictor variables. The most common form, [[Linear Regression]], assumes a linear relationship between the dependent variable $y$ and independent variables $x_1, x_2, \\dots, x_n$. The goal is to minimize the residual sum of squares (RSS) between observed and predicted values. Other forms, such as [[Logistic Regression]], handle classification problems. Regression models can incorporate techniques like regularization ($L_1$, $L_2$) to improve performance and prevent overfitting, especially with high-dimensional data. Advanced methods like [[Polynomial Regression]] address non-linearity, while generalized linear models (GLMs) extend regression..."
  },
  "regression_logistic_metrics.ipynb": {
    "title": "Regression_Logistic_Metrics.ipynb",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regression_Logistic_Metrics.ipynb.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Regression_Logistic_Metrics.ipynb"
  },
  "regularisation_of_tree_based_models": {
    "title": "Regularisation of Tree based models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regularisation of Tree based models.md",
    "tags": [
      "ml_process",
      "ml_optimisation",
      "evaluation",
      "model_explainability"
    ],
    "aliases": [],
    "outlinks": [
      "Regularisation",
      "hyperparameters",
      "Model Ensemble",
      "Decision Tree",
      "Random Forests"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "Tree models, such as Random Forests and Gradient Boosting, can also be regularized, although they don\u2019t use L1 or L2 regularization directly. Instead, they are regularized through hyperparameters like max depth, min samples split, and learning rate to control the complexity of the trees. In tree-based models, regularization is not applied in the same way as it is in linear models (i.e., using L1 or L2 penalties). In tree models, [[Regularisation]] is done by controlling the growth of the trees using [[hyperparameters]] like max_depth, min_samples_split, min_samples_leaf. These hyperparameters restrict the growth of the tree, preventing it from becoming too complex...."
  },
  "regularisation": {
    "title": "Regularization in Machine Learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regularisation.md",
    "tags": [
      "deleted",
      "ml_process",
      "data_visualization",
      "statistics",
      "ml_optimisation",
      "model_explainability"
    ],
    "aliases": [
      "Regulation in ML",
      "Regularisation techniques"
    ],
    "outlinks": [
      "Loss function",
      "Lasso",
      "Ridge",
      "When and why not to us regularisation",
      "Model Parameters Tuning",
      "loss function",
      "Feature Selection",
      "interpretability",
      "Model Selection",
      "Neural network",
      "Regularisation of Tree based models",
      "ML_Tools",
      "Regularisation.py"
    ],
    "inlinks": [
      "xgboost",
      "dropout",
      "bias_and_variance",
      "hyperparameter_tuning",
      "fitting_weights_and_biases_of_a_neural_network",
      "backpropagation",
      "embedded_methods",
      "lightgbm_vs_xgboost_vs_catboost",
      "explain_the_curse_of_dimensionality",
      "regularisation_of_tree_based_models",
      "overfitting",
      "ds_&_ml_portal",
      "orthogonalization"
    ],
    "summary": "Regularization is a technique in machine learning that reduces the risk of overfitting by adding a penalty to the [[Loss function]] during model training. This penalty term restricts the magnitude of the model's parameters, thereby controlling the complexity of the model. It is especially useful in linear models but can also be applied to more complex models like neural networks. Key Concepts $L_1$ Regularization ([[Lasso]]): Adds the absolute value of the coefficients to the loss function, encouraging sparsity by driving some coefficients to zero, effectively selecting a subset of features. $L_2$ Regularization ([[Ridge]]): Adds the square of the coefficients to..."
  },
  "regularisation.py": {
    "title": "Regularisation.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Regularisation.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "regularisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Optimisation/Regularisation/Regularisation.py"
  },
  "reinforcement_learning": {
    "title": "Reinforcement learning",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Reinforcement learning.md",
    "tags": [
      "field",
      "reinforcement_learning"
    ],
    "aliases": null,
    "outlinks": [
      "Reinforcement learning|RL",
      "Exploration vs. Exploitation",
      "Multi-Agent Reinforcement Learning",
      "Q-Learning",
      "Deep Q-Learning",
      "Sarsa",
      "Policy",
      "Q-Learning",
      "Markov Decision Processes",
      "Bellman Equations",
      "Policy|polices"
    ],
    "inlinks": [
      "cost_function",
      "industries_of_interest",
      "sarsa",
      "policy",
      "llm",
      "q-learning",
      "how_is_reinforcement_learning_being_combined_with_deep_learning",
      "deep_q-learning",
      "exploration_vs._exploitation",
      "ds_&_ml_portal"
    ],
    "summary": "Reinforcement Learning ( [[Reinforcement learning|RL]]) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions, and its goal is to ==maximize cumulative reward.== Current Research Problems Sample Efficiency: Many RL algorithms require a large number of interactions with the environment to learn effectively. Research is focused on developing methods to improve sample efficiency, such as model-based approaches and transfer learning. [[Exploration vs. Exploitation]]: Balancing exploration (trying new actions) and exploitation (choosing known rewarding actions) remains a challenge. New strategies, such as curiosity-driven..."
  },
  "relating_tables_together": {
    "title": "Relating Tables Together",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Relating Tables Together.md",
    "tags": [
      "data_integrity",
      "database_design"
    ],
    "aliases": [],
    "outlinks": [
      "Data Integrity",
      "Primary Key",
      "Foreign Key",
      "Many-to-Many Relationships",
      "Junction Tables",
      "ER Diagrams"
    ],
    "inlinks": [
      "database",
      "data_engineering_portal"
    ],
    "summary": "Implementing these concepts, database tables can be effectively related, ensuring [[Data Integrity]], efficient retrieval, and easy maintenance. Resources: - LINK Notes on Relating Database Tables [[Primary Key]] [[Foreign Key]] One-to-One Relationships: - Each record in Table A relates to one record in Table B and vice versa. - Example: Each employee has one unique profile. One-to-Many Relationships: - A single record in Table A can relate to multiple records in Table B. - Example: One department has many employees. [[Many-to-Many Relationships]] [[Junction Tables]] - Used to manage many-to-many relationships. - Contains foreign keys from both tables it connects. - Example:..."
  },
  "relational_database": {
    "title": "Relational Database",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Relational Database.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "fabric"
    ],
    "summary": ""
  },
  "relationships_in_memory": {
    "title": "Relationships in memory",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Relationships in memory.md",
    "tags": [
      "memory_management",
      "language_models"
    ],
    "aliases": [
      "Managing LLM memory"
    ],
    "outlinks": [
      "RAG",
      "semantic relationships",
      "Vector Database",
      "GraphRAG"
    ],
    "inlinks": [],
    "summary": "In managing the memory of a large language model (LLM), several key concepts and techniques play a crucial role in forming and maintaining relationships between data points: [[RAG]] (Retrieval-Augmented Generation): This technique enhances LLMs by integrating external data retrieval with generative capabilities. By employing chunking and reranking, RAG refines outputs, ensuring that the model can access and utilize relevant information efficiently. This process strengthens the model's ability to form and maintain relationships between different pieces of information, improving its memory and response accuracy. Ontology and Correlating Data Points: Ontologies establish [[semantic relationships]] between data points by defining a structured framework..."
  },
  "requirements.txt": {
    "title": "requirements.txt",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\requirements.txt.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "dependency_manager"
    ],
    "summary": ""
  },
  "rest_api": {
    "title": "REST API",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\REST API.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "CRUD",
      "JSON"
    ],
    "inlinks": [
      "api"
    ],
    "summary": "REST API REST stands for Representational State Transfer. It is a ==standardized== software architecture style used for API communication between a client and a server. Benefits of REST APIs: 1. Simplicity and Standardization: - Data formatting and request structuring are standardized and widely adopted. 2. Scalability and Statelessness: - Easily modifiable as service complexity grows without tracking data states across client and server. 3. High Performance: - Supports ==caching==, maintaining high performance even as complexity increases. Main Building Blocks: 1. ==Request==: - Actions (==[[CRUD]]==): Create (POST), Read (GET), Update (PUT), Delete (DELETE). - Components: Operation (==HTTP method==), Endpoint, Parameters/Body, Headers...."
  },
  "reverse_etl": {
    "title": "What is Reverse ETL?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\reverse etl.md",
    "tags": [
      "data_transformation"
    ],
    "aliases": [
      "Data Activation",
      "Operational Analytics"
    ],
    "outlinks": [
      "ETL"
    ],
    "inlinks": [],
    "summary": "Reverse [[ETL]] is the flip side of the ETL/ELT. With Reverse ETL, the data warehouse becomes the source rather than the destination. Data is taken from the warehouse, transformed to match the destination's data formatting requirements, and loaded into an application \u2013 for example, a CRM like Salesforce \u2013 to enable action. In a way, the Reverse ETL concept is not new to data engineers, who have been enabling data movement warehouses to business applications for a long time. As Maxime Beauchemin mentions in his article, Reverse ETL \u201cappears to be a modern new means of addressing a subset of..."
  },
  "reward_function": {
    "title": "Reward Function",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Reward Function.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Recurrent Neural Networks|RNN"
    ],
    "inlinks": [
      "cost_function"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]]"
  },
  "ridge": {
    "title": "Ridge",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Ridge.md",
    "tags": [
      "drafting"
    ],
    "aliases": [
      "L2"
    ],
    "outlinks": [
      "loss function",
      "Logistic Regression",
      "overfitting",
      "Multicollinearity",
      "Feature Selection",
      "linear regression",
      "Ridge",
      "Neural network"
    ],
    "inlinks": [
      "regression",
      "fitting_weights_and_biases_of_a_neural_network",
      "ridge",
      "regularisation",
      "embedded_methods",
      "optimising_a_logistic_regression_model",
      "elastic_net"
    ],
    "summary": "L2 Regularization, also known as Ridge Regularization, adds a penalty term proportional to the square of the weights to the [[loss function]]. This technique enhances the robustness of linear regression models (and [[Logistic Regression]]) by penalizing large coefficients, encouraging smaller weights overall, and ==distributing weight values more evenly across all features.== Key Points Overfitting Mitigation: Ridge helps mitigate [[overfitting]], especially in high-dimensional datasets, and is effective in managing [[Multicollinearity]] among predictors. Coefficient Shrinkage: Unlike Lasso regularization (L1), which can eliminate some features entirely by driving their coefficients to zero, Ridge reduces the magnitudes of coefficients but retains all features. Multicollinearity..."
  },
  "roc_(receiver_operating_characteristic)": {
    "title": "ROC (Receiver Operating Characteristic)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ROC (Receiver Operating Characteristic).md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [
      "ML_Tools",
      "ROC_Curve.py",
      "Recall",
      "Specificity",
      "Logistic Regression"
    ],
    "inlinks": [
      "determining_threshold_values",
      "auc",
      "model_observability",
      "precision-recall_curve",
      "choosing_a_threshold"
    ],
    "summary": "ROC (Receiver Operating Characteristic) is a graphical representation of a classifier's performance across different thresholds, showing the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate). A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. In [[ML_Tools]] see: [[ROC_Curve.py]] Why Use Predicted Probabilities? In ROC analysis, predicted probabilities (y_probs) are used instead of predicted classes (y_pred) because the ROC curve evaluates the model's performance across different threshold levels. Probabilities..."
  },
  "roc_curve.py": {
    "title": "ROC_Curve.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\ROC_Curve.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/ROC_Curve.py Overview This script demonstrates how to compute and interpret Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC) scores using Random Forest and Naive Bayes classifiers. Below is the step-by-step breakdown: Script Flow Generate Synthetic Dataset Creates a binary classification dataset with 2,000 samples and 10 features. Simulates a realistic classification problem. Add Noisy Features Appends random, irrelevant features to increase the dataset's complexity. Mimics challenging real-world scenarios where not all features are informative. Split the Data Divides the dataset into training (80%) and testing (20%) subsets. Ensures unbiased model evaluation on unseen data. Train Classification..."
  },
  "rollup": {
    "title": "rollup",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\rollup.md",
    "tags": [
      "database"
    ],
    "aliases": null,
    "outlinks": [
      "granularity",
      "Database"
    ],
    "inlinks": [
      "business_intelligence"
    ],
    "summary": "Rollup refers to aggregating data to a higher level of [[granularity]], such as summarizing hourly data into daily totals. [[Database]]"
  },
  "row-based_storage": {
    "title": "Row-based Storage",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Row-based Storage.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "CRUD",
      "OLTP",
      "Columnar Storage",
      "OLTP"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Data is stored in consecutive rows, allows [[CRUD]] Row-based storage is well-suited for transactional systems ([[OLTP]]) Less efficient than [[Columnar Storage]] in largedatasets. Row-based Storage Example (Transactional Workloads): For the same table, if the goal is to efficiently handle transactions like inserting or updating an order, row-based storage** organizes data row by row. | order_id | customer_id | order_date | order_amount | | ---------- | ------------- | ------------ | -------------- | | 1 | 101 | 2024-10-01 | $100 | | 2 | 102 | 2024-10-02 | $150 | | 3 | 103 | 2024-10-03 | $200 | In row-based..."
  },
  "sarsa": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sarsa.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Reinforcement learning"
    ],
    "inlinks": [
      "reinforcement_learning",
      "policy"
    ],
    "summary": "SARSA stands for State-Action-Reward-State-Action SARSA is another value-based [[Reinforcement learning]] algorithm, differing from Q-learning in that it updates the Q-values based on the action actually taken by the policy. SARSA update rule: $$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] $$ Explanation: $Q(s_t, a_t)$: The Q-value of the current state $s_t$ and action $a_t$. $\\alpha$: The learning rate, determining how much new information overrides old information. $r_{t+1}$: The reward received after taking action $a_t$ from state $s_t$. $\\gamma$: The discount factor, balancing immediate and future rewards. $Q(s_{t+1}, a_{t+1})$: The Q-value..."
  },
  "scala": {
    "title": "Scala",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scala.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "big data",
      "Apache Spark",
      "Lambdas",
      "Java"
    ],
    "inlinks": [
      "big_data",
      "fabric"
    ],
    "summary": "[!Summary] Scala is a functional programming language primarily used for [[big data]] processing, particularly with frameworks like [[Apache Spark]]. It is known for its concise syntax and its ability to integrate seamlessly with the Java ecosystem, running on the JVM (Java Virtual Machine). While Scala has a smaller user base and is considered hard to learn, it is highly expressive and offers strong support for managing distributed systems and building large-scale data pipelines. Its robust features make it a top choice for big data engineers. Key Features of Scala Functional Programming: Scala is built around functional programming principles, offering key..."
  },
  "scalability": {
    "title": "Scalability",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scalability.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "distributed_computing",
      "model_deployment",
      "spreadsheets_vs_databases",
      "event_driven",
      "performance_dimensions",
      "apache_kafka",
      "data_ingestion",
      "machine_learning_algorithms",
      "publish_and_subscribe"
    ],
    "summary": "Scalability refers to the capability of a system, network, or process to handle a growing amount of work or its potential to accommodate growth. Key Benefits of Scalability: Performance Improvement: As demand increases, scalable systems can maintain or improve performance levels. Cost Efficiency: Organizations can manage costs by scaling resources according to demand, avoiding over-provisioning. Flexibility: Scalable systems can adapt to changing workloads and business needs, making it easier to accommodate growth. Reliability: Distributing workloads across multiple nodes can enhance system reliability and reduce the risk of a single point of failure. Vertical Scalability (Scaling Up): This involves ==adding more..."
  },
  "scaling_agentic_systems": {
    "title": "Scaling Agentic Systems",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scaling Agentic Systems.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Agentic solutions",
      "LLM",
      "Small Language Models|SLM",
      "Small Language Models|SLM"
    ],
    "inlinks": [],
    "summary": "[[Agentic solutions]] propose an improvement over traditional Large Language Model ([[LLM]]) usage by employing networks of Small Language Models (SLMs). These systems aim to strike a balance between scalability, control, and performance, addressing specific tasks with precision while maintaining overall system adaptability. Ideas from MLOPs talk by MaltedAI. Agentic solutions represent a pragmatic approach to AI systems by focusing on modularity, task-specific efficiency, and the thoughtful integration of human expertise. These architectures show promise for enhancing scalability, control, and adaptability in real-world applications. Contrasting SLMs and LLMs [[Small Language Models|SLM]] (Small Language Models): - Intent-based conversations and decision trees. -..."
  },
  "scaling_server": {
    "title": "Scaling Server",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scaling Server.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "cloud_providers"
    ],
    "summary": "Scaling Server - Horizontal Scaling: Adding more servers, preferred for scalability. - Vertical Scaling: Adding more resources (memory, CPU) to existing servers."
  },
  "scheduled_tasks": {
    "title": "Scheduled Tasks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scheduled Tasks.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Cron jobs ",
      "Unix"
    ],
    "inlinks": [],
    "summary": "Similar to [[Cron jobs ]]in [[Unix]] Using schtasks (Command-Line) Windows provides schtasks, a command-line tool to schedule tasks. Example Commands Run a Python script every 5 minutes: cmd CopyEdit schtasks /create /tn \"MyPythonScript\" /tr \"C:\\Python\\python.exe C:\\path\\to\\script.py\" /sc minute /mo 5 /f Run a batch file daily at 3 AM: cmd CopyEdit schtasks /create /tn \"DailyBackup\" /tr \"C:\\path\\to\\backup.bat\" /sc daily /st 03:00 /f Delete a scheduled task: cmd CopyEdit schtasks /delete /tn \"DailyBackup\" /f"
  },
  "schema_evolution": {
    "title": "What is Schema Evolution?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Schema Evolution.md",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "Database Schema|Schema",
      "DE_Tools",
      "ACID Transaction"
    ],
    "inlinks": [
      "parquet"
    ],
    "summary": "[[Database Schema|Schema]] Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might break backward compatibilities. Still, we can change one table, and the table format takes care of switching it on all distributed files. Best of all does not require rewrite of your table and underlying files. How is schema evolution done in practice In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Writing/Schema_Evolution.ipynb See also: - [[ACID Transaction]]"
  },
  "scientific_method": {
    "title": "Scientific Method",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Scientific Method.md",
    "tags": [
      "field",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "EDA",
      "Problem Definition"
    ],
    "inlinks": [
      "knowledge_work",
      "data_science",
      "thinking_systems"
    ],
    "summary": "Step 1: Start with Data Collect Data: Gather all relevant data sources that might be useful for your analysis. Understand Data: Familiarize yourself with the data types, structures, and any existing metadata. Clean Data: Perform data cleaning to handle missing values, outliers, and inconsistencies. Step 2: Develop Intuitions Explore Data: Use exploratory data analysis ([[EDA]]) techniques to visualize and summarize the data. Identify Patterns: Look for trends, correlations, and anomalies that might inform your understanding. Ask Preliminary Questions: Consider what initial questions the data might help answer. Step 3: Formulate Your Question [[Problem Definition]]: Clearly articulate the problem you are..."
  },
  "search": {
    "title": "Search",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Search.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "tf-idf"
    ],
    "summary": ""
  },
  "security": {
    "title": "Security",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Security.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Common Security Vulnerabilities in Software Development"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development",
      "cryptography",
      "software_design_patterns",
      "data_principles",
      "checksum",
      "deepseek"
    ],
    "summary": "[[Common Security Vulnerabilities in Software Development]]"
  },
  "semantic_layer": {
    "title": "semantic layer",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\semantic layer.md",
    "tags": [
      "database",
      "data_storage"
    ],
    "aliases": null,
    "outlinks": [
      "Metric",
      "Tableau",
      "PowerBI",
      "Data Virtualization"
    ],
    "inlinks": [
      "granularity"
    ],
    "summary": "A Semantic Layer is much more flexible and makes the most sense on top of transformed data in a Data Warehouse. A semantic layer in the context of a data warehouse is an abstraction layer that sits between the raw data stored in the warehouse and the end users who need to access and analyze that data. Its primary purpose is to simplify complex data structures and present them in a more user-friendly and business-oriented way. This allows users to interact with the data without needing to understand the underlying complexities of the database schema or query languages. Bridging the..."
  },
  "semantic_relationships": {
    "title": "Semantic Relationships",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Semantic Relationships.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "relationships_in_memory",
      "memory",
      "vector_embedding",
      "word2vec",
      "syntactic_relationships"
    ],
    "summary": ""
  },
  "semi-structured_data": {
    "title": "What is semi-structured data?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\semi-structured data.md",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "XML",
      "structured data",
      "JSON"
    ],
    "inlinks": [],
    "summary": "Semi-structured data is data that lacks a rigid structure and that does not conform directly to a data model, but that has tags, metadata, or elements that describe the data. Examples of semi-structured data are JSON or [[XML]] files. ==Semi-structured data often contains enough information that it can be relatively easily converted into == [[structured data]]. [[JSON]] data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet. | |..."
  },
  "sentence_similarity": {
    "title": "Sentence Similarity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sentence Similarity.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "NLP"
    ],
    "inlinks": [
      "bert"
    ],
    "summary": "Sentence similarity refers to the degree to which two sentences are alike in meaning. It is a crucial concept in natural language processing ([[NLP]]) tasks such as information retrieval, text summarization, and paraphrase detection. Measuring sentence similarity involves comparing the semantic content of sentences to determine how closely they relate to each other. There are several methods to measure sentence similarity: Lexical Similarity: This involves comparing the words in the sentences directly. Common techniques include: Jaccard Similarity: Measures the overlap of words between two sentences. Cosine Similarity: Represents sentences as vectors (e.g., using TF-IDF) and measures the cosine of the..."
  },
  "shapefile": {
    "title": "shapefile",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\shapefile.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "GIS"
    ],
    "inlinks": [
      "gis"
    ],
    "summary": "A shapefile is a popular geospatial vector data format for geographic information system (GIS) software. It is widely used for storing the location, shape, and attributes of geographic features. Developed by Esri, shapefiles are commonly used in the GIS community for exchanging and managing geospatial data. A shapefile is a widely used [[GIS]] vector data format consisting of multiple files that store both spatial geometry and attribute data. Its ease of use and broad compatibility have made it a standard format for geospatial data exchange and analysis in the GIS community. Components of a Shapefile A shapefile is not a..."
  },
  "shapley_additive_explanations": {
    "title": "SHapley Additive exPlanations",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SHapley Additive exPlanations.md",
    "tags": [],
    "aliases": [
      "SHAP"
    ],
    "outlinks": [
      "Feature Importance"
    ],
    "inlinks": [
      "model_interpretability",
      "feature_importance"
    ],
    "summary": "SHAP provides a unified approach to measure [[Feature Importance]] by computing the contribution of each feature to each prediction, based on game theory. Key Points Purpose: SHAP provides consistent and locally accurate explanations by assigning each feature ==an importance value based== on Shapley values from cooperative game theory. How it Works: It calculates how each feature contributes to the model's output by comparing predictions with and without the feature, across various feature value combinations. Use Cases: Suitable for complex models like neural networks, random forests, or gradient boosting machines, where internal logic is difficult to understand. Advantage: Provides global explanations..."
  },
  "sharepoint": {
    "title": "Sharepoint",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sharepoint.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Microsoft"
    ],
    "inlinks": [],
    "summary": "SharePoint is a web-based collaboration platform developed by [[Microsoft]]. It is primarily used for creating intranet sites, document management, and team collaboration, providing a centralized platform for managing content and communication. SharePoint integrates with Microsoft Office and is highly customizable, making it a versatile tool for organizations of all sizes. Key Features Intranet Sites: Create internal websites for team collaboration and communication. Share news, updates, and resources within an organization. Document Repository: Store, organize, and manage documents in a centralized location. Version control and access permissions ensure document integrity and security. Lists and Libraries: Create lists to manage data and..."
  },
  "silhouette_analysis": {
    "title": "Silhouette Analysis",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Silhouette Analysis.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "clustering",
      "Pasted image 20241231172403.png",
      "Pasted image 20241231172459.png"
    ],
    "inlinks": [
      "choosing_the_number_of_clusters"
    ],
    "summary": "Sklearn link Silhouette analysis is a technique used to evaluate the quality of clustering results. It provides a measure of how similar an object is to its own cluster compared to other clusters. This analysis helps in determining the appropriateness of the number of clusters and the consistency within clusters. Overall, silhouette analysis is a valuable tool for assessing the quality of [[clustering]] results and making informed decisions about the number of clusters and the clustering algorithm's effectiveness. Key Concepts: Silhouette Score: For each data point, the silhouette score is calculated using the following formula: $s(i) = \\frac{b(i) a(i)}{\\max(a(i), b(i))}$..."
  },
  "single_source_of_truth": {
    "title": "Single Source of Truth",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Single Source of Truth.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Database",
      "Data Warehouse",
      "Data Lakehouse",
      "Data Lakehouse",
      "master data management",
      "Single Source of Truth"
    ],
    "inlinks": [
      "business_intelligence",
      "single_source_of_truth",
      "data_integration"
    ],
    "summary": "Sending data from across an enterprise into a centralized system such as a: [[Database]] [[Data Warehouse]] [[Data Lakehouse]] [[Data Lakehouse]] [[master data management]] results in a single unified location for accessing and analyzing all the information that is flowing through an organization. [[Single Source of Truth]] Tags: #data_management, #data_storage"
  },
  "sklearn_datasets": {
    "title": "sklearn datasets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\sklearn datasets.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "make a dataframe by ```python ds = datasets.load_dataset() df = pd.DataFrame(ds.data,columns=ds.feature_names) df.head() add target column df['target'] = ds.target"
  },
  "sklearn_pipiline": {
    "title": "Sklearn Pipiline",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sklearn Pipiline.md",
    "tags": [
      "code_snippet",
      "data_transformation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "```python Naivebayesfor email prediction from sklearn.pipeline import Pipeline clf = Pipeline([ ('vectorizer', CountVectorizer()), ('nb', MultinomialNB()) ]) clf.fit(X_train, y_train) clf.score(X_test,y_test) clf.predict(user_input) ```"
  },
  "sklearn": {
    "title": "Sklearn",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sklearn.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "p-values in linear regression in sklearn",
      "Sklearn"
    ],
    "inlinks": [
      "lbfgs",
      "logistic_regression_statsmodel_summary_table",
      "optimisation_techniques",
      "transformed_target_regressor",
      "ds_&_ml_portal",
      "sklearn",
      "optimising_a_logistic_regression_model"
    ],
    "summary": "Terms of interest: Also called Scikit-learn. train_test_split X and y are separate things (y is the target variable/column) and X is multiple is columns used to get y. Given any pandas df use .to_numpy to convert first. classifier score? -0.018 bad 0.72 good import #data_cleaning (puts all values between -1 and 1) skileanr.pipeline allows you to combine steps. to save model use pickle with with open(out_file,\"wb\") as out: pickle.dump(pipe,out) [[p-values in linear regression in sklearn]] [[Sklearn]]"
  },
  "slowly_changing_dimension": {
    "title": "What is Slowly Changing Dimension?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Slowly Changing Dimension.md",
    "tags": [
      "database"
    ],
    "aliases": [
      "SCD"
    ],
    "outlinks": [
      "ETL",
      "database",
      "dimension table"
    ],
    "inlinks": [],
    "summary": "A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a Data Warehouse. It is considered and implemented as one of the most critical [[ETL]] tasks in tracking the history of dimension records. How do you track slowly changing dimensions in a [[database]] Take a customer dimension in a retail database. Consider a retail company that tracks customer information, including attributes such as name, address, and membership status. Over time, customers may change their addresses or upgrade their membership levels. Implementation of SCD The original record for John Doe is..."
  },
  "small_language_models": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Small Language Models.md",
    "tags": [
      "NLP",
      "language_models"
    ],
    "aliases": [
      "SLM"
    ],
    "outlinks": [
      "LLM|LLMs",
      "Language Models",
      "interpretability",
      "Edge Machine Learning Models",
      "interpretability",
      "Contrastive Decoding",
      "inference",
      "Distillation",
      "BERT",
      "Data Synthesis:",
      "Model Cascading",
      "interpretability|interpretable",
      "RAG",
      "inference",
      "Interpretability"
    ],
    "inlinks": [
      "distillation"
    ],
    "summary": "[[LLM|LLMs]] dominate many general-purpose NLP tasks, small [[Language Models]] have their own place in specialized tasks, where they excel due to computational efficiency, [[interpretability]], and task-specific fine-tuning. SLMs remain highly relevant for [[Edge Machine Learning Models]] and edge computing, ==domain-specific tasks==, and applications requiring [[interpretability]], making them a crucial tool in the NLP landscape. Use Cases for Small Language Models (SLMs) [[Contrastive Decoding]]: Improve the quality of generated content by filtering out low-quality outputs, by having a SLM guide and critique a LLM or other way ([[inference]]) Mitigate hallucinations Augmented Reasoning [[Distillation]]: Transfer the knowledge from a larger model to..."
  },
  "smart_grids": {
    "title": "Smart Grids",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Smart Grids.md",
    "tags": [
      "energy"
    ],
    "aliases": null,
    "outlinks": [
      "RL"
    ],
    "inlinks": [
      "energy"
    ],
    "summary": "Smart Grids Want adaptive grid that can handle the volatility of energy coming on or off. This occurs more often due to the variety of sources i.e wind. Help with carbon commitment Overview: Smart grids utilize advanced technology and data analytics to improve the efficiency and reliability of electricity distribution. [[RL]] can optimize the operation and management of these grids. Applications: Demand Forecasting: RL algorithms predict electricity demand based on historical data and real-time inputs. They adjust energy production and distribution to match forecasted demand. Load Balancing: RL can manage the distribution of electricity by dynamically balancing load across different..."
  },
  "smote_(synthetic_minority_over-sampling_technique)": {
    "title": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SMOTE (Synthetic Minority Over-sampling Technique).md",
    "tags": [],
    "aliases": [
      "SMOTE"
    ],
    "outlinks": [],
    "inlinks": [
      "imbalanced_datasets"
    ],
    "summary": "SMOTE (Synthetic Minority Over-sampling Technique) Generate synthetic samples for the minority class by interpolating between existing samples. SMOTE: This technique generates synthetic samples for the minority class (female resumes) by creating new instances that are interpolations of existing ones."
  },
  "smss": {
    "title": "SMSS",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SMSS.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "microsoft sql server management."
  },
  "snowflake_schema": {
    "title": "Snowflake Schema",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Snowflake Schema.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Normalised Schema",
      "Star Schema"
    ],
    "inlinks": [
      "types_of_database_schema"
    ],
    "summary": "Snowflake Schema - Description: A more [[Normalised Schema]] normalized form of a star schema where dimension tables are further broken down into additional tables. - Advantages: Reduces data redundancy and can save storage space, but may be more complex to query. - A variation of the [[Star Schema]], the snowflake schema normalizes dimension tables into multiple related tables. This can reduce data redundancy and improve data integrity but may complicate queries due to the additional joins required."
  },
  "snowflake": {
    "title": "Snowflake",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Snowflake.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Cloud",
      "Data Warehouse"
    ],
    "inlinks": [
      "difference_between_snowflake_to_hadoop",
      "databricks_vs_snowflake"
    ],
    "summary": "Snowflake Architecture: Cloud-Native: Snowflake is a fully managed, cloud-native data warehousing service. It operates entirely on cloud platforms like AWS, Azure, and Google [[Cloud]]. Separation of Storage and Compute: Snowflake separates storage from compute, allowing for independent scaling of each. This means you can scale up compute resources without affecting storage capacity and vice versa. Multi-Cluster Shared Data Architecture: Snowflake uses a multi-cluster architecture to handle concurrent workloads, ensuring high performance and minimal contention. Data Storage: Structured Data: Primarily designed for structured data and optimized for SQL queries and analytics. Semi-Structured Data: Also supports semi-structured data like JSON, Avro, and..."
  },
  "soft_deletion": {
    "title": "What is a Soft Delete?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Soft Deletion.md",
    "tags": [
      "data_integrity",
      "data_management"
    ],
    "aliases": [],
    "outlinks": [
      "data integrity",
      "Incremental Synchronization",
      "incremental synchronization",
      "Database Schema|schema",
      "Querying|queries",
      "data integrity"
    ],
    "inlinks": [
      "database_techniques",
      "views"
    ],
    "summary": "Soft deletion is a technique used in databases to ==mark records as deleted without physically removing them from the database==. This approach is particularly useful in scenarios where [[data integrity]] and synchronization are important, such as during [[Incremental Synchronization]]. When using [[incremental synchronization]] modes, fully deleted records from a source system are not replicated. To handle this, a field can be added to each record to indicate whether it should be treated as deleted. This allows the system to maintain a complete history of records while still functioning as if certain records are removed. Implementation A common way to implement..."
  },
  "software_design_patterns": {
    "title": "Software Design Patterns",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Software Design Patterns.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Database",
      "Security"
    ],
    "inlinks": [],
    "summary": "10 Design Patterns Explained in 10 Minutes Software Design Patterns What Are Software Design Patterns? Software design patterns provide reusable solutions to common software design problems. They help standardize approaches, making code easier to understand, maintain, and extend. The influential book Design Patterns by the \"Gang of Four\" categorizes design patterns into three types: Creational Patterns: Handle object creation mechanisms. Structural Patterns: Define how objects and components relate. Behavioral Patterns: Govern object communication and workflows. Using design patterns effectively can improve code quality, but excessive or incorrect use may introduce unnecessary complexity. Key Software Design Patterns Singleton Pattern The Singleton..."
  },
  "software_development_life_cycle": {
    "title": "Software Development Life Cycle",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Software Development Life Cycle.md",
    "tags": [
      "#data_orchestration"
    ],
    "aliases": [
      "sdlc"
    ],
    "outlinks": [
      "DevOps"
    ],
    "inlinks": [
      "generative_ai_from_theory_to_practice",
      "data_lifecycle_management",
      "ci-cd",
      "debugging"
    ],
    "summary": "A structured approach like the Software Development Life Cycle (SDLC) ensures ==systematic progression through various stages of development==. SDLC remains relevant today by outlining the essential stages a product must undergo to achieve success. SDLC Stages The SDLC comprises several phases, each critical to the overall development process: Planning and Analysis Purpose: Collect business and user requirements, perform cost and time estimation, and conduct scoping activities. Activities: Define what the final product must do and how it should work. This phase may include a separate requirements analysis stage. Designing Purpose: Prepare the product's architecture and design. Activities: A software architect..."
  },
  "software_development_portal": {
    "title": "Software Development Portal",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Software Development Portal.md",
    "tags": [
      "portal"
    ],
    "aliases": null,
    "outlinks": [
      "tool.uv",
      "tool.ruff",
      "Justfile",
      "TOML",
      "Makefile",
      "Json",
      "Testing",
      "Documentation & Meetings",
      "DevOps"
    ],
    "inlinks": [
      "devops",
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "Tools: - [[tool.uv]] - [[tool.ruff]] File types: - [[Justfile]] - [[TOML]] - [[Makefile]] - [[Json]] Practices: - [[Testing]] - [[Documentation & Meetings]] Related to: - [[DevOps]] dv TABLE file.name AS \"Note\", length(file.inlinks) AS \"Backlinks\" FROM \"\" SORT length(file.inlinks) DESC LIMIT 100"
  },
  "sparsecategorialcrossentropy_or_categoricalcrossentropy": {
    "title": "SparseCategorialCrossentropy or CategoricalCrossEntropy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SparseCategorialCrossentropy or CategoricalCrossEntropy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "TensorFlow",
      "loss function",
      "One-hot encoding",
      "cross entropy"
    ],
    "inlinks": [
      "neural_network_in_practice"
    ],
    "summary": "To understand the differences and use cases for SparseCategoricalCrossentropy and CategoricalCrossentropy in [[TensorFlow]], let's break down each one: CategoricalCrossentropy Use Case: This [[loss function]] is used when you have one-hot encoded labels. [[One-hot encoding]] means that each label is represented as a vector with a length equal to the number of classes, where the correct class is marked with a 1 and all other classes are marked with 0s. Example: If you have three classes, a label might look like [0, 1, 0] for class 2. Functionality: It calculates the [[cross entropy]] loss between the true labels and the predicted..."
  },
  "specificity": {
    "title": "Specificity",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Specificity.md",
    "tags": [
      "evaluation"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "confusion_matrix",
      "evaluation_metrics",
      "roc_(receiver_operating_characteristic)"
    ],
    "summary": "Specificity, also known as the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. It indicates how well the model is at identifying negative instances. Formula: $$\\text{Specificity} = \\frac{TN}{TN + FP}$$ Importance - Specificity is crucial in scenarios where it is important to correctly identify negative instances, such as in medical testing where a false positive could lead to unnecessary treatment."
  },
  "spreadsheets_vs_databases": {
    "title": "Spreadsheets vs Databases",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Spreadsheets vs Databases.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Scalability",
      "data management"
    ],
    "inlinks": [
      "database",
      "data_engineering_portal"
    ],
    "summary": "Compared to spreadsheets, databases offer: [[Scalability]]: Databases are designed to handle large volumes of data, making them suitable for applications with millions or even billions of records. In contrast, spreadsheets can become unwieldy and slow when dealing with large datasets. Update Frequency: Databases support real-time updates and continuous operations, allowing for dynamic [[data management]]. Spreadsheets, on the other hand, are more static and may require manual updates, which can lead to outdated information. Speed: Databases are optimized for quick access, retrieval, and manipulation of data. They can efficiently handle multiple concurrent users, making them ideal for environments where data is..."
  },
  "sql_groupby": {
    "title": "SQL Groupby",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL Groupby.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "SQL",
      "Querying|query",
      "SQL Groupby"
    ],
    "inlinks": [
      "sql_groupby"
    ],
    "summary": "The [[SQL]] GROUP BY clause is used to group rows that have the same values in specified columns into summary rows, like \"total sales per region\" or \"average age per department.\" It is often used in conjunction with aggregate functions such as COUNT(), SUM(), AVG(), MAX(), and MIN() to perform calculations on each group. Basic Syntax sql SELECT column1, aggregate_function(column2) FROM table_name WHERE condition GROUP BY column1; Example Usage Let's say you have a table called sales with the following columns: id: Unique identifier for each sale product: The name of the product sold amount: The sale amount region: The..."
  },
  "sql_injection": {
    "title": "SQL Injection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL Injection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development",
      "querying"
    ],
    "summary": "SQL injection is a code injection technique that targets applications using SQL databases. It occurs when a malicious user injects harmful SQL code into a query, potentially compromising the security of the database. How SQL Injection Works Consider a scenario where a website prompts users to log in with their username and password. The application might execute a query like this: sql SELECT `id` FROM `users` WHERE `user` = 'Carter' AND `password` = 'password'; If the user named Carter enters their credentials correctly, the query functions as intended. However, a malicious user could input a different string, such as: password'..."
  },
  "sql_joins": {
    "title": "SQL Joins",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL Joins.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DE_Tools",
      "Pasted image 20250323083319.png|800"
    ],
    "inlinks": [
      "database_techniques",
      "querying",
      "joining_datasets"
    ],
    "summary": "In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/Joining.ipynb ![[Pasted image 20250323083319.png|800]]"
  },
  "sql_vs_nosql": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL vs NoSQL.md",
    "tags": [
      "#question",
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "NoSQL"
    ],
    "inlinks": [],
    "summary": "[[NoSQL]]"
  },
  "sql_window_functions": {
    "title": "SQL Window functions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL Window functions.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Querying|Queries",
      "SQL Window functions"
    ],
    "inlinks": [
      "sql_window_functions"
    ],
    "summary": "SQL Window Functions are a feature in SQL that allow you to perform calculations across a set of table rows that are somehow related to the current row. Unlike regular aggregate functions, which return a single value for a group of rows, window functions return a value for each row in the result set while still allowing access to the individual row data. Key Characteristics of Window Functions Non-Aggregating: Window functions do not collapse rows into a single output row. Instead, they perform calculations across a defined \"window\" of rows related to the current row. OVER Clause: Window functions are..."
  },
  "sql": {
    "title": "What is SQL?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQL.md",
    "tags": [
      "software",
      "database",
      "query_language"
    ],
    "aliases": [],
    "outlinks": [
      "Querying",
      "Database",
      "Database Techniques"
    ],
    "inlinks": [
      "declarative",
      "pyspark",
      "sqlalchemy",
      "dbt",
      "data_engineering_tools",
      "sqlite",
      "unstructured_data",
      "google_cloud_platform",
      "sql_groupby"
    ],
    "summary": "Structured Query Language (SQL) is the standard language for interacting with relational databases, enabling efficient data [[Querying]] and manipulation. It serves as a common interface for [[Database]]s and data lakes. Features: - Declarative language for storing and querying structured data. - Transactional properties enhance speed and efficiency. Good Practices Capitalization: - Use uppercase for SQL keywords for better readability. - Use lowercase for table and column names. Quotes: - Use double quotes for SQL identifiers (table and column names). - Use single quotes for string values. Related terms [[Database Techniques]]"
  },
  "sqlalchemy_vs._sqlite3": {
    "title": "SQLAlchemy vs. sqlite3",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQLAlchemy vs. sqlite3.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "SQLAlchemy",
      "SQLite"
    ],
    "inlinks": [
      "sqlalchemy"
    ],
    "summary": "SQLAlchemy vs. sqlite3: Which One Should You Use? The choice between [[SQLAlchemy]] and [[SQLite]] depends on your specific needs. Here\u2019s a comparison based on key factors: 1. Abstraction and Ease of Use | Feature | SQLAlchemy | sqlite3 | | ----------- | ------------------------------------------ | -------------------------------- | | Abstraction | High-level ORM (Object Relational Mapping) | Low-level, direct SQL execution | | Ease of Use | Pythonic API for working with databases | Requires writing raw SQL queries | | Best for | Large projects, scalable applications | Simple scripts, small projects | \u2705 Use SQLAlchemy if you want to work..."
  },
  "sqlalchemy": {
    "title": "SQLAlchemy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQLAlchemy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "SQL",
      "SQLAlchemy vs. sqlite3"
    ],
    "inlinks": [
      "sqlalchemy_vs._sqlite3"
    ],
    "summary": "What is SQLAlchemy? SQLAlchemy is a Python SQL toolkit and ==Object Relational Mapper== (ORM) that provides tools to interact with databases in a more Pythonic way. It allows you to work with relational databases such as MySQL, PostgreSQL, SQLite, and others without writing raw [[SQL]] queries manually. [[SQLAlchemy vs. sqlite3]] Key Features of SQLAlchemy Database Connectivity Provides a unified interface to connect to different databases. Uses connection strings to establish a database connection. Example: ```python from sqlalchemy import create_engine engine = create_engine('mysql+pymysql://user:password@localhost:3306/database_name') ``` SQL Query Execution Allows execution of raw SQL queries using Pandas: python import pandas as pd df..."
  },
  "sqlite_studio": {
    "title": "SQLite Studio",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQLite Studio.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "sqlite"
    ],
    "summary": ""
  },
  "sqlite": {
    "title": "SQLite",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SQLite.md",
    "tags": [
      "database_management"
    ],
    "aliases": null,
    "outlinks": [
      "Database Management System (DBMS)|DBMS",
      "SQLite Studio",
      "Querying",
      "Concurrency",
      "SQL"
    ],
    "inlinks": [
      "views",
      "duckdb_vs_sqlite",
      "transaction",
      "database_management_system_(dbms)",
      "sqlalchemy_vs._sqlite3",
      "data_storage"
    ],
    "summary": "Lightweight [[Database Management System (DBMS)|DBMS]] used in various applications (phone apps, desktop apps, websites). Note [[SQLite Studio]] exists To get in terminal enter: sqlite3 database.db Related notes: - [[Querying]] - [[Concurrency]] - [[SQL]]"
  },
  "stacking": {
    "title": "Stacking",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Stacking.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Stacking",
      "Model Ensemble"
    ],
    "inlinks": [
      "stacking",
      "model_ensemble"
    ],
    "summary": "What is [[Stacking]]?;; is an [[Model Ensemble]] combines predictions of multiple base models ==by training a meta-model== on the outputs of the base models."
  },
  "standard_deviation": {
    "title": "Standard deviation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Standard deviation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Variance",
      "interpretability|interpretable",
      "Distributions|Distribution",
      "standardised/Outliers"
    ],
    "inlinks": [
      "t-test",
      "eda",
      "z-test"
    ],
    "summary": "Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a set of data values. It indicates how much individual data points deviate from the mean (average) of the dataset. Formula For a dataset with $n$ observations $X_1, X_2, \\ldots, X_n$, the standard deviation $\\sigma$ is calculated using the formula: $$ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu)^2} $$ Where: - $\\sigma$ = standard deviation - $n$ = number of observations - $X_i$ = each individual observation - $\\mu$ = mean of the dataset, calculated as: $$ \\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$ Why Standard..."
  },
  "standardisation": {
    "title": "Standardisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Standardisation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Preprocessing|data preprocessing",
      "Data Transformation",
      "principal component analysis",
      "K-nearest neighbours|KNN",
      "Gradient Descent",
      "Gaussian Distribution"
    ],
    "inlinks": [
      "statistical_tests",
      "normalisation_vs_standardisation",
      "normalisation",
      "anomaly_detection",
      "feature_scaling"
    ],
    "summary": "Standardisation is a [[Preprocessing|data preprocessing]] technique used to [[Data Transformation]] features so that they have a mean of 0 and a standard deviation of 1. Centers data with zero mean and unit variance, suitable for algorithms sensitive to variance. Definition: Standardisation involves rescaling the features of your data so that they have a mean of 0 and a standard deviation of 1. This is achieved by subtracting the mean of the feature from each data point and then dividing by the standard deviation. Purpose: - Useful for algorithms that assume the data is normally distributed. - Uniformity: It helps in..."
  },
  "star_schema": {
    "title": "Star Schema",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Star Schema.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Star Schema"
    ],
    "inlinks": [
      "star_schema",
      "dimension_table",
      "types_of_database_schema",
      "snowflake_schema",
      "dimensional_modelling"
    ],
    "summary": "Star Schema - This schema consists of a central fact table surrounded by dimension tables. The fact table contains quantitative data, while the dimension tables provide descriptive attributes. The star schema is easy to understand and query, making it popular for OLAP applications. [[Star Schema]] - Description: A simple and widely used form of dimensional modeling where a central fact table is connected to multiple dimension tables. - Advantages: Easy to understand and query, with straightforward joins between fact and dimension tables."
  },
  "statistical_assumptions": {
    "title": "Statistical Assumptions",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Statistical Assumptions.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Data Analysis",
      "Interpretability",
      "Assumption of Normality",
      "Statistical Tests",
      "hypothesis testing",
      "Logistic Regression",
      "Statistical Tests"
    ],
    "inlinks": [
      "statistical_tests",
      "parametric_vs_non-parametric_tests",
      "statistics"
    ],
    "summary": "Statistical assumptions are essential conditions that must be met for various statistical methods and models to produce valid results. Necessary for robustness and reliability of statistical analyses. If any assumptions are violated, it may be necessary to employ alternative statistical methods or to transform the data accordingly. Purpose of Statistical Assumptions: - [[Data Analysis]]: Ensures that the chosen statistical methods are appropriate for the data. - [[Interpretability]]: Facilitates accurate interpretation of results and conclusions drawn from analyses. Key Assumptions: [[Assumption of Normality]]: This assumption posits that the data follows a normal distribution. Many [[Statistical Tests]], such as t-tests and ANOVA,..."
  },
  "statistical_tests": {
    "title": "Statistical Tests",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Statistical Tests.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Statistical Assumptions",
      "Z-Test",
      "T-test",
      "Chi-Squared Test",
      "Proportion Test",
      "Hypothesis testing",
      "estimator",
      "Standardisation"
    ],
    "inlinks": [
      "statistics",
      "statistical_assumptions"
    ],
    "summary": "Statistical tests are methods used to determine if there is a significant difference between groups or if a relationship exists between variables. Each test has its specific [[Statistical Assumptions]] and applications. Types of Statistical Tests [[Z-Test]] [[T-test]] [[Chi-Squared Test]] [[Proportion Test]] Test Statistics For each statistical test, a test statistic is calculated. This statistic measures the degree of deviation from the null hypothesis ([[Hypothesis testing]]). The [[estimator]] is centered by the population mean, and then it is divided by the population standard deviation, a process known as [[Standardisation]]."
  },
  "statistics": {
    "title": "Statistics",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Statistics.md",
    "tags": [
      "statistics",
      "portal"
    ],
    "aliases": [],
    "outlinks": [
      "Statistical Assumptions",
      "Type 1 error and Power",
      "Distributions",
      "Statistical Tests",
      "Monte Carlo Simulation",
      "Logistic Regression",
      "Proportional Hazard Model",
      "Hypothesis testing",
      "p values",
      "Confidence Interval",
      "Central Limit Theorem",
      "Correlation vs Causation",
      "Markov chain",
      "parametric vs non-parametric tests",
      "Multicollinearity",
      "univariate vs multivariate",
      "R",
      "tidyverse",
      "Over parameterised models",
      "Casual Inference",
      "Bootstrap",
      "Adaptive decision analysis",
      "Maximum Likelihood Estimation",
      "Expectation Maximisation Algorithm",
      "Likelihood ratio",
      "Type 1 error and Power",
      "T-test",
      "Estimator"
    ],
    "inlinks": [
      "data_science",
      "data_analyst",
      "hypothesis_testing",
      "parametric_vs_non-parametric_models",
      "ds_&_ml_portal",
      "covariance_structures",
      "data_analysis"
    ],
    "summary": "Statistics want to understand the world. The world is made of probabilities, we model probabilities with functions, and we model functions with parameters. \"Observe data and construct models, infer and refine hypotheses \" Portal for all statistics notes: Statistical theorems: - Asymptotic Theorem: Law of large numbers: Sample mean approaches the population mean. - finite mean assumption [[Statistical Assumptions]] [[Type 1 error and Power]] [[Distributions]] [[Statistical Tests]] [[Monte Carlo Simulation]] [[Logistic Regression]]: model how change and covariance influence the odds of an event [[Proportional Hazard Model]]: time to an event [[Hypothesis testing]] [[p values]] [[Confidence Interval]] [[Central Limit Theorem]] [[Correlation..."
  },
  "stemming": {
    "title": "Stemming",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Stemming.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "normalisation_of_text"
    ],
    "summary": "Shorting words to the key term."
  },
  "stochastic_gradient_descent": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Stochastic Gradient Descent.md",
    "tags": [],
    "aliases": [
      "SGD"
    ],
    "outlinks": [
      "Gradient Descent"
    ],
    "inlinks": [
      "pytorch",
      "optimisation_techniques",
      "gradient_descent"
    ],
    "summary": "[[Gradient Descent]]"
  },
  "storage_layer_object_store": {
    "title": "What is a Storage Layer / Object Store?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\storage layer object store.md",
    "tags": [
      "data_storage"
    ],
    "aliases": [
      "Object Store"
    ],
    "outlinks": [
      "Cloud Providers",
      "S3 bucket"
    ],
    "inlinks": [],
    "summary": "A storage layer or object storage are services from the three big [[Cloud Providers]], AWS S3,[[S3 bucket]] Azure Blob Storage, and Google Cloud Storage. The web user interface is easy to use. Its features are very basic, where, in fact, these object stores store distributed files exceptionally well. They are also highly configurable, with solid security and reliability built-in."
  },
  "stored_procedures": {
    "title": "Stored Procedures",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Stored Procedures.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": "Stored procedures are a way to automate SQL statements, allowing them to be executed repeatedly without rewriting the code. Demonstration with the Boston MFA Database We will use the Boston MFA database to illustrate stored procedures. Previously, we implemented a soft-delete feature for the collections table using views. Now, we will create a stored procedure to achieve similar functionality. Select the Database: sql USE `mfa`; Add a Deleted Column: The deleted column needs to be added to the collections table to track soft deletions. sql ALTER TABLE `collections` ADD COLUMN `deleted` TINYINT DEFAULT 0; The TINYINT type is appropriate since..."
  },
  "strongly_vs_weakly_typed_language": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Strongly vs Weakly typed language.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Java",
      "JavaScript"
    ],
    "inlinks": [],
    "summary": "A strongly typed programming language is one where ==types== are strictly enforced. This means that once a variable is assigned a type, it cannot be implicitly converted to another type without an explicit conversion. The goal is to ==minimize errors related to incorrect type handling,== as the compiler or interpreter will detect type mismatches early in the development process. Characteristics of a Strongly Typed Language: Type Enforcement: The language does not allow operations between incompatible types (e.g., trying to add a string to an integer). Explicit Conversions: If you need to change the type of a variable, you must explicitly..."
  },
  "structured_data": {
    "title": "What is structured data?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\structured data.md",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "Database Schema",
      "Database",
      "JSON",
      "XML"
    ],
    "inlinks": [
      "semi-structured_data",
      "database",
      "data_lake",
      "named_entity_recognition",
      "data_engineering_portal"
    ],
    "summary": "Structured data refers to data that has been formatted into a well-defined schema ([[Database Schema]]). An example would be data that is stored with precisely defined columns in a relational [[Database]] or excel spreadsheet. Examples of ==structured fields could be age, name, phone number, credit card numbers or address.== Storing data in a structured format allows it to be easily understood and queried by machines and with tools such as SQL. Example of structure data Below is an example of structured data as it would appear in a database: | | age| name| phone| |---------|-----|------|-----| |Record 1| 29 | Bob..."
  },
  "structuring_and_organizing_data": {
    "title": "Structuring and organizing data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Structuring and organizing data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DE_Tools",
      "Multi-level index"
    ],
    "inlinks": [
      "data_transformation"
    ],
    "summary": "Structuring and organizing data. In [[DE_Tools]] see: - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/multi_index.ipynb - https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb Related terms: - [[Multi-level index]]"
  },
  "summarisation": {
    "title": "Summarisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Summarisation.md",
    "tags": [
      "NLP"
    ],
    "aliases": [],
    "outlinks": [
      "Extraction",
      "Abstraction"
    ],
    "inlinks": [
      "nlp",
      "bert"
    ],
    "summary": "Summarization in NLP Summarization in natural language processing (NLP) is the process of condensing a text document into a shorter version while retaining its main ideas and key information. There are two primary forms of summarization: The unsupervised summarization process involves ==splitting text, tokenizing sentences, assigning scores based on importance, and selecting top sentences==. Effective scoring methods include calculating sentence ==similarity== and analyzing ==word frequencies== to ensure that the summary captures the essence of the original text. [[Extraction]]: - This method involves selecting specific words or sentences directly from the original text to create a summary. It focuses on identifying..."
  },
  "supervised_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Supervised Learning.md",
    "tags": [
      "field"
    ],
    "aliases": null,
    "outlinks": [
      "Machine Learning Algorithms",
      "K-nearest neighbours",
      "Naive Bayes",
      "Decision Tree",
      "Linear Regression",
      "Support Vector Machines",
      "Classification",
      "Regression",
      "Pasted image 20241012152141.png"
    ],
    "inlinks": [
      "regression",
      "active_learning",
      "decision_tree",
      "backpropagation",
      "classification",
      "machine_learning_algorithms",
      "data_transformation_in_machine_learning",
      "transfer_learning",
      "ds_&_ml_portal",
      "feed_forward_neural_network"
    ],
    "summary": "Supervised learning is a type of machine learning where an algorithm learns from ==labeled data== to make predictions or decisions. In supervised learning, the training data consists of input-output pairs, where each input (features) is associated with a known output (label or target). The algorithm's goal is to learn a mapping from the input to the output so that it can predict the output for new, unseen data. Examples of Supervised [[Machine Learning Algorithms]]: [[K-nearest neighbours]] (KNN) [[Naive Bayes]] [[Decision Tree]] [[Linear Regression]] [[Support Vector Machines]] (SVM) Key Characteristics of Supervised Learning: Labeled Data: The training dataset contains both the..."
  },
  "support_vector_classifier_(svc)": {
    "title": "Support Vector Classifier (SVC)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Support Vector Classifier (SVC).md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Support Vector Machines|SVM"
    ],
    "inlinks": [],
    "summary": "Overview Support Vector Classifiers (SVCs) are a fundamental component of [[Support Vector Machines|SVM]]s, designed to find the optimal hyperplane that separates data into distinct classes. The primary objective of an SVC is to maximize the margin between different classes, ensuring that the separation is as clear as possible. Key Concepts Hyperplane: A decision boundary that separates different classes in the feature space. In a two-dimensional space, this is a line; in higher dimensions, it becomes a plane or hyperplane. Support Vectors: The data points that are closest to the hyperplane. These points are critical as they define the position and..."
  },
  "support_vector_machines": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Support Vector Machines.md",
    "tags": [
      "classifier",
      "clustering"
    ],
    "aliases": [
      "SVM"
    ],
    "outlinks": [
      "classification",
      "Kernelling",
      "ML_Tools",
      "SVM_Example.py",
      "Pasted image 20240128193726.png|700",
      "Pasted image 20240128193838.png|700"
    ],
    "inlinks": [
      "supervised_learning",
      "model_parameters",
      "unsupervised_learning",
      "logistic_regression",
      "classification",
      "machine_learning_algorithms",
      "parametric_vs_non-parametric_models",
      "ds_&_ml_portal"
    ],
    "summary": "Support Vector Machines (SVM) are a type of supervised learning algorithm primarily used for [[classification]] tasks, though they can also be adapted for regression. The main idea is to find an optimal hyperplane that divides data into different classes by maximizing the margin between them. The support vectors are the data points closest to the hyperplane, influencing its position and orientation. Key Features - Hyperplane: Finds a hyperplane that maximizes the margin between classes. - High-Dimensional Spaces: Robust in high-dimensional spaces, such as image and text classification. Advantages - Highly effective for high-dimensional data (datasets with many features). - Useful..."
  },
  "support_vector_regression": {
    "title": "Support Vector Regression",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Support Vector Regression.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Support Vector Machines|SVM"
    ],
    "inlinks": [
      "machine_learning_algorithms"
    ],
    "summary": "Support Vector Regression use similar principles to [[Support Vector Machines|SVM]]s but for predicting continuous variables."
  },
  "svm_example.py": {
    "title": "SVM_Example.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\SVM_Example.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "support_vector_machines"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main\\Explorations/Build/Classifiers/SVM/SVM_Example.py Overview Objective: To classify Iris flowers using SVM and explore various hyperparameters like kernel type, regularization (C), and gamma. Dataset: The Iris dataset contains information about sepal and petal dimensions for three flower species. To explore the effect of soft boundaries in SVMs, you can adjust the regularization parameter CCC. A smaller CCC allows a softer boundary (more margin violations), prioritizing generalization. A larger CCC enforces a harder boundary with fewer margin violations, but may lead to overfitting. Here's an extended version of the script to include this exploration: Steps in the Script 1. Data Loading and Preparation The..."
  },
  "symbolic_computation": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Symbolic computation.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "Mathematical Reasoning in Transformers",
      "Mathematica"
    ],
    "inlinks": [],
    "summary": "[[Mathematical Reasoning in Transformers]] Summary of Wolfram Alpha\u2019s Approach: Uses symbolic computation with precise algorithms. Leverages predefined mathematical rules for various domains. Provides step-by-step solutions to explain problem-solving processes. Handles natural language inputs and translates them into mathematical expressions. Produces both exact and numerical solutions, depending on the problem type. Visualizes results with graphs and interactive displays. Accesses a curated knowledge base for real-world data integration. Example: Wolfram alpha Wolfram Alpha is designed specifically for symbolic computation and uses rule-based algorithms to perform exact and precise mathematical operations. Here's an overview of how Wolfram Alpha processes math problems: 1. Symbolic..."
  },
  "sympy": {
    "title": "Sympy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Sympy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "backpropagation"
    ],
    "summary": ""
  },
  "syntactic_relationships": {
    "title": "syntactic relationships",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\syntactic relationships.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Semantic relationships"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "Syntactic relationships refer to the structural connections between words or phrases in a sentence, focusing on grammar and the arrangement of words. They determine how words combine to form phrases, clauses, and sentences, following the rules of syntax. [[Semantic relationships]], on the other hand, deal with the meaning and interpretation of words and phrases. They focus on how words relate to each other in terms of meaning, such as synonyms, antonyms, and hierarchical relationships like hypernyms and hyponyms. The key difference is that syntactic relationships are concerned with the form and structure of language, while semantic relationships are concerned with..."
  },
  "t-sne": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\t-SNE.md",
    "tags": [
      "data_visualization",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "Dimensionality Reduction",
      "Principal Component Analysis|PCA",
      "t-SNE",
      "Pasted image 20241015211844.png"
    ],
    "inlinks": [
      "t-sne",
      "dimensionality_reduction",
      "vector_embedding",
      "principal_component_analysis"
    ],
    "summary": "t-SNE (t-distributed Stochastic Neighbor Embedding) is a [[Dimensionality Reduction]] technique used primarily for visualizing high-dimensional data. Unlike methods such as [[Principal Component Analysis|PCA]] (Principal Component Analysis), which are linear, t-SNE is a non-linear method that excels at preserving the local structure of the data. Key Characteristics of t-SNE: Non-linear Mapping: It attempts to capture non-linear relationships in the data by embedding it in a lower-dimensional space (usually 2D or 3D). Local Similarities: t-SNE preserves the local structure of the data. This means that points that are close in the high-dimensional space remain close in the lower-dimensional space. Global Structure: t-SNE..."
  },
  "t-test": {
    "title": "T-test",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\T-test.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard deviation",
      "Variance",
      "Distributions|distribution",
      "hypothesis testing"
    ],
    "inlinks": [
      "statistical_tests",
      "general_linear_regression",
      "statistics"
    ],
    "summary": "The T-test is a statistical method ==used to determine if there is a significant difference between the means of two groups, especially when the population [[standard deviation]] is unknown.== It is particularly useful when dealing with small sample sizes. Types of T-tests One-Sample T-test: This test compares the mean of a single sample to a known value (often the population mean). It helps determine if the sample mean significantly differs from the population mean. Two-Sample T-test: This test compares the means of two independent samples. It can be further categorized into: Two-Sample T-test with Known [[Variance]]: Used when the variances..."
  },
  "tableau": {
    "title": "Tableau",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Tableau.md",
    "tags": [
      "data_visualization"
    ],
    "aliases": null,
    "outlinks": [
      "PostgreSQL",
      "Data Visualisation"
    ],
    "inlinks": [
      "semantic_layer",
      "postgresql",
      "data_visualisation"
    ],
    "summary": "Next Steps Load a [[PostgreSQL]] database and perform analytics as an example. Resources Tableau How-To Videos Tutorial Link Example Usage Video Features Can publish to blogs and embed dashboards online ([[Data Visualisation]]) Dashboards can be shared online. Easier than doing visualizations in Python."
  },
  "technical_debt": {
    "title": "Technical Debt",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Technical Debt.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": "Technical debt refers to the concept in software development where developers take shortcuts or make suboptimal decisions to spped up the delivery of a project. These shortcuts can lead to increased complexity and potential issues in the codebase, which may require additional effort to address in the future. Just like financial debt, technical debt incurs \"interest,\" meaning that the longer it remains unaddressed, the more costly it becomes to fix. How Can Businesses Reduce Technical Debt? Automate Testing and Code Quality Checks: Implement automated tests to ensure code quality and catch issues early. Tools like RUFF, mypy, and fixit can..."
  },
  "technical_design_doc_template": {
    "title": "Technical Design Doc Template",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Technical Design Doc Template.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "documentation_&_meetings"
    ],
    "summary": "[Project name] Design Doc About this doc Metadata about this document. Describe the scope and current status. This doc describes the technical approach, milestones, and work planned for the [Project name linked to Product Requirements Doc] | | | |---|---| |Sign off deadline|Date| |Status|Draft| |Author(s)|Name 1, Name 2| Sign offs Name 1 Name 2 Add your name here to sign off Context A sentence or two on the \u201cwhat.\u201d What is being built. Then include the \u201cwhy\u201d (metrics we intend for). Link off to Product Requirements Doc for details. Non-goals What is out of scope for this project that we..."
  },
  "telecommunications": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Telecommunications.md",
    "tags": null,
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": "Network Optimization Overview: In telecommunications, RL is used to enhance network performance, optimize resource allocation, and manage traffic efficiently. Applications: Traffic Management: RL algorithms can analyze real-time network traffic to optimize routing and minimize congestion, ensuring that data packets are transmitted through the least congested paths. Quality of Service (QoS): RL can be used to allocate bandwidth dynamically based on current demand and service-level agreements (SLAs), improving user experience by maintaining high QoS standards. Fault Detection and Recovery: RL systems can learn to identify and respond to network anomalies or failures, automatically rerouting traffic or reallocating resources to maintain service..."
  },
  "tensorflow": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Tensorflow.md",
    "tags": [
      "deep_learning",
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Machine Learning",
      "Handwritten Digit Classification",
      "Pytorch vs Tensorflow"
    ],
    "inlinks": [
      "pytorch_vs_tensorflow",
      "pytorch",
      "deep_learning",
      "sparsecategorialcrossentropy_or_categoricalcrossentropy"
    ],
    "summary": "Open sourced by Google Based on a dataflow graph Text summarization with TensorFlow Open-source library for numerical computation and large-scale [[Machine Learning]], focusing on static dataflow graphs. Get same code use of tensorflow example: Basic example is [[Handwritten Digit Classification]] [[Pytorch vs Tensorflow]]"
  },
  "terminal_commands": {
    "title": "Terminal commands",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Terminal commands.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "jupyter nbconvert K-Means_VideoGames_Raw.ipynb --to python --no-prompt"
  },
  "test_loss_when_evaluating_models": {
    "title": "Test Loss When Evaluating Models",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Test Loss When Evaluating Models.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Evaluation",
      "Accuracy",
      "Evaluation Metrics",
      "loss function",
      "Hyperparameter Tuning",
      "Model Selection",
      "standardised/Outliers",
      "standardised/Outliers|Handling Outliers"
    ],
    "inlinks": [],
    "summary": "Test loss is used for [[Model Evaluation]] to assess how well a model generalizes to unseen data, which is essential for evaluating its performance in real-world applications. Importance of Test Loss Test Accuracy: Indicates the percentage of correct predictions. ==Test Loss: Measures the magnitude of errors in predictions, providing complementary information to accuracy.== Balancing Metrics: Depending on the application, you might prioritize [[Accuracy]] (e.g., in classification tasks) or loss (e.g., when evaluating prediction confidence or calibrating probabilistic models). Balancing both is crucial for most real-world problems. Test loss is an [[Evaluation Metrics]] that uses the [[loss function]] to measure the..."
  },
  "testing": {
    "title": "Testing",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Testing.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Model Deployment|deployment",
      "Python",
      "unittest",
      "Common Security Vulnerabilities in Software Development",
      "Continuous integration",
      "Pytest",
      "Hypothesis testing",
      "Types of Computational Bugs"
    ],
    "inlinks": [
      "maintainable_code",
      "hypothesis_testing",
      "software_development_portal",
      "debugging"
    ],
    "summary": "Testing in coding projects refers to the systematic process of evaluating software to ensure it meets specified requirements and functions correctly. It enhances software robustness, reduces maintenance costs, and improves user satisfaction. Testing is crucial for: - Identifying bugs - Ensuring code quality - Validating that the software behaves as expected under various conditions Key Insights - Testing reduces the probability of software failure, $P(\\text{failure})$, by identifying defects before [[Model Deployment|deployment]]. - Effective testing strategies can lead to a decrease in the expected cost of errors, $E(\\text{cost})$, associated with software bugs. Comprehensive Python Testing Strategy Testing a [[Python]] program effectively..."
  },
  "testing_pytest.py": {
    "title": "Testing_Pytest.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Testing_Pytest.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Testing_Pytest.py",
      "Pytest"
    ],
    "inlinks": [
      "debugging",
      "testing_pytest.py"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_Pytest.py In [[ML_Tools]] see: [[Testing_Pytest.py]] The pytest example script demonstrates several key features of the [[Pytest]] testing framework: Fixtures: The script uses a fixture named sample_data to provide common test data that can be reused across multiple test functions. This helps reduce code duplication and enhances test maintainability. Parametrization: The script employs the @pytest.mark.parametrize decorator to run a test function with multiple sets of arguments. This allows for testing a function with various inputs without writing separate test cases for each scenario. Custom Markers: A custom marker @pytest.mark.slow is used to categorize tests. This enables selective test execution based on..."
  },
  "testing_unittest.py": {
    "title": "Testing_unittest.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Testing_unittest.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pytest"
    ],
    "inlinks": [
      "debugging"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Testing_unittest.py To explore testing in Python, let's focus on some key concepts and provide a simple example using the unittest framework, which is a built-in module for writing and running tests. By writing and running tests, you can ensure that your code behaves as expected and catch bugs early in the development process. [[Pytest]] Key Concepts in Testing Unit Testing: Testing individual components or functions in isolation to ensure they work as expected. Unit tests are typically small and fast. Test-Driven Development (TDD): A development approach where tests are written before the actual code. This helps define the expected behavior..."
  },
  "text2cypher": {
    "title": "Text2Cypher",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Text2Cypher.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "GraphRAG|graph database",
      "Neo4j",
      "interpretability|interpretable",
      "Cypher"
    ],
    "inlinks": [
      "how_to_search_within_a_graph",
      "graphrag"
    ],
    "summary": "Text2Cypher is a concept that allows users to convert natural language queries into Cypher queries, which are used to interact with [[GraphRAG|graph database]] like [[Neo4j]]. This functionality enables users to ask questions in a more intuitive/[[interpretability|interpretable]], conversational manner, rather than needing to know the specific syntax of [[Cypher]]. Allows the user to ask vague questions. Allows for multihop queries on the graph Overall, Text2Cypher aims to simplify the interaction with graph databases, making it accessible to users who may not be familiar with query languages. Key Features of Text2Cypher: Natural Language Processing: It utilizes natural language processing (NLP) techniques to..."
  },
  "tf-idf": {
    "title": "TF-IDF",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\TF-IDF.md",
    "tags": [
      "NLP"
    ],
    "aliases": [],
    "outlinks": [
      "Bag of words",
      "Clustering",
      "Search",
      "Recommender systems"
    ],
    "inlinks": [
      "nlp",
      "cosine_similarity",
      "bag_of_words"
    ],
    "summary": "Short term frequency inverse document frequency Improves on [[Bag of words]] Reflects how important each word is to a document in a corpus. which takes into account both the frequency of a term in a document and the rarity of the term in the entire corpus High values mean more important TF-IDF equation: - term frequency $tf_{i,j} = \\frac{n_{i,j}}{\\sum_k n_{k,j}}$ - inverse document frequency $idf(w) = \\mbox{log} \\frac{N}{df_i}$ - term frequency\u2013inverse document frequency $w_{i,j} = tf_{i,j} \\times \\mbox{log}\\frac{N}{df_i}$ where: - $i$ - index of term - $j$ - index of document - $k$ - number of terms in document -..."
  },
  "thinking_systems": {
    "title": "Thinking Systems",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Thinking Systems.md",
    "tags": [
      "career",
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "Knowledge Work",
      "Scientific Method"
    ],
    "inlinks": [],
    "summary": "A thinking system is a point of view that helps solve a problem. Part of [[Knowledge Work]]. We view problems through the view of our own specialism (mathematics). Thinking systems help with perspective. Types of thinking systems: - Design - Engineering Design thinking Historically we trained people to use a product. users have choices, be user focused, empathy and contextual enquiry for the product/system This puts human at the center, and focus on thinking about ==pain points== of their experience. Remember: - User segments are not the same and should be handled separately. - It is important to understand how..."
  },
  "time_series_forecasting": {
    "title": "Time Series Forecasting",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Time Series Forecasting.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Time Series",
      "Forecasting_Baseline.py",
      "Forecasting_Exponential_Smoothing.py",
      "Forecasting_AutoArima.py",
      "XGBoost",
      "LightGBM"
    ],
    "inlinks": [
      "time_series",
      "forecasting_autoarima.py",
      "forecasting_exponential_smoothing.py",
      "use_cases_for_a_simple_neural_network_like"
    ],
    "summary": "With [[Time Series]] dataset we often want to predict future terms. These are methods to do so. Resources: TimeSeries Forecasting Statistical Methods [[Forecasting_Baseline.py]] [[Forecasting_Exponential_Smoothing.py]] [[Forecasting_AutoArima.py]] Machine Learning Methods [[XGBoost]] [[LightGBM]]"
  },
  "time_series_identify_trends_and_patterns": {
    "title": "Time Series Identify Trends and Patterns",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Time Series Identify Trends and Patterns.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "time_series"
    ],
    "summary": "Analyze long-term trends, seasonal patterns, and cyclical behaviors."
  },
  "time_series": {
    "title": "Time Series",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Time Series.md",
    "tags": null,
    "aliases": [],
    "outlinks": [
      "ML_Tools",
      "Time Series Forecasting",
      "Time Series Identify Trends and Patterns",
      "Anomaly Detection"
    ],
    "inlinks": [
      "anomaly_detection_in_time_series",
      "recurrent_neural_networks",
      "time_series_forecasting",
      "cross_validation",
      "datasets"
    ],
    "summary": "Time series data is a sequence of data points collected or recorded at successive points in time, typically at uniform intervals. It captures the temporal ordering of data, which is crucial for analyzing trends, patterns, and changes over time. Time series data is widely used across various domains, including: - Finance: Stock prices, interest rates, and economic indicators. - Weather Forecasting: Temperature, precipitation, and wind speed data. In [[ML_Tools]] see: TimeSeries folder \"Explorations\\Build\\TimeSeries\" What Can You Do with Time Series Data? With time series data, you can: [[Time Series Forecasting]] [[Time Series Identify Trends and Patterns]] [[Anomaly Detection]] ```python We..."
  },
  "tokenisation": {
    "title": "Tokenisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Tokenisation.md",
    "tags": [
      "NLP",
      "code_snippet"
    ],
    "aliases": [],
    "outlinks": [
      "NLP"
    ],
    "inlinks": [
      "normalisation_of_text",
      "generative_ai_from_theory_to_practice"
    ],
    "summary": "Tokenisation is a fundamental process in natural language processing ([[NLP]]) that involves breaking down text into smaller units called tokens. These tokens can be words, sentences, or ==subwords==, depending on the level of tokenization. Word tokenisation ```python from nltk.tokenize import word_tokenize #keeps punctuation text_word_tokens_nltk = word_tokenize(text_original) print(text_word_tokens_nltk) ``` Sentence tokenisation python from nltk.tokenize import sent_tokenize text_sentence_tokens_nltk = sent_tokenize(text_original) print(text_sentence_tokens_nltk) Basic tokenisation python temp = text_original.lower() temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp) # just letters and numbers temp = re.sub(r\"\\[[0-9]+\\]\", \"\", temp) #remove weird stuff temp = word_tokenize(temp) #break up text to word list tokens_no_stopwords = [token for token in temp..."
  },
  "toml": {
    "title": "TOML",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\TOML.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "tool.ruff",
      "tool.bandit",
      "tool.uv",
      "Pytest"
    ],
    "inlinks": [
      "software_development_portal",
      "tool.ruff",
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "A .toml file is a configuration file format that stands for \"Tom's Obvious, Minimal Language.\" It is designed to be easy to read due to its simple syntax. TOML files are often used for configuration because they are straightforward to parse and write for both humans and machines. The format supports basic data types like strings, integers, floats, booleans, arrays, and tables (which are similar to dictionaries or objects in other programming languages). ```toml title = \"TOML Example\" [owner] name = \"Tom Preston-Werner\" dob = 1979-05-27T07:32:00Z [database] server = \"192.168.1.1\" ports = [ 8001, 8001, 8002 ] connection_max = 5000..."
  },
  "tool.bandit": {
    "title": "tool.bandit",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\tool.bandit.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Bandit example output",
      "ML_Tools",
      "Bandit_Example_Nonfixed.py",
      "Common Security Vulnerabilities in Software Development"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development",
      "toml"
    ],
    "summary": "Bandit: A Security Linter for Python Resources Bandit Documentation How to Use Bandit Installation To install Bandit, use pip by running the following command in your terminal: bash pip install bandit Running Bandit After installation, you can run Bandit on your Python files or directories. For example, to scan a file named example.py, use: bash bandit example.py This command will analyze the file and report any security issues it finds. 3. Customizing Bandit You can customize Bandit's behavior by specifying options. For example, to scan a directory and exclude certain subdirectories, use: bash bandit -r example_directory -x example_directory/venv -r specifies..."
  },
  "tool.ruff": {
    "title": "tool.ruff",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\tool.ruff.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Python",
      "TOML"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Ruff is a fast [[Python]] linter and code formatter. It is designed to enforce coding style and catch potential errors in Python code. Ruff aims to be efficient and comprehensive, supporting a wide range of linting rules and style checks. It can be used to automatically format code to adhere to a specified style guide, making it a useful tool for maintaining consistent code quality across a project. in [[TOML]] have: ```toml [tool.ruff.format] Like Black, use double quotes for strings. quote-style = \"double\" Like Black, indent with spaces, rather than tabs. indent-style = \"space\" Like Black, respect magic trailing commas...."
  },
  "tool.uv": {
    "title": "tool.uv",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\tool.uv.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "TOML",
      "Virtual environments"
    ],
    "inlinks": [
      "software_development_portal",
      "toml"
    ],
    "summary": "Appears in [[TOML]] file Link: https://github.com/astral-sh/uv uv is a tool for managing Python development [[Virtual environments]], projects, and dependencies. It offers a range of features that streamline various aspects of Python development, from installing Python itself to managing projects and dependencies. Python Version Management: uv allows you to install, list, find, pin, and uninstall Python versions. This is useful for managing multiple Python versions across different projects, ensuring compatibility and ease of switching between environments. Script Execution: You can run standalone Python scripts and manage their dependencies directly with uv. This simplifies the process of executing scripts with specific dependencies..."
  },
  "train-dev-test_sets": {
    "title": "Train-Dev-Test Sets",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Train-Dev-Test Sets.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Building",
      "training data",
      "validation data",
      "model parameters",
      "Model Evaluation",
      "Datasets",
      "Distributions",
      "evaluation metrics",
      "Handling Different Distributions",
      "Cross Validation"
    ],
    "inlinks": [
      "model_building"
    ],
    "summary": "In [[Model Building]] train the model using the prepared data to learn patterns and make predictions. The model is trained on your dataset, which is typically divided into three main subsets: training, development (dev), and test sets. Purpose of Each Set Training Set ([[training data]]) : Used to fit the model. This is where the model learns the patterns and relationships within the data. The majority of the data is allocated here to ensure the model has enough information to learn effectively. Development (Dev) Set: Also known as the [[validation data]], it is used to tune the [[model parameters]] and..."
  },
  "transaction": {
    "title": "Transaction",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Transaction.md",
    "tags": null,
    "aliases": [
      "Transactions"
    ],
    "outlinks": [
      "Data Integrity",
      "ACID Transaction",
      "queries",
      "Transaction|Transactions",
      "Granularity",
      "SQLite",
      "Database Management System (DBMS)|DBMS",
      "Concurrency"
    ],
    "inlinks": [
      "query_optimisation",
      "acid_transaction"
    ],
    "summary": "Transactions are used for maintaining [[Data Integrity]] and should adhere to the [[ACID Transaction]]. Transaction Operations Commit: Saves all changes made during the transaction. Rollback: Reverts the database to its previous state if an error occurs during the transaction. Concurrency and Transactions Concurrency: Allows multiple [[queries]] to run simultaneously, essential for high-traffic applications. Race Conditions: Occur when concurrent transactions access and modify shared data, potentially causing inconsistencies. Transaction Locks To prevent ==race conditions,== transactions and locking mechanisms are employed to ensure that operations occur sequentially. Locks manage access to the database during [[Transaction|Transactions]]: UNLOCK: Allows anyone to read or add..."
  },
  "transfer_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Transfer Learning.md",
    "tags": [
      "model_algorithm"
    ],
    "aliases": null,
    "outlinks": [
      "Hugging Face",
      "Keras",
      "PyTorch",
      "Supervised Learning",
      "Performance Drift",
      "ML_Tools",
      "transfer_learning.py"
    ],
    "inlinks": [
      "transfer_learning.py",
      "llm",
      "bert",
      "imbalanced_datasets",
      "deep_learning"
    ],
    "summary": "Transfer learning is a technique in machine learning that ==leverages knowledge gained from one setting (source domain) to improve performance on a different but related setting (target domain).== The core idea is to train a model on a large dataset in the source domain, learning rich feature representations that capture general patterns and relationships in the data.== These learned representations can then be transferred to the target domain, where they can be fine-tuned with a smaller dataset to achieve good performance on the target task. Transfer learning makes sense when: - It makes sense to do when there is lots..."
  },
  "transfer_learning.py": {
    "title": "transfer_learning.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\transfer_learning.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Transfer Learning"
    ],
    "inlinks": [
      "transfer_learning"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Neural_Network/transfer_learning.py For deep learning, to do [[Transfer Learning]] we take out and replace a few end layers of the network. We can then train just the last layer of weights of a neural network. The number of layers to remove and then added from pretrained depends on the similarity between tasks. Higher layers in networks are able to recognise higher detail components."
  },
  "transformed_target_regressor": {
    "title": "Transformed Target Regressor",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Transformed Target Regressor.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Sklearn"
    ],
    "inlinks": [],
    "summary": "[[Sklearn]] The TransformedTargetRegressor is a utility class in scikit-learn that applies a transformation to the target values in a regression problem. This can be useful in several scenarios: Non-normal target distribution: Many regression algorithms assume that the target variable is normally distributed. If your target variable has a skewed distribution, applying a transformation (like a log transformation) can help improve the performance of the model. Heteroscedasticity: This is a situation where the variance of the error terms in a regression model is not constant. In such cases, applying a transformation to the target variable can help stabilize the variance and..."
  },
  "transformer": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Transformer.md",
    "tags": [
      "deep_learning",
      "NLP"
    ],
    "aliases": [
      "Transformers"
    ],
    "outlinks": [
      "NLP",
      "standardised/Attention Is All You Need",
      "Attention mechanism",
      "BERT",
      "Feed Forward Neural Network",
      "Attention Mechanism",
      "Multi-head attention",
      "Recurrent Neural Networks|RNNs",
      "Unsupervised Learning|unsupervised",
      "Recurrent Neural Networks",
      "Transformers vs RNNs"
    ],
    "inlinks": [
      "hugging_face",
      "llm",
      "rag",
      "bert",
      "lstm",
      "types_of_neural_networks",
      "ds_&_ml_portal",
      "transformers_vs_rnns"
    ],
    "summary": "A transformer in machine learning (ML) refers to a deep learning model architecture designed to process sequential data, such as natural language processing ([[NLP]]). It was introduced in the paper \"[[standardised/Attention Is All You Need]]\" and has since become a cornerstone in NLP tasks. Transformers excel at handling sequence-based data and are particularly known for their self-attention mechanisms [[Attention mechanism]], which allow them to process long-range dependencies in data. Key Concepts of a Transformer Architecture Overview: A transformer model consists of an encoder and a decoder, although some models use only the encoder (like [[BERT]] only consists of encoders) or..."
  },
  "transformers_vs_rnns": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Transformers vs RNNs.md",
    "tags": [
      "deep_learning"
    ],
    "aliases": null,
    "outlinks": [
      "Transformer|Transformers",
      "Recurrent Neural Networks",
      "LSTM",
      "Gated Recurrent Units",
      "Transformer",
      "vanishing and exploding gradients problem",
      "Attention mechanism",
      "Longformer",
      "Reformer",
      "time-series data",
      "BERT",
      "BERT",
      "Recurrent Neural Networks|RNNs",
      "Attention Mechanism",
      "Transformers vs RNNs"
    ],
    "inlinks": [
      "transformers_vs_rnns",
      "transformer"
    ],
    "summary": "[[Transformer|Transformers]] and Recurrent Neural Networks ([[Recurrent Neural Networks]]) are both deep learning architectures ==used for processing sequential data==, but they differ significantly in structure, operation, and performance. While RNNs have been essential for sequence modeling, transformers have become the dominant architecture in ML due to their ability to handle large-scale data and long-range dependencies more efficiently. RNNs still have use cases, especially for tasks where memory constraints are critical ==or for smaller datasets==, but transformers are the go-to solution for most modern ML applications. Summary Table: | Aspect | RNNs | Transformers | | --------------------------- | ---------------------------------- | -------------------------------- |..."
  },
  "ts_anomaly_detection": {
    "title": "TS_Anomaly_Detection",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\TS_Anomaly_Detection.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "ts_anomaly_detection.py": {
    "title": "TS_Anomaly_Detection.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\TS_Anomaly_Detection.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "anomaly_detection_in_time_series"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/TS_Anomaly_Detection.py"
  },
  "turning_a_flat_file_into_a_database": {
    "title": "Turning a flat file into a database",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Turning a flat file into a database.md",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "Foreign Key",
      "'customer_id', 'customer_name', 'contact_name', 'country'",
      "'order_id', 'order_date', 'customer_id', 'amount'"
    ],
    "inlinks": [
      "database",
      "data_engineering_portal",
      "melt"
    ],
    "summary": "Summary: Read and Clean the Data: Load the data from the Excel sheet and clean it. Split the Data: Separate the data into two DataFrames, one for customers and one for orders. Create Tables: Create the SQLite tables with appropriate foreign key relationships. Insert Data: Insert the cleaned and separated data into the respective tables. Verify [[Foreign Key]]: Ensure that the foreign key relationships are valid. Example Data Structure Combined Excel Data (Sheet1): | order_id | order_date | customer_id | customer_name | contact_name | country | amount | |----------|------------|-------------|---------------|--------------|---------|--------| | 1 | 2024-01-15 | 101 | John Doe | Jane..."
  },
  "types_of_computational_bugs": {
    "title": "Types of Computational Bugs",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Types of Computational Bugs.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Debugging"
    ],
    "inlinks": [
      "testing",
      "debugging"
    ],
    "summary": "Each of these types of bugs can have significant impacts on software functionality and performance, and understanding them is crucial for effective [[Debugging]] and software development. Types of Computational Bugs Cumulative Rounding Error: This occurs when small rounding errors accumulate over time, potentially leading to significant inaccuracies. An example is the Vancouver Stock Exchange issue. Cascades: These are bugs that trigger a series of failures or errors in a system. Integer Overflow: This happens when an arithmetic operation attempts to create a numeric value that is outside the range that can be represented with a given number of bits. Backend..."
  },
  "types_of_database_schema": {
    "title": "Types of Database Schema",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Types of Database Schema.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Star Schema",
      "Snowflake Schema",
      "Normalised Schema",
      "ER Diagrams",
      "Columnar Storage"
    ],
    "inlinks": [
      "database_schema"
    ],
    "summary": "There are several types of database schemas commonly used in data warehousing and database design. [[Star Schema]] [[Snowflake Schema]] Galaxy Schema (or Fact Constellation Schema): - This schema consists of multiple fact tables that share dimension tables. It is useful for complex data models that require analysis across different business processes. The galaxy schema allows for more flexibility in querying and reporting. [[Normalised Schema]] Denormalized Schema: - A denormalized schema combines data from multiple tables into fewer tables to improve query performance. This approach is often used in data marts and data warehouses where read performance is prioritized over write..."
  },
  "types_of_neural_networks": {
    "title": "Types of Neural Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Types of Neural Networks.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Neural network",
      "Feed Forward Neural Network",
      "Convolutional Neural Networks",
      "Recurrent Neural Networks",
      "Generative Adversarial Networks",
      "Transformer"
    ],
    "inlinks": [
      "neural_network"
    ],
    "summary": "Types of [[Neural network]]: [[Feed Forward Neural Network]] [[Convolutional Neural Networks]] [[Recurrent Neural Networks]] [[Generative Adversarial Networks]] [[Transformer]]"
  },
  "typescript": {
    "title": "TypeScript",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\TypeScript.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "data_validation",
      "debugging"
    ],
    "summary": "Superset of JavaScript adding static typing and object-oriented features for building large-scale applications."
  },
  "typical_output_formats_in_neural_networks": {
    "title": "Typical Output Formats in Neural Networks",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Typical Output Formats in Neural Networks.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Neural network",
      "Binary Classification",
      "Classification",
      "Regression",
      "Generative AI",
      "Activation Function",
      "loss function"
    ],
    "inlinks": [],
    "summary": "The output format of a [[Neural network]] is largely determined by the specific task it is designed to perform. Classification [[Binary Classification]] Single Output Node: This involves a single output node with a value between 0 and 1, representing the probability of the input belonging to the positive class. Example: A spam classifier might output a value close to 1 for a spam email and a value close to 0 for a legitimate email. Multiclass [[Classification]] Multiple Output Nodes: Each class has its own output node, with values typically between 0 and 1, representing the probability of the input belonging..."
  },
  "ubuntu": {
    "title": "Ubuntu",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Ubuntu.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Linux"
    ],
    "inlinks": [
      "windows_subsystem_for_linux"
    ],
    "summary": "Ubuntu is a popular open-source operating system based on the [[Linux]] kernel. It is designed to be user-friendly: Desktop Environment: Ubuntu provides a graphical user interface (GUI) that makes it accessible to users who may not be familiar with command-line interfaces. It is often used as a desktop operating system for personal computers. Server Use: Ubuntu Server is a version of Ubuntu designed for server environments. It is commonly used for hosting websites, applications, and databases due to its stability and security. Development: Many developers use Ubuntu for software development because it supports a wide range of programming languages and..."
  },
  "uml": {
    "title": "UML",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\UML.md",
    "tags": [
      "data_modeling"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": "https://www.drawio.com/ https://www.reddit.com/r/SoftwareEngineering/comments/133iw7n/is_there_any_free_handy_tool_to_create_uml/ https://plantuml.com/"
  },
  "unittest": {
    "title": "unittest",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\unittest.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [
      "testing"
    ],
    "summary": "@patch (from unittest.mock) Explanation @patch is used to replace objects/functions with mock versions during tests. It is part of Python\u2019s unittest.mock module. Example & Usage python Copy code from unittest.mock import patch def fetch_data(): \"\"\"Simulated function that fetches data from an API\"\"\" return \"Real Data\" @patch(\"__main__.fetch_data\", return_value=\"Mocked Data\") def test_fetch_data(mock_fetch): assert fetch_data() == \"Mocked Data\" \ud83d\udd39 How it works: @patch(\"__main__.fetch_data\", return_value=\"Mocked Data\") replaces fetch_data() with a mocked version returning \"Mocked Data\". Inside the test, fetch_data() will always return \"Mocked Data\" instead of calling the real function. Why use @patch? Prevents tests from making actual API/database calls. Speeds up testing by..."
  },
  "univariate_vs_multivariate": {
    "title": "univariate vs multivariate",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\univariate vs multivariate.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "statistics"
    ],
    "summary": "Single feature versus multiple features"
  },
  "unstructured_data": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\unstructured data.md",
    "tags": [
      "data_modeling",
      "data_storage"
    ],
    "aliases": [],
    "outlinks": [
      "SQL"
    ],
    "inlinks": [
      "data_lake",
      "vector_database",
      "data_science"
    ],
    "summary": "[!Important] Unstructured data is data that does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used by programs, and is difficult to analyze. Examples of unstructured data could be the contents of an ==email, contents of a word document, data from social media, photos, videos, survey results==, etc. An example of unstructured data An simple example of unstructured data is a string that contains interesting information inside of it, but that has not been formatted into a well defined schema. An example is given below: | | UnstructuredString| |---------| -----------| |Record..."
  },
  "unsupervised_learning": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Unsupervised Learning.md",
    "tags": [
      "#clustering",
      "field"
    ],
    "aliases": [
      "unsupervised"
    ],
    "outlinks": [
      "Clustering",
      "Isolated Forest",
      "Clustering",
      "K-means",
      "DBScan",
      "Support Vector Machines",
      "K-nearest neighbours",
      "Dimensionality Reduction",
      "Principal Component Analysis",
      "Isolated Forest",
      "standardised/Outliers",
      "standardised/Outliers|anomalies"
    ],
    "inlinks": [
      "learning_styles",
      "machine_learning_algorithms",
      "principal_component_analysis",
      "k-means",
      "ds_&_ml_portal",
      "clustering"
    ],
    "summary": "Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or predefined outputs. Unsupervised learning involves discovering hidden patterns in data without predefined labels. It is valuable for exploratory data analysis, [[Clustering]], and [[Isolated Forest]]. The goal is to find hidden patterns, relationships, or structures in the data. Unlike supervised learning, which uses labeled input-output pairs, unsupervised learning relies solely on input data, allowing the algorithm to uncover insights independently. Key Concepts No Labeled Data: There is no ground truth or correct output associated with the input data. Data Patterns: The algorithm..."
  },
  "untitled": {
    "title": "Untitled",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Untitled.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "use_cases_for_a_simple_neural_network_like": {
    "title": "Use Cases for a Simple Neural Network Like",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Use Cases for a Simple Neural Network Like.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Neural network|Neural Network",
      "Regression",
      "Binary Classification",
      "Time Series Forecasting"
    ],
    "inlinks": [
      "pytorch"
    ],
    "summary": "Scenarios where a simple [[Neural network|Neural Network]] work like this might be useful: [[Regression]] with Multiple Features If you have multiple input features and you want to predict a continuous output, this network can learn the appropriate weights for each feature. For instance: - Predicting fuel efficiency of a car based on features like engine size, horsepower, and weight. - Predicting sales based on multiple factors like marketing spend, seasonality, and economic indicators. [[Binary Classification]] With slight modification (e.g., adding a Sigmoid activation to the output layer), you could use this network for binary classification tasks. For example: - Classifying..."
  },
  "use_of_rnns_in_energy_sector": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Use of RNNs in energy sector.md",
    "tags": [
      "time_series",
      "deep_learning",
      "energy",
      "anomaly_detection"
    ],
    "aliases": [],
    "outlinks": [
      "Machine Learning Algorithms",
      "Demand forecasting",
      "Gradient Boosting",
      "SHapley Additive exPlanations|SHAP",
      "SHapley Additive exPlanations|SHAP"
    ],
    "inlinks": [
      "recurrent_neural_networks"
    ],
    "summary": "For energy data problems, many interpretable machine learning algorithms can be applied in place of or alongside RNNs. These models offer transparency, making it easier to understand the relationships between features and predictions, which is critical in areas like energy management, where interpretability can be as important as accuracy. For each of the energy data questions that RNNs might solve, interpretable alternatives [[Machine Learning Algorithms]]: such as linear regression, decision trees, random forests, and ARIMA models can be employed. These models provide transparency by revealing which features (e.g., weather, demand) influence predictions the most, making them suitable for stakeholders who..."
  },
  "utilities": {
    "title": "Utilities",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Utilities.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "vacuum": {
    "title": "Vacuum",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vacuum.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "database_techniques"
    ],
    "summary": ""
  },
  "vanishing_and_exploding_gradients_problem": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\vanishing and exploding gradients problem.md",
    "tags": [
      "drafting"
    ],
    "aliases": null,
    "outlinks": [
      "Recurrent Neural Networks|RNN",
      "vanishing and exploding gradients problem"
    ],
    "inlinks": [
      "recurrent_neural_networks",
      "backpropagation",
      "vanishing_and_exploding_gradients_problem",
      "batch_normalisation",
      "lstm",
      "forward_propagation",
      "ds_&_ml_portal",
      "transformers_vs_rnns"
    ],
    "summary": "[[Recurrent Neural Networks|RNN]] [[vanishing and exploding gradients problem]] In standard RNNs, the difficulty lies in retaining useful information over long sequences due to the exponential decrease in the gradient values, which results in poor learning of long-term dependencies."
  },
  "variance": {
    "title": "Variance",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Variance.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Boxplot",
      "Distributions"
    ],
    "inlinks": [
      "data_reduction",
      "standard_deviation",
      "principal_component_analysis",
      "t-test",
      "regression_metrics",
      "covariance_structures",
      "feature_selection"
    ],
    "summary": "Variance in a dataset is a statistical measure that represents the degree of spread or dispersion of the data points around the mean (average) of the dataset. It quantifies how much the individual data points differ from the mean value. A higher variance indicates that the data points are more spread out from the mean, while a lower variance indicates that they are closer to the mean. Variance is calculated as the average of the squared differences between each data point and the mean. See also: [[Boxplot]] [[Distributions]] Variance: - Measures how much a single variable deviates from its mean...."
  },
  "vector_database": {
    "title": "Vector Database",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vector Database.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Vector Embedding",
      "unstructured data",
      "LLM",
      "Langchain",
      "standardised/Vector Embedding",
      "standardised/Vector Embedding",
      "Semantic Search",
      "Cosine Similarity"
    ],
    "inlinks": [
      "relationships_in_memory"
    ],
    "summary": "Overview Vector databases are specialized systems designed to handle and manage [[Vector Embedding]]. As most real-world data is unstructured, such as text, images, and audio, vector databases play a role in organizing and querying this data effectively. Why use them: - data is [[unstructured data]] i.e image - Key Features Vector Embeddings: At the core, vector databases store embeddings generated by machine learning models. These embeddings transform complex data into fixed-size vectors that encapsulate semantic information. Similarity Search: By leveraging the geometric properties of vector spaces, vector databases can quickly identify similar items. This is achieved by measuring distances (e.g.,..."
  },
  "vector_embedding": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vector Embedding.md",
    "tags": [
      "math",
      "language_models",
      "drafting"
    ],
    "aliases": [
      "embedding",
      "word embedding"
    ],
    "outlinks": [
      "Dimensionality Reduction",
      "Semantic Relationships",
      "Pasted image 20241015211934.png",
      "Language Models",
      "Attention mechanism",
      "BERT",
      "t-SNE",
      "Pasted image 20241015211844.png",
      "PyTorch",
      "Semantic Relationships",
      "ML_Tools",
      "Vector_Embedding.py",
      "How to search within a graph",
      "How would you decide between using TF-IDF and Word2Vec for text vectorization"
    ],
    "inlinks": [
      "how_llms_store_facts",
      "vector_database"
    ],
    "summary": "Vector Embedding is a technique used in machine learning and natural language processing to represent data in a continuous vector space. This representation captures the semantic meaning of data, such as words or sentences, allowing similar items to be positioned close to each other in the vector space. Key Concepts Data Compression: Embeddings compress data into a lower-dimensional space, making it easier to process and analyze. This is particularly useful for high-dimensional data like text or images. Semantic Similarity: In the embedding space, similar items are positioned close to each other. This proximity reflects semantic similarity, meaning that items with..."
  },
  "vectorisation": {
    "title": "Vectorisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vectorisation.md",
    "tags": [
      "software"
    ],
    "aliases": null,
    "outlinks": [
      "Pasted image 20241217204829.png|500",
      "Numpy",
      "gpu"
    ],
    "inlinks": [],
    "summary": "Link ![[Pasted image 20241217204829.png|500]] Numpy dot is better than for loop and summing. Why does it run faster? A: Designed to run in parallel Sequentially versus simultaneously in parallel. Related concepts: - [[Numpy]] - [[gpu]]"
  },
  "vectorized_engine": {
    "title": "Vectorized Engine",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vectorized Engine.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "DuckDB",
      "Columnar Storage"
    ],
    "inlinks": [
      "database_storage"
    ],
    "summary": "Vectorized Engine A modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of modern CPUs. Vectorized engines, such as [[DuckDB]], process data in large blocks or batches using SIMD instructions, allowing for improved parallelism, cache locality, and reduced overhead compared to traditional row-at-a-time processing engines, using [[Columnar Storage]]."
  },
  "vector_embedding.py": {
    "title": "Vector_Embedding.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vector_Embedding.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "vector_embedding"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Vector_Embedding.py Explanation of the Script Vocabulary and Embedding Layer: Terms are mapped to indices using a dictionary. The embedding layer learns continuous vector representations for these terms. Cosine Similarity: The cosine similarity function measures how similar two terms are in the embedding space. Higher values indicate closer relationships. Visualization: Embeddings are plotted in a 2D space to show semantic relationships. Terms with similar meanings (e.g., \"king\" and \"queen\") are expected to cluster together. t-SNE for Dimensionality Reduction: If the embedding dimension is higher than 2, t-SNE can reduce it to 2D for visualization while preserving semantic relationships. Outputs Cosine Similarities:..."
  },
  "vercel": {
    "title": "Vercel",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Vercel.md",
    "tags": null,
    "aliases": null,
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "view_use_case": {
    "title": "View Use Case",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\View Use Case.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "views"
    ],
    "summary": "View Use Case Scenario A company wants to generate monthly performance reports for its employees. The performance data is spread across multiple tables, including employees, departments, and performance_reviews. Instead of writing complex queries every time a report is needed, the company can create a view that simplifies data retrieval. Step 1: Define the Tables Assume we have the following tables: employees: Contains employee details. employee_id name department_id departments: Contains department details. department_id department_name performance_reviews: Contains performance review data. review_id employee_id review_score review_date Step 2: Create a View To simplify the reporting process, we create a view that joins these tables..."
  },
  "views": {
    "title": "Views",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Views.md",
    "tags": [
      "database"
    ],
    "aliases": [],
    "outlinks": [
      "queries",
      "DE_Tools",
      "Common Table Expression",
      "Common Table Expression",
      "Soft Deletion",
      "View Use Case",
      "SQLite",
      "Database Schema|schema"
    ],
    "inlinks": [
      "common_table_expression"
    ],
    "summary": "Views are virtual tables defined by SQL [[queries]] that ==simplify complex data representation.== They can remove unnecessary columns, aggregate results, partition data, and secure sensitive information. In [[DE_Tools]] see: https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Viewing/Viewing.ipynb Basic Usage: - Simplification - Aggregation (using GROUP) - [[Common Table Expression]] - Securing data: can give all values in a field the same value. Advanced Usage - Temporary Views: Exist only for the ==duration of the database connection.== - [[Common Table Expression]]: Serve as temporary views for a single query. - [[Soft Deletion]]: Use views and triggers to mark records as deleted without physically removing them from the table...."
  },
  "violin_plot": {
    "title": "Violin plot",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Violin plot.md",
    "tags": [
      "statistics"
    ],
    "aliases": [],
    "outlinks": [
      "Boxplot"
    ],
    "inlinks": [
      "distributions"
    ],
    "summary": "An extension of a [[Boxplot]] showing the data distribution. Useful when comparing distributions, skewness. python data = [...] # Your data sns.violinplot(data=data, color=\"purple\", fill=\"lightblue\", scale=\"area\") plt.show()"
  },
  "virtual_environments": {
    "title": "Virtual environments",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Virtual environments.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Poetry"
    ],
    "inlinks": [
      "tool.uv",
      "dependency_manager"
    ],
    "summary": "Setting up virtual env For windows (need to not be in a venv before del) cmd rmdir /s /q venv python -m venv venv venv\\Scripts\\activate pip freeze > requirements.txt .gitignore https://www.youtube.com/watch?v=_vejzukmn4s Remember to set python interpreter Related terms: - [[Poetry]]"
  },
  "wcss_and_elbow_method": {
    "title": "WCSS and elbow method",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\WCSS and elbow method.md",
    "tags": [
      "clustering"
    ],
    "aliases": [],
    "outlinks": [
      "clustering"
    ],
    "inlinks": [
      "k-means",
      "choosing_the_number_of_clusters"
    ],
    "summary": "USE: WCSS (within-cluster sum of squares) WCSS is a measure developed within the ANOVA framework. It gives a very good idea about the different distance between different clusters and within clusters, thus providing us a rule for deciding the appropriate number of clusters. The plot will resemble an \"elbow,\" and the goal is to find the point where the decrease in WCSS slows down, forming an elbow-like shape. Elbow numbers are the point where the rate of decrease in WCSS starts to flatten out The rationale behind the elbow method is that Rationale: as you increase the number of clusters..."
  },
  "weak_learners": {
    "title": "Weak Learners",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Weak Learners.md",
    "tags": null,
    "aliases": null,
    "outlinks": [
      "Model Ensemble",
      "Decision Tree",
      "Model Ensemble",
      "learning rate",
      "Hyperparameter"
    ],
    "inlinks": [
      "gradient_boosting",
      "boosting"
    ],
    "summary": "Weak learners are simple models that perform slightly better than random guessing. They are often used as the building blocks in [[Model Ensemble]] methods to create a strong predictive model. Characteristics Simplicity: Weak learners are typically simple models, such as [[Decision Tree]] stumps, which split the data based on a single feature. Performance: Individually, they may not perform well, but when combined, they can produce a powerful ensemble model. Role in Model Ensembling Weak learners are a crucial component of [[Model Ensemble]] techniques, such as boosting and bagging, where multiple weak learners are combined to improve overall model performance. Learning..."
  },
  "web_feature_server_(wfs)": {
    "title": "Web Feature Server (WFS)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Web Feature Server (WFS).md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "GIS"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Feature Server (WFS) Purpose: WFS is designed to serve raw geographic features (vector data) over the web. Functionality: - Feature-Based: It delivers geographic features (such as points, lines, and polygons) and their associated attribute data in formats like GML (Geography Markup Language). - Interactivity: Allows clients to query and retrieve specific features, perform spatial and attribute queries, and even support transactions (e.g., inserting, updating, deleting features). - Data Access: Provides access to the actual data behind the map, enabling more detailed and customized analysis and processing compared to image-based services. - Standardization: Also standardized by the OGC, ensuring..."
  },
  "web_map_tile_service_(wmts)": {
    "title": "Web Map Tile Service (WMTS)",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Web Map Tile Service (WMTS).md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "GIS"
    ],
    "inlinks": [
      "key_differences_of_web_feature_server_(wfs)_and_web_feature_server_(wfs)",
      "gis"
    ],
    "summary": "[[GIS]] Web Map Tile Service (WMTS) Purpose: WMTS is designed to serve pre-rendered, cached image tiles of maps. Functionality: - Tile-Based: It serves map images as small, fixed-size tiles, usually in a format such as PNG or JPEG. - Performance: By using cached tiles, WMTS can quickly deliver map images, making it highly efficient for applications requiring fast map rendering, like web mapping applications. - Scalability: The tile-based approach allows for easy scaling and efficient handling of high load, as the same tiles can be reused for multiple requests. - Standardization: It is standardized by the Open Geospatial Consortium (OGC),..."
  },
  "what_algorithms_or_models_are_used_within_the_energy_sector": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What algorithms or models are used within the energy sector.md",
    "tags": [
      "#question",
      "energy"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": ""
  },
  "what_algorithms_or_models_are_used_within_the_telecommunication_sector": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What algorithms or models are used within the telecommunication sector.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "industries_of_interest"
    ],
    "summary": ""
  },
  "what_are_the_best_practices_for_evaluating_the_effectiveness_of_different_prompts": {
    "title": "What are the best practices for evaluating the effectiveness of different prompts",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What are the best practices for evaluating the effectiveness of different prompts.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "how_do_we_evaluate_of_llm_outputs",
      "prompt_engineering"
    ],
    "summary": ""
  },
  "what_can_abm_solve_within_the_energy_sector": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What can ABM solve within the energy sector.md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [
      "Agent-Based Modelling"
    ],
    "inlinks": [],
    "summary": "[[Agent-Based Modelling]] energy systems analysis"
  },
  "what_is_the_difference_between_odds_and_probability": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What is the difference between odds and probability.md",
    "tags": [
      "#question",
      "math"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "logistic_regression_does_not_predict_probabilities"
    ],
    "summary": ""
  },
  "what_is_the_role_of_gradient-based_optimization_in_training_deep_learning_models.": {
    "title": null,
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\What is the role of gradient-based optimization in training deep learning models..md",
    "tags": [
      "#question"
    ],
    "aliases": [],
    "outlinks": [],
    "inlinks": [],
    "summary": ""
  },
  "when_and_why_not_to_us_regularisation": {
    "title": "When and why not to us regularisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\When and why not to us regularisation.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "training data",
      "validation data"
    ],
    "inlinks": [
      "regularisation"
    ],
    "summary": "While regularization is tool to combat overfitting, it is not a always useful. It is crucial to consider the model's - complexity, - the quality and - quantity of data, - and the appropriateness of the regularization parameters to ensure effective performance on validation data. If your model is performing well on [[training data]] but poorly on [[validation data]], regularization might not always solve this issue for several reasons: Underfitting: While regularization aims to reduce overfitting, it can also lead to underfitting if the penalty is too strong. This occurs when the model becomes too simplistic and fails to capture..."
  },
  "why_and_when_is_feature_scaling_necessary": {
    "title": "Why and when is feature scaling necessary",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why and when is feature scaling necessary.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Feature Scaling",
      "Support Vector Machines|SVM",
      "K-means",
      "Decision Tree",
      "Random Forests"
    ],
    "inlinks": [],
    "summary": "[[Feature Scaling]] is useful for models that use distances like [[Support Vector Machines|SVM]] and [[K-means]] When Scaling Is Unnecessary Tree-based Algorithms: Algorithms like [[Decision Tree]], [[Random Forests]], and Gradient Boosted Trees are invariant to feature scaling because they split data based on thresholds, not distances. Example: Splits are determined by feature values, not their magnitude. Data with Uniform Scales: If all features have the same range or are already normalized (e.g., percentages), scaling may not be required."
  },
  "why_does_increasing_the_number_of_models_in_a_ensemble_not_necessarily_improve_the_accuracy": {
    "title": "Why does increasing the number of models in a ensemble not necessarily improve the accuracy",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why does increasing the number of models in a ensemble not necessarily improve the accuracy.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Model Ensemble"
    ],
    "inlinks": [],
    "summary": "Increasing the number of models in an ensemble ([[Model Ensemble]]) does not always lead to improved accuracy due to several limiting factors: Convergence of Predictions: Additional models may lead to similar predictions, resulting in minimal changes to the overall output. Limited Data Representation: If the dataset is noisy or incomplete, more models will only aggregate existing noise without capturing new patterns. Diminishing Returns: Each new model contributes less unique information, and performance is ultimately limited by the irreducible error in the data. Increased Complexity: More models increase computational costs and training times without necessarily improving accuracy. Overfitting Risk: Adding complex..."
  },
  "why_does_label_encoding_give_different_predictions_from_one-hot_encoding": {
    "title": "Why does label encoding give different predictions from one-hot encoding",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why does label encoding give different predictions from one-hot encoding.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "one-hot_encoding"
    ],
    "summary": "Label Encoding and One-Hot Encoding give different predictions because they represent categorical variables in fundamentally different ways. Label Encoding might cause issues by implying an ordinal relationship between categories, leading to biased predictions. One-Hot Encoding prevents this by treating categories independently, resulting in more accurate predictions when there's no natural order among the categories. Label Encoding: How It Works: Label Encoding assigns an integer value to each unique category in a feature. For example, if you have three towns: ['West Windsor', 'Robbinsville', 'Princeton'], Label Encoding would convert them into numerical values like this: West Windsor \u2192 0 Robbinsville \u2192 1..."
  },
  "why_does_the_adam_optimizer_converge": {
    "title": "Why does the Adam Optimizer converge",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why does the Adam Optimizer converge.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "adam_optimizer"
    ],
    "summary": "Why the Adam Optimizer Converges The Adam optimizer is able to efficiently handle sparse gradients and adaptively adjust learning rates. The convergence of Adam, often observed as a flattening of the cost function, can be attributed to several factors inherent to its design and the characteristics of the dataset being used. The convergence of the Adam optimizer, resulting in a stable cost value, is a product of its adaptive learning rate, regularization effects, numerical stability mechanisms, and the dataset's characteristics. 1. Convergence to Local Minimum or Saddle Point Adaptive Learning Rate: Adam adjusts the learning rate for each parameter individually,..."
  },
  "why_is_named_entity_recognition_(ner)_a_challenging_task": {
    "title": "Why is named entity recognition (NER) a challenging task",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why is named entity recognition (NER) a challenging task.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "named_entity_recognition"
    ],
    "summary": "Named Entity Recognition (NER) is considered a challenging task for several reasons: Ambiguity: Entities can be ambiguous, meaning the same word or phrase can refer to different entities depending on the context. For example, \"Washington\" could refer to a city, a state, or a person. Disambiguating these entities requires a deep understanding of context. Variability in Language: Natural language is highly variable and can include slang, idioms, and different syntactic structures. This variability makes it difficult for NER models to consistently identify entities across different texts. Named Entity Diversity: Entities can take many forms, including names, organizations, locations, dates, and..."
  },
  "why_is_the_central_limit_theorem_important_when_working_with_small_sample_sizes": {
    "title": "Why is the Central Limit Theorem important when working with small sample sizes",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why is the Central Limit Theorem important when working with small sample sizes.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Central Limit Theorem",
      "Distributions|distribution",
      "assumption of normality",
      "Hypothesis Testing"
    ],
    "inlinks": [
      "central_limit_theorem"
    ],
    "summary": "The [[Central Limit Theorem]] (CLT) is particularly important for data scientists working with small sample sizes. It enables the use of various statistical methods, and helps in making valid inferences about the population from limited data. Assumption of Normality: The CLT states that the sampling [[Distributions|distribution]] of the sample means will approximate a normal distribution, regardless of the underlying population distribution, as long as the sample size is sufficiently large. This is crucial for data scientists because many statistical methods and tests (such as t-tests, ANOVA, and regression analysis) rely on the [[assumption of normality]]. Even with small sample sizes,..."
  },
  "why_json_is_better_than_pickle_for_untrusted_data": {
    "title": "Why JSON is Better than Pickle for Untrusted Data",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why JSON is Better than Pickle for Untrusted Data.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pickle"
    ],
    "inlinks": [
      "common_security_vulnerabilities_in_software_development"
    ],
    "summary": "JSON vs. [[Pickle]]: Security: JSON: JSON is a text-based data format that is inherently safer for handling untrusted data. It ==only== supports basic data types like strings, numbers, arrays, and objects, which reduces the risk of executing arbitrary code. Pickle: Pickle is a Python-specific binary serialization format that ==can serialize and deserialize complex Python objects.== However, it can ==execute arbitrary code== during deserialization, making it unsafe for handling untrusted data. Interoperability (the ability of computer systems or software to exchange and make use of information): JSON: JSON is language-agnostic and widely used across different programming environments, making it ideal for..."
  },
  "why_type_1_and_type_2_matter": {
    "title": "Why Type 1 and Type 2 matter",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why Type 1 and Type 2 matter.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Pasted image 20250312064809.png"
    ],
    "inlinks": [
      "evaluation_metrics"
    ],
    "summary": "Type I and Type II errors are used in evaluating the performance of classification models, and understanding their differences is essential for interpreting model results effectively. ![[Pasted image 20250312064809.png]] Type I Error (False Positive) Definition: A Type I error occurs when the model incorrectly predicts the positive class. In other words, it identifies a negative instance as positive. Example: If a model predicts that an email is spam (positive) when it is actually not spam (negative), this is a Type I error. Consequences: Type I errors can lead to unnecessary actions or consequences, such as misclassifying legitimate emails as spam,..."
  },
  "why_use_er_diagrams": {
    "title": "Why use ER diagrams",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Why use ER diagrams.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Why use ER diagrams",
      "ER Diagrams",
      "Data Quality",
      "Normalised Schema",
      "Data Cleansing"
    ],
    "inlinks": [
      "why_use_er_diagrams",
      "er_diagrams"
    ],
    "summary": "[[Why use ER diagrams]] Cleaning a dataset before creating an [[ER Diagrams]] is crucial for ensuring accuracy and reliability in your database design [[Data Quality]]: Cleaning the dataset helps identify and rectify errors, inconsistencies, and missing values. This ensures that the data accurately represents the real-world entities and relationships you intend to model. [[Normalised Schema]]: Before creating an ER diagram, it's essential to normalize the data, which involves organizing it efficiently to reduce redundancy and dependency. Cleaning the dataset beforehand allows you to identify redundant information and eliminate it, leading to a more streamlined ER diagram. Entity Identification: Through data..."
  },
  "wikipedia_api.py": {
    "title": "Wikipedia_API.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Wikipedia_API.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [],
    "inlinks": [
      "api",
      "graphrag"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Wikipedia_API.py"
  },
  "windows_subsystem_for_linux": {
    "title": "Windows Subsystem for Linux",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Windows Subsystem for Linux.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Windows Subsystem for Linux",
      "Linux",
      "Ubuntu"
    ],
    "inlinks": [
      "windows_subsystem_for_linux",
      "powershell_vs_bash"
    ],
    "summary": "[[Windows Subsystem for Linux]] (WSL) is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows 11. It allows users to run a Linux environment directly on Windows without the need for a virtual machine or dual-boot setup. Key features of WSL include: Integration with Windows: Users can access files from both Windows and the [[Linux]] environment seamlessly. Multiple Distributions: WSL supports various Linux distributions, such as [[Ubuntu]], Debian, and Fedora, which can be installed from the Microsoft Store. Command-Line Tools: Users can run Linux command-line tools and applications directly in Windows, making it easier for..."
  },
  "word2vec": {
    "title": "Word2vec",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Word2vec.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "neural network",
      "standardised/Vector Embedding|word embedding",
      "Semantic Relationships",
      "syntactic relationships",
      "ML_Tools",
      "Word2Vec.py",
      "Bag of Words"
    ],
    "inlinks": [
      "word2vec.py"
    ],
    "summary": "Word2Vec is a technique for generating vector representations of words. Developed by researchers at Google, it uses a shallow [[neural network]] to produce [[standardised/Vector Embedding|word embedding]] that capture [[Semantic Relationships]] and [[syntactic relationships]]. Word2Vec has two main architectures: In [[ML_Tools]] see: [[Word2Vec.py]] CBOW (Continuous [[Bag of Words]]): Predicts a target word given its context (neighboring words). Efficient for smaller datasets. Skip-Gram: Predicts the context words given a target word. Performs better on larger datasets. Word2Vec generates dense, continuous vector representations where words with similar meanings are close to each other in the embedding space. For example: vector(\"king\") - vector(\"man\") +..."
  },
  "word2vec.py": {
    "title": "Word2Vec.py",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Word2Vec.py.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "cosine similarity",
      "Word2Vec",
      "BERT",
      "Transformer|Transformers"
    ],
    "inlinks": [
      "word2vec"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Word2Vec.py The script can benefit from Word2Vec embeddings by replacing the randomly initialized embeddings with pretrained or trained embeddings generated using Word2Vec. These embeddings provide a meaningful semantic structure that is learned from a corpus of text, enhancing the visualization and [[cosine similarity]] calculations. Benefits: Meaningful Relationships: Words like \"king\" and \"queen\" will naturally be closer than \"king\" and \"apple.\" Analogy Solving: Word2Vec supports vector arithmetic to solve word analogies (e.g., \"man is to king as woman is to queen\"). Improved Visualizations: The embeddings reflect real-world semantic and syntactic relationships, making the 2D plots more interpretable. Further Enhancements Train Your..."
  },
  "wrapper_methods": {
    "title": "Wrapper Methods",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Wrapper Methods.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Feature Selection",
      "Filter method",
      "Model Evaluation"
    ],
    "inlinks": [
      "embedded_methods",
      "feature_selection"
    ],
    "summary": "Used in [[Feature Selection]]. Wrapper methods are powerful because they directly optimize the performance of the machine learning model by selecting the most informative subset of features. Iterative Approach: Unlike [[Filter method]], which assess the relevance of features based on statistical properties, wrapper methods ==directly involve the machine learning algorithm in the feature selection process.== Subset Selection: Wrapper methods work by creating different subsets of features from the original dataset and training a model on each subset. These subsets can be combinations of different features or a subset of all features. [[Model Evaluation]]: After training a model on each subset..."
  },
  "xgboost": {
    "title": "XGBoost",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\XGBoost.md",
    "tags": [
      "ml_optimisation"
    ],
    "aliases": [
      "XGM"
    ],
    "outlinks": [
      "Gradient Boosting",
      "Model Ensemble",
      "Loss function",
      "Regularisation",
      "Gradient Descent",
      "learning rate",
      "Handling Missing Data",
      "Decision Tree",
      "Regularisation",
      "Interpretability",
      "Hyperparameter Tuning"
    ],
    "inlinks": [
      "time_series_forecasting",
      "gradient_boosting",
      "lightgbm_vs_xgboost_vs_catboost",
      "boosting",
      "feature_importance",
      "ds_&_ml_portal",
      "optuna"
    ],
    "summary": "XGBoost (eXtreme Gradient Boosting) is a highly efficient and flexible implementation of [[Gradient Boosting]] that is widely used for its accuracy and performance in machine learning tasks. How does XGBoost work It works by building an [[Model Ensemble]] - ensemble of decision trees, where each tree is trained to correct the errors made by the previous ones. Here's a breakdown of how XGBoost works: Key Concepts Gradient Boosting Framework: XGBoost is based on the gradient boosting framework, which builds models sequentially. Each new model aims to reduce the errors (residuals) of the combined ensemble of previous models. Decision Trees: XGBoost..."
  },
  "yaml": {
    "title": "What is YAML?",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\yaml.md",
    "tags": [
      "software"
    ],
    "aliases": [],
    "outlinks": [
      "Json"
    ],
    "inlinks": [
      "json_to_yaml"
    ],
    "summary": "Stands for YAML ain't markup language and is a superset of JSON lists begin with a hyphen dependent on whitespace / indentation better suited for configuration than [[Json]] YAML is a data serialization language often used to write configuration files. Depending on whom you ask, YAML stands for yet another markup language, or YAML isn\u2019t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents."
  },
  "z-normalisation": {
    "title": "Z-Normalisation",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Z-Normalisation.md",
    "tags": null,
    "aliases": [
      "Z-Score"
    ],
    "outlinks": [
      "machine learning algorithms",
      "gradient descent",
      "learning rate",
      "Gradient Descent",
      "Pasted image 20241224091151.png",
      "Pasted image 20241224091157.png",
      "Pasted image 20241224091007.png"
    ],
    "inlinks": [
      "normalisation"
    ],
    "summary": "https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Outliers/outliers_z_score.py Z-normalisation, also known as z-score normalization, is a technique used to standardize the range of independent variables or features of data. This process is used in preparing data for [[machine learning algorithms]], especially those that rely on distance calculations, such as k-nearest neighbors and [[gradient descent]] optimization. Why Normalize? Consistency Across Features: By normalizing, the peak-to-peak range of each column is reduced from a factor of thousands to a factor of 2-3. This ensures that each feature contributes equally to the distance calculations, preventing features with larger ranges from dominating the results. Centered Data: The range of the normalized..."
  },
  "z-normalisationz-score": {
    "title": "Z-NormalisationZ-Score",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Z-NormalisationZ-Score.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "Z-Normalisation|Z-Score"
    ],
    "inlinks": [],
    "summary": "[[Z-Normalisation|Z-Score]] - Formula: $Z = \\frac{(X - \\mu)}{\\sigma}$ - $X$: Data point - $\\mu$: Mean of the dataset - $\\sigma$: Standard deviation of the dataset - Procedure: - Set a threshold (commonly $|Z| > 3$). Points exceeding the threshold are anomalies. 2. Modified Z-Score Formula: $M = \\frac{0.6745 \\cdot (X - \\text{median})}{\\text{MAD}}$ $MAD$: Median Absolute Deviation Procedure: Use this method for datasets with extreme outliers. Points with $M > 3.5$ are typically anomalies."
  },
  "z-test": {
    "title": "Z-Test",
    "path": "C:\\Users\\RhysL\\Desktop\\Data-Archive\\content\\standardised\\Z-Test.md",
    "tags": [],
    "aliases": [],
    "outlinks": [
      "standard deviation",
      "Central Limit Theorem"
    ],
    "inlinks": [
      "statistical_tests"
    ],
    "summary": "The Z-test is a statistical method used to determine if there is a ==significant difference between the means of two groups or to compare a sample mean to a known population mean when the population [[standard deviation]] is known==. It is typically applied when the sample size is large (usually n > 30). Types of Z-tests One-Sample Z-test: This test compares the mean of a single sample to a known population mean. It assesses whether the sample mean significantly differs from the population mean. Two-Sample Z-test: This test compares the means of two independent samples. It is used when both..."
  }
}